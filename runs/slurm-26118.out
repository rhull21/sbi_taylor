Loading parflow-ml/latest
  Loading requirement: openmpi/gcc/4.1.0 parflow/3.9.0 gdal/3.2.1
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  warnings.warn(
/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/05_utils/sbiutils.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y_out)
Running 1000 simulations.:   0%|          | 0/1000 [00:00<?, ?it/s]Running 1000 simulations.:   3%|▎         | 30/1000 [00:00<00:03, 291.69it/s]Running 1000 simulations.:   6%|▌         | 60/1000 [00:00<00:03, 293.30it/s]Running 1000 simulations.:   9%|▉         | 90/1000 [00:00<00:03, 293.06it/s]Running 1000 simulations.:  12%|█▏        | 120/1000 [00:00<00:03, 292.84it/s]Running 1000 simulations.:  15%|█▌        | 150/1000 [00:00<00:02, 292.79it/s]Running 1000 simulations.:  18%|█▊        | 180/1000 [00:00<00:02, 292.66it/s]Running 1000 simulations.:  21%|██        | 210/1000 [00:00<00:02, 292.63it/s]Running 1000 simulations.:  24%|██▍       | 240/1000 [00:00<00:02, 292.67it/s]Running 1000 simulations.:  27%|██▋       | 270/1000 [00:00<00:02, 292.63it/s]Running 1000 simulations.:  30%|███       | 300/1000 [00:01<00:02, 293.77it/s]Running 1000 simulations.:  33%|███▎      | 330/1000 [00:01<00:02, 295.03it/s]Running 1000 simulations.:  36%|███▌      | 360/1000 [00:01<00:02, 296.07it/s]Running 1000 simulations.:  39%|███▉      | 390/1000 [00:01<00:02, 296.92it/s]Running 1000 simulations.:  42%|████▏     | 420/1000 [00:01<00:01, 297.77it/s]Running 1000 simulations.:  45%|████▌     | 450/1000 [00:01<00:01, 298.19it/s]Running 1000 simulations.:  48%|████▊     | 480/1000 [00:01<00:01, 297.79it/s]Running 1000 simulations.:  51%|█████     | 510/1000 [00:01<00:01, 297.92it/s]Running 1000 simulations.:  54%|█████▍    | 540/1000 [00:01<00:01, 297.69it/s]Running 1000 simulations.:  57%|█████▋    | 570/1000 [00:01<00:01, 297.53it/s]Running 1000 simulations.:  60%|██████    | 600/1000 [00:02<00:01, 298.20it/s]Running 1000 simulations.:  63%|██████▎   | 630/1000 [00:02<00:01, 298.66it/s]Running 1000 simulations.:  66%|██████▌   | 660/1000 [00:02<00:01, 298.76it/s]Running 1000 simulations.:  69%|██████▉   | 690/1000 [00:02<00:01, 298.69it/s]Running 1000 simulations.:  72%|███████▏  | 720/1000 [00:02<00:00, 298.90it/s]Running 1000 simulations.:  75%|███████▌  | 750/1000 [00:02<00:00, 299.16it/s]Running 1000 simulations.:  78%|███████▊  | 780/1000 [00:02<00:00, 299.28it/s]Running 1000 simulations.:  81%|████████  | 810/1000 [00:02<00:00, 299.38it/s]Running 1000 simulations.:  84%|████████▍ | 840/1000 [00:02<00:00, 299.28it/s]Running 1000 simulations.:  87%|████████▋ | 870/1000 [00:02<00:00, 299.08it/s]Running 1000 simulations.:  90%|█████████ | 900/1000 [00:03<00:00, 298.58it/s]Running 1000 simulations.:  93%|█████████▎| 930/1000 [00:03<00:00, 298.44it/s]Running 1000 simulations.:  96%|█████████▌| 960/1000 [00:03<00:00, 298.42it/s]Running 1000 simulations.:  99%|█████████▉| 990/1000 [00:03<00:00, 297.55it/s]Running 1000 simulations.: 100%|██████████| 1000/1000 [00:03<00:00, 296.85it/s]
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8680it [00:00, 128143.38it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9547it [00:00, 145778.50it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9347it [00:00, 139714.97it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9677it [00:00, 143893.84it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8022it [00:00, 71979.57it/s]            Drawing 5000 posterior samples: 8022it [00:00, 71724.56it/s]
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 7336it [00:00, 110018.46it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8991it [00:00, 133326.45it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 6083it [00:00, 89859.69it/s]            
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 10080it [00:00, 151143.93it/s]          
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9883it [00:00, 147881.28it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9118it [00:00, 139154.40it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8106it [00:00, 72802.43it/s]            Drawing 5000 posterior samples: 8106it [00:00, 72519.19it/s]
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9848it [00:00, 147853.39it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9415it [00:00, 140993.19it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 11275it [00:00, 166774.03it/s]          
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9481it [00:00, 140923.43it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9599it [00:00, 144482.21it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 6008it [00:00, 90859.65it/s]            
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9357it [00:00, 141231.96it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9446it [00:00, 141590.89it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 10079it [00:00, 151211.10it/s]          
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
warning: file exists
warning: file exists
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Neural network successfully converged after 339 epochs.
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0593, 0.0621, 0.0677, 0.0697, 0.0695, 0.0708, 0.0683, 0.0631, 0.0611,
        0.0638, 0.0687, 0.0719, 0.0716, 0.0689, 0.0641, 0.0654, 0.0659, 0.0637,
        0.0625, 0.0614, 0.0670, 0.0677, 0.0718, 0.0757, 0.0748, 0.0778, 0.0712,
        0.0700, 0.0695, 0.0659, 0.0662, 0.0709, 0.0694, 0.0661, 0.0630, 0.0648,
        0.0698, 0.0654, 0.0657, 0.0637, 0.0624, 0.0617, 0.0623, 0.0618, 0.0629,
        0.0644, 0.0646, 0.0634, 0.0613, 0.0615, 0.0627, 0.0632, 0.0644, 0.0623,
        0.0633, 0.0645, 0.0640, 0.0634, 0.0622, 0.0645, 0.0649, 0.0634, 0.0644,
        0.0638, 0.0646, 0.0640, 0.0641, 0.0650, 0.0640, 0.0637, 0.0649, 0.0633,
        0.0625, 0.0613, 0.0598, 0.0599, 0.0601, 0.0634, 0.0634, 0.0636, 0.0630,
        0.0624, 0.0638, 0.0629, 0.0638, 0.0661, 0.0653, 0.0650, 0.0642, 0.0644,
        0.0645, 0.0642, 0.0632, 0.0610, 0.0612, 0.0620, 0.0634, 0.0634, 0.0622,
        0.0611, 0.0606, 0.0601, 0.0625, 0.0621, 0.0588, 0.0583, 0.0605, 0.0624,
        0.0609, 0.0591, 0.0593, 0.0584, 0.0590, 0.0578, 0.0569, 0.0559, 0.0567,
        0.0565, 0.0572, 0.0585, 0.0574, 0.0553, 0.0597, 0.0586, 0.0573, 0.0568,
        0.0565, 0.0557, 0.0560, 0.0554, 0.0538, 0.0540, 0.0542, 0.0551, 0.0555,
        0.0544, 0.0558, 0.0564, 0.0573, 0.0603, 0.0588, 0.0573, 0.0576, 0.0600,
        0.0596, 0.0571, 0.0554, 0.0551, 0.0557, 0.0572, 0.0587, 0.0596, 0.0568,
        0.0559, 0.0565, 0.0572, 0.0560, 0.0530, 0.0528, 0.0540, 0.0550, 0.0541,
        0.0550, 0.0552, 0.0554, 0.0531, 0.0516, 0.0522, 0.0527, 0.0526, 0.0517,
        0.0526, 0.0527, 0.0531, 0.0537, 0.0592, 0.0547, 0.0533, 0.0545, 0.0542,
        0.0506, 0.0482, 0.0497, 0.0506, 0.0500, 0.0497, 0.0504, 0.0512, 0.0505,
        0.0494, 0.0502, 0.0512, 0.0516, 0.0498, 0.0517, 0.0519, 0.0540, 0.0617,
        0.0614, 0.0593, 0.0592, 0.0556, 0.0568, 0.0609, 0.0643, 0.0577, 0.0598,
        0.0619, 0.0636, 0.0648, 0.0663, 0.0652, 0.0652, 0.0795, 0.0869, 0.0932,
        0.0920, 0.0880, 0.0911, 0.0905, 0.0940, 0.1020, 0.1025, 0.1023, 0.1071,
        0.1111, 0.1071, 0.1050, 0.1047, 0.1017, 0.1017, 0.1067, 0.1156, 0.1232,
        0.1157, 0.1227, 0.1259, 0.1336, 0.1456, 0.1449, 0.1394, 0.1424, 0.1414,
        0.1456, 0.1419, 0.1579, 0.1606, 0.1608, 0.1858, 0.2071, 0.1852, 0.1772,
        0.1734, 0.1670, 0.1508, 0.1517, 0.1435, 0.1390, 0.1354, 0.1307, 0.1414,
        0.1443, 0.1241, 0.1344, 0.1312, 0.1302, 0.1191, 0.1102, 0.1027, 0.1015,
        0.0982, 0.0964, 0.0956, 0.0933, 0.0915, 0.0882, 0.0861, 0.0833, 0.0826,
        0.0802, 0.0818, 0.0827, 0.0799, 0.0789, 0.0738, 0.0661, 0.0622, 0.0594,
        0.0558, 0.0563, 0.0584, 0.0542, 0.0511, 0.0502, 0.0486, 0.0455, 0.0430,
        0.0423, 0.0442, 0.0458, 0.0464, 0.0488, 0.0508, 0.0479, 0.0449, 0.0439,
        0.0438, 0.0474, 0.0486, 0.0461, 0.0535, 0.0532, 0.0586, 0.0566, 0.0578,
        0.0645, 0.0641, 0.0640, 0.0664, 0.0584, 0.0582, 0.0534, 0.0509, 0.0519,
        0.0534, 0.0512, 0.0495, 0.0539, 0.0418, 0.0553, 0.0587, 0.0564, 0.0554,
        0.0547, 0.0553, 0.0556, 0.0545, 0.0530, 0.0519, 0.0522, 0.0522, 0.0479,
        0.0505, 0.0496, 0.0474, 0.0483, 0.0502, 0.0491, 0.0584, 0.0689],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0523, 0.0540, 0.0594, 0.0603, 0.0610, 0.0597, 0.0583, 0.0527, 0.0528,
        0.0555, 0.0597, 0.0623, 0.0623, 0.0604, 0.0562, 0.0573, 0.0573, 0.0557,
        0.0548, 0.0542, 0.0587, 0.0594, 0.0627, 0.0658, 0.0645, 0.0669, 0.0613,
        0.0600, 0.0592, 0.0562, 0.0570, 0.0610, 0.0600, 0.0576, 0.0551, 0.0574,
        0.0614, 0.0575, 0.0583, 0.0567, 0.0557, 0.0550, 0.0556, 0.0552, 0.0563,
        0.0580, 0.0580, 0.0568, 0.0549, 0.0550, 0.0561, 0.0564, 0.0576, 0.0554,
        0.0566, 0.0578, 0.0575, 0.0570, 0.0559, 0.0580, 0.0582, 0.0567, 0.0579,
        0.0572, 0.0578, 0.0572, 0.0573, 0.0581, 0.0570, 0.0565, 0.0574, 0.0558,
        0.0553, 0.0542, 0.0528, 0.0529, 0.0530, 0.0558, 0.0561, 0.0567, 0.0564,
        0.0558, 0.0573, 0.0564, 0.0573, 0.0593, 0.0587, 0.0583, 0.0576, 0.0575,
        0.0578, 0.0576, 0.0567, 0.0547, 0.0547, 0.0557, 0.0571, 0.0570, 0.0559,
        0.0549, 0.0544, 0.0541, 0.0562, 0.0557, 0.0527, 0.0523, 0.0544, 0.0559,
        0.0546, 0.0530, 0.0533, 0.0525, 0.0532, 0.0521, 0.0514, 0.0505, 0.0511,
        0.0508, 0.0514, 0.0528, 0.0517, 0.0495, 0.0540, 0.0528, 0.0518, 0.0513,
        0.0509, 0.0502, 0.0504, 0.0501, 0.0486, 0.0487, 0.0490, 0.0497, 0.0500,
        0.0492, 0.0505, 0.0509, 0.0516, 0.0546, 0.0534, 0.0519, 0.0522, 0.0541,
        0.0538, 0.0517, 0.0500, 0.0496, 0.0502, 0.0516, 0.0529, 0.0537, 0.0515,
        0.0506, 0.0512, 0.0518, 0.0507, 0.0484, 0.0480, 0.0490, 0.0500, 0.0490,
        0.0497, 0.0497, 0.0498, 0.0480, 0.0467, 0.0473, 0.0478, 0.0475, 0.0467,
        0.0475, 0.0476, 0.0479, 0.0483, 0.0530, 0.0491, 0.0479, 0.0493, 0.0497,
        0.0465, 0.0443, 0.0460, 0.0466, 0.0455, 0.0450, 0.0455, 0.0459, 0.0457,
        0.0450, 0.0461, 0.0473, 0.0477, 0.0458, 0.0472, 0.0474, 0.0497, 0.0560,
        0.0556, 0.0540, 0.0537, 0.0505, 0.0513, 0.0554, 0.0597, 0.0535, 0.0554,
        0.0585, 0.0585, 0.0592, 0.0597, 0.0595, 0.0592, 0.0725, 0.0783, 0.0844,
        0.0851, 0.0812, 0.0850, 0.0844, 0.0870, 0.0939, 0.0942, 0.0940, 0.0985,
        0.1025, 0.0984, 0.0971, 0.0972, 0.0948, 0.0945, 0.0997, 0.1084, 0.1157,
        0.1080, 0.1149, 0.1182, 0.1269, 0.1382, 0.1372, 0.1323, 0.1363, 0.1362,
        0.1397, 0.1344, 0.1488, 0.1497, 0.1528, 0.1670, 0.1940, 0.1866, 0.1910,
        0.1711, 0.1682, 0.1573, 0.1637, 0.1536, 0.1451, 0.1354, 0.1282, 0.1374,
        0.1376, 0.1185, 0.1287, 0.1256, 0.1265, 0.1170, 0.1093, 0.1022, 0.1010,
        0.0974, 0.0953, 0.0942, 0.0915, 0.0902, 0.0867, 0.0850, 0.0822, 0.0815,
        0.0790, 0.0805, 0.0814, 0.0784, 0.0771, 0.0718, 0.0643, 0.0609, 0.0585,
        0.0547, 0.0545, 0.0565, 0.0524, 0.0494, 0.0484, 0.0465, 0.0433, 0.0407,
        0.0397, 0.0413, 0.0427, 0.0433, 0.0457, 0.0477, 0.0450, 0.0419, 0.0408,
        0.0410, 0.0444, 0.0454, 0.0428, 0.0486, 0.0485, 0.0537, 0.0522, 0.0532,
        0.0592, 0.0589, 0.0587, 0.0609, 0.0536, 0.0533, 0.0491, 0.0471, 0.0480,
        0.0496, 0.0475, 0.0460, 0.0498, 0.0386, 0.0505, 0.0539, 0.0519, 0.0508,
        0.0499, 0.0508, 0.0509, 0.0497, 0.0486, 0.0480, 0.0482, 0.0481, 0.0440,
        0.0462, 0.0454, 0.0437, 0.0445, 0.0460, 0.0455, 0.0543, 0.0634],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0584, 0.0610, 0.0667, 0.0686, 0.0687, 0.0696, 0.0673, 0.0617, 0.0601,
        0.0629, 0.0678, 0.0709, 0.0707, 0.0681, 0.0634, 0.0647, 0.0650, 0.0629,
        0.0618, 0.0607, 0.0663, 0.0670, 0.0710, 0.0747, 0.0737, 0.0767, 0.0701,
        0.0690, 0.0684, 0.0649, 0.0653, 0.0699, 0.0685, 0.0653, 0.0622, 0.0641,
        0.0690, 0.0647, 0.0650, 0.0631, 0.0618, 0.0611, 0.0617, 0.0613, 0.0624,
        0.0639, 0.0641, 0.0628, 0.0608, 0.0609, 0.0621, 0.0626, 0.0639, 0.0617,
        0.0627, 0.0639, 0.0634, 0.0629, 0.0617, 0.0640, 0.0643, 0.0628, 0.0639,
        0.0633, 0.0640, 0.0634, 0.0636, 0.0644, 0.0635, 0.0631, 0.0642, 0.0627,
        0.0619, 0.0606, 0.0591, 0.0593, 0.0594, 0.0627, 0.0627, 0.0630, 0.0625,
        0.0619, 0.0633, 0.0624, 0.0633, 0.0655, 0.0648, 0.0644, 0.0637, 0.0638,
        0.0640, 0.0637, 0.0627, 0.0605, 0.0606, 0.0615, 0.0629, 0.0629, 0.0617,
        0.0606, 0.0601, 0.0596, 0.0620, 0.0615, 0.0583, 0.0578, 0.0600, 0.0619,
        0.0604, 0.0586, 0.0588, 0.0579, 0.0585, 0.0574, 0.0565, 0.0555, 0.0562,
        0.0560, 0.0567, 0.0580, 0.0570, 0.0549, 0.0592, 0.0581, 0.0569, 0.0563,
        0.0560, 0.0553, 0.0555, 0.0550, 0.0534, 0.0535, 0.0538, 0.0547, 0.0551,
        0.0539, 0.0554, 0.0559, 0.0568, 0.0598, 0.0584, 0.0569, 0.0571, 0.0596,
        0.0592, 0.0567, 0.0550, 0.0547, 0.0552, 0.0567, 0.0583, 0.0591, 0.0564,
        0.0554, 0.0561, 0.0567, 0.0555, 0.0527, 0.0524, 0.0536, 0.0546, 0.0537,
        0.0545, 0.0547, 0.0549, 0.0526, 0.0512, 0.0518, 0.0523, 0.0522, 0.0513,
        0.0521, 0.0523, 0.0527, 0.0532, 0.0586, 0.0541, 0.0528, 0.0541, 0.0539,
        0.0503, 0.0479, 0.0495, 0.0503, 0.0497, 0.0493, 0.0500, 0.0507, 0.0501,
        0.0491, 0.0499, 0.0509, 0.0513, 0.0495, 0.0513, 0.0515, 0.0537, 0.0611,
        0.0608, 0.0588, 0.0586, 0.0551, 0.0563, 0.0604, 0.0639, 0.0575, 0.0594,
        0.0618, 0.0631, 0.0642, 0.0656, 0.0646, 0.0646, 0.0787, 0.0858, 0.0920,
        0.0912, 0.0873, 0.0905, 0.0899, 0.0932, 0.1010, 0.1015, 0.1013, 0.1060,
        0.1101, 0.1061, 0.1041, 0.1039, 0.1010, 0.1009, 0.1059, 0.1147, 0.1223,
        0.1146, 0.1216, 0.1247, 0.1326, 0.1446, 0.1439, 0.1385, 0.1416, 0.1407,
        0.1447, 0.1406, 0.1561, 0.1580, 0.1588, 0.1813, 0.2048, 0.1852, 0.1813,
        0.1728, 0.1674, 0.1523, 0.1549, 0.1461, 0.1404, 0.1357, 0.1307, 0.1414,
        0.1435, 0.1233, 0.1337, 0.1305, 0.1297, 0.1190, 0.1104, 0.1031, 0.1021,
        0.0987, 0.0968, 0.0960, 0.0936, 0.0919, 0.0886, 0.0865, 0.0837, 0.0830,
        0.0806, 0.0822, 0.0831, 0.0803, 0.0792, 0.0741, 0.0662, 0.0624, 0.0595,
        0.0559, 0.0563, 0.0584, 0.0541, 0.0511, 0.0501, 0.0485, 0.0454, 0.0428,
        0.0421, 0.0440, 0.0456, 0.0462, 0.0486, 0.0506, 0.0477, 0.0447, 0.0436,
        0.0436, 0.0472, 0.0484, 0.0459, 0.0531, 0.0527, 0.0582, 0.0562, 0.0574,
        0.0641, 0.0637, 0.0636, 0.0660, 0.0580, 0.0578, 0.0531, 0.0506, 0.0517,
        0.0532, 0.0510, 0.0492, 0.0536, 0.0416, 0.0548, 0.0583, 0.0561, 0.0551,
        0.0544, 0.0551, 0.0554, 0.0542, 0.0527, 0.0516, 0.0519, 0.0519, 0.0476,
        0.0501, 0.0492, 0.0471, 0.0480, 0.0499, 0.0488, 0.0581, 0.0685],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0534, 0.0552, 0.0608, 0.0619, 0.0628, 0.0615, 0.0602, 0.0543, 0.0543,
        0.0571, 0.0615, 0.0643, 0.0642, 0.0623, 0.0580, 0.0591, 0.0591, 0.0573,
        0.0565, 0.0558, 0.0606, 0.0613, 0.0648, 0.0679, 0.0666, 0.0691, 0.0633,
        0.0620, 0.0612, 0.0582, 0.0589, 0.0631, 0.0620, 0.0595, 0.0568, 0.0590,
        0.0633, 0.0593, 0.0601, 0.0584, 0.0573, 0.0566, 0.0572, 0.0569, 0.0580,
        0.0596, 0.0596, 0.0585, 0.0564, 0.0566, 0.0577, 0.0580, 0.0592, 0.0570,
        0.0582, 0.0594, 0.0591, 0.0586, 0.0575, 0.0596, 0.0598, 0.0584, 0.0595,
        0.0589, 0.0595, 0.0589, 0.0590, 0.0598, 0.0587, 0.0582, 0.0591, 0.0576,
        0.0570, 0.0559, 0.0544, 0.0545, 0.0546, 0.0575, 0.0578, 0.0583, 0.0580,
        0.0574, 0.0589, 0.0581, 0.0590, 0.0610, 0.0604, 0.0600, 0.0593, 0.0593,
        0.0595, 0.0593, 0.0583, 0.0563, 0.0563, 0.0573, 0.0587, 0.0586, 0.0575,
        0.0565, 0.0560, 0.0556, 0.0578, 0.0572, 0.0542, 0.0538, 0.0559, 0.0575,
        0.0562, 0.0545, 0.0548, 0.0540, 0.0546, 0.0536, 0.0528, 0.0519, 0.0525,
        0.0522, 0.0528, 0.0542, 0.0531, 0.0510, 0.0554, 0.0542, 0.0532, 0.0527,
        0.0523, 0.0516, 0.0518, 0.0514, 0.0499, 0.0501, 0.0503, 0.0510, 0.0513,
        0.0505, 0.0518, 0.0522, 0.0530, 0.0561, 0.0548, 0.0533, 0.0535, 0.0556,
        0.0554, 0.0531, 0.0514, 0.0511, 0.0516, 0.0530, 0.0543, 0.0551, 0.0528,
        0.0519, 0.0525, 0.0531, 0.0520, 0.0496, 0.0492, 0.0502, 0.0512, 0.0502,
        0.0509, 0.0510, 0.0510, 0.0492, 0.0479, 0.0484, 0.0489, 0.0487, 0.0479,
        0.0487, 0.0489, 0.0492, 0.0496, 0.0543, 0.0503, 0.0491, 0.0506, 0.0508,
        0.0475, 0.0453, 0.0470, 0.0477, 0.0466, 0.0461, 0.0466, 0.0471, 0.0469,
        0.0461, 0.0472, 0.0484, 0.0487, 0.0469, 0.0483, 0.0486, 0.0509, 0.0573,
        0.0569, 0.0552, 0.0549, 0.0517, 0.0525, 0.0566, 0.0608, 0.0547, 0.0564,
        0.0596, 0.0597, 0.0604, 0.0611, 0.0606, 0.0605, 0.0739, 0.0796, 0.0858,
        0.0864, 0.0826, 0.0864, 0.0857, 0.0884, 0.0953, 0.0956, 0.0954, 0.0999,
        0.1040, 0.1000, 0.0986, 0.0987, 0.0962, 0.0959, 0.1010, 0.1096, 0.1168,
        0.1090, 0.1159, 0.1190, 0.1276, 0.1392, 0.1384, 0.1334, 0.1373, 0.1369,
        0.1404, 0.1351, 0.1493, 0.1498, 0.1530, 0.1660, 0.1931, 0.1867, 0.1914,
        0.1718, 0.1691, 0.1584, 0.1652, 0.1555, 0.1470, 0.1369, 0.1297, 0.1389,
        0.1392, 0.1195, 0.1295, 0.1265, 0.1270, 0.1176, 0.1100, 0.1031, 0.1022,
        0.0987, 0.0966, 0.0954, 0.0928, 0.0915, 0.0880, 0.0861, 0.0833, 0.0827,
        0.0802, 0.0818, 0.0828, 0.0799, 0.0785, 0.0731, 0.0653, 0.0618, 0.0592,
        0.0554, 0.0553, 0.0573, 0.0531, 0.0501, 0.0490, 0.0472, 0.0439, 0.0413,
        0.0404, 0.0421, 0.0436, 0.0442, 0.0466, 0.0485, 0.0458, 0.0426, 0.0416,
        0.0417, 0.0452, 0.0462, 0.0437, 0.0497, 0.0495, 0.0549, 0.0533, 0.0543,
        0.0606, 0.0603, 0.0600, 0.0623, 0.0548, 0.0545, 0.0503, 0.0481, 0.0491,
        0.0507, 0.0486, 0.0470, 0.0509, 0.0395, 0.0516, 0.0551, 0.0531, 0.0521,
        0.0513, 0.0522, 0.0523, 0.0510, 0.0498, 0.0491, 0.0492, 0.0491, 0.0450,
        0.0472, 0.0464, 0.0447, 0.0455, 0.0470, 0.0464, 0.0555, 0.0648],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0889, 0.0939, 0.1030, 0.1101, 0.1103, 0.1142, 0.1104, 0.1065, 0.0995,
        0.1017, 0.1091, 0.1152, 0.1153, 0.1103, 0.1031, 0.1054, 0.1072, 0.1025,
        0.1003, 0.0977, 0.1085, 0.1098, 0.1178, 0.1239, 0.1236, 0.1284, 0.1201,
        0.1209, 0.1232, 0.1191, 0.1189, 0.1293, 0.1244, 0.1137, 0.1079, 0.1078,
        0.1170, 0.1108, 0.1052, 0.0998, 0.0971, 0.0958, 0.0959, 0.0953, 0.0965,
        0.0974, 0.0978, 0.0959, 0.0932, 0.0940, 0.0960, 0.0970, 0.0982, 0.0958,
        0.0954, 0.0970, 0.0962, 0.0952, 0.0934, 0.0966, 0.0976, 0.0962, 0.0966,
        0.0964, 0.0976, 0.0975, 0.0983, 0.0995, 0.0992, 0.0996, 0.1019, 0.1005,
        0.0991, 0.0976, 0.0962, 0.0966, 0.0970, 0.1021, 0.1006, 0.0994, 0.0970,
        0.0958, 0.0965, 0.0956, 0.0969, 0.0997, 0.0982, 0.0980, 0.0970, 0.0982,
        0.0982, 0.0974, 0.0959, 0.0928, 0.0939, 0.0935, 0.0955, 0.0958, 0.0943,
        0.0923, 0.0914, 0.0903, 0.0931, 0.0928, 0.0889, 0.0880, 0.0911, 0.0944,
        0.0926, 0.0899, 0.0896, 0.0881, 0.0888, 0.0869, 0.0852, 0.0835, 0.0844,
        0.0845, 0.0856, 0.0868, 0.0859, 0.0829, 0.0856, 0.0853, 0.0834, 0.0831,
        0.0831, 0.0825, 0.0826, 0.0813, 0.0792, 0.0805, 0.0811, 0.0826, 0.0834,
        0.0799, 0.0821, 0.0828, 0.0844, 0.0868, 0.0846, 0.0835, 0.0843, 0.0889,
        0.0872, 0.0832, 0.0815, 0.0815, 0.0813, 0.0840, 0.0873, 0.0894, 0.0825,
        0.0811, 0.0819, 0.0830, 0.0810, 0.0749, 0.0759, 0.0776, 0.0789, 0.0783,
        0.0791, 0.0803, 0.0802, 0.0763, 0.0746, 0.0747, 0.0753, 0.0760, 0.0750,
        0.0764, 0.0767, 0.0774, 0.0787, 0.0873, 0.0788, 0.0763, 0.0785, 0.0743,
        0.0691, 0.0660, 0.0677, 0.0701, 0.0713, 0.0712, 0.0724, 0.0737, 0.0720,
        0.0694, 0.0681, 0.0687, 0.0685, 0.0669, 0.0715, 0.0721, 0.0725, 0.0838,
        0.0847, 0.0807, 0.0808, 0.0760, 0.0788, 0.0831, 0.0840, 0.0781, 0.0794,
        0.0808, 0.0860, 0.0881, 0.0945, 0.0903, 0.0908, 0.1058, 0.1156, 0.1223,
        0.1169, 0.1126, 0.1151, 0.1156, 0.1199, 0.1289, 0.1307, 0.1300, 0.1365,
        0.1409, 0.1379, 0.1333, 0.1320, 0.1276, 0.1281, 0.1308, 0.1388, 0.1462,
        0.1388, 0.1464, 0.1480, 0.1538, 0.1691, 0.1696, 0.1629, 0.1629, 0.1581,
        0.1608, 0.1579, 0.1732, 0.1754, 0.1761, 0.2052, 0.2170, 0.1897, 0.1751,
        0.1842, 0.1774, 0.1606, 0.1563, 0.1552, 0.1546, 0.1521, 0.1512, 0.1651,
        0.1699, 0.1423, 0.1529, 0.1515, 0.1460, 0.1347, 0.1257, 0.1184, 0.1189,
        0.1167, 0.1153, 0.1150, 0.1148, 0.1116, 0.1068, 0.1020, 0.0989, 0.0988,
        0.0973, 0.0998, 0.1009, 0.0985, 0.0977, 0.0934, 0.0838, 0.0777, 0.0727,
        0.0681, 0.0709, 0.0738, 0.0683, 0.0649, 0.0639, 0.0627, 0.0591, 0.0564,
        0.0572, 0.0608, 0.0644, 0.0656, 0.0672, 0.0677, 0.0625, 0.0601, 0.0593,
        0.0587, 0.0629, 0.0658, 0.0632, 0.0774, 0.0766, 0.0820, 0.0786, 0.0805,
        0.0903, 0.0906, 0.0902, 0.0950, 0.0822, 0.0816, 0.0748, 0.0709, 0.0731,
        0.0739, 0.0709, 0.0678, 0.0746, 0.0603, 0.0797, 0.0833, 0.0792, 0.0794,
        0.0818, 0.0809, 0.0803, 0.0810, 0.0783, 0.0728, 0.0725, 0.0729, 0.0672,
        0.0712, 0.0696, 0.0657, 0.0679, 0.0723, 0.0668, 0.0798, 0.0952],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0587, 0.0617, 0.0665, 0.0683, 0.0673, 0.0690, 0.0663, 0.0627, 0.0600,
        0.0621, 0.0670, 0.0699, 0.0695, 0.0666, 0.0617, 0.0632, 0.0639, 0.0620,
        0.0607, 0.0596, 0.0647, 0.0653, 0.0692, 0.0730, 0.0723, 0.0752, 0.0689,
        0.0679, 0.0673, 0.0637, 0.0640, 0.0682, 0.0667, 0.0638, 0.0609, 0.0628,
        0.0674, 0.0630, 0.0633, 0.0615, 0.0603, 0.0596, 0.0602, 0.0597, 0.0607,
        0.0621, 0.0623, 0.0612, 0.0592, 0.0593, 0.0605, 0.0609, 0.0623, 0.0602,
        0.0612, 0.0624, 0.0618, 0.0613, 0.0601, 0.0624, 0.0626, 0.0611, 0.0622,
        0.0616, 0.0623, 0.0618, 0.0618, 0.0626, 0.0617, 0.0613, 0.0626, 0.0611,
        0.0604, 0.0591, 0.0576, 0.0578, 0.0580, 0.0612, 0.0612, 0.0615, 0.0609,
        0.0602, 0.0617, 0.0607, 0.0615, 0.0636, 0.0630, 0.0626, 0.0620, 0.0620,
        0.0622, 0.0619, 0.0609, 0.0588, 0.0589, 0.0598, 0.0612, 0.0611, 0.0600,
        0.0590, 0.0584, 0.0580, 0.0605, 0.0600, 0.0567, 0.0562, 0.0583, 0.0602,
        0.0588, 0.0569, 0.0572, 0.0563, 0.0569, 0.0558, 0.0550, 0.0541, 0.0547,
        0.0545, 0.0552, 0.0565, 0.0554, 0.0533, 0.0578, 0.0566, 0.0554, 0.0549,
        0.0546, 0.0539, 0.0541, 0.0537, 0.0520, 0.0520, 0.0523, 0.0532, 0.0536,
        0.0527, 0.0541, 0.0546, 0.0554, 0.0582, 0.0569, 0.0554, 0.0556, 0.0578,
        0.0574, 0.0550, 0.0533, 0.0531, 0.0538, 0.0553, 0.0568, 0.0576, 0.0550,
        0.0542, 0.0548, 0.0555, 0.0542, 0.0515, 0.0512, 0.0525, 0.0534, 0.0526,
        0.0535, 0.0536, 0.0540, 0.0516, 0.0501, 0.0508, 0.0513, 0.0511, 0.0502,
        0.0509, 0.0510, 0.0514, 0.0520, 0.0579, 0.0534, 0.0519, 0.0528, 0.0527,
        0.0492, 0.0469, 0.0483, 0.0490, 0.0485, 0.0483, 0.0491, 0.0498, 0.0491,
        0.0479, 0.0486, 0.0496, 0.0502, 0.0484, 0.0502, 0.0501, 0.0524, 0.0606,
        0.0602, 0.0581, 0.0581, 0.0544, 0.0557, 0.0599, 0.0630, 0.0559, 0.0586,
        0.0597, 0.0622, 0.0636, 0.0650, 0.0641, 0.0641, 0.0787, 0.0869, 0.0932,
        0.0914, 0.0869, 0.0897, 0.0890, 0.0930, 0.1016, 0.1019, 0.1020, 0.1066,
        0.1105, 0.1063, 0.1040, 0.1037, 0.1005, 0.1006, 0.1061, 0.1154, 0.1234,
        0.1163, 0.1235, 0.1272, 0.1347, 0.1458, 0.1448, 0.1393, 0.1423, 0.1418,
        0.1466, 0.1444, 0.1625, 0.1691, 0.1688, 0.1974, 0.2104, 0.1863, 0.1698,
        0.1737, 0.1636, 0.1452, 0.1424, 0.1355, 0.1341, 0.1325, 0.1283, 0.1386,
        0.1439, 0.1241, 0.1346, 0.1306, 0.1299, 0.1180, 0.1081, 0.0997, 0.0977,
        0.0941, 0.0925, 0.0921, 0.0897, 0.0879, 0.0846, 0.0829, 0.0801, 0.0792,
        0.0766, 0.0782, 0.0790, 0.0760, 0.0752, 0.0706, 0.0635, 0.0599, 0.0573,
        0.0542, 0.0546, 0.0567, 0.0526, 0.0496, 0.0488, 0.0473, 0.0444, 0.0420,
        0.0413, 0.0429, 0.0444, 0.0448, 0.0474, 0.0495, 0.0468, 0.0440, 0.0429,
        0.0427, 0.0462, 0.0474, 0.0448, 0.0525, 0.0519, 0.0570, 0.0549, 0.0561,
        0.0626, 0.0620, 0.0621, 0.0644, 0.0567, 0.0565, 0.0517, 0.0492, 0.0501,
        0.0515, 0.0495, 0.0479, 0.0523, 0.0405, 0.0539, 0.0570, 0.0545, 0.0535,
        0.0525, 0.0529, 0.0533, 0.0523, 0.0508, 0.0500, 0.0506, 0.0507, 0.0465,
        0.0491, 0.0482, 0.0459, 0.0468, 0.0487, 0.0477, 0.0566, 0.0668],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0553, 0.0575, 0.0628, 0.0641, 0.0642, 0.0644, 0.0624, 0.0571, 0.0560,
        0.0587, 0.0632, 0.0660, 0.0658, 0.0635, 0.0590, 0.0602, 0.0605, 0.0587,
        0.0577, 0.0568, 0.0617, 0.0624, 0.0660, 0.0694, 0.0683, 0.0709, 0.0650,
        0.0637, 0.0630, 0.0598, 0.0603, 0.0645, 0.0633, 0.0606, 0.0579, 0.0600,
        0.0644, 0.0602, 0.0609, 0.0591, 0.0580, 0.0573, 0.0579, 0.0575, 0.0586,
        0.0601, 0.0602, 0.0591, 0.0571, 0.0572, 0.0584, 0.0587, 0.0600, 0.0578,
        0.0590, 0.0602, 0.0597, 0.0592, 0.0581, 0.0603, 0.0605, 0.0590, 0.0601,
        0.0595, 0.0602, 0.0595, 0.0596, 0.0604, 0.0594, 0.0590, 0.0600, 0.0585,
        0.0578, 0.0567, 0.0552, 0.0553, 0.0555, 0.0585, 0.0587, 0.0591, 0.0587,
        0.0581, 0.0595, 0.0587, 0.0595, 0.0616, 0.0610, 0.0606, 0.0599, 0.0599,
        0.0601, 0.0598, 0.0589, 0.0569, 0.0569, 0.0579, 0.0592, 0.0591, 0.0580,
        0.0570, 0.0565, 0.0561, 0.0584, 0.0579, 0.0547, 0.0544, 0.0564, 0.0581,
        0.0568, 0.0550, 0.0554, 0.0545, 0.0551, 0.0540, 0.0533, 0.0523, 0.0530,
        0.0527, 0.0534, 0.0547, 0.0536, 0.0515, 0.0559, 0.0548, 0.0537, 0.0532,
        0.0528, 0.0521, 0.0523, 0.0519, 0.0504, 0.0505, 0.0507, 0.0515, 0.0518,
        0.0510, 0.0523, 0.0528, 0.0535, 0.0565, 0.0553, 0.0537, 0.0540, 0.0561,
        0.0557, 0.0535, 0.0518, 0.0515, 0.0521, 0.0535, 0.0549, 0.0557, 0.0533,
        0.0524, 0.0530, 0.0537, 0.0525, 0.0500, 0.0497, 0.0508, 0.0517, 0.0508,
        0.0516, 0.0517, 0.0519, 0.0498, 0.0484, 0.0490, 0.0496, 0.0493, 0.0485,
        0.0493, 0.0494, 0.0497, 0.0502, 0.0553, 0.0512, 0.0499, 0.0511, 0.0513,
        0.0479, 0.0457, 0.0472, 0.0479, 0.0470, 0.0466, 0.0473, 0.0479, 0.0474,
        0.0465, 0.0475, 0.0486, 0.0490, 0.0472, 0.0487, 0.0489, 0.0511, 0.0582,
        0.0578, 0.0560, 0.0558, 0.0524, 0.0534, 0.0575, 0.0613, 0.0548, 0.0569,
        0.0594, 0.0603, 0.0613, 0.0622, 0.0616, 0.0615, 0.0754, 0.0821, 0.0883,
        0.0880, 0.0839, 0.0874, 0.0867, 0.0899, 0.0974, 0.0977, 0.0976, 0.1021,
        0.1062, 0.1021, 0.1004, 0.1003, 0.0975, 0.0974, 0.1027, 0.1116, 0.1192,
        0.1116, 0.1186, 0.1220, 0.1303, 0.1416, 0.1406, 0.1355, 0.1391, 0.1386,
        0.1426, 0.1383, 0.1538, 0.1561, 0.1571, 0.1793, 0.2033, 0.1853, 0.1834,
        0.1714, 0.1662, 0.1520, 0.1550, 0.1452, 0.1390, 0.1337, 0.1280, 0.1383,
        0.1400, 0.1209, 0.1314, 0.1281, 0.1281, 0.1176, 0.1091, 0.1016, 0.1001,
        0.0966, 0.0946, 0.0937, 0.0912, 0.0897, 0.0863, 0.0845, 0.0817, 0.0809,
        0.0784, 0.0800, 0.0809, 0.0779, 0.0768, 0.0717, 0.0642, 0.0607, 0.0582,
        0.0546, 0.0547, 0.0568, 0.0526, 0.0496, 0.0487, 0.0470, 0.0439, 0.0414,
        0.0405, 0.0422, 0.0436, 0.0442, 0.0466, 0.0487, 0.0460, 0.0429, 0.0419,
        0.0419, 0.0454, 0.0465, 0.0439, 0.0505, 0.0502, 0.0554, 0.0536, 0.0547,
        0.0610, 0.0606, 0.0605, 0.0628, 0.0552, 0.0550, 0.0505, 0.0483, 0.0492,
        0.0508, 0.0487, 0.0471, 0.0512, 0.0396, 0.0522, 0.0555, 0.0533, 0.0523,
        0.0514, 0.0521, 0.0523, 0.0511, 0.0499, 0.0492, 0.0495, 0.0495, 0.0453,
        0.0477, 0.0469, 0.0449, 0.0457, 0.0474, 0.0466, 0.0556, 0.0652],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0662, 0.0701, 0.0754, 0.0785, 0.0772, 0.0798, 0.0767, 0.0740, 0.0698,
        0.0716, 0.0771, 0.0808, 0.0803, 0.0766, 0.0712, 0.0728, 0.0739, 0.0714,
        0.0698, 0.0682, 0.0747, 0.0753, 0.0800, 0.0846, 0.0842, 0.0879, 0.0807,
        0.0799, 0.0798, 0.0757, 0.0755, 0.0808, 0.0785, 0.0743, 0.0709, 0.0721,
        0.0778, 0.0729, 0.0724, 0.0700, 0.0684, 0.0677, 0.0682, 0.0677, 0.0688,
        0.0701, 0.0703, 0.0691, 0.0670, 0.0672, 0.0685, 0.0691, 0.0706, 0.0685,
        0.0692, 0.0704, 0.0697, 0.0690, 0.0677, 0.0702, 0.0707, 0.0692, 0.0701,
        0.0695, 0.0704, 0.0699, 0.0701, 0.0710, 0.0702, 0.0700, 0.0716, 0.0701,
        0.0692, 0.0677, 0.0661, 0.0664, 0.0667, 0.0704, 0.0700, 0.0699, 0.0690,
        0.0683, 0.0696, 0.0687, 0.0695, 0.0719, 0.0710, 0.0707, 0.0700, 0.0703,
        0.0704, 0.0700, 0.0689, 0.0665, 0.0669, 0.0675, 0.0690, 0.0690, 0.0678,
        0.0665, 0.0659, 0.0654, 0.0680, 0.0676, 0.0641, 0.0635, 0.0658, 0.0680,
        0.0665, 0.0644, 0.0646, 0.0635, 0.0641, 0.0628, 0.0619, 0.0608, 0.0615,
        0.0614, 0.0622, 0.0635, 0.0625, 0.0603, 0.0645, 0.0635, 0.0620, 0.0615,
        0.0613, 0.0606, 0.0608, 0.0601, 0.0582, 0.0585, 0.0588, 0.0599, 0.0604,
        0.0589, 0.0606, 0.0612, 0.0623, 0.0650, 0.0634, 0.0619, 0.0622, 0.0650,
        0.0644, 0.0616, 0.0599, 0.0597, 0.0602, 0.0620, 0.0638, 0.0648, 0.0615,
        0.0605, 0.0612, 0.0619, 0.0605, 0.0571, 0.0571, 0.0585, 0.0595, 0.0587,
        0.0597, 0.0600, 0.0605, 0.0577, 0.0560, 0.0565, 0.0571, 0.0571, 0.0561,
        0.0570, 0.0571, 0.0576, 0.0584, 0.0652, 0.0598, 0.0581, 0.0590, 0.0580,
        0.0541, 0.0515, 0.0529, 0.0540, 0.0540, 0.0539, 0.0549, 0.0559, 0.0548,
        0.0532, 0.0534, 0.0542, 0.0547, 0.0530, 0.0554, 0.0555, 0.0575, 0.0671,
        0.0668, 0.0641, 0.0642, 0.0602, 0.0620, 0.0662, 0.0684, 0.0613, 0.0640,
        0.0645, 0.0684, 0.0702, 0.0727, 0.0708, 0.0711, 0.0863, 0.0956, 0.1022,
        0.0988, 0.0939, 0.0961, 0.0957, 0.1003, 0.1095, 0.1102, 0.1102, 0.1152,
        0.1193, 0.1152, 0.1122, 0.1115, 0.1079, 0.1081, 0.1130, 0.1222, 0.1304,
        0.1233, 0.1305, 0.1336, 0.1403, 0.1525, 0.1519, 0.1458, 0.1478, 0.1463,
        0.1516, 0.1510, 0.1700, 0.1782, 0.1783, 0.2058, 0.2122, 0.1891, 0.1691,
        0.1763, 0.1643, 0.1446, 0.1402, 0.1360, 0.1360, 0.1361, 0.1336, 0.1451,
        0.1521, 0.1299, 0.1394, 0.1356, 0.1333, 0.1208, 0.1105, 0.1021, 0.1005,
        0.0973, 0.0959, 0.0958, 0.0941, 0.0918, 0.0883, 0.0861, 0.0833, 0.0826,
        0.0802, 0.0819, 0.0827, 0.0799, 0.0793, 0.0748, 0.0673, 0.0631, 0.0599,
        0.0568, 0.0578, 0.0601, 0.0558, 0.0527, 0.0519, 0.0506, 0.0476, 0.0452,
        0.0447, 0.0467, 0.0486, 0.0491, 0.0515, 0.0536, 0.0505, 0.0478, 0.0467,
        0.0464, 0.0501, 0.0515, 0.0489, 0.0581, 0.0575, 0.0628, 0.0604, 0.0618,
        0.0690, 0.0684, 0.0686, 0.0713, 0.0626, 0.0624, 0.0569, 0.0539, 0.0551,
        0.0564, 0.0542, 0.0522, 0.0573, 0.0447, 0.0598, 0.0630, 0.0602, 0.0593,
        0.0587, 0.0589, 0.0592, 0.0585, 0.0567, 0.0550, 0.0556, 0.0558, 0.0513,
        0.0543, 0.0532, 0.0505, 0.0516, 0.0540, 0.0522, 0.0618, 0.0735],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([-0.0009, -0.0036, -0.0066, -0.0010,  0.0083,  0.0120,  0.0098,  0.0086,
         0.0079,  0.0083,  0.0083,  0.0062,  0.0070,  0.0160,  0.0226,  0.0210,
         0.0200,  0.0210,  0.0188,  0.0187,  0.0144,  0.0133,  0.0164,  0.0140,
         0.0140,  0.0136,  0.0133,  0.0120,  0.0120,  0.0104,  0.0118,  0.0120,
         0.0115,  0.0120,  0.0125,  0.0089,  0.0080,  0.0153,  0.0177,  0.0173,
         0.0158,  0.0205,  0.0230,  0.0269,  0.0299,  0.0316,  0.0306,  0.0307,
         0.0296,  0.0306,  0.0316,  0.0317,  0.0321,  0.0321,  0.0315,  0.0330,
         0.0341,  0.0334,  0.0324,  0.0304,  0.0281,  0.0286,  0.0291,  0.0288,
         0.0284,  0.0267,  0.0276,  0.0312,  0.0328,  0.0274,  0.0239,  0.0229,
         0.0246,  0.0277,  0.0289,  0.0269,  0.0289,  0.0265,  0.0264,  0.0285,
         0.0309,  0.0303,  0.0304,  0.0316,  0.0344,  0.0346,  0.0355,  0.0358,
         0.0363,  0.0365,  0.0386,  0.0404,  0.0397,  0.0380,  0.0380,  0.0384,
         0.0383,  0.0383,  0.0378,  0.0380,  0.0381,  0.0374,  0.0388,  0.0370,
         0.0371,  0.0401,  0.0422,  0.0417,  0.0415,  0.0427,  0.0420,  0.0418,
         0.0433,  0.0431,  0.0418,  0.0421,  0.0444,  0.0458,  0.0473,  0.0477,
         0.0482,  0.0503,  0.0525,  0.0523,  0.0514,  0.0496,  0.0467,  0.0465,
         0.0464,  0.0451,  0.0446,  0.0485,  0.0481,  0.0461,  0.0448,  0.0420,
         0.0419,  0.0451,  0.0472,  0.0511,  0.0511,  0.0496,  0.0486,  0.0543,
         0.0581,  0.0595,  0.0613,  0.0620,  0.0569,  0.0556,  0.0511,  0.0494,
         0.0448,  0.0453,  0.0461,  0.0485,  0.0501,  0.0470,  0.0446,  0.0466,
         0.0466,  0.0475,  0.0460,  0.0445,  0.0435,  0.0417,  0.0413,  0.0434,
         0.0452,  0.0445,  0.0460,  0.0463,  0.0466,  0.0472,  0.0480,  0.0498,
         0.0488,  0.0506,  0.0555,  0.0558,  0.0514,  0.0507,  0.0487,  0.0507,
         0.0467,  0.0470,  0.0470,  0.0430,  0.0478,  0.0457,  0.0449,  0.0475,
         0.0483,  0.0468,  0.0460,  0.0494,  0.0510,  0.0468,  0.0447,  0.0467,
         0.0482,  0.0509,  0.0488,  0.0461,  0.0498,  0.0468,  0.0497,  0.0476,
         0.0508,  0.0540,  0.0485,  0.0537,  0.0519,  0.0506,  0.0477,  0.0509,
         0.0623,  0.0632,  0.0602,  0.0597,  0.0553,  0.0533,  0.0522,  0.0517,
         0.0534,  0.0549,  0.0528,  0.0509,  0.0535,  0.0426,  0.0406,  0.0407,
         0.0490,  0.0523,  0.0536,  0.0523,  0.0463,  0.0485,  0.0549,  0.0559,
         0.0559,  0.0572,  0.0567,  0.0589,  0.0549,  0.0543,  0.0594,  0.0577,
         0.0549,  0.0564,  0.0567,  0.0593,  0.0599,  0.0598,  0.0613,  0.0608,
         0.0673,  0.0660,  0.0713,  0.0706,  0.0701,  0.0624,  0.0677,  0.0711,
         0.0746,  0.0752,  0.0747,  0.0742,  0.0782,  0.0886,  0.0953,  0.0969,
         0.0985,  0.1001,  0.0980,  0.0971,  0.0986,  0.0994,  0.1066,  0.1068,
         0.1065,  0.1073,  0.1069,  0.1092,  0.1084,  0.1108,  0.1147,  0.1141,
         0.1209,  0.1230,  0.1272,  0.1221,  0.1267,  0.1243,  0.1241,  0.1171,
         0.1185,  0.1116,  0.1141,  0.1100,  0.1097,  0.1139,  0.1193,  0.1201,
         0.1081,  0.1034,  0.0967,  0.1009,  0.1014,  0.0935,  0.0931,  0.0895,
         0.0861,  0.0850,  0.0842,  0.0828,  0.0843,  0.0844,  0.0838,  0.0839,
         0.0781,  0.0790,  0.0743,  0.0768,  0.0789,  0.0753,  0.0814,  0.0850,
         0.0759,  0.0729,  0.0678,  0.0648,  0.0618,  0.0563,  0.0555,  0.0560,
         0.0563,  0.0547,  0.0559,  0.0580,  0.0615,  0.0636,  0.0590,  0.0579,
         0.0572,  0.0519,  0.0459,  0.0420,  0.0429,  0.0429],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0562, 0.0584, 0.0643, 0.0659, 0.0667, 0.0664, 0.0646, 0.0583, 0.0578,
        0.0608, 0.0655, 0.0685, 0.0684, 0.0662, 0.0617, 0.0629, 0.0629, 0.0609,
        0.0599, 0.0591, 0.0644, 0.0651, 0.0690, 0.0725, 0.0712, 0.0740, 0.0677,
        0.0664, 0.0658, 0.0625, 0.0631, 0.0676, 0.0664, 0.0634, 0.0604, 0.0625,
        0.0672, 0.0631, 0.0635, 0.0617, 0.0604, 0.0598, 0.0603, 0.0600, 0.0611,
        0.0627, 0.0628, 0.0616, 0.0595, 0.0597, 0.0608, 0.0612, 0.0624, 0.0603,
        0.0613, 0.0626, 0.0622, 0.0616, 0.0605, 0.0627, 0.0630, 0.0616, 0.0627,
        0.0620, 0.0627, 0.0621, 0.0622, 0.0631, 0.0621, 0.0616, 0.0626, 0.0611,
        0.0604, 0.0592, 0.0577, 0.0578, 0.0579, 0.0610, 0.0612, 0.0616, 0.0611,
        0.0606, 0.0620, 0.0612, 0.0621, 0.0643, 0.0636, 0.0632, 0.0625, 0.0625,
        0.0627, 0.0624, 0.0614, 0.0593, 0.0594, 0.0603, 0.0617, 0.0617, 0.0605,
        0.0595, 0.0589, 0.0585, 0.0608, 0.0603, 0.0571, 0.0567, 0.0588, 0.0606,
        0.0592, 0.0574, 0.0577, 0.0568, 0.0574, 0.0563, 0.0554, 0.0545, 0.0551,
        0.0549, 0.0556, 0.0569, 0.0559, 0.0538, 0.0581, 0.0570, 0.0559, 0.0553,
        0.0550, 0.0542, 0.0544, 0.0540, 0.0524, 0.0526, 0.0528, 0.0536, 0.0540,
        0.0529, 0.0543, 0.0548, 0.0557, 0.0588, 0.0575, 0.0559, 0.0562, 0.0585,
        0.0582, 0.0558, 0.0540, 0.0537, 0.0542, 0.0556, 0.0571, 0.0580, 0.0553,
        0.0544, 0.0550, 0.0557, 0.0545, 0.0518, 0.0515, 0.0526, 0.0536, 0.0526,
        0.0534, 0.0536, 0.0536, 0.0516, 0.0502, 0.0507, 0.0512, 0.0511, 0.0503,
        0.0511, 0.0513, 0.0517, 0.0521, 0.0571, 0.0528, 0.0516, 0.0530, 0.0530,
        0.0495, 0.0472, 0.0488, 0.0497, 0.0488, 0.0483, 0.0489, 0.0495, 0.0491,
        0.0482, 0.0492, 0.0503, 0.0506, 0.0488, 0.0504, 0.0508, 0.0530, 0.0598,
        0.0595, 0.0576, 0.0573, 0.0540, 0.0550, 0.0590, 0.0630, 0.0568, 0.0585,
        0.0615, 0.0620, 0.0629, 0.0640, 0.0632, 0.0631, 0.0768, 0.0830, 0.0892,
        0.0893, 0.0855, 0.0892, 0.0885, 0.0914, 0.0986, 0.0991, 0.0988, 0.1035,
        0.1076, 0.1037, 0.1020, 0.1019, 0.0992, 0.0990, 0.1039, 0.1125, 0.1198,
        0.1120, 0.1189, 0.1218, 0.1301, 0.1421, 0.1415, 0.1363, 0.1397, 0.1389,
        0.1426, 0.1376, 0.1520, 0.1525, 0.1550, 0.1703, 0.1971, 0.1862, 0.1897,
        0.1721, 0.1689, 0.1570, 0.1633, 0.1537, 0.1457, 0.1375, 0.1312, 0.1414,
        0.1420, 0.1217, 0.1318, 0.1289, 0.1286, 0.1187, 0.1108, 0.1040, 0.1033,
        0.1000, 0.0980, 0.0968, 0.0944, 0.0928, 0.0894, 0.0874, 0.0845, 0.0840,
        0.0815, 0.0832, 0.0842, 0.0814, 0.0801, 0.0747, 0.0666, 0.0629, 0.0600,
        0.0562, 0.0563, 0.0584, 0.0542, 0.0511, 0.0501, 0.0483, 0.0450, 0.0424,
        0.0417, 0.0435, 0.0452, 0.0458, 0.0481, 0.0501, 0.0472, 0.0440, 0.0430,
        0.0430, 0.0467, 0.0478, 0.0453, 0.0519, 0.0517, 0.0572, 0.0555, 0.0565,
        0.0631, 0.0628, 0.0626, 0.0649, 0.0571, 0.0568, 0.0523, 0.0500, 0.0511,
        0.0527, 0.0505, 0.0487, 0.0529, 0.0410, 0.0538, 0.0574, 0.0553, 0.0544,
        0.0537, 0.0546, 0.0548, 0.0535, 0.0521, 0.0511, 0.0512, 0.0511, 0.0469,
        0.0492, 0.0484, 0.0465, 0.0474, 0.0491, 0.0482, 0.0576, 0.0675],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0538, 0.0557, 0.0610, 0.0620, 0.0622, 0.0620, 0.0602, 0.0548, 0.0542,
        0.0568, 0.0612, 0.0639, 0.0637, 0.0615, 0.0572, 0.0584, 0.0586, 0.0569,
        0.0560, 0.0552, 0.0598, 0.0605, 0.0639, 0.0672, 0.0660, 0.0685, 0.0627,
        0.0615, 0.0607, 0.0576, 0.0583, 0.0623, 0.0612, 0.0587, 0.0561, 0.0583,
        0.0625, 0.0584, 0.0592, 0.0576, 0.0565, 0.0558, 0.0564, 0.0560, 0.0571,
        0.0587, 0.0587, 0.0576, 0.0556, 0.0558, 0.0569, 0.0572, 0.0584, 0.0562,
        0.0574, 0.0586, 0.0582, 0.0577, 0.0566, 0.0588, 0.0590, 0.0575, 0.0586,
        0.0580, 0.0586, 0.0580, 0.0581, 0.0589, 0.0578, 0.0573, 0.0583, 0.0568,
        0.0562, 0.0551, 0.0536, 0.0538, 0.0539, 0.0568, 0.0570, 0.0575, 0.0572,
        0.0566, 0.0581, 0.0572, 0.0580, 0.0600, 0.0595, 0.0590, 0.0584, 0.0583,
        0.0586, 0.0583, 0.0574, 0.0554, 0.0554, 0.0564, 0.0577, 0.0577, 0.0566,
        0.0556, 0.0551, 0.0548, 0.0570, 0.0565, 0.0534, 0.0530, 0.0550, 0.0566,
        0.0553, 0.0536, 0.0540, 0.0532, 0.0538, 0.0527, 0.0520, 0.0511, 0.0517,
        0.0514, 0.0521, 0.0534, 0.0523, 0.0502, 0.0546, 0.0534, 0.0524, 0.0519,
        0.0516, 0.0508, 0.0511, 0.0507, 0.0492, 0.0493, 0.0495, 0.0503, 0.0506,
        0.0498, 0.0511, 0.0516, 0.0522, 0.0552, 0.0540, 0.0525, 0.0527, 0.0547,
        0.0544, 0.0523, 0.0505, 0.0502, 0.0509, 0.0522, 0.0536, 0.0544, 0.0521,
        0.0512, 0.0518, 0.0525, 0.0513, 0.0490, 0.0486, 0.0496, 0.0506, 0.0496,
        0.0504, 0.0504, 0.0506, 0.0487, 0.0473, 0.0479, 0.0484, 0.0482, 0.0474,
        0.0481, 0.0482, 0.0486, 0.0490, 0.0539, 0.0499, 0.0486, 0.0499, 0.0502,
        0.0469, 0.0448, 0.0463, 0.0469, 0.0460, 0.0456, 0.0462, 0.0467, 0.0463,
        0.0455, 0.0465, 0.0477, 0.0481, 0.0463, 0.0477, 0.0478, 0.0501, 0.0569,
        0.0565, 0.0548, 0.0545, 0.0512, 0.0521, 0.0562, 0.0603, 0.0538, 0.0559,
        0.0586, 0.0592, 0.0600, 0.0607, 0.0603, 0.0601, 0.0738, 0.0801, 0.0863,
        0.0864, 0.0823, 0.0860, 0.0853, 0.0883, 0.0956, 0.0958, 0.0957, 0.1002,
        0.1042, 0.1001, 0.0986, 0.0986, 0.0959, 0.0958, 0.1011, 0.1100, 0.1175,
        0.1099, 0.1169, 0.1204, 0.1288, 0.1400, 0.1389, 0.1339, 0.1377, 0.1375,
        0.1413, 0.1366, 0.1518, 0.1536, 0.1553, 0.1752, 0.2005, 0.1857, 0.1867,
        0.1707, 0.1663, 0.1534, 0.1577, 0.1473, 0.1401, 0.1335, 0.1273, 0.1374,
        0.1384, 0.1196, 0.1301, 0.1268, 0.1273, 0.1172, 0.1089, 0.1014, 0.1000,
        0.0963, 0.0943, 0.0934, 0.0908, 0.0894, 0.0859, 0.0842, 0.0814, 0.0806,
        0.0781, 0.0796, 0.0805, 0.0775, 0.0763, 0.0712, 0.0638, 0.0604, 0.0580,
        0.0544, 0.0543, 0.0563, 0.0522, 0.0492, 0.0483, 0.0465, 0.0434, 0.0409,
        0.0400, 0.0415, 0.0429, 0.0435, 0.0459, 0.0479, 0.0453, 0.0422, 0.0412,
        0.0412, 0.0447, 0.0457, 0.0431, 0.0494, 0.0491, 0.0543, 0.0526, 0.0536,
        0.0598, 0.0594, 0.0592, 0.0615, 0.0541, 0.0539, 0.0496, 0.0474, 0.0483,
        0.0499, 0.0478, 0.0463, 0.0502, 0.0389, 0.0511, 0.0544, 0.0523, 0.0512,
        0.0503, 0.0510, 0.0512, 0.0500, 0.0489, 0.0483, 0.0486, 0.0485, 0.0444,
        0.0467, 0.0459, 0.0441, 0.0449, 0.0464, 0.0458, 0.0547, 0.0639],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0747, 0.0793, 0.0859, 0.0906, 0.0896, 0.0929, 0.0895, 0.0865, 0.0811,
        0.0830, 0.0892, 0.0937, 0.0934, 0.0892, 0.0831, 0.0851, 0.0864, 0.0830,
        0.0812, 0.0792, 0.0874, 0.0881, 0.0941, 0.0994, 0.0991, 0.1034, 0.0954,
        0.0950, 0.0957, 0.0913, 0.0909, 0.0980, 0.0947, 0.0884, 0.0840, 0.0846,
        0.0916, 0.0863, 0.0842, 0.0809, 0.0790, 0.0781, 0.0786, 0.0780, 0.0792,
        0.0804, 0.0807, 0.0792, 0.0769, 0.0773, 0.0788, 0.0796, 0.0810, 0.0789,
        0.0792, 0.0804, 0.0797, 0.0789, 0.0774, 0.0803, 0.0810, 0.0795, 0.0801,
        0.0797, 0.0807, 0.0804, 0.0807, 0.0818, 0.0812, 0.0812, 0.0830, 0.0816,
        0.0804, 0.0788, 0.0773, 0.0775, 0.0779, 0.0822, 0.0813, 0.0808, 0.0794,
        0.0786, 0.0797, 0.0788, 0.0798, 0.0825, 0.0813, 0.0811, 0.0802, 0.0809,
        0.0809, 0.0803, 0.0791, 0.0764, 0.0771, 0.0773, 0.0790, 0.0791, 0.0778,
        0.0763, 0.0755, 0.0748, 0.0775, 0.0772, 0.0735, 0.0728, 0.0754, 0.0780,
        0.0763, 0.0740, 0.0740, 0.0728, 0.0734, 0.0719, 0.0707, 0.0693, 0.0702,
        0.0702, 0.0711, 0.0723, 0.0714, 0.0690, 0.0728, 0.0720, 0.0703, 0.0698,
        0.0697, 0.0690, 0.0691, 0.0682, 0.0662, 0.0668, 0.0672, 0.0684, 0.0691,
        0.0669, 0.0688, 0.0695, 0.0708, 0.0735, 0.0716, 0.0702, 0.0706, 0.0742,
        0.0733, 0.0700, 0.0683, 0.0681, 0.0684, 0.0704, 0.0728, 0.0740, 0.0695,
        0.0684, 0.0691, 0.0700, 0.0684, 0.0640, 0.0643, 0.0658, 0.0670, 0.0662,
        0.0672, 0.0679, 0.0682, 0.0649, 0.0632, 0.0636, 0.0642, 0.0644, 0.0634,
        0.0645, 0.0647, 0.0653, 0.0662, 0.0737, 0.0672, 0.0653, 0.0666, 0.0644,
        0.0600, 0.0572, 0.0587, 0.0603, 0.0607, 0.0606, 0.0617, 0.0629, 0.0616,
        0.0596, 0.0593, 0.0600, 0.0603, 0.0586, 0.0618, 0.0621, 0.0636, 0.0739,
        0.0740, 0.0708, 0.0709, 0.0666, 0.0688, 0.0730, 0.0746, 0.0679, 0.0702,
        0.0709, 0.0755, 0.0773, 0.0813, 0.0784, 0.0789, 0.0942, 0.1038, 0.1105,
        0.1061, 0.1014, 0.1036, 0.1035, 0.1081, 0.1174, 0.1185, 0.1182, 0.1238,
        0.1281, 0.1244, 0.1207, 0.1198, 0.1159, 0.1162, 0.1202, 0.1289, 0.1367,
        0.1295, 0.1367, 0.1391, 0.1455, 0.1590, 0.1589, 0.1525, 0.1536, 0.1507,
        0.1552, 0.1540, 0.1716, 0.1774, 0.1774, 0.2073, 0.2125, 0.1897, 0.1711,
        0.1793, 0.1691, 0.1502, 0.1456, 0.1428, 0.1426, 0.1421, 0.1403, 0.1530,
        0.1594, 0.1350, 0.1445, 0.1416, 0.1378, 0.1257, 0.1158, 0.1079, 0.1072,
        0.1044, 0.1030, 0.1029, 0.1018, 0.0992, 0.0953, 0.0921, 0.0893, 0.0888,
        0.0867, 0.0888, 0.0897, 0.0871, 0.0864, 0.0819, 0.0735, 0.0685, 0.0646,
        0.0610, 0.0628, 0.0653, 0.0605, 0.0573, 0.0564, 0.0551, 0.0519, 0.0493,
        0.0493, 0.0519, 0.0544, 0.0550, 0.0573, 0.0589, 0.0551, 0.0525, 0.0515,
        0.0510, 0.0550, 0.0569, 0.0543, 0.0651, 0.0646, 0.0701, 0.0673, 0.0689,
        0.0772, 0.0768, 0.0770, 0.0802, 0.0700, 0.0697, 0.0637, 0.0603, 0.0619,
        0.0629, 0.0605, 0.0581, 0.0639, 0.0504, 0.0672, 0.0706, 0.0675, 0.0669,
        0.0673, 0.0672, 0.0674, 0.0671, 0.0648, 0.0617, 0.0620, 0.0623, 0.0574,
        0.0607, 0.0595, 0.0564, 0.0579, 0.0608, 0.0579, 0.0686, 0.0819],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0578, 0.0602, 0.0662, 0.0681, 0.0689, 0.0690, 0.0671, 0.0606, 0.0598,
        0.0628, 0.0678, 0.0709, 0.0708, 0.0684, 0.0638, 0.0650, 0.0651, 0.0629,
        0.0619, 0.0609, 0.0666, 0.0673, 0.0714, 0.0751, 0.0739, 0.0768, 0.0702,
        0.0690, 0.0684, 0.0650, 0.0655, 0.0703, 0.0689, 0.0656, 0.0624, 0.0644,
        0.0694, 0.0652, 0.0655, 0.0635, 0.0622, 0.0616, 0.0621, 0.0618, 0.0629,
        0.0645, 0.0646, 0.0633, 0.0612, 0.0614, 0.0626, 0.0631, 0.0643, 0.0621,
        0.0631, 0.0644, 0.0639, 0.0634, 0.0622, 0.0645, 0.0648, 0.0634, 0.0644,
        0.0638, 0.0645, 0.0639, 0.0641, 0.0649, 0.0640, 0.0635, 0.0646, 0.0631,
        0.0623, 0.0610, 0.0596, 0.0597, 0.0598, 0.0630, 0.0631, 0.0634, 0.0629,
        0.0623, 0.0638, 0.0629, 0.0639, 0.0662, 0.0654, 0.0650, 0.0643, 0.0644,
        0.0646, 0.0642, 0.0632, 0.0611, 0.0612, 0.0620, 0.0635, 0.0634, 0.0623,
        0.0612, 0.0606, 0.0602, 0.0624, 0.0620, 0.0587, 0.0583, 0.0605, 0.0624,
        0.0609, 0.0591, 0.0593, 0.0584, 0.0590, 0.0579, 0.0570, 0.0560, 0.0567,
        0.0564, 0.0571, 0.0585, 0.0574, 0.0554, 0.0597, 0.0586, 0.0574, 0.0568,
        0.0565, 0.0557, 0.0559, 0.0554, 0.0538, 0.0540, 0.0543, 0.0551, 0.0555,
        0.0543, 0.0557, 0.0563, 0.0572, 0.0603, 0.0590, 0.0574, 0.0576, 0.0601,
        0.0598, 0.0573, 0.0555, 0.0552, 0.0557, 0.0571, 0.0587, 0.0596, 0.0567,
        0.0558, 0.0564, 0.0571, 0.0559, 0.0530, 0.0528, 0.0539, 0.0549, 0.0540,
        0.0548, 0.0550, 0.0550, 0.0529, 0.0515, 0.0520, 0.0525, 0.0524, 0.0516,
        0.0525, 0.0526, 0.0530, 0.0535, 0.0587, 0.0542, 0.0529, 0.0544, 0.0542,
        0.0506, 0.0482, 0.0499, 0.0508, 0.0500, 0.0496, 0.0502, 0.0508, 0.0504,
        0.0494, 0.0503, 0.0514, 0.0516, 0.0498, 0.0516, 0.0520, 0.0541, 0.0612,
        0.0609, 0.0589, 0.0587, 0.0552, 0.0564, 0.0604, 0.0641, 0.0580, 0.0597,
        0.0626, 0.0633, 0.0642, 0.0656, 0.0646, 0.0646, 0.0785, 0.0849, 0.0911,
        0.0908, 0.0872, 0.0907, 0.0901, 0.0931, 0.1005, 0.1010, 0.1006, 0.1054,
        0.1096, 0.1057, 0.1038, 0.1037, 0.1009, 0.1007, 0.1056, 0.1141, 0.1214,
        0.1136, 0.1205, 0.1233, 0.1314, 0.1437, 0.1432, 0.1378, 0.1410, 0.1400,
        0.1437, 0.1389, 0.1534, 0.1539, 0.1561, 0.1723, 0.1989, 0.1859, 0.1886,
        0.1726, 0.1691, 0.1565, 0.1624, 0.1531, 0.1455, 0.1381, 0.1322, 0.1428,
        0.1437, 0.1230, 0.1330, 0.1302, 0.1294, 0.1194, 0.1114, 0.1046, 0.1040,
        0.1008, 0.0988, 0.0977, 0.0953, 0.0936, 0.0903, 0.0881, 0.0853, 0.0847,
        0.0824, 0.0840, 0.0850, 0.0823, 0.0810, 0.0756, 0.0674, 0.0635, 0.0606,
        0.0567, 0.0570, 0.0591, 0.0548, 0.0517, 0.0507, 0.0490, 0.0457, 0.0431,
        0.0424, 0.0444, 0.0461, 0.0467, 0.0490, 0.0509, 0.0479, 0.0448, 0.0438,
        0.0438, 0.0475, 0.0487, 0.0462, 0.0531, 0.0529, 0.0585, 0.0567, 0.0578,
        0.0646, 0.0643, 0.0641, 0.0665, 0.0584, 0.0581, 0.0535, 0.0511, 0.0523,
        0.0538, 0.0516, 0.0497, 0.0540, 0.0419, 0.0551, 0.0587, 0.0566, 0.0557,
        0.0551, 0.0560, 0.0562, 0.0549, 0.0534, 0.0522, 0.0523, 0.0522, 0.0479,
        0.0504, 0.0495, 0.0475, 0.0485, 0.0502, 0.0492, 0.0587, 0.0690],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0634, 0.0667, 0.0730, 0.0758, 0.0759, 0.0778, 0.0751, 0.0694, 0.0668,
        0.0696, 0.0750, 0.0786, 0.0783, 0.0754, 0.0703, 0.0717, 0.0722, 0.0695,
        0.0682, 0.0669, 0.0735, 0.0742, 0.0789, 0.0832, 0.0824, 0.0858, 0.0785,
        0.0775, 0.0772, 0.0733, 0.0735, 0.0790, 0.0770, 0.0729, 0.0692, 0.0708,
        0.0764, 0.0719, 0.0716, 0.0693, 0.0677, 0.0670, 0.0676, 0.0672, 0.0683,
        0.0698, 0.0699, 0.0686, 0.0664, 0.0667, 0.0680, 0.0686, 0.0699, 0.0677,
        0.0685, 0.0697, 0.0692, 0.0686, 0.0673, 0.0698, 0.0702, 0.0688, 0.0697,
        0.0691, 0.0699, 0.0694, 0.0696, 0.0705, 0.0697, 0.0694, 0.0708, 0.0692,
        0.0683, 0.0669, 0.0654, 0.0655, 0.0657, 0.0693, 0.0691, 0.0691, 0.0683,
        0.0677, 0.0691, 0.0682, 0.0692, 0.0716, 0.0707, 0.0704, 0.0696, 0.0699,
        0.0700, 0.0696, 0.0685, 0.0662, 0.0664, 0.0671, 0.0686, 0.0686, 0.0674,
        0.0662, 0.0656, 0.0650, 0.0675, 0.0671, 0.0637, 0.0631, 0.0655, 0.0676,
        0.0661, 0.0640, 0.0642, 0.0632, 0.0638, 0.0625, 0.0615, 0.0604, 0.0611,
        0.0610, 0.0618, 0.0631, 0.0621, 0.0600, 0.0642, 0.0631, 0.0618, 0.0612,
        0.0609, 0.0602, 0.0604, 0.0597, 0.0579, 0.0582, 0.0586, 0.0595, 0.0600,
        0.0585, 0.0601, 0.0607, 0.0618, 0.0648, 0.0632, 0.0617, 0.0620, 0.0649,
        0.0644, 0.0616, 0.0598, 0.0596, 0.0600, 0.0616, 0.0634, 0.0644, 0.0610,
        0.0600, 0.0607, 0.0614, 0.0601, 0.0567, 0.0566, 0.0579, 0.0589, 0.0580,
        0.0590, 0.0593, 0.0595, 0.0569, 0.0555, 0.0559, 0.0565, 0.0565, 0.0556,
        0.0565, 0.0567, 0.0571, 0.0578, 0.0637, 0.0586, 0.0572, 0.0585, 0.0577,
        0.0538, 0.0513, 0.0528, 0.0540, 0.0536, 0.0533, 0.0541, 0.0550, 0.0542,
        0.0529, 0.0534, 0.0543, 0.0546, 0.0529, 0.0551, 0.0555, 0.0574, 0.0656,
        0.0654, 0.0630, 0.0629, 0.0592, 0.0606, 0.0647, 0.0677, 0.0613, 0.0632,
        0.0653, 0.0673, 0.0686, 0.0709, 0.0691, 0.0694, 0.0839, 0.0916, 0.0980,
        0.0962, 0.0923, 0.0953, 0.0948, 0.0984, 0.1066, 0.1073, 0.1069, 0.1120,
        0.1162, 0.1124, 0.1099, 0.1094, 0.1062, 0.1062, 0.1108, 0.1195, 0.1271,
        0.1194, 0.1264, 0.1291, 0.1366, 0.1493, 0.1490, 0.1432, 0.1456, 0.1440,
        0.1480, 0.1443, 0.1599, 0.1619, 0.1621, 0.1864, 0.2081, 0.1853, 0.1766,
        0.1751, 0.1692, 0.1526, 0.1534, 0.1465, 0.1422, 0.1386, 0.1346, 0.1459,
        0.1488, 0.1272, 0.1373, 0.1345, 0.1324, 0.1214, 0.1125, 0.1054, 0.1047,
        0.1016, 0.0999, 0.0991, 0.0971, 0.0950, 0.0917, 0.0892, 0.0863, 0.0858,
        0.0835, 0.0853, 0.0862, 0.0836, 0.0825, 0.0774, 0.0691, 0.0648, 0.0615,
        0.0578, 0.0586, 0.0609, 0.0564, 0.0533, 0.0523, 0.0508, 0.0476, 0.0451,
        0.0446, 0.0468, 0.0487, 0.0493, 0.0516, 0.0535, 0.0503, 0.0474, 0.0463,
        0.0462, 0.0499, 0.0513, 0.0489, 0.0571, 0.0568, 0.0624, 0.0602, 0.0615,
        0.0688, 0.0685, 0.0684, 0.0710, 0.0622, 0.0620, 0.0569, 0.0542, 0.0555,
        0.0568, 0.0545, 0.0525, 0.0573, 0.0446, 0.0590, 0.0627, 0.0602, 0.0594,
        0.0591, 0.0597, 0.0600, 0.0589, 0.0571, 0.0553, 0.0556, 0.0556, 0.0511,
        0.0538, 0.0528, 0.0504, 0.0516, 0.0537, 0.0521, 0.0619, 0.0733],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0505, 0.0526, 0.0587, 0.0601, 0.0645, 0.0574, 0.0584, 0.0530, 0.0551,
        0.0583, 0.0630, 0.0660, 0.0665, 0.0653, 0.0612, 0.0619, 0.0610, 0.0589,
        0.0583, 0.0579, 0.0633, 0.0638, 0.0675, 0.0704, 0.0686, 0.0714, 0.0652,
        0.0640, 0.0633, 0.0606, 0.0615, 0.0664, 0.0655, 0.0625, 0.0595, 0.0619,
        0.0666, 0.0629, 0.0636, 0.0617, 0.0605, 0.0598, 0.0604, 0.0603, 0.0616,
        0.0633, 0.0634, 0.0619, 0.0597, 0.0600, 0.0612, 0.0615, 0.0623, 0.0602,
        0.0613, 0.0627, 0.0625, 0.0620, 0.0609, 0.0631, 0.0635, 0.0621, 0.0632,
        0.0626, 0.0630, 0.0624, 0.0625, 0.0634, 0.0624, 0.0617, 0.0624, 0.0607,
        0.0600, 0.0590, 0.0576, 0.0575, 0.0574, 0.0605, 0.0608, 0.0615, 0.0613,
        0.0608, 0.0622, 0.0616, 0.0629, 0.0651, 0.0643, 0.0638, 0.0630, 0.0631,
        0.0633, 0.0630, 0.0620, 0.0599, 0.0599, 0.0609, 0.0624, 0.0623, 0.0611,
        0.0600, 0.0595, 0.0591, 0.0610, 0.0603, 0.0574, 0.0571, 0.0593, 0.0610,
        0.0596, 0.0578, 0.0580, 0.0572, 0.0579, 0.0568, 0.0559, 0.0549, 0.0555,
        0.0552, 0.0559, 0.0573, 0.0563, 0.0544, 0.0585, 0.0575, 0.0564, 0.0558,
        0.0553, 0.0545, 0.0547, 0.0542, 0.0528, 0.0532, 0.0533, 0.0540, 0.0543,
        0.0531, 0.0544, 0.0549, 0.0560, 0.0594, 0.0582, 0.0566, 0.0568, 0.0594,
        0.0592, 0.0568, 0.0550, 0.0545, 0.0547, 0.0560, 0.0574, 0.0582, 0.0555,
        0.0545, 0.0550, 0.0556, 0.0546, 0.0520, 0.0516, 0.0526, 0.0538, 0.0527,
        0.0532, 0.0534, 0.0528, 0.0515, 0.0503, 0.0505, 0.0509, 0.0508, 0.0502,
        0.0512, 0.0514, 0.0517, 0.0520, 0.0561, 0.0522, 0.0511, 0.0533, 0.0533,
        0.0497, 0.0476, 0.0497, 0.0508, 0.0494, 0.0484, 0.0485, 0.0489, 0.0491,
        0.0486, 0.0499, 0.0513, 0.0510, 0.0491, 0.0506, 0.0515, 0.0539, 0.0592,
        0.0587, 0.0570, 0.0564, 0.0536, 0.0545, 0.0585, 0.0633, 0.0582, 0.0589,
        0.0634, 0.0619, 0.0622, 0.0626, 0.0621, 0.0622, 0.0745, 0.0785, 0.0846,
        0.0868, 0.0838, 0.0883, 0.0878, 0.0895, 0.0949, 0.0954, 0.0947, 0.0996,
        0.1038, 0.1001, 0.0991, 0.0993, 0.0974, 0.0969, 0.1011, 0.1087, 0.1153,
        0.1069, 0.1134, 0.1152, 0.1236, 0.1369, 0.1369, 0.1322, 0.1363, 0.1355,
        0.1379, 0.1312, 0.1430, 0.1415, 0.1485, 0.1460, 0.1605, 0.1735, 0.1914,
        0.1921, 0.1890, 0.1789, 0.1858, 0.1829, 0.1816, 0.1579, 0.1484, 0.1455,
        0.1444, 0.1215, 0.1278, 0.1254, 0.1261, 0.1192, 0.1138, 0.1086, 0.1099,
        0.1073, 0.1048, 0.1029, 0.1004, 0.0990, 0.0955, 0.0934, 0.0904, 0.0903,
        0.0882, 0.0899, 0.0910, 0.0888, 0.0867, 0.0802, 0.0709, 0.0673, 0.0640,
        0.0593, 0.0591, 0.0612, 0.0567, 0.0535, 0.0521, 0.0497, 0.0456, 0.0428,
        0.0421, 0.0444, 0.0462, 0.0469, 0.0490, 0.0508, 0.0476, 0.0441, 0.0431,
        0.0434, 0.0471, 0.0481, 0.0459, 0.0509, 0.0512, 0.0573, 0.0561, 0.0570,
        0.0636, 0.0636, 0.0631, 0.0652, 0.0573, 0.0569, 0.0530, 0.0510, 0.0524,
        0.0539, 0.0515, 0.0496, 0.0534, 0.0419, 0.0536, 0.0578, 0.0562, 0.0554,
        0.0552, 0.0567, 0.0565, 0.0548, 0.0534, 0.0521, 0.0517, 0.0514, 0.0471,
        0.0492, 0.0484, 0.0471, 0.0480, 0.0495, 0.0488, 0.0586, 0.0683],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0536, 0.0555, 0.0611, 0.0623, 0.0632, 0.0620, 0.0606, 0.0546, 0.0546,
        0.0575, 0.0619, 0.0647, 0.0646, 0.0626, 0.0584, 0.0594, 0.0595, 0.0577,
        0.0568, 0.0561, 0.0609, 0.0616, 0.0651, 0.0683, 0.0671, 0.0696, 0.0637,
        0.0624, 0.0616, 0.0586, 0.0593, 0.0635, 0.0624, 0.0598, 0.0571, 0.0593,
        0.0637, 0.0597, 0.0604, 0.0587, 0.0575, 0.0569, 0.0575, 0.0571, 0.0583,
        0.0599, 0.0599, 0.0587, 0.0567, 0.0569, 0.0580, 0.0583, 0.0595, 0.0573,
        0.0585, 0.0597, 0.0593, 0.0589, 0.0578, 0.0599, 0.0601, 0.0587, 0.0598,
        0.0592, 0.0598, 0.0592, 0.0592, 0.0601, 0.0590, 0.0585, 0.0594, 0.0579,
        0.0573, 0.0562, 0.0547, 0.0548, 0.0549, 0.0579, 0.0581, 0.0586, 0.0583,
        0.0577, 0.0592, 0.0583, 0.0593, 0.0613, 0.0607, 0.0603, 0.0596, 0.0596,
        0.0598, 0.0595, 0.0586, 0.0566, 0.0566, 0.0576, 0.0589, 0.0589, 0.0577,
        0.0568, 0.0562, 0.0559, 0.0580, 0.0575, 0.0544, 0.0541, 0.0562, 0.0578,
        0.0565, 0.0547, 0.0550, 0.0542, 0.0549, 0.0538, 0.0530, 0.0521, 0.0527,
        0.0524, 0.0531, 0.0544, 0.0534, 0.0513, 0.0556, 0.0545, 0.0535, 0.0529,
        0.0526, 0.0518, 0.0520, 0.0517, 0.0501, 0.0503, 0.0505, 0.0513, 0.0516,
        0.0507, 0.0520, 0.0525, 0.0532, 0.0563, 0.0551, 0.0535, 0.0538, 0.0559,
        0.0556, 0.0534, 0.0516, 0.0513, 0.0519, 0.0532, 0.0546, 0.0554, 0.0530,
        0.0521, 0.0527, 0.0533, 0.0522, 0.0498, 0.0494, 0.0504, 0.0514, 0.0504,
        0.0511, 0.0512, 0.0513, 0.0494, 0.0481, 0.0486, 0.0491, 0.0489, 0.0482,
        0.0490, 0.0491, 0.0494, 0.0498, 0.0546, 0.0506, 0.0493, 0.0508, 0.0510,
        0.0477, 0.0455, 0.0472, 0.0479, 0.0468, 0.0463, 0.0468, 0.0473, 0.0471,
        0.0463, 0.0474, 0.0486, 0.0489, 0.0470, 0.0485, 0.0488, 0.0511, 0.0575,
        0.0571, 0.0554, 0.0551, 0.0519, 0.0528, 0.0568, 0.0610, 0.0549, 0.0566,
        0.0598, 0.0599, 0.0607, 0.0613, 0.0609, 0.0607, 0.0741, 0.0799, 0.0861,
        0.0867, 0.0828, 0.0867, 0.0860, 0.0887, 0.0956, 0.0959, 0.0957, 0.1003,
        0.1043, 0.1003, 0.0989, 0.0990, 0.0965, 0.0962, 0.1013, 0.1099, 0.1171,
        0.1093, 0.1162, 0.1193, 0.1278, 0.1395, 0.1387, 0.1337, 0.1375, 0.1371,
        0.1406, 0.1353, 0.1496, 0.1500, 0.1532, 0.1664, 0.1936, 0.1866, 0.1913,
        0.1718, 0.1690, 0.1582, 0.1650, 0.1553, 0.1468, 0.1369, 0.1298, 0.1392,
        0.1394, 0.1197, 0.1297, 0.1268, 0.1271, 0.1177, 0.1100, 0.1031, 0.1023,
        0.0988, 0.0967, 0.0955, 0.0930, 0.0916, 0.0881, 0.0862, 0.0834, 0.0828,
        0.0803, 0.0819, 0.0829, 0.0800, 0.0787, 0.0732, 0.0654, 0.0619, 0.0593,
        0.0555, 0.0554, 0.0574, 0.0532, 0.0502, 0.0491, 0.0473, 0.0440, 0.0414,
        0.0405, 0.0422, 0.0437, 0.0443, 0.0467, 0.0487, 0.0459, 0.0427, 0.0417,
        0.0418, 0.0454, 0.0464, 0.0438, 0.0499, 0.0497, 0.0551, 0.0535, 0.0545,
        0.0608, 0.0605, 0.0603, 0.0625, 0.0550, 0.0547, 0.0505, 0.0483, 0.0493,
        0.0509, 0.0488, 0.0471, 0.0511, 0.0396, 0.0518, 0.0553, 0.0533, 0.0523,
        0.0515, 0.0524, 0.0526, 0.0512, 0.0500, 0.0493, 0.0494, 0.0493, 0.0451,
        0.0474, 0.0466, 0.0449, 0.0457, 0.0472, 0.0466, 0.0557, 0.0651],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0573, 0.0597, 0.0654, 0.0671, 0.0675, 0.0679, 0.0658, 0.0599, 0.0588,
        0.0616, 0.0664, 0.0695, 0.0693, 0.0669, 0.0623, 0.0635, 0.0637, 0.0616,
        0.0606, 0.0597, 0.0651, 0.0657, 0.0697, 0.0733, 0.0722, 0.0750, 0.0686,
        0.0674, 0.0668, 0.0634, 0.0639, 0.0684, 0.0671, 0.0640, 0.0610, 0.0630,
        0.0678, 0.0636, 0.0640, 0.0621, 0.0609, 0.0602, 0.0608, 0.0604, 0.0615,
        0.0630, 0.0631, 0.0619, 0.0599, 0.0600, 0.0612, 0.0616, 0.0629, 0.0607,
        0.0618, 0.0630, 0.0625, 0.0620, 0.0608, 0.0631, 0.0634, 0.0619, 0.0630,
        0.0624, 0.0631, 0.0625, 0.0626, 0.0634, 0.0625, 0.0620, 0.0631, 0.0616,
        0.0608, 0.0596, 0.0581, 0.0583, 0.0584, 0.0616, 0.0617, 0.0620, 0.0615,
        0.0609, 0.0624, 0.0615, 0.0624, 0.0646, 0.0639, 0.0635, 0.0628, 0.0629,
        0.0631, 0.0628, 0.0618, 0.0597, 0.0597, 0.0606, 0.0620, 0.0620, 0.0608,
        0.0598, 0.0592, 0.0588, 0.0611, 0.0606, 0.0574, 0.0570, 0.0591, 0.0609,
        0.0596, 0.0577, 0.0580, 0.0571, 0.0577, 0.0566, 0.0557, 0.0547, 0.0554,
        0.0552, 0.0559, 0.0572, 0.0562, 0.0541, 0.0584, 0.0573, 0.0561, 0.0556,
        0.0553, 0.0545, 0.0547, 0.0543, 0.0526, 0.0528, 0.0531, 0.0539, 0.0543,
        0.0532, 0.0546, 0.0551, 0.0560, 0.0591, 0.0577, 0.0562, 0.0564, 0.0588,
        0.0584, 0.0560, 0.0543, 0.0540, 0.0545, 0.0559, 0.0574, 0.0583, 0.0556,
        0.0547, 0.0553, 0.0560, 0.0548, 0.0520, 0.0517, 0.0529, 0.0539, 0.0529,
        0.0537, 0.0539, 0.0540, 0.0519, 0.0505, 0.0511, 0.0516, 0.0514, 0.0506,
        0.0514, 0.0516, 0.0520, 0.0524, 0.0577, 0.0533, 0.0520, 0.0533, 0.0532,
        0.0497, 0.0474, 0.0490, 0.0498, 0.0490, 0.0486, 0.0493, 0.0499, 0.0494,
        0.0484, 0.0493, 0.0504, 0.0508, 0.0489, 0.0507, 0.0509, 0.0531, 0.0603,
        0.0600, 0.0580, 0.0578, 0.0544, 0.0555, 0.0595, 0.0632, 0.0569, 0.0588,
        0.0614, 0.0624, 0.0634, 0.0646, 0.0637, 0.0637, 0.0777, 0.0844, 0.0906,
        0.0901, 0.0862, 0.0897, 0.0890, 0.0922, 0.0998, 0.1002, 0.0999, 0.1047,
        0.1088, 0.1048, 0.1029, 0.1028, 0.1000, 0.0998, 0.1048, 0.1136, 0.1210,
        0.1133, 0.1203, 0.1234, 0.1315, 0.1434, 0.1427, 0.1374, 0.1406, 0.1398,
        0.1437, 0.1393, 0.1544, 0.1558, 0.1571, 0.1773, 0.2023, 0.1854, 0.1849,
        0.1723, 0.1677, 0.1537, 0.1577, 0.1483, 0.1417, 0.1359, 0.1304, 0.1410,
        0.1425, 0.1225, 0.1328, 0.1297, 0.1291, 0.1187, 0.1104, 0.1032, 0.1022,
        0.0989, 0.0969, 0.0960, 0.0936, 0.0919, 0.0886, 0.0865, 0.0837, 0.0831,
        0.0806, 0.0823, 0.0832, 0.0803, 0.0792, 0.0740, 0.0661, 0.0623, 0.0595,
        0.0558, 0.0561, 0.0582, 0.0539, 0.0509, 0.0499, 0.0482, 0.0451, 0.0425,
        0.0418, 0.0436, 0.0452, 0.0458, 0.0481, 0.0501, 0.0473, 0.0442, 0.0432,
        0.0432, 0.0468, 0.0479, 0.0454, 0.0523, 0.0520, 0.0575, 0.0556, 0.0568,
        0.0634, 0.0630, 0.0629, 0.0652, 0.0573, 0.0571, 0.0525, 0.0501, 0.0512,
        0.0527, 0.0505, 0.0488, 0.0530, 0.0411, 0.0541, 0.0577, 0.0555, 0.0545,
        0.0538, 0.0545, 0.0548, 0.0536, 0.0522, 0.0511, 0.0514, 0.0513, 0.0470,
        0.0495, 0.0486, 0.0466, 0.0475, 0.0493, 0.0483, 0.0576, 0.0677],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0747, 0.0792, 0.0862, 0.0911, 0.0905, 0.0940, 0.0905, 0.0865, 0.0813,
        0.0836, 0.0899, 0.0945, 0.0942, 0.0902, 0.0842, 0.0861, 0.0872, 0.0837,
        0.0819, 0.0799, 0.0884, 0.0891, 0.0953, 0.1006, 0.1002, 0.1045, 0.0963,
        0.0959, 0.0966, 0.0923, 0.0919, 0.0993, 0.0961, 0.0895, 0.0849, 0.0855,
        0.0926, 0.0874, 0.0853, 0.0819, 0.0799, 0.0790, 0.0795, 0.0789, 0.0801,
        0.0813, 0.0817, 0.0802, 0.0778, 0.0782, 0.0797, 0.0806, 0.0819, 0.0797,
        0.0800, 0.0813, 0.0806, 0.0798, 0.0783, 0.0812, 0.0819, 0.0804, 0.0811,
        0.0807, 0.0817, 0.0813, 0.0817, 0.0828, 0.0822, 0.0822, 0.0840, 0.0825,
        0.0813, 0.0798, 0.0782, 0.0784, 0.0788, 0.0831, 0.0822, 0.0817, 0.0803,
        0.0795, 0.0806, 0.0797, 0.0808, 0.0835, 0.0823, 0.0821, 0.0812, 0.0819,
        0.0819, 0.0813, 0.0801, 0.0774, 0.0780, 0.0782, 0.0799, 0.0801, 0.0787,
        0.0772, 0.0764, 0.0757, 0.0784, 0.0781, 0.0744, 0.0736, 0.0763, 0.0790,
        0.0773, 0.0749, 0.0749, 0.0737, 0.0743, 0.0728, 0.0715, 0.0701, 0.0710,
        0.0710, 0.0718, 0.0731, 0.0722, 0.0698, 0.0735, 0.0728, 0.0711, 0.0706,
        0.0705, 0.0697, 0.0699, 0.0689, 0.0670, 0.0676, 0.0680, 0.0692, 0.0699,
        0.0675, 0.0694, 0.0701, 0.0716, 0.0743, 0.0724, 0.0710, 0.0714, 0.0751,
        0.0742, 0.0708, 0.0691, 0.0690, 0.0691, 0.0712, 0.0736, 0.0748, 0.0702,
        0.0691, 0.0698, 0.0707, 0.0691, 0.0645, 0.0649, 0.0664, 0.0676, 0.0668,
        0.0678, 0.0685, 0.0687, 0.0654, 0.0638, 0.0641, 0.0647, 0.0650, 0.0640,
        0.0651, 0.0654, 0.0659, 0.0668, 0.0741, 0.0676, 0.0657, 0.0672, 0.0650,
        0.0605, 0.0577, 0.0592, 0.0609, 0.0613, 0.0612, 0.0622, 0.0634, 0.0621,
        0.0602, 0.0599, 0.0606, 0.0608, 0.0591, 0.0624, 0.0628, 0.0641, 0.0741,
        0.0743, 0.0711, 0.0712, 0.0669, 0.0691, 0.0732, 0.0750, 0.0685, 0.0705,
        0.0717, 0.0758, 0.0775, 0.0816, 0.0786, 0.0791, 0.0943, 0.1035, 0.1101,
        0.1061, 0.1017, 0.1042, 0.1041, 0.1084, 0.1174, 0.1186, 0.1182, 0.1238,
        0.1281, 0.1245, 0.1210, 0.1201, 0.1162, 0.1165, 0.1204, 0.1289, 0.1365,
        0.1291, 0.1363, 0.1386, 0.1451, 0.1589, 0.1590, 0.1527, 0.1537, 0.1507,
        0.1547, 0.1524, 0.1689, 0.1727, 0.1724, 0.2019, 0.2126, 0.1882, 0.1717,
        0.1796, 0.1713, 0.1529, 0.1494, 0.1462, 0.1450, 0.1435, 0.1414, 0.1539,
        0.1592, 0.1347, 0.1446, 0.1420, 0.1382, 0.1264, 0.1169, 0.1095, 0.1091,
        0.1063, 0.1049, 0.1046, 0.1034, 0.1008, 0.0969, 0.0936, 0.0907, 0.0903,
        0.0883, 0.0903, 0.0913, 0.0888, 0.0880, 0.0833, 0.0746, 0.0694, 0.0654,
        0.0616, 0.0634, 0.0659, 0.0611, 0.0579, 0.0569, 0.0556, 0.0523, 0.0497,
        0.0498, 0.0524, 0.0550, 0.0557, 0.0578, 0.0594, 0.0554, 0.0528, 0.0518,
        0.0514, 0.0554, 0.0574, 0.0549, 0.0656, 0.0651, 0.0708, 0.0680, 0.0696,
        0.0779, 0.0777, 0.0778, 0.0811, 0.0707, 0.0704, 0.0644, 0.0611, 0.0628,
        0.0638, 0.0613, 0.0588, 0.0646, 0.0509, 0.0677, 0.0713, 0.0682, 0.0678,
        0.0683, 0.0683, 0.0684, 0.0681, 0.0658, 0.0625, 0.0626, 0.0629, 0.0579,
        0.0613, 0.0600, 0.0569, 0.0584, 0.0614, 0.0584, 0.0693, 0.0827],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0528, 0.0545, 0.0599, 0.0609, 0.0615, 0.0604, 0.0590, 0.0534, 0.0533,
        0.0560, 0.0603, 0.0629, 0.0629, 0.0609, 0.0567, 0.0578, 0.0579, 0.0562,
        0.0553, 0.0547, 0.0592, 0.0599, 0.0633, 0.0664, 0.0652, 0.0675, 0.0619,
        0.0606, 0.0598, 0.0568, 0.0575, 0.0616, 0.0605, 0.0581, 0.0555, 0.0578,
        0.0620, 0.0580, 0.0588, 0.0572, 0.0561, 0.0554, 0.0560, 0.0557, 0.0568,
        0.0584, 0.0584, 0.0573, 0.0553, 0.0554, 0.0565, 0.0568, 0.0580, 0.0558,
        0.0570, 0.0583, 0.0579, 0.0574, 0.0563, 0.0584, 0.0586, 0.0571, 0.0583,
        0.0577, 0.0583, 0.0576, 0.0577, 0.0585, 0.0575, 0.0569, 0.0578, 0.0563,
        0.0557, 0.0546, 0.0532, 0.0533, 0.0534, 0.0563, 0.0566, 0.0571, 0.0568,
        0.0562, 0.0577, 0.0569, 0.0577, 0.0597, 0.0592, 0.0587, 0.0581, 0.0580,
        0.0582, 0.0580, 0.0571, 0.0551, 0.0551, 0.0561, 0.0575, 0.0574, 0.0563,
        0.0553, 0.0548, 0.0545, 0.0566, 0.0561, 0.0530, 0.0527, 0.0548, 0.0563,
        0.0550, 0.0533, 0.0537, 0.0529, 0.0535, 0.0525, 0.0517, 0.0509, 0.0515,
        0.0512, 0.0518, 0.0531, 0.0520, 0.0499, 0.0543, 0.0532, 0.0522, 0.0517,
        0.0513, 0.0505, 0.0508, 0.0505, 0.0489, 0.0491, 0.0493, 0.0500, 0.0503,
        0.0495, 0.0508, 0.0513, 0.0519, 0.0550, 0.0538, 0.0523, 0.0525, 0.0545,
        0.0542, 0.0521, 0.0503, 0.0500, 0.0506, 0.0519, 0.0533, 0.0541, 0.0518,
        0.0510, 0.0515, 0.0522, 0.0510, 0.0487, 0.0483, 0.0494, 0.0503, 0.0493,
        0.0500, 0.0501, 0.0501, 0.0484, 0.0470, 0.0476, 0.0481, 0.0478, 0.0471,
        0.0478, 0.0479, 0.0483, 0.0487, 0.0534, 0.0495, 0.0482, 0.0496, 0.0500,
        0.0467, 0.0446, 0.0462, 0.0468, 0.0458, 0.0453, 0.0458, 0.0463, 0.0461,
        0.0453, 0.0464, 0.0476, 0.0479, 0.0461, 0.0475, 0.0477, 0.0500, 0.0564,
        0.0560, 0.0544, 0.0540, 0.0509, 0.0517, 0.0557, 0.0600, 0.0538, 0.0556,
        0.0587, 0.0589, 0.0596, 0.0601, 0.0598, 0.0596, 0.0730, 0.0788, 0.0850,
        0.0856, 0.0816, 0.0854, 0.0848, 0.0875, 0.0944, 0.0947, 0.0945, 0.0990,
        0.1030, 0.0990, 0.0977, 0.0977, 0.0952, 0.0950, 0.1002, 0.1089, 0.1162,
        0.1085, 0.1154, 0.1187, 0.1273, 0.1386, 0.1377, 0.1327, 0.1367, 0.1365,
        0.1401, 0.1349, 0.1494, 0.1503, 0.1532, 0.1681, 0.1949, 0.1864, 0.1906,
        0.1711, 0.1680, 0.1568, 0.1630, 0.1529, 0.1445, 0.1353, 0.1282, 0.1377,
        0.1380, 0.1189, 0.1291, 0.1260, 0.1267, 0.1171, 0.1094, 0.1022, 0.1010,
        0.0975, 0.0954, 0.0943, 0.0916, 0.0903, 0.0868, 0.0850, 0.0823, 0.0816,
        0.0791, 0.0806, 0.0815, 0.0785, 0.0773, 0.0719, 0.0644, 0.0610, 0.0586,
        0.0548, 0.0546, 0.0566, 0.0525, 0.0495, 0.0485, 0.0466, 0.0434, 0.0408,
        0.0399, 0.0415, 0.0429, 0.0435, 0.0459, 0.0479, 0.0452, 0.0421, 0.0410,
        0.0411, 0.0446, 0.0456, 0.0430, 0.0490, 0.0488, 0.0540, 0.0525, 0.0535,
        0.0596, 0.0592, 0.0590, 0.0612, 0.0539, 0.0536, 0.0494, 0.0473, 0.0482,
        0.0499, 0.0478, 0.0462, 0.0501, 0.0388, 0.0508, 0.0542, 0.0521, 0.0511,
        0.0502, 0.0511, 0.0512, 0.0500, 0.0488, 0.0482, 0.0484, 0.0484, 0.0442,
        0.0464, 0.0457, 0.0440, 0.0447, 0.0462, 0.0457, 0.0546, 0.0637],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([ 1.3411e-03, -1.6135e-03, -4.9048e-03,  1.1384e-05,  9.4074e-03,
         1.2263e-02,  1.0188e-02,  7.9929e-03,  7.0218e-03,  7.3820e-03,
         7.5863e-03,  5.8844e-03,  6.9379e-03,  1.5375e-02,  2.1652e-02,
         2.0161e-02,  1.9270e-02,  2.0466e-02,  1.8293e-02,  1.8407e-02,
         1.4202e-02,  1.3099e-02,  1.6168e-02,  1.3498e-02,  1.3344e-02,
         1.2325e-02,  1.2239e-02,  1.0912e-02,  1.0940e-02,  9.1799e-03,
         1.1004e-02,  1.1019e-02,  1.0492e-02,  1.0901e-02,  1.1077e-02,
         7.5933e-03,  6.9020e-03,  1.4033e-02,  1.6401e-02,  1.6234e-02,
         1.5005e-02,  1.9782e-02,  2.2316e-02,  2.6278e-02,  2.9159e-02,
         3.0781e-02,  2.9703e-02,  2.9930e-02,  2.8886e-02,  2.9973e-02,
         3.1174e-02,  3.1291e-02,  3.1788e-02,  3.1722e-02,  3.0910e-02,
         3.2440e-02,  3.3480e-02,  3.2845e-02,  3.1859e-02,  2.9865e-02,
         2.7507e-02,  2.7935e-02,  2.8415e-02,  2.8045e-02,  2.7800e-02,
         2.6175e-02,  2.7105e-02,  3.0693e-02,  3.2462e-02,  2.7209e-02,
         2.3908e-02,  2.2875e-02,  2.4479e-02,  2.7392e-02,  2.8568e-02,
         2.6404e-02,  2.8516e-02,  2.6005e-02,  2.5670e-02,  2.7798e-02,
         3.0239e-02,  2.9472e-02,  2.9694e-02,  3.0787e-02,  3.3417e-02,
         3.3377e-02,  3.4359e-02,  3.4775e-02,  3.5498e-02,  3.5820e-02,
         3.7927e-02,  3.9815e-02,  3.9005e-02,  3.7368e-02,  3.7574e-02,
         3.7955e-02,  3.7781e-02,  3.7866e-02,  3.7359e-02,  3.7493e-02,
         3.7529e-02,  3.6846e-02,  3.8400e-02,  3.6690e-02,  3.6859e-02,
         3.9828e-02,  4.1959e-02,  4.1571e-02,  4.1406e-02,  4.2451e-02,
         4.1540e-02,  4.1211e-02,  4.2679e-02,  4.2435e-02,  4.1088e-02,
         4.1566e-02,  4.4079e-02,  4.5701e-02,  4.7219e-02,  4.7497e-02,
         4.7925e-02,  4.9878e-02,  5.1897e-02,  5.1762e-02,  5.0998e-02,
         4.9470e-02,  4.6737e-02,  4.6616e-02,  4.6645e-02,  4.5333e-02,
         4.4787e-02,  4.8778e-02,  4.8157e-02,  4.5944e-02,  4.4587e-02,
         4.1644e-02,  4.1546e-02,  4.4767e-02,  4.7052e-02,  5.0420e-02,
         5.0237e-02,  4.8655e-02,  4.7799e-02,  5.3802e-02,  5.7539e-02,
         5.9429e-02,  6.1402e-02,  6.2050e-02,  5.6734e-02,  5.5338e-02,
         5.0800e-02,  4.9261e-02,  4.4914e-02,  4.5516e-02,  4.6337e-02,
         4.8644e-02,  5.0275e-02,  4.6861e-02,  4.4361e-02,  4.6441e-02,
         4.6574e-02,  4.7562e-02,  4.6268e-02,  4.4623e-02,  4.3656e-02,
         4.2069e-02,  4.1599e-02,  4.3792e-02,  4.5721e-02,  4.5009e-02,
         4.6561e-02,  4.6786e-02,  4.7063e-02,  4.7533e-02,  4.8365e-02,
         5.0161e-02,  4.9121e-02,  5.0828e-02,  5.5489e-02,  5.5573e-02,
         5.1341e-02,  5.0881e-02,  4.8998e-02,  5.0977e-02,  4.6962e-02,
         4.7146e-02,  4.7104e-02,  4.3200e-02,  4.7975e-02,  4.5852e-02,
         4.5039e-02,  4.7973e-02,  4.8362e-02,  4.6892e-02,  4.6023e-02,
         4.9387e-02,  5.1083e-02,  4.6888e-02,  4.4579e-02,  4.6667e-02,
         4.8068e-02,  5.0873e-02,  4.8710e-02,  4.6205e-02,  5.0044e-02,
         4.7051e-02,  4.9530e-02,  4.7600e-02,  5.0598e-02,  5.3698e-02,
         4.7946e-02,  5.3299e-02,  5.1247e-02,  5.0104e-02,  4.7388e-02,
         5.1061e-02,  6.1106e-02,  6.2295e-02,  5.9420e-02,  5.9188e-02,
         5.4989e-02,  5.3170e-02,  5.1850e-02,  5.1260e-02,  5.3036e-02,
         5.4321e-02,  5.2558e-02,  5.0295e-02,  5.2931e-02,  4.1672e-02,
         4.0153e-02,  4.0825e-02,  4.8707e-02,  5.2204e-02,  5.3704e-02,
         5.2660e-02,  4.6897e-02,  4.8701e-02,  5.4491e-02,  5.5456e-02,
         5.5718e-02,  5.6898e-02,  5.6488e-02,  5.8852e-02,  5.5307e-02,
         5.4642e-02,  6.0107e-02,  5.7856e-02,  5.5215e-02,  5.6597e-02,
         5.6698e-02,  5.8658e-02,  5.9160e-02,  5.8738e-02,  5.9888e-02,
         5.8959e-02,  6.5321e-02,  6.4417e-02,  6.9663e-02,  6.9428e-02,
         6.9162e-02,  6.1870e-02,  6.7040e-02,  7.0512e-02,  7.3636e-02,
         7.4290e-02,  7.3326e-02,  7.1768e-02,  7.3689e-02,  8.2378e-02,
         8.9597e-02,  9.2565e-02,  9.4342e-02,  9.5917e-02,  9.4233e-02,
         9.3238e-02,  9.3556e-02,  9.4178e-02,  1.0107e-01,  1.0188e-01,
         1.0161e-01,  1.0268e-01,  1.0280e-01,  1.0449e-01,  1.0381e-01,
         1.0625e-01,  1.0995e-01,  1.1016e-01,  1.1701e-01,  1.1973e-01,
         1.2424e-01,  1.1993e-01,  1.2495e-01,  1.2438e-01,  1.2545e-01,
         1.1926e-01,  1.2086e-01,  1.1412e-01,  1.1669e-01,  1.1272e-01,
         1.1245e-01,  1.1662e-01,  1.2191e-01,  1.2221e-01,  1.1014e-01,
         1.0555e-01,  9.9000e-02,  1.0315e-01,  1.0363e-01,  9.5836e-02,
         9.5983e-02,  9.2568e-02,  8.9125e-02,  8.7691e-02,  8.7031e-02,
         8.5707e-02,  8.7189e-02,  8.7374e-02,  8.7296e-02,  8.7517e-02,
         8.1843e-02,  8.2663e-02,  7.7721e-02,  8.0387e-02,  8.2565e-02,
         7.8788e-02,  8.4381e-02,  8.7791e-02,  7.8762e-02,  7.5896e-02,
         7.0804e-02,  6.7725e-02,  6.4792e-02,  5.9253e-02,  5.8266e-02,
         5.8599e-02,  5.8764e-02,  5.7100e-02,  5.8430e-02,  6.0858e-02,
         6.4562e-02,  6.6754e-02,  6.1790e-02,  6.0489e-02,  5.9555e-02,
         5.4385e-02,  4.8640e-02,  4.4583e-02,  4.5702e-02,  4.5966e-02],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0571, 0.0595, 0.0655, 0.0673, 0.0683, 0.0680, 0.0662, 0.0596, 0.0591,
        0.0622, 0.0670, 0.0701, 0.0701, 0.0678, 0.0632, 0.0644, 0.0645, 0.0622,
        0.0613, 0.0604, 0.0660, 0.0667, 0.0707, 0.0743, 0.0731, 0.0759, 0.0694,
        0.0682, 0.0675, 0.0642, 0.0648, 0.0695, 0.0682, 0.0650, 0.0618, 0.0639,
        0.0687, 0.0646, 0.0650, 0.0631, 0.0618, 0.0611, 0.0617, 0.0613, 0.0625,
        0.0641, 0.0642, 0.0629, 0.0608, 0.0610, 0.0622, 0.0626, 0.0638, 0.0616,
        0.0627, 0.0639, 0.0635, 0.0629, 0.0618, 0.0640, 0.0644, 0.0629, 0.0640,
        0.0634, 0.0640, 0.0635, 0.0636, 0.0645, 0.0635, 0.0630, 0.0641, 0.0625,
        0.0618, 0.0605, 0.0591, 0.0592, 0.0593, 0.0625, 0.0625, 0.0629, 0.0624,
        0.0619, 0.0633, 0.0625, 0.0635, 0.0657, 0.0650, 0.0645, 0.0638, 0.0639,
        0.0641, 0.0638, 0.0628, 0.0607, 0.0607, 0.0616, 0.0630, 0.0630, 0.0618,
        0.0607, 0.0602, 0.0598, 0.0620, 0.0615, 0.0583, 0.0579, 0.0601, 0.0619,
        0.0605, 0.0586, 0.0589, 0.0580, 0.0586, 0.0575, 0.0566, 0.0556, 0.0563,
        0.0560, 0.0567, 0.0581, 0.0570, 0.0550, 0.0593, 0.0582, 0.0570, 0.0564,
        0.0561, 0.0553, 0.0556, 0.0550, 0.0534, 0.0537, 0.0539, 0.0548, 0.0551,
        0.0539, 0.0554, 0.0559, 0.0568, 0.0600, 0.0586, 0.0571, 0.0573, 0.0598,
        0.0594, 0.0570, 0.0552, 0.0549, 0.0553, 0.0568, 0.0583, 0.0592, 0.0564,
        0.0554, 0.0560, 0.0567, 0.0556, 0.0527, 0.0524, 0.0535, 0.0546, 0.0536,
        0.0544, 0.0546, 0.0546, 0.0525, 0.0512, 0.0516, 0.0522, 0.0521, 0.0512,
        0.0521, 0.0523, 0.0527, 0.0531, 0.0582, 0.0538, 0.0525, 0.0541, 0.0539,
        0.0503, 0.0480, 0.0496, 0.0505, 0.0497, 0.0493, 0.0498, 0.0505, 0.0501,
        0.0491, 0.0501, 0.0511, 0.0514, 0.0496, 0.0513, 0.0517, 0.0539, 0.0608,
        0.0605, 0.0585, 0.0583, 0.0549, 0.0560, 0.0600, 0.0638, 0.0578, 0.0594,
        0.0624, 0.0630, 0.0638, 0.0651, 0.0641, 0.0641, 0.0779, 0.0841, 0.0904,
        0.0903, 0.0866, 0.0903, 0.0896, 0.0925, 0.0998, 0.1003, 0.0999, 0.1047,
        0.1088, 0.1050, 0.1032, 0.1031, 0.1004, 0.1002, 0.1050, 0.1135, 0.1208,
        0.1129, 0.1197, 0.1225, 0.1307, 0.1430, 0.1425, 0.1372, 0.1405, 0.1395,
        0.1431, 0.1381, 0.1524, 0.1526, 0.1553, 0.1697, 0.1967, 0.1863, 0.1902,
        0.1726, 0.1695, 0.1577, 0.1644, 0.1551, 0.1471, 0.1386, 0.1324, 0.1426,
        0.1432, 0.1225, 0.1325, 0.1297, 0.1291, 0.1192, 0.1114, 0.1047, 0.1042,
        0.1010, 0.0990, 0.0978, 0.0954, 0.0937, 0.0905, 0.0883, 0.0854, 0.0849,
        0.0825, 0.0842, 0.0852, 0.0825, 0.0812, 0.0757, 0.0674, 0.0636, 0.0606,
        0.0567, 0.0570, 0.0591, 0.0548, 0.0517, 0.0506, 0.0489, 0.0455, 0.0429,
        0.0422, 0.0442, 0.0459, 0.0465, 0.0488, 0.0507, 0.0477, 0.0446, 0.0436,
        0.0436, 0.0473, 0.0485, 0.0460, 0.0527, 0.0526, 0.0582, 0.0564, 0.0575,
        0.0642, 0.0639, 0.0638, 0.0661, 0.0580, 0.0578, 0.0533, 0.0509, 0.0521,
        0.0536, 0.0513, 0.0495, 0.0537, 0.0417, 0.0547, 0.0584, 0.0563, 0.0554,
        0.0548, 0.0557, 0.0559, 0.0546, 0.0532, 0.0520, 0.0521, 0.0520, 0.0477,
        0.0501, 0.0492, 0.0473, 0.0482, 0.0499, 0.0490, 0.0585, 0.0687],
       grad_fn=<SelectBackward>)
