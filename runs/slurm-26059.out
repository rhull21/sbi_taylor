Loading parflow-ml/latest
  Loading requirement: openmpi/gcc/4.1.0 parflow/3.9.0 gdal/3.2.1
/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/05_utils/lstmutils.py:177: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  data_window[f'AOC_{l}'] = AOC_scale[l]
/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/05_utils/lstmutils.py:179: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  data_window['Y'] = data[member_name]
warning: file exists
saving
Epoch: 0, loss: 0.00048
Epoch: 1, loss: 0.00121
Epoch: 2, loss: 0.00199
Epoch: 3, loss: 0.00314
Epoch: 4, loss: 0.00542
Epoch: 5, loss: 0.00781
Epoch: 6, loss: 0.00864
Epoch: 7, loss: 0.00889
Epoch: 8, loss: 0.00899
Epoch: 9, loss: 0.00903
Epoch: 10, loss: 0.00906
Epoch: 11, loss: 0.00906
Epoch: 12, loss: 0.00905
Epoch: 13, loss: 0.00901
Epoch: 14, loss: 0.00894
Epoch: 15, loss: 0.00883
Epoch: 16, loss: 0.00868
Epoch: 17, loss: 0.00844
Epoch: 18, loss: 0.00809
Epoch: 19, loss: 0.00756
Epoch: 20, loss: 0.00683
Epoch: 21, loss: 0.00599
Epoch: 22, loss: 0.00523
Epoch: 23, loss: 0.00465
Epoch: 24, loss: 0.00420
Epoch: 25, loss: 0.00386
Epoch: 26, loss: 0.00361
Epoch: 27, loss: 0.00341
Epoch: 28, loss: 0.00326
Epoch: 29, loss: 0.00314
Epoch: 30, loss: 0.00305
Epoch: 31, loss: 0.00298
Epoch: 32, loss: 0.00294
Epoch: 33, loss: 0.00290
Epoch: 34, loss: 0.00288
Epoch: 35, loss: 0.00286
Epoch: 36, loss: 0.00285
Epoch: 37, loss: 0.00283
Epoch: 38, loss: 0.00282
Epoch: 39, loss: 0.00281
Epoch: 40, loss: 0.00280
Epoch: 41, loss: 0.00278
Epoch: 42, loss: 0.00277
Epoch: 43, loss: 0.00275
Epoch: 44, loss: 0.00273
Epoch: 45, loss: 0.00271
Epoch: 46, loss: 0.00268
Epoch: 47, loss: 0.00266
Epoch: 48, loss: 0.00263
Epoch: 49, loss: 0.00261
Epoch: 50, loss: 0.00258
Epoch: 51, loss: 0.00255
Epoch: 52, loss: 0.00253
Epoch: 53, loss: 0.00251
Epoch: 54, loss: 0.00249
Epoch: 55, loss: 0.00247
Epoch: 56, loss: 0.00245
Epoch: 57, loss: 0.00243
Epoch: 58, loss: 0.00242
Epoch: 59, loss: 0.00241
Epoch: 60, loss: 0.00239
Epoch: 61, loss: 0.00238
Epoch: 62, loss: 0.00237
Epoch: 63, loss: 0.00236
Epoch: 64, loss: 0.00234
Epoch: 65, loss: 0.00233
Epoch: 66, loss: 0.00232
Epoch: 67, loss: 0.00231
Epoch: 68, loss: 0.00230
Epoch: 69, loss: 0.00228
Epoch: 70, loss: 0.00227
Epoch: 71, loss: 0.00226
Epoch: 72, loss: 0.00224
Epoch: 73, loss: 0.00223
Epoch: 74, loss: 0.00222
Epoch: 75, loss: 0.00220
Epoch: 76, loss: 0.00219
Epoch: 77, loss: 0.00218
Epoch: 78, loss: 0.00216
Epoch: 79, loss: 0.00215
Epoch: 80, loss: 0.00214
Epoch: 81, loss: 0.00212
Epoch: 82, loss: 0.00211
Epoch: 83, loss: 0.00210
Epoch: 84, loss: 0.00208
Epoch: 85, loss: 0.00207
Epoch: 86, loss: 0.00206
Epoch: 87, loss: 0.00205
Epoch: 88, loss: 0.00203
Epoch: 89, loss: 0.00202
Epoch: 90, loss: 0.00201
Epoch: 91, loss: 0.00200
Epoch: 92, loss: 0.00198
Epoch: 93, loss: 0.00197
Epoch: 94, loss: 0.00196
Epoch: 95, loss: 0.00195
Epoch: 96, loss: 0.00194
Epoch: 97, loss: 0.00193
Epoch: 98, loss: 0.00192
Epoch: 99, loss: 0.00191
Epoch: 100, loss: 0.00190
Epoch: 101, loss: 0.00189
Epoch: 102, loss: 0.00188
Epoch: 103, loss: 0.00187
Epoch: 104, loss: 0.00186
Epoch: 105, loss: 0.00185
Epoch: 106, loss: 0.00184
Epoch: 107, loss: 0.00183
Epoch: 108, loss: 0.00182
Epoch: 109, loss: 0.00181
Epoch: 110, loss: 0.00181
Epoch: 111, loss: 0.00180
Epoch: 112, loss: 0.00179
Epoch: 113, loss: 0.00178
Epoch: 114, loss: 0.00177
Epoch: 115, loss: 0.00176
Epoch: 116, loss: 0.00176
Epoch: 117, loss: 0.00175
Epoch: 118, loss: 0.00174
Epoch: 119, loss: 0.00173
Epoch: 120, loss: 0.00173
Epoch: 121, loss: 0.00172
Epoch: 122, loss: 0.00171
Epoch: 123, loss: 0.00171
Epoch: 124, loss: 0.00170
Epoch: 125, loss: 0.00169
Epoch: 126, loss: 0.00168
Epoch: 127, loss: 0.00168
Epoch: 128, loss: 0.00167
Epoch: 129, loss: 0.00166
Epoch: 130, loss: 0.00166
Epoch: 131, loss: 0.00165
Epoch: 132, loss: 0.00164
Epoch: 133, loss: 0.00163
Epoch: 134, loss: 0.00163
Epoch: 135, loss: 0.00162
Epoch: 136, loss: 0.00161
Epoch: 137, loss: 0.00161
Epoch: 138, loss: 0.00160
Epoch: 139, loss: 0.00159
Epoch: 140, loss: 0.00159
Epoch: 141, loss: 0.00158
Epoch: 142, loss: 0.00157
Epoch: 143, loss: 0.00157
Epoch: 144, loss: 0.00156
Epoch: 145, loss: 0.00156
Epoch: 146, loss: 0.00155
Epoch: 147, loss: 0.00154
Epoch: 148, loss: 0.00154
Epoch: 149, loss: 0.00153
Epoch: 150, loss: 0.00153
Epoch: 151, loss: 0.00153
Epoch: 152, loss: 0.00152
Epoch: 153, loss: 0.00152
Epoch: 154, loss: 0.00151
Epoch: 155, loss: 0.00151
Epoch: 156, loss: 0.00151
Epoch: 157, loss: 0.00151
Epoch: 158, loss: 0.00150
Epoch: 159, loss: 0.00150
Epoch: 160, loss: 0.00150
Epoch: 161, loss: 0.00150
Epoch: 162, loss: 0.00150
Epoch: 163, loss: 0.00150
Epoch: 164, loss: 0.00150
Epoch: 165, loss: 0.00150
Epoch: 166, loss: 0.00151
Epoch: 167, loss: 0.00151
Epoch: 168, loss: 0.00151
Epoch: 169, loss: 0.00152
Epoch: 170, loss: 0.00152
Epoch: 171, loss: 0.00153
Epoch: 172, loss: 0.00154
Epoch: 173, loss: 0.00156
Epoch: 174, loss: 0.00158
Epoch: 175, loss: 0.00160
Epoch: 176, loss: 0.00162
Epoch: 177, loss: 0.00165
Epoch: 178, loss: 0.00167
Epoch: 179, loss: 0.00169
Epoch: 180, loss: 0.00169
Epoch: 181, loss: 0.00168
Epoch: 182, loss: 0.00167
Epoch: 183, loss: 0.00166
Epoch: 184, loss: 0.00166
Epoch: 185, loss: 0.00166
Epoch: 186, loss: 0.00166
Epoch: 187, loss: 0.00166
Epoch: 188, loss: 0.00165
Epoch: 189, loss: 0.00164
Epoch: 190, loss: 0.00163
Epoch: 191, loss: 0.00162
Epoch: 192, loss: 0.00161
Epoch: 193, loss: 0.00160
Epoch: 194, loss: 0.00159
Epoch: 195, loss: 0.00157
Epoch: 196, loss: 0.00155
Epoch: 197, loss: 0.00153
Epoch: 198, loss: 0.00151
Epoch: 199, loss: 0.00149
Epoch: 200, loss: 0.00146
Epoch: 201, loss: 0.00143
Epoch: 202, loss: 0.00141
Epoch: 203, loss: 0.00138
Epoch: 204, loss: 0.00135
Epoch: 205, loss: 0.00132
Epoch: 206, loss: 0.00130
Epoch: 207, loss: 0.00128
Epoch: 208, loss: 0.00129
Epoch: 209, loss: 0.00133
Epoch: 210, loss: 0.00142
Epoch: 211, loss: 0.00174
Epoch: 212, loss: 0.00237
Epoch: 213, loss: 0.00186
Epoch: 214, loss: 0.00153
Epoch: 215, loss: 0.00145
Epoch: 216, loss: 0.00140
Epoch: 217, loss: 0.00135
Epoch: 218, loss: 0.00132
Epoch: 219, loss: 0.00129
Epoch: 220, loss: 0.00127
Epoch: 221, loss: 0.00125
Epoch: 222, loss: 0.00123
Epoch: 223, loss: 0.00122
Epoch: 224, loss: 0.00120
Epoch: 225, loss: 0.00119
Epoch: 226, loss: 0.00118
Epoch: 227, loss: 0.00116
Epoch: 228, loss: 0.00115
Epoch: 229, loss: 0.00114
Epoch: 230, loss: 0.00112
Epoch: 231, loss: 0.00111
Epoch: 232, loss: 0.00110
Epoch: 233, loss: 0.00110
Epoch: 234, loss: 0.00112
Epoch: 235, loss: 0.00122
Epoch: 236, loss: 0.00165
Epoch: 237, loss: 0.00153
Epoch: 238, loss: 0.00121
Epoch: 239, loss: 0.00118
Epoch: 240, loss: 0.00112
Epoch: 241, loss: 0.00108
Epoch: 242, loss: 0.00105
Epoch: 243, loss: 0.00104
Epoch: 244, loss: 0.00101
Epoch: 245, loss: 0.00100
Epoch: 246, loss: 0.00099
Epoch: 247, loss: 0.00097
Epoch: 248, loss: 0.00096
Epoch: 249, loss: 0.00094
Epoch: 250, loss: 0.00093
Epoch: 251, loss: 0.00092
Epoch: 252, loss: 0.00091
Epoch: 253, loss: 0.00090
Epoch: 254, loss: 0.00089
Epoch: 255, loss: 0.00088
Epoch: 256, loss: 0.00087
Epoch: 257, loss: 0.00086
Epoch: 258, loss: 0.00086
Epoch: 259, loss: 0.00085
Epoch: 260, loss: 0.00084
Epoch: 261, loss: 0.00084
Epoch: 262, loss: 0.00082
Epoch: 263, loss: 0.00082
Epoch: 264, loss: 0.00083
Epoch: 265, loss: 0.00084
Epoch: 266, loss: 0.00085
Epoch: 267, loss: 0.00094
Epoch: 268, loss: 0.00183
Epoch: 269, loss: 0.00138
Epoch: 270, loss: 0.00111
Epoch: 271, loss: 0.00097
Epoch: 272, loss: 0.00089
Epoch: 273, loss: 0.00087
Epoch: 274, loss: 0.00082
Epoch: 275, loss: 0.00079
Epoch: 276, loss: 0.00079
Epoch: 277, loss: 0.00078
Epoch: 278, loss: 0.00074
Epoch: 279, loss: 0.00078
Epoch: 280, loss: 0.00081
Epoch: 281, loss: 0.00088
Epoch: 282, loss: 0.00106
Epoch: 283, loss: 0.00093
Epoch: 284, loss: 0.00077
Epoch: 285, loss: 0.00089
Epoch: 286, loss: 0.00077
Epoch: 287, loss: 0.00076
Epoch: 288, loss: 0.00078
Epoch: 289, loss: 0.00076
Epoch: 290, loss: 0.00065
Epoch: 291, loss: 0.00083
Epoch: 292, loss: 0.00066
Epoch: 293, loss: 0.00086
Epoch: 294, loss: 0.00093
Epoch: 295, loss: 0.00091
Epoch: 296, loss: 0.00080
Epoch: 297, loss: 0.00071
Epoch: 298, loss: 0.00085
Epoch: 299, loss: 0.00077

0:00:42.832634
the relationship between loss and epochs for given hyper parameters
The test R-squared value for lstm-parflow is: 0.7291460050855717
The test summary stats (RMSE, NSE, KGE) for lstm-parflow are: 
 [0.21131347 0.72914603 0.66348659]
Traceback (most recent call last):
  File "lstm_sbi.py", line 62, in <module>
    buildLSTM(lstm_name, save_path, save, shuffle_it_in, num_members, ensemble_name, ensemble_path)
  File "/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/03_sbi_lstm/lstm_build.py", line 242, in buildLSTM
    evallstm(lstm_out, dataY=y_hat_list[idx], dataX=x_list[idx],
  File "/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/03_sbi_lstm/lstm_build_utils.py", line 304, in evallstm
    plotSeries(dataY_predict, dataY_hat, cond, member_name_l, series_len, save_path_out, save=True)
  File "/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/03_sbi_lstm/lstm_build_utils.py", line 260, in plotSeries
    plot_stuff(df_all.iloc[idx_1:idx_2,:], same=True, ylabel=f'flow-{cond}-{nm}', title=f'PFvLSTM-{cond}-{nm}',
NameError: name 'plot_stuff' is not defined
