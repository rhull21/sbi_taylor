Loading parflow-ml/latest
  Loading requirement: openmpi/gcc/4.1.0 parflow/3.9.0 gdal/3.2.1
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  warnings.warn(
/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/05_utils/sbiutils.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y_out)
Running 1000 simulations.:   0%|          | 0/1000 [00:00<?, ?it/s]Running 1000 simulations.:   3%|▎         | 30/1000 [00:00<00:03, 291.86it/s]Running 1000 simulations.:   6%|▌         | 60/1000 [00:00<00:03, 291.58it/s]Running 1000 simulations.:   9%|▉         | 89/1000 [00:00<00:03, 290.86it/s]Running 1000 simulations.:  12%|█▏        | 119/1000 [00:00<00:03, 290.72it/s]Running 1000 simulations.:  15%|█▍        | 149/1000 [00:00<00:02, 290.79it/s]Running 1000 simulations.:  18%|█▊        | 179/1000 [00:00<00:02, 291.26it/s]Running 1000 simulations.:  21%|██        | 209/1000 [00:00<00:02, 291.31it/s]Running 1000 simulations.:  24%|██▍       | 239/1000 [00:00<00:02, 292.08it/s]Running 1000 simulations.:  27%|██▋       | 269/1000 [00:00<00:02, 291.68it/s]Running 1000 simulations.:  30%|██▉       | 299/1000 [00:01<00:02, 292.05it/s]Running 1000 simulations.:  33%|███▎      | 328/1000 [00:01<00:02, 288.20it/s]Running 1000 simulations.:  36%|███▌      | 358/1000 [00:01<00:02, 289.82it/s]Running 1000 simulations.:  39%|███▉      | 388/1000 [00:01<00:02, 289.88it/s]Running 1000 simulations.:  42%|████▏     | 418/1000 [00:01<00:02, 290.63it/s]Running 1000 simulations.:  45%|████▍     | 448/1000 [00:01<00:01, 291.45it/s]Running 1000 simulations.:  48%|████▊     | 478/1000 [00:01<00:01, 291.68it/s]Running 1000 simulations.:  51%|█████     | 508/1000 [00:01<00:01, 291.24it/s]Running 1000 simulations.:  54%|█████▍    | 538/1000 [00:01<00:01, 291.02it/s]Running 1000 simulations.:  57%|█████▋    | 568/1000 [00:01<00:01, 291.37it/s]Running 1000 simulations.:  60%|█████▉    | 598/1000 [00:02<00:01, 291.94it/s]Running 1000 simulations.:  63%|██████▎   | 628/1000 [00:02<00:01, 291.81it/s]Running 1000 simulations.:  66%|██████▌   | 658/1000 [00:02<00:01, 290.35it/s]Running 1000 simulations.:  69%|██████▉   | 688/1000 [00:02<00:01, 290.24it/s]Running 1000 simulations.:  72%|███████▏  | 718/1000 [00:02<00:00, 291.54it/s]Running 1000 simulations.:  75%|███████▍  | 748/1000 [00:02<00:00, 291.79it/s]Running 1000 simulations.:  78%|███████▊  | 778/1000 [00:02<00:00, 291.98it/s]Running 1000 simulations.:  81%|████████  | 808/1000 [00:02<00:00, 291.92it/s]Running 1000 simulations.:  84%|████████▍ | 838/1000 [00:02<00:00, 291.65it/s]Running 1000 simulations.:  87%|████████▋ | 868/1000 [00:02<00:00, 289.85it/s]Running 1000 simulations.:  90%|████████▉ | 897/1000 [00:03<00:00, 288.28it/s]Running 1000 simulations.:  93%|█████████▎| 926/1000 [00:03<00:00, 287.62it/s]Running 1000 simulations.:  96%|█████████▌| 955/1000 [00:03<00:00, 286.94it/s]Running 1000 simulations.:  98%|█████████▊| 985/1000 [00:03<00:00, 288.02it/s]Running 1000 simulations.: 100%|██████████| 1000/1000 [00:03<00:00, 290.45it/s]
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8840it [00:00, 131718.30it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8482it [00:00, 125506.55it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8985it [00:00, 130014.77it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8786it [00:00, 133019.37it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9356it [00:00, 139838.53it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8338it [00:00, 125582.11it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8717it [00:00, 132755.81it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8262it [00:00, 120961.24it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples:  50%|████▉     | 2491/5000 [00:00<00:00, 22617.88it/s]Drawing 5000 posterior samples: 5467it [00:00, 22578.87it/s]                          Drawing 5000 posterior samples: 5467it [00:00, 22505.27it/s]
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8830it [00:00, 133261.27it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8826it [00:00, 134284.67it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9115it [00:00, 138399.57it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8841it [00:00, 132843.17it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8313it [00:00, 125350.07it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 11306it [00:00, 172030.16it/s]          
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8616it [00:00, 130168.37it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8853it [00:00, 133994.09it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 9209it [00:00, 139732.75it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8537it [00:00, 129155.82it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples:  50%|█████     | 2509/5000 [00:00<00:00, 22414.05it/s]Drawing 5000 posterior samples: 5428it [00:00, 22271.07it/s]                          Drawing 5000 posterior samples: 5428it [00:00, 22120.65it/s]
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 8547it [00:00, 130226.85it/s]           
lstm_sbi.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
warning: file exists
warning: file exists
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Neural network successfully converged after 226 epochs.
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473,
        0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473, 0.2473],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565,
        0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565, 0.2565],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415,
        0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415, 0.2415],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487,
        0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487, 0.2487],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266,
        0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307,
        0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307, 0.2307],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447,
        0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447, 0.2447],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409,
        0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409, 0.2409],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983,
        0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983, 0.0983],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468,
        0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468, 0.2468],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429,
        0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429, 0.2429],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295,
        0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295, 0.2295],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403,
        0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403, 0.2403],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422,
        0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422, 0.2422],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377,
        0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377, 0.2377],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443,
        0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443, 0.2443],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301,
        0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301, 0.2301],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523,
        0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523, 0.2523],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017,
        0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017, 0.1017],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561,
        0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561, 0.2561],
       grad_fn=<SelectBackward>)
