Loading parflow-ml/latest
  Loading requirement: openmpi/gcc/4.1.0 parflow/3.9.0 gdal/3.2.1
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  warnings.warn(
/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/05_utils/sbiutils.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y_out)
Running 1000 simulations.:   0%|          | 0/1000 [00:00<?, ?it/s]Running 1000 simulations.:   3%|▎         | 30/1000 [00:00<00:03, 293.00it/s]Running 1000 simulations.:   6%|▌         | 60/1000 [00:00<00:03, 292.99it/s]Running 1000 simulations.:   9%|▉         | 90/1000 [00:00<00:03, 292.68it/s]Running 1000 simulations.:  12%|█▏        | 120/1000 [00:00<00:03, 293.31it/s]Running 1000 simulations.:  15%|█▌        | 150/1000 [00:00<00:02, 292.46it/s]Running 1000 simulations.:  18%|█▊        | 180/1000 [00:00<00:02, 291.78it/s]Running 1000 simulations.:  21%|██        | 210/1000 [00:00<00:02, 291.60it/s]Running 1000 simulations.:  24%|██▍       | 240/1000 [00:00<00:02, 292.09it/s]Running 1000 simulations.:  27%|██▋       | 270/1000 [00:00<00:02, 293.52it/s]Running 1000 simulations.:  30%|███       | 300/1000 [00:01<00:02, 293.89it/s]Running 1000 simulations.:  33%|███▎      | 330/1000 [00:01<00:02, 294.45it/s]Running 1000 simulations.:  36%|███▌      | 360/1000 [00:01<00:02, 295.18it/s]Running 1000 simulations.:  39%|███▉      | 390/1000 [00:01<00:02, 295.01it/s]Running 1000 simulations.:  42%|████▏     | 420/1000 [00:01<00:01, 293.27it/s]Running 1000 simulations.:  45%|████▌     | 450/1000 [00:01<00:01, 293.45it/s]Running 1000 simulations.:  48%|████▊     | 480/1000 [00:01<00:01, 293.35it/s]Running 1000 simulations.:  51%|█████     | 510/1000 [00:01<00:01, 293.15it/s]Running 1000 simulations.:  54%|█████▍    | 540/1000 [00:01<00:01, 292.50it/s]Running 1000 simulations.:  57%|█████▋    | 570/1000 [00:01<00:01, 292.59it/s]Running 1000 simulations.:  60%|██████    | 600/1000 [00:02<00:01, 293.48it/s]Running 1000 simulations.:  63%|██████▎   | 630/1000 [00:02<00:01, 293.55it/s]Running 1000 simulations.:  66%|██████▌   | 660/1000 [00:02<00:01, 294.14it/s]Running 1000 simulations.:  69%|██████▉   | 690/1000 [00:02<00:01, 293.37it/s]Running 1000 simulations.:  72%|███████▏  | 720/1000 [00:02<00:00, 292.43it/s]Running 1000 simulations.:  75%|███████▌  | 750/1000 [00:02<00:00, 293.00it/s]Running 1000 simulations.:  78%|███████▊  | 780/1000 [00:02<00:00, 293.25it/s]Running 1000 simulations.:  81%|████████  | 810/1000 [00:02<00:00, 293.63it/s]Running 1000 simulations.:  84%|████████▍ | 840/1000 [00:02<00:00, 294.64it/s]Running 1000 simulations.:  87%|████████▋ | 870/1000 [00:02<00:00, 295.20it/s]Running 1000 simulations.:  90%|█████████ | 900/1000 [00:03<00:00, 294.30it/s]Running 1000 simulations.:  93%|█████████▎| 930/1000 [00:03<00:00, 293.16it/s]Running 1000 simulations.:  96%|█████████▌| 960/1000 [00:03<00:00, 293.83it/s]Running 1000 simulations.:  99%|█████████▉| 990/1000 [00:03<00:00, 294.73it/s]Running 1000 simulations.: 100%|██████████| 1000/1000 [00:03<00:00, 293.56it/s]
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 11924it [00:00, 174650.37it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 12268it [00:00, 186279.89it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 12059it [00:00, 182487.37it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 11640it [00:00, 174190.26it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 7799it [00:00, 69964.27it/s]            Drawing 5000 posterior samples: 7799it [00:00, 69702.63it/s]
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples:  49%|████▉     | 2468/5000 [00:00<00:00, 22628.70it/s]Drawing 5000 posterior samples: 5367it [00:00, 22422.48it/s]                          Drawing 5000 posterior samples: 5367it [00:00, 22224.29it/s]
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 11231it [00:00, 170578.33it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples:  76%|███████▌  | 3809/5000 [00:00<00:00, 34418.65it/s]Drawing 5000 posterior samples: 5300it [00:00, 34040.19it/s]                          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 5504it [00:00, 83782.27it/s]            
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 12467it [00:00, 187052.67it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 10494it [00:00, 158533.30it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 6373it [00:00, 96064.39it/s]            
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 12404it [00:00, 187553.93it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 10538it [00:00, 133138.07it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 10990it [00:00, 165896.13it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 10427it [00:00, 157458.74it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 12106it [00:00, 178807.22it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 5883it [00:00, 87890.53it/s]            
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 12236it [00:00, 185935.35it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples:  84%|████████▎ | 4186/5000 [00:00<00:00, 37478.56it/s]Drawing 5000 posterior samples: 5796it [00:00, 36846.40it/s]                          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 5000 posterior samples:   0%|          | 0/5000 [00:00<?, ?it/s]Drawing 5000 posterior samples: 12096it [00:00, 179402.47it/s]          
lstm_sbi.py:368: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
warning: file exists
warning: file exists
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Training neural network. Epochs trained:  340Training neural network. Epochs trained:  341Training neural network. Epochs trained:  342Training neural network. Epochs trained:  343Training neural network. Epochs trained:  344Training neural network. Epochs trained:  345Training neural network. Epochs trained:  346Training neural network. Epochs trained:  347Training neural network. Epochs trained:  348Training neural network. Epochs trained:  349Training neural network. Epochs trained:  350Training neural network. Epochs trained:  351Training neural network. Epochs trained:  352Training neural network. Epochs trained:  353Training neural network. Epochs trained:  354Training neural network. Epochs trained:  355Training neural network. Epochs trained:  356Training neural network. Epochs trained:  357Training neural network. Epochs trained:  358Training neural network. Epochs trained:  359Training neural network. Epochs trained:  360Training neural network. Epochs trained:  361Training neural network. Epochs trained:  362Training neural network. Epochs trained:  363Training neural network. Epochs trained:  364Training neural network. Epochs trained:  365Training neural network. Epochs trained:  366Training neural network. Epochs trained:  367Training neural network. Epochs trained:  368Training neural network. Epochs trained:  369Training neural network. Epochs trained:  370Training neural network. Epochs trained:  371Training neural network. Epochs trained:  372Training neural network. Epochs trained:  373Training neural network. Epochs trained:  374Training neural network. Epochs trained:  375Training neural network. Epochs trained:  376Training neural network. Epochs trained:  377Training neural network. Epochs trained:  378Training neural network. Epochs trained:  379Training neural network. Epochs trained:  380Training neural network. Epochs trained:  381Training neural network. Epochs trained:  382Training neural network. Epochs trained:  383Training neural network. Epochs trained:  384Training neural network. Epochs trained:  385Training neural network. Epochs trained:  386Training neural network. Epochs trained:  387Training neural network. Epochs trained:  388Training neural network. Epochs trained:  389Training neural network. Epochs trained:  390Training neural network. Epochs trained:  391Training neural network. Epochs trained:  392Training neural network. Epochs trained:  393Training neural network. Epochs trained:  394Training neural network. Epochs trained:  395Training neural network. Epochs trained:  396Training neural network. Epochs trained:  397Training neural network. Epochs trained:  398Training neural network. Epochs trained:  399Training neural network. Epochs trained:  400Training neural network. Epochs trained:  401Training neural network. Epochs trained:  402Training neural network. Epochs trained:  403Training neural network. Epochs trained:  404Training neural network. Epochs trained:  405Training neural network. Epochs trained:  406Training neural network. Epochs trained:  407Training neural network. Epochs trained:  408Training neural network. Epochs trained:  409Training neural network. Epochs trained:  410Training neural network. Epochs trained:  411Training neural network. Epochs trained:  412Training neural network. Epochs trained:  413Training neural network. Epochs trained:  414Training neural network. Epochs trained:  415Training neural network. Epochs trained:  416Training neural network. Epochs trained:  417Training neural network. Epochs trained:  418Training neural network. Epochs trained:  419Training neural network. Epochs trained:  420Training neural network. Epochs trained:  421Training neural network. Epochs trained:  422Training neural network. Epochs trained:  423Training neural network. Epochs trained:  424Training neural network. Epochs trained:  425Training neural network. Epochs trained:  426Training neural network. Epochs trained:  427Training neural network. Epochs trained:  428Training neural network. Epochs trained:  429Training neural network. Epochs trained:  430Training neural network. Epochs trained:  431Training neural network. Epochs trained:  432Training neural network. Epochs trained:  433Training neural network. Epochs trained:  434Training neural network. Epochs trained:  435Training neural network. Epochs trained:  436Training neural network. Epochs trained:  437Training neural network. Epochs trained:  438Training neural network. Epochs trained:  439Training neural network. Epochs trained:  440Training neural network. Epochs trained:  441Training neural network. Epochs trained:  442Training neural network. Epochs trained:  443Training neural network. Epochs trained:  444Training neural network. Epochs trained:  445Training neural network. Epochs trained:  446Training neural network. Epochs trained:  447Training neural network. Epochs trained:  448Training neural network. Epochs trained:  449Training neural network. Epochs trained:  450Training neural network. Epochs trained:  451Training neural network. Epochs trained:  452Training neural network. Epochs trained:  453Training neural network. Epochs trained:  454Training neural network. Epochs trained:  455Training neural network. Epochs trained:  456Training neural network. Epochs trained:  457Training neural network. Epochs trained:  458Training neural network. Epochs trained:  459Training neural network. Epochs trained:  460Training neural network. Epochs trained:  461Training neural network. Epochs trained:  462Training neural network. Epochs trained:  463Training neural network. Epochs trained:  464Training neural network. Epochs trained:  465Training neural network. Epochs trained:  466Training neural network. Epochs trained:  467Training neural network. Epochs trained:  468Training neural network. Epochs trained:  469Training neural network. Epochs trained:  470Training neural network. Epochs trained:  471Training neural network. Epochs trained:  472Training neural network. Epochs trained:  473Training neural network. Epochs trained:  474Training neural network. Epochs trained:  475Training neural network. Epochs trained:  476Training neural network. Epochs trained:  477Training neural network. Epochs trained:  478Training neural network. Epochs trained:  479Training neural network. Epochs trained:  480Training neural network. Epochs trained:  481Training neural network. Epochs trained:  482Training neural network. Epochs trained:  483Training neural network. Epochs trained:  484Training neural network. Epochs trained:  485Training neural network. Epochs trained:  486Training neural network. Epochs trained:  487Training neural network. Epochs trained:  488Training neural network. Epochs trained:  489Training neural network. Epochs trained:  490Training neural network. Epochs trained:  491Training neural network. Epochs trained:  492Training neural network. Epochs trained:  493Training neural network. Epochs trained:  494Training neural network. Epochs trained:  495Training neural network. Epochs trained:  496Training neural network. Epochs trained:  497Training neural network. Epochs trained:  498Training neural network. Epochs trained:  499Training neural network. Epochs trained:  500Training neural network. Epochs trained:  501Training neural network. Epochs trained:  502Training neural network. Epochs trained:  503Training neural network. Epochs trained:  504Training neural network. Epochs trained:  505Training neural network. Epochs trained:  506Training neural network. Epochs trained:  507Training neural network. Epochs trained:  508Training neural network. Epochs trained:  509Training neural network. Epochs trained:  510Training neural network. Epochs trained:  511Training neural network. Epochs trained:  512Training neural network. Epochs trained:  513Training neural network. Epochs trained:  514Training neural network. Epochs trained:  515Training neural network. Epochs trained:  516Training neural network. Epochs trained:  517Training neural network. Epochs trained:  518Training neural network. Epochs trained:  519Training neural network. Epochs trained:  520Training neural network. Epochs trained:  521Training neural network. Epochs trained:  522Training neural network. Epochs trained:  523Training neural network. Epochs trained:  524Training neural network. Epochs trained:  525Training neural network. Epochs trained:  526Training neural network. Epochs trained:  527Training neural network. Epochs trained:  528Training neural network. Epochs trained:  529Training neural network. Epochs trained:  530Training neural network. Epochs trained:  531Training neural network. Epochs trained:  532Training neural network. Epochs trained:  533Training neural network. Epochs trained:  534Training neural network. Epochs trained:  535Training neural network. Epochs trained:  536Training neural network. Epochs trained:  537Training neural network. Epochs trained:  538Training neural network. Epochs trained:  539Training neural network. Epochs trained:  540Training neural network. Epochs trained:  541Training neural network. Epochs trained:  542Training neural network. Epochs trained:  543Training neural network. Epochs trained:  544Training neural network. Epochs trained:  545Training neural network. Epochs trained:  546Training neural network. Epochs trained:  547Training neural network. Epochs trained:  548Training neural network. Epochs trained:  549Training neural network. Epochs trained:  550Training neural network. Epochs trained:  551Training neural network. Epochs trained:  552Training neural network. Epochs trained:  553Training neural network. Epochs trained:  554Training neural network. Epochs trained:  555Training neural network. Epochs trained:  556Training neural network. Epochs trained:  557Training neural network. Epochs trained:  558Training neural network. Epochs trained:  559Training neural network. Epochs trained:  560Training neural network. Epochs trained:  561Training neural network. Epochs trained:  562Training neural network. Epochs trained:  563Training neural network. Epochs trained:  564Training neural network. Epochs trained:  565Training neural network. Epochs trained:  566Training neural network. Epochs trained:  567Training neural network. Epochs trained:  568Training neural network. Epochs trained:  569Training neural network. Epochs trained:  570Training neural network. Epochs trained:  571Training neural network. Epochs trained:  572Training neural network. Epochs trained:  573Training neural network. Epochs trained:  574Training neural network. Epochs trained:  575Training neural network. Epochs trained:  576Training neural network. Epochs trained:  577Training neural network. Epochs trained:  578Training neural network. Epochs trained:  579Training neural network. Epochs trained:  580Training neural network. Epochs trained:  581Training neural network. Epochs trained:  582Training neural network. Epochs trained:  583Training neural network. Epochs trained:  584Training neural network. Epochs trained:  585Training neural network. Epochs trained:  586Training neural network. Epochs trained:  587Training neural network. Epochs trained:  588Training neural network. Epochs trained:  589Training neural network. Epochs trained:  590Training neural network. Epochs trained:  591Training neural network. Epochs trained:  592Training neural network. Epochs trained:  593Training neural network. Epochs trained:  594Training neural network. Epochs trained:  595Training neural network. Epochs trained:  596Training neural network. Epochs trained:  597Training neural network. Epochs trained:  598Training neural network. Epochs trained:  599Training neural network. Epochs trained:  600Training neural network. Epochs trained:  601Training neural network. Epochs trained:  602Training neural network. Epochs trained:  603Training neural network. Epochs trained:  604Training neural network. Epochs trained:  605Training neural network. Epochs trained:  606Training neural network. Epochs trained:  607Training neural network. Epochs trained:  608Training neural network. Epochs trained:  609Training neural network. Epochs trained:  610Training neural network. Epochs trained:  611Training neural network. Epochs trained:  612Training neural network. Epochs trained:  613Training neural network. Epochs trained:  614Neural network successfully converged after 614 epochs.
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0476, 0.0486, 0.0532, 0.0532, 0.0533, 0.0517, 0.0506, 0.0463, 0.0464,
        0.0486, 0.0521, 0.0542, 0.0540, 0.0524, 0.0487, 0.0498, 0.0500, 0.0490,
        0.0481, 0.0477, 0.0508, 0.0517, 0.0544, 0.0569, 0.0557, 0.0574, 0.0530,
        0.0517, 0.0509, 0.0482, 0.0492, 0.0524, 0.0517, 0.0499, 0.0480, 0.0505,
        0.0537, 0.0499, 0.0511, 0.0498, 0.0490, 0.0483, 0.0489, 0.0485, 0.0495,
        0.0511, 0.0511, 0.0501, 0.0483, 0.0484, 0.0494, 0.0496, 0.0507, 0.0484,
        0.0499, 0.0511, 0.0507, 0.0504, 0.0494, 0.0513, 0.0513, 0.0498, 0.0511,
        0.0504, 0.0510, 0.0503, 0.0503, 0.0510, 0.0499, 0.0493, 0.0501, 0.0486,
        0.0483, 0.0473, 0.0460, 0.0462, 0.0463, 0.0486, 0.0492, 0.0500, 0.0498,
        0.0491, 0.0506, 0.0497, 0.0504, 0.0521, 0.0518, 0.0513, 0.0508, 0.0504,
        0.0508, 0.0507, 0.0499, 0.0481, 0.0479, 0.0492, 0.0503, 0.0502, 0.0492,
        0.0484, 0.0480, 0.0478, 0.0498, 0.0493, 0.0463, 0.0461, 0.0479, 0.0492,
        0.0481, 0.0466, 0.0471, 0.0464, 0.0470, 0.0461, 0.0455, 0.0448, 0.0453,
        0.0449, 0.0455, 0.0468, 0.0456, 0.0433, 0.0478, 0.0466, 0.0459, 0.0455,
        0.0451, 0.0444, 0.0447, 0.0446, 0.0432, 0.0432, 0.0434, 0.0440, 0.0442,
        0.0439, 0.0449, 0.0453, 0.0456, 0.0484, 0.0475, 0.0460, 0.0463, 0.0476,
        0.0473, 0.0457, 0.0440, 0.0437, 0.0445, 0.0457, 0.0467, 0.0476, 0.0459,
        0.0452, 0.0457, 0.0463, 0.0452, 0.0436, 0.0429, 0.0439, 0.0447, 0.0438,
        0.0444, 0.0443, 0.0445, 0.0430, 0.0416, 0.0424, 0.0429, 0.0424, 0.0417,
        0.0423, 0.0423, 0.0426, 0.0430, 0.0473, 0.0439, 0.0426, 0.0438, 0.0449,
        0.0420, 0.0401, 0.0416, 0.0419, 0.0405, 0.0401, 0.0406, 0.0409, 0.0408,
        0.0401, 0.0414, 0.0427, 0.0433, 0.0415, 0.0423, 0.0422, 0.0447, 0.0505,
        0.0501, 0.0488, 0.0484, 0.0455, 0.0460, 0.0500, 0.0548, 0.0482, 0.0506,
        0.0534, 0.0533, 0.0540, 0.0536, 0.0542, 0.0537, 0.0667, 0.0724, 0.0785,
        0.0796, 0.0750, 0.0791, 0.0784, 0.0811, 0.0879, 0.0878, 0.0881, 0.0921,
        0.0959, 0.0917, 0.0907, 0.0910, 0.0886, 0.0885, 0.0942, 0.1034, 0.1108,
        0.1036, 0.1109, 0.1152, 0.1244, 0.1341, 0.1323, 0.1278, 0.1325, 0.1331,
        0.1364, 0.1312, 0.1464, 0.1491, 0.1520, 0.1708, 0.1971, 0.1865, 0.1901,
        0.1687, 0.1650, 0.1533, 0.1579, 0.1467, 0.1381, 0.1293, 0.1217, 0.1307,
        0.1309, 0.1143, 0.1255, 0.1218, 0.1245, 0.1150, 0.1069, 0.0987, 0.0963,
        0.0922, 0.0901, 0.0891, 0.0862, 0.0851, 0.0814, 0.0801, 0.0774, 0.0763,
        0.0736, 0.0749, 0.0758, 0.0725, 0.0714, 0.0665, 0.0600, 0.0572, 0.0554,
        0.0519, 0.0513, 0.0532, 0.0493, 0.0465, 0.0456, 0.0437, 0.0407, 0.0381,
        0.0369, 0.0381, 0.0390, 0.0396, 0.0421, 0.0441, 0.0419, 0.0387, 0.0377,
        0.0379, 0.0411, 0.0420, 0.0392, 0.0443, 0.0440, 0.0487, 0.0474, 0.0482,
        0.0536, 0.0532, 0.0529, 0.0550, 0.0485, 0.0483, 0.0445, 0.0427, 0.0433,
        0.0450, 0.0431, 0.0419, 0.0453, 0.0351, 0.0458, 0.0488, 0.0467, 0.0455,
        0.0444, 0.0450, 0.0452, 0.0441, 0.0433, 0.0434, 0.0437, 0.0437, 0.0398,
        0.0418, 0.0412, 0.0396, 0.0402, 0.0415, 0.0413, 0.0494, 0.0571],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0689, 0.0728, 0.0797, 0.0836, 0.0836, 0.0864, 0.0833, 0.0778, 0.0740,
        0.0769, 0.0828, 0.0869, 0.0866, 0.0832, 0.0777, 0.0793, 0.0801, 0.0768,
        0.0754, 0.0737, 0.0814, 0.0821, 0.0876, 0.0925, 0.0917, 0.0956, 0.0877,
        0.0870, 0.0871, 0.0830, 0.0829, 0.0894, 0.0869, 0.0815, 0.0772, 0.0783,
        0.0848, 0.0800, 0.0788, 0.0760, 0.0742, 0.0734, 0.0740, 0.0735, 0.0747,
        0.0760, 0.0763, 0.0748, 0.0725, 0.0728, 0.0743, 0.0750, 0.0763, 0.0741,
        0.0747, 0.0759, 0.0753, 0.0746, 0.0732, 0.0759, 0.0765, 0.0751, 0.0758,
        0.0754, 0.0763, 0.0758, 0.0761, 0.0771, 0.0764, 0.0763, 0.0778, 0.0763,
        0.0752, 0.0737, 0.0722, 0.0723, 0.0726, 0.0765, 0.0760, 0.0758, 0.0747,
        0.0740, 0.0753, 0.0744, 0.0755, 0.0781, 0.0770, 0.0767, 0.0759, 0.0764,
        0.0764, 0.0759, 0.0747, 0.0722, 0.0727, 0.0731, 0.0747, 0.0748, 0.0735,
        0.0721, 0.0714, 0.0708, 0.0734, 0.0730, 0.0695, 0.0688, 0.0713, 0.0737,
        0.0721, 0.0699, 0.0699, 0.0688, 0.0694, 0.0680, 0.0669, 0.0656, 0.0664,
        0.0664, 0.0672, 0.0685, 0.0675, 0.0653, 0.0693, 0.0684, 0.0669, 0.0663,
        0.0661, 0.0654, 0.0655, 0.0647, 0.0628, 0.0633, 0.0637, 0.0647, 0.0653,
        0.0633, 0.0651, 0.0658, 0.0671, 0.0700, 0.0683, 0.0668, 0.0671, 0.0704,
        0.0698, 0.0667, 0.0650, 0.0647, 0.0650, 0.0668, 0.0689, 0.0700, 0.0660,
        0.0649, 0.0656, 0.0664, 0.0650, 0.0609, 0.0611, 0.0624, 0.0636, 0.0627,
        0.0637, 0.0642, 0.0644, 0.0615, 0.0600, 0.0603, 0.0609, 0.0610, 0.0601,
        0.0611, 0.0614, 0.0619, 0.0626, 0.0691, 0.0634, 0.0617, 0.0632, 0.0617,
        0.0574, 0.0548, 0.0563, 0.0578, 0.0578, 0.0575, 0.0584, 0.0594, 0.0584,
        0.0569, 0.0570, 0.0578, 0.0580, 0.0563, 0.0591, 0.0595, 0.0611, 0.0700,
        0.0701, 0.0673, 0.0673, 0.0633, 0.0651, 0.0692, 0.0716, 0.0653, 0.0671,
        0.0689, 0.0718, 0.0732, 0.0764, 0.0740, 0.0744, 0.0893, 0.0975, 0.1041,
        0.1013, 0.0973, 0.1001, 0.0998, 0.1037, 0.1122, 0.1132, 0.1127, 0.1181,
        0.1224, 0.1187, 0.1157, 0.1150, 0.1116, 0.1116, 0.1158, 0.1243, 0.1319,
        0.1242, 0.1313, 0.1337, 0.1407, 0.1542, 0.1542, 0.1481, 0.1498, 0.1473,
        0.1512, 0.1479, 0.1636, 0.1655, 0.1655, 0.1914, 0.2108, 0.1860, 0.1738,
        0.1774, 0.1712, 0.1536, 0.1528, 0.1478, 0.1446, 0.1417, 0.1386, 0.1505,
        0.1542, 0.1310, 0.1411, 0.1385, 0.1354, 0.1241, 0.1151, 0.1080, 0.1077,
        0.1049, 0.1032, 0.1026, 0.1010, 0.0985, 0.0950, 0.0921, 0.0891, 0.0887,
        0.0866, 0.0886, 0.0896, 0.0870, 0.0860, 0.0811, 0.0723, 0.0675, 0.0638,
        0.0600, 0.0613, 0.0637, 0.0590, 0.0558, 0.0548, 0.0534, 0.0501, 0.0475,
        0.0474, 0.0498, 0.0522, 0.0528, 0.0549, 0.0567, 0.0530, 0.0502, 0.0492,
        0.0490, 0.0529, 0.0546, 0.0522, 0.0615, 0.0612, 0.0669, 0.0644, 0.0659,
        0.0738, 0.0735, 0.0735, 0.0764, 0.0668, 0.0665, 0.0610, 0.0580, 0.0596,
        0.0607, 0.0583, 0.0560, 0.0613, 0.0479, 0.0636, 0.0673, 0.0646, 0.0640,
        0.0641, 0.0645, 0.0648, 0.0640, 0.0619, 0.0593, 0.0595, 0.0595, 0.0548,
        0.0578, 0.0567, 0.0540, 0.0553, 0.0578, 0.0555, 0.0659, 0.0784],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0462, 0.0470, 0.0515, 0.0514, 0.0516, 0.0496, 0.0486, 0.0445, 0.0449,
        0.0470, 0.0504, 0.0523, 0.0522, 0.0507, 0.0472, 0.0483, 0.0484, 0.0475,
        0.0467, 0.0463, 0.0492, 0.0502, 0.0527, 0.0551, 0.0538, 0.0554, 0.0512,
        0.0499, 0.0491, 0.0466, 0.0476, 0.0507, 0.0500, 0.0484, 0.0466, 0.0491,
        0.0522, 0.0484, 0.0497, 0.0484, 0.0477, 0.0470, 0.0475, 0.0472, 0.0482,
        0.0498, 0.0498, 0.0488, 0.0470, 0.0471, 0.0481, 0.0482, 0.0494, 0.0471,
        0.0486, 0.0498, 0.0494, 0.0491, 0.0481, 0.0499, 0.0499, 0.0484, 0.0498,
        0.0491, 0.0496, 0.0489, 0.0489, 0.0496, 0.0485, 0.0479, 0.0486, 0.0472,
        0.0469, 0.0460, 0.0447, 0.0448, 0.0449, 0.0472, 0.0478, 0.0486, 0.0485,
        0.0478, 0.0493, 0.0484, 0.0491, 0.0507, 0.0504, 0.0500, 0.0494, 0.0490,
        0.0495, 0.0494, 0.0486, 0.0468, 0.0466, 0.0480, 0.0491, 0.0489, 0.0479,
        0.0472, 0.0467, 0.0466, 0.0486, 0.0480, 0.0451, 0.0449, 0.0467, 0.0479,
        0.0468, 0.0454, 0.0459, 0.0453, 0.0459, 0.0450, 0.0444, 0.0437, 0.0442,
        0.0438, 0.0443, 0.0457, 0.0444, 0.0421, 0.0466, 0.0454, 0.0447, 0.0444,
        0.0440, 0.0432, 0.0435, 0.0435, 0.0421, 0.0421, 0.0423, 0.0430, 0.0431,
        0.0428, 0.0439, 0.0442, 0.0444, 0.0472, 0.0463, 0.0449, 0.0452, 0.0463,
        0.0461, 0.0445, 0.0429, 0.0426, 0.0433, 0.0445, 0.0456, 0.0464, 0.0448,
        0.0441, 0.0446, 0.0452, 0.0441, 0.0426, 0.0419, 0.0429, 0.0437, 0.0428,
        0.0433, 0.0432, 0.0433, 0.0420, 0.0405, 0.0414, 0.0419, 0.0414, 0.0407,
        0.0413, 0.0413, 0.0415, 0.0419, 0.0461, 0.0428, 0.0415, 0.0428, 0.0439,
        0.0411, 0.0392, 0.0408, 0.0410, 0.0395, 0.0391, 0.0396, 0.0398, 0.0398,
        0.0392, 0.0405, 0.0419, 0.0424, 0.0406, 0.0414, 0.0413, 0.0437, 0.0492,
        0.0489, 0.0477, 0.0472, 0.0444, 0.0448, 0.0488, 0.0538, 0.0472, 0.0497,
        0.0526, 0.0523, 0.0528, 0.0522, 0.0530, 0.0524, 0.0653, 0.0707, 0.0767,
        0.0782, 0.0735, 0.0777, 0.0770, 0.0796, 0.0862, 0.0860, 0.0863, 0.0903,
        0.0941, 0.0898, 0.0890, 0.0894, 0.0871, 0.0869, 0.0926, 0.1018, 0.1091,
        0.1019, 0.1093, 0.1137, 0.1231, 0.1326, 0.1308, 0.1264, 0.1313, 0.1319,
        0.1350, 0.1296, 0.1444, 0.1468, 0.1505, 0.1671, 0.1940, 0.1870, 0.1916,
        0.1693, 0.1661, 0.1548, 0.1599, 0.1491, 0.1398, 0.1295, 0.1213, 0.1298,
        0.1295, 0.1132, 0.1243, 0.1206, 0.1238, 0.1147, 0.1069, 0.0986, 0.0962,
        0.0920, 0.0898, 0.0888, 0.0859, 0.0848, 0.0810, 0.0797, 0.0771, 0.0761,
        0.0733, 0.0746, 0.0754, 0.0721, 0.0710, 0.0661, 0.0597, 0.0570, 0.0553,
        0.0518, 0.0510, 0.0529, 0.0490, 0.0462, 0.0453, 0.0433, 0.0403, 0.0377,
        0.0364, 0.0375, 0.0384, 0.0390, 0.0415, 0.0435, 0.0413, 0.0381, 0.0371,
        0.0373, 0.0405, 0.0414, 0.0386, 0.0434, 0.0431, 0.0477, 0.0465, 0.0473,
        0.0525, 0.0521, 0.0519, 0.0539, 0.0476, 0.0473, 0.0436, 0.0419, 0.0424,
        0.0442, 0.0424, 0.0412, 0.0445, 0.0345, 0.0449, 0.0479, 0.0458, 0.0446,
        0.0435, 0.0441, 0.0442, 0.0431, 0.0425, 0.0426, 0.0428, 0.0429, 0.0390,
        0.0409, 0.0403, 0.0388, 0.0394, 0.0406, 0.0405, 0.0485, 0.0559],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0471, 0.0482, 0.0525, 0.0525, 0.0524, 0.0511, 0.0498, 0.0457, 0.0457,
        0.0478, 0.0513, 0.0533, 0.0531, 0.0514, 0.0478, 0.0489, 0.0492, 0.0482,
        0.0473, 0.0469, 0.0499, 0.0508, 0.0534, 0.0559, 0.0547, 0.0563, 0.0521,
        0.0508, 0.0500, 0.0474, 0.0483, 0.0515, 0.0507, 0.0490, 0.0472, 0.0497,
        0.0528, 0.0490, 0.0502, 0.0489, 0.0482, 0.0475, 0.0480, 0.0477, 0.0487,
        0.0502, 0.0502, 0.0493, 0.0475, 0.0476, 0.0485, 0.0487, 0.0499, 0.0476,
        0.0491, 0.0503, 0.0499, 0.0495, 0.0486, 0.0504, 0.0504, 0.0489, 0.0502,
        0.0495, 0.0501, 0.0494, 0.0494, 0.0501, 0.0490, 0.0484, 0.0492, 0.0478,
        0.0475, 0.0465, 0.0452, 0.0453, 0.0454, 0.0478, 0.0484, 0.0491, 0.0489,
        0.0483, 0.0498, 0.0488, 0.0495, 0.0511, 0.0509, 0.0504, 0.0499, 0.0495,
        0.0499, 0.0498, 0.0490, 0.0472, 0.0470, 0.0484, 0.0495, 0.0494, 0.0484,
        0.0476, 0.0471, 0.0470, 0.0490, 0.0485, 0.0455, 0.0453, 0.0471, 0.0483,
        0.0472, 0.0458, 0.0463, 0.0456, 0.0462, 0.0453, 0.0447, 0.0441, 0.0445,
        0.0441, 0.0447, 0.0461, 0.0448, 0.0425, 0.0471, 0.0458, 0.0451, 0.0447,
        0.0444, 0.0436, 0.0439, 0.0438, 0.0425, 0.0424, 0.0426, 0.0433, 0.0435,
        0.0432, 0.0442, 0.0446, 0.0448, 0.0476, 0.0467, 0.0452, 0.0455, 0.0467,
        0.0464, 0.0449, 0.0432, 0.0429, 0.0437, 0.0449, 0.0460, 0.0468, 0.0452,
        0.0445, 0.0450, 0.0456, 0.0445, 0.0429, 0.0423, 0.0433, 0.0441, 0.0431,
        0.0437, 0.0436, 0.0438, 0.0424, 0.0409, 0.0418, 0.0423, 0.0418, 0.0411,
        0.0416, 0.0416, 0.0419, 0.0423, 0.0467, 0.0433, 0.0420, 0.0431, 0.0442,
        0.0414, 0.0395, 0.0410, 0.0412, 0.0398, 0.0395, 0.0400, 0.0403, 0.0402,
        0.0395, 0.0408, 0.0421, 0.0427, 0.0409, 0.0417, 0.0415, 0.0439, 0.0498,
        0.0495, 0.0482, 0.0478, 0.0449, 0.0453, 0.0494, 0.0542, 0.0474, 0.0500,
        0.0526, 0.0526, 0.0533, 0.0529, 0.0535, 0.0530, 0.0661, 0.0720, 0.0780,
        0.0790, 0.0744, 0.0784, 0.0777, 0.0805, 0.0874, 0.0873, 0.0876, 0.0916,
        0.0953, 0.0911, 0.0900, 0.0903, 0.0879, 0.0878, 0.0937, 0.1030, 0.1105,
        0.1034, 0.1109, 0.1154, 0.1246, 0.1340, 0.1321, 0.1276, 0.1324, 0.1330,
        0.1363, 0.1313, 0.1469, 0.1502, 0.1529, 0.1739, 0.1995, 0.1863, 0.1890,
        0.1680, 0.1638, 0.1516, 0.1554, 0.1439, 0.1357, 0.1279, 0.1205, 0.1294,
        0.1300, 0.1140, 0.1255, 0.1216, 0.1245, 0.1147, 0.1064, 0.0979, 0.0952,
        0.0910, 0.0890, 0.0881, 0.0851, 0.0840, 0.0803, 0.0790, 0.0764, 0.0752,
        0.0725, 0.0738, 0.0746, 0.0713, 0.0702, 0.0655, 0.0592, 0.0564, 0.0547,
        0.0514, 0.0507, 0.0526, 0.0487, 0.0459, 0.0451, 0.0432, 0.0403, 0.0377,
        0.0365, 0.0376, 0.0385, 0.0391, 0.0416, 0.0436, 0.0414, 0.0383, 0.0373,
        0.0374, 0.0406, 0.0415, 0.0387, 0.0439, 0.0435, 0.0481, 0.0467, 0.0476,
        0.0528, 0.0524, 0.0522, 0.0542, 0.0479, 0.0477, 0.0438, 0.0421, 0.0426,
        0.0444, 0.0425, 0.0413, 0.0447, 0.0346, 0.0453, 0.0482, 0.0460, 0.0448,
        0.0437, 0.0442, 0.0443, 0.0433, 0.0426, 0.0427, 0.0431, 0.0431, 0.0392,
        0.0412, 0.0406, 0.0390, 0.0396, 0.0409, 0.0407, 0.0486, 0.0562],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0622, 0.0663, 0.0695, 0.0716, 0.0692, 0.0711, 0.0684, 0.0683, 0.0651,
        0.0654, 0.0702, 0.0734, 0.0727, 0.0686, 0.0632, 0.0650, 0.0665, 0.0649,
        0.0631, 0.0616, 0.0664, 0.0669, 0.0706, 0.0749, 0.0749, 0.0783, 0.0723,
        0.0717, 0.0716, 0.0675, 0.0671, 0.0711, 0.0690, 0.0660, 0.0635, 0.0649,
        0.0695, 0.0644, 0.0645, 0.0627, 0.0615, 0.0607, 0.0613, 0.0605, 0.0615,
        0.0627, 0.0630, 0.0621, 0.0601, 0.0601, 0.0613, 0.0619, 0.0635, 0.0615,
        0.0625, 0.0634, 0.0627, 0.0621, 0.0608, 0.0632, 0.0635, 0.0619, 0.0629,
        0.0623, 0.0633, 0.0627, 0.0627, 0.0635, 0.0626, 0.0623, 0.0639, 0.0625,
        0.0618, 0.0603, 0.0587, 0.0591, 0.0596, 0.0630, 0.0628, 0.0628, 0.0619,
        0.0612, 0.0627, 0.0615, 0.0619, 0.0642, 0.0635, 0.0633, 0.0627, 0.0627,
        0.0629, 0.0625, 0.0616, 0.0594, 0.0597, 0.0605, 0.0618, 0.0618, 0.0607,
        0.0596, 0.0590, 0.0586, 0.0614, 0.0610, 0.0574, 0.0568, 0.0589, 0.0608,
        0.0594, 0.0575, 0.0579, 0.0569, 0.0574, 0.0563, 0.0555, 0.0546, 0.0553,
        0.0552, 0.0559, 0.0572, 0.0561, 0.0539, 0.0585, 0.0572, 0.0559, 0.0554,
        0.0553, 0.0545, 0.0548, 0.0543, 0.0524, 0.0523, 0.0527, 0.0537, 0.0541,
        0.0534, 0.0549, 0.0555, 0.0562, 0.0587, 0.0572, 0.0558, 0.0560, 0.0582,
        0.0576, 0.0552, 0.0536, 0.0534, 0.0543, 0.0560, 0.0575, 0.0583, 0.0557,
        0.0549, 0.0555, 0.0562, 0.0549, 0.0522, 0.0520, 0.0536, 0.0544, 0.0538,
        0.0547, 0.0547, 0.0554, 0.0530, 0.0509, 0.0518, 0.0524, 0.0521, 0.0510,
        0.0516, 0.0516, 0.0521, 0.0531, 0.0606, 0.0560, 0.0538, 0.0537, 0.0534,
        0.0498, 0.0473, 0.0484, 0.0491, 0.0491, 0.0491, 0.0503, 0.0514, 0.0502,
        0.0484, 0.0488, 0.0494, 0.0503, 0.0487, 0.0507, 0.0501, 0.0528, 0.0640,
        0.0632, 0.0600, 0.0605, 0.0566, 0.0586, 0.0632, 0.0646, 0.0561, 0.0603,
        0.0575, 0.0639, 0.0669, 0.0682, 0.0667, 0.0674, 0.0837, 0.0956, 0.1018,
        0.0969, 0.0904, 0.0912, 0.0909, 0.0972, 0.1078, 0.1082, 0.1089, 0.1131,
        0.1165, 0.1117, 0.1078, 0.1072, 0.1031, 0.1036, 0.1104, 0.1214, 0.1313,
        0.1264, 0.1341, 0.1374, 0.1436, 0.1528, 0.1494, 0.1425, 0.1451, 0.1471,
        0.1565, 0.1650, 0.1936, 0.2138, 0.2128, 0.2072, 0.2132, 0.1976, 0.1655,
        0.1630, 0.1422, 0.1246, 0.1187, 0.1179, 0.1224, 0.1284, 0.1288, 0.1430,
        0.1543, 0.1321, 0.1380, 0.1321, 0.1296, 0.1147, 0.1012, 0.0906, 0.0880,
        0.0851, 0.0846, 0.0857, 0.0837, 0.0813, 0.0781, 0.0769, 0.0743, 0.0729,
        0.0703, 0.0717, 0.0722, 0.0694, 0.0691, 0.0656, 0.0601, 0.0566, 0.0542,
        0.0521, 0.0529, 0.0551, 0.0512, 0.0484, 0.0478, 0.0468, 0.0442, 0.0420,
        0.0413, 0.0428, 0.0440, 0.0443, 0.0471, 0.0495, 0.0471, 0.0447, 0.0435,
        0.0430, 0.0463, 0.0476, 0.0448, 0.0544, 0.0532, 0.0576, 0.0552, 0.0567,
        0.0630, 0.0618, 0.0624, 0.0647, 0.0572, 0.0571, 0.0515, 0.0487, 0.0493,
        0.0506, 0.0487, 0.0472, 0.0523, 0.0408, 0.0553, 0.0575, 0.0544, 0.0530,
        0.0517, 0.0514, 0.0519, 0.0513, 0.0496, 0.0491, 0.0505, 0.0510, 0.0469,
        0.0497, 0.0487, 0.0458, 0.0468, 0.0490, 0.0479, 0.0561, 0.0670],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2337, 0.2442, 0.2618, 0.2536, 0.2417, 0.2267, 0.2230, 0.2326, 0.2365,
        0.2389, 0.2528, 0.2692, 0.2790, 0.2643, 0.2373, 0.2282, 0.2228, 0.2206,
        0.2187, 0.2251, 0.2318, 0.2391, 0.2421, 0.2452, 0.2501, 0.2562, 0.2687,
        0.2750, 0.2805, 0.2843, 0.2907, 0.2907, 0.2864, 0.2653, 0.2608, 0.2719,
        0.2762, 0.2636, 0.2604, 0.2576, 0.2556, 0.2448, 0.2356, 0.2212, 0.2237,
        0.2269, 0.2273, 0.2165, 0.2213, 0.2195, 0.2129, 0.2152, 0.2106, 0.2068,
        0.2089, 0.2135, 0.2093, 0.2048, 0.2055, 0.2082, 0.2132, 0.2108, 0.2097,
        0.2101, 0.2101, 0.2134, 0.2147, 0.2135, 0.2096, 0.2133, 0.2157, 0.2182,
        0.2212, 0.2228, 0.2210, 0.2279, 0.2289, 0.2353, 0.2375, 0.2297, 0.2247,
        0.2280, 0.2277, 0.2259, 0.2227, 0.2205, 0.2207, 0.2213, 0.2203, 0.2168,
        0.2170, 0.2100, 0.2089, 0.2070, 0.2099, 0.2137, 0.2144, 0.2067, 0.2069,
        0.2110, 0.2120, 0.2104, 0.2032, 0.1973, 0.1988, 0.2006, 0.2048, 0.2014,
        0.1987, 0.1938, 0.1937, 0.1948, 0.1953, 0.1929, 0.1966, 0.1990, 0.1974,
        0.1868, 0.1871, 0.1905, 0.1865, 0.1684, 0.1628, 0.1651, 0.1664, 0.1691,
        0.1718, 0.1732, 0.1743, 0.1755, 0.1760, 0.1743, 0.1738, 0.1757, 0.1951,
        0.2033, 0.2037, 0.1795, 0.1733, 0.1694, 0.1700, 0.1733, 0.1860, 0.1921,
        0.1793, 0.1776, 0.1700, 0.1668, 0.1649, 0.1733, 0.1948, 0.1826, 0.1706,
        0.1692, 0.1702, 0.1745, 0.1609, 0.1611, 0.1693, 0.1724, 0.1729, 0.1712,
        0.1636, 0.1630, 0.1556, 0.1507, 0.1550, 0.1542, 0.1539, 0.1556, 0.1561,
        0.1591, 0.1609, 0.1668, 0.1850, 0.1981, 0.1798, 0.1540, 0.1568, 0.1400,
        0.1348, 0.1311, 0.1347, 0.1413, 0.1456, 0.1443, 0.1459, 0.1454, 0.1380,
        0.1256, 0.1181, 0.1191, 0.1136, 0.1103, 0.1266, 0.1234, 0.1377, 0.1643,
        0.1632, 0.1516, 0.1458, 0.1390, 0.1480, 0.1638, 0.1636, 0.1596, 0.1487,
        0.1565, 0.1572, 0.1750, 0.1846, 0.1890, 0.1725, 0.1740, 0.1807, 0.1791,
        0.1770, 0.1804, 0.1861, 0.1859, 0.1861, 0.1924, 0.1929, 0.1900, 0.2000,
        0.2047, 0.2026, 0.1907, 0.1906, 0.1818, 0.1837, 0.1878, 0.1980, 0.2094,
        0.2027, 0.2147, 0.2096, 0.2095, 0.2238, 0.2162, 0.2063, 0.2108, 0.2127,
        0.2133, 0.2128, 0.2250, 0.2271, 0.2320, 0.3051, 0.2054, 0.2169, 0.2070,
        0.2004, 0.2054, 0.2116, 0.2156, 0.2178, 0.2280, 0.2221, 0.2248, 0.2333,
        0.2307, 0.2041, 0.1994, 0.2065, 0.2034, 0.1954, 0.1945, 0.1898, 0.1915,
        0.1901, 0.1874, 0.1853, 0.1920, 0.1920, 0.1843, 0.1685, 0.1661, 0.1679,
        0.1687, 0.1711, 0.1705, 0.1667, 0.1653, 0.1628, 0.1585, 0.1561, 0.1530,
        0.1526, 0.1574, 0.1606, 0.1502, 0.1458, 0.1444, 0.1446, 0.1410, 0.1402,
        0.1410, 0.1408, 0.1401, 0.1386, 0.1506, 0.1561, 0.1417, 0.1355, 0.1410,
        0.1405, 0.1454, 0.1518, 0.1529, 0.1961, 0.1863, 0.1648, 0.1585, 0.1621,
        0.1750, 0.1779, 0.1757, 0.1753, 0.1642, 0.1593, 0.1480, 0.1428, 0.1404,
        0.1427, 0.1392, 0.1346, 0.1400, 0.1469, 0.1774, 0.1778, 0.1666, 0.1616,
        0.1604, 0.1589, 0.1555, 0.1544, 0.1546, 0.1457, 0.1425, 0.1495, 0.1417,
        0.1525, 0.1502, 0.1375, 0.1467, 0.1662, 0.1431, 0.1831, 0.1964],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0489, 0.0503, 0.0545, 0.0546, 0.0543, 0.0537, 0.0521, 0.0481, 0.0475,
        0.0497, 0.0533, 0.0555, 0.0552, 0.0533, 0.0494, 0.0506, 0.0510, 0.0499,
        0.0490, 0.0484, 0.0516, 0.0525, 0.0552, 0.0579, 0.0569, 0.0586, 0.0541,
        0.0529, 0.0521, 0.0493, 0.0501, 0.0533, 0.0525, 0.0507, 0.0487, 0.0512,
        0.0545, 0.0505, 0.0516, 0.0503, 0.0495, 0.0489, 0.0494, 0.0490, 0.0500,
        0.0515, 0.0515, 0.0506, 0.0488, 0.0489, 0.0499, 0.0501, 0.0513, 0.0490,
        0.0505, 0.0516, 0.0512, 0.0508, 0.0498, 0.0517, 0.0518, 0.0502, 0.0515,
        0.0509, 0.0515, 0.0508, 0.0508, 0.0515, 0.0504, 0.0498, 0.0507, 0.0493,
        0.0489, 0.0479, 0.0465, 0.0467, 0.0469, 0.0493, 0.0498, 0.0505, 0.0502,
        0.0496, 0.0511, 0.0501, 0.0508, 0.0525, 0.0522, 0.0518, 0.0512, 0.0509,
        0.0513, 0.0511, 0.0503, 0.0485, 0.0483, 0.0496, 0.0507, 0.0506, 0.0496,
        0.0488, 0.0484, 0.0482, 0.0503, 0.0498, 0.0468, 0.0465, 0.0483, 0.0496,
        0.0485, 0.0470, 0.0475, 0.0468, 0.0474, 0.0464, 0.0459, 0.0451, 0.0457,
        0.0453, 0.0459, 0.0472, 0.0460, 0.0437, 0.0483, 0.0470, 0.0462, 0.0459,
        0.0455, 0.0448, 0.0451, 0.0449, 0.0435, 0.0435, 0.0437, 0.0444, 0.0446,
        0.0442, 0.0453, 0.0457, 0.0460, 0.0488, 0.0478, 0.0464, 0.0466, 0.0479,
        0.0476, 0.0460, 0.0443, 0.0440, 0.0449, 0.0461, 0.0472, 0.0480, 0.0463,
        0.0456, 0.0461, 0.0467, 0.0456, 0.0439, 0.0433, 0.0444, 0.0451, 0.0442,
        0.0448, 0.0448, 0.0451, 0.0435, 0.0419, 0.0429, 0.0434, 0.0429, 0.0421,
        0.0427, 0.0427, 0.0429, 0.0434, 0.0481, 0.0446, 0.0432, 0.0442, 0.0452,
        0.0423, 0.0403, 0.0418, 0.0420, 0.0408, 0.0405, 0.0411, 0.0415, 0.0412,
        0.0404, 0.0417, 0.0429, 0.0435, 0.0417, 0.0427, 0.0425, 0.0449, 0.0512,
        0.0509, 0.0495, 0.0491, 0.0461, 0.0467, 0.0507, 0.0552, 0.0483, 0.0510,
        0.0531, 0.0537, 0.0546, 0.0544, 0.0549, 0.0544, 0.0679, 0.0744, 0.0804,
        0.0808, 0.0762, 0.0800, 0.0792, 0.0823, 0.0897, 0.0897, 0.0900, 0.0940,
        0.0977, 0.0934, 0.0921, 0.0923, 0.0897, 0.0897, 0.0956, 0.1051, 0.1128,
        0.1059, 0.1134, 0.1180, 0.1267, 0.1362, 0.1343, 0.1297, 0.1342, 0.1347,
        0.1384, 0.1341, 0.1507, 0.1553, 0.1568, 0.1822, 0.2051, 0.1855, 0.1831,
        0.1682, 0.1622, 0.1483, 0.1498, 0.1386, 0.1326, 0.1273, 0.1207, 0.1298,
        0.1319, 0.1156, 0.1273, 0.1232, 0.1255, 0.1150, 0.1060, 0.0973, 0.0945,
        0.0903, 0.0884, 0.0877, 0.0847, 0.0835, 0.0799, 0.0787, 0.0760, 0.0748,
        0.0721, 0.0733, 0.0741, 0.0709, 0.0699, 0.0653, 0.0590, 0.0562, 0.0544,
        0.0512, 0.0507, 0.0526, 0.0488, 0.0460, 0.0451, 0.0434, 0.0405, 0.0381,
        0.0369, 0.0381, 0.0390, 0.0395, 0.0421, 0.0441, 0.0420, 0.0389, 0.0379,
        0.0379, 0.0412, 0.0421, 0.0393, 0.0449, 0.0444, 0.0491, 0.0476, 0.0485,
        0.0539, 0.0534, 0.0532, 0.0554, 0.0488, 0.0487, 0.0446, 0.0428, 0.0433,
        0.0450, 0.0432, 0.0419, 0.0455, 0.0351, 0.0462, 0.0491, 0.0469, 0.0457,
        0.0445, 0.0449, 0.0451, 0.0441, 0.0433, 0.0434, 0.0439, 0.0439, 0.0400,
        0.0421, 0.0414, 0.0397, 0.0403, 0.0417, 0.0414, 0.0494, 0.0574],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.2675, 0.2561, 0.2957, 0.2857, 0.2669, 0.2504, 0.2443, 0.2630, 0.2559,
        0.2598, 0.2786, 0.2962, 0.3048, 0.2948, 0.2651, 0.2508, 0.2425, 0.2428,
        0.2421, 0.2529, 0.2539, 0.2638, 0.2654, 0.2678, 0.2747, 0.2875, 0.3045,
        0.3072, 0.3078, 0.3099, 0.3178, 0.3122, 0.3086, 0.2894, 0.2868, 0.2976,
        0.2978, 0.2843, 0.2855, 0.2902, 0.2887, 0.2764, 0.2685, 0.2503, 0.2517,
        0.2554, 0.2561, 0.2444, 0.2550, 0.2515, 0.2391, 0.2405, 0.2344, 0.2307,
        0.2352, 0.2394, 0.2351, 0.2293, 0.2323, 0.2350, 0.2425, 0.2396, 0.2386,
        0.2383, 0.2372, 0.2410, 0.2409, 0.2384, 0.2324, 0.2366, 0.2382, 0.2424,
        0.2473, 0.2488, 0.2453, 0.2531, 0.2527, 0.2583, 0.2631, 0.2544, 0.2496,
        0.2557, 0.2572, 0.2562, 0.2526, 0.2492, 0.2520, 0.2535, 0.2523, 0.2444,
        0.2438, 0.2340, 0.2337, 0.2343, 0.2353, 0.2422, 0.2415, 0.2300, 0.2310,
        0.2393, 0.2416, 0.2405, 0.2296, 0.2224, 0.2275, 0.2297, 0.2340, 0.2256,
        0.2218, 0.2161, 0.2163, 0.2188, 0.2189, 0.2168, 0.2242, 0.2287, 0.2265,
        0.2111, 0.2105, 0.2157, 0.2099, 0.1863, 0.1822, 0.1853, 0.1892, 0.1924,
        0.1963, 0.1976, 0.1990, 0.2022, 0.2037, 0.1953, 0.1950, 0.2019, 0.2097,
        0.2382, 0.2371, 0.2027, 0.1932, 0.1888, 0.1912, 0.1919, 0.2032, 0.2131,
        0.2092, 0.2087, 0.1984, 0.1910, 0.1865, 0.1979, 0.2211, 0.1964, 0.1991,
        0.1906, 0.2022, 0.1882, 0.1796, 0.1848, 0.1933, 0.1945, 0.1951, 0.1932,
        0.1885, 0.1827, 0.1691, 0.1722, 0.1762, 0.1751, 0.1753, 0.1773, 0.1789,
        0.1820, 0.1851, 0.1955, 0.2135, 0.2200, 0.2078, 0.1808, 0.1847, 0.1602,
        0.1572, 0.1543, 0.1584, 0.1671, 0.1726, 0.1660, 0.1654, 0.1650, 0.1590,
        0.1457, 0.1387, 0.1380, 0.1325, 0.1255, 0.1408, 0.1449, 0.1682, 0.1887,
        0.1841, 0.1738, 0.1647, 0.1581, 0.1687, 0.1839, 0.1934, 0.1947, 0.1764,
        0.1838, 0.1834, 0.1966, 0.2016, 0.2092, 0.1893, 0.1849, 0.1904, 0.1865,
        0.1887, 0.1936, 0.1987, 0.1964, 0.1951, 0.2017, 0.2000, 0.1975, 0.2087,
        0.2161, 0.2156, 0.2027, 0.2036, 0.1952, 0.1990, 0.2037, 0.2110, 0.2165,
        0.2067, 0.2200, 0.2156, 0.2180, 0.2331, 0.2277, 0.2183, 0.2209, 0.2222,
        0.2216, 0.2170, 0.2264, 0.2235, 0.2299, 0.2697, 0.2614, 0.2471, 0.2389,
        0.2306, 0.2392, 0.2469, 0.2534, 0.2524, 0.2527, 0.2428, 0.2453, 0.2481,
        0.2440, 0.2178, 0.2171, 0.2261, 0.2248, 0.2184, 0.2202, 0.2184, 0.2212,
        0.2186, 0.2127, 0.2082, 0.2148, 0.2167, 0.2122, 0.1960, 0.1923, 0.1934,
        0.1942, 0.1959, 0.1945, 0.1902, 0.1885, 0.1868, 0.1834, 0.1821, 0.1788,
        0.1824, 0.1911, 0.1947, 0.1850, 0.1817, 0.1815, 0.1810, 0.1773, 0.1717,
        0.1623, 0.1571, 0.1575, 0.1579, 0.1684, 0.1813, 0.1700, 0.1639, 0.1706,
        0.1724, 0.1777, 0.1805, 0.1781, 0.2145, 0.2115, 0.1898, 0.1797, 0.1820,
        0.1938, 0.1968, 0.1953, 0.1925, 0.1829, 0.1774, 0.1661, 0.1626, 0.1613,
        0.1638, 0.1598, 0.1543, 0.1594, 0.1667, 0.1917, 0.1952, 0.1883, 0.1810,
        0.1775, 0.1780, 0.1771, 0.1751, 0.1758, 0.1668, 0.1602, 0.1675, 0.1594,
        0.1745, 0.1749, 0.1583, 0.1699, 0.1955, 0.1747, 0.2017, 0.2049],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([-0.0035, -0.0070, -0.0108, -0.0074, -0.0007,  0.0008, -0.0009, -0.0025,
        -0.0027, -0.0019, -0.0019, -0.0038, -0.0037,  0.0014,  0.0058,  0.0038,
         0.0033,  0.0047,  0.0023,  0.0026, -0.0019, -0.0014,  0.0025, -0.0008,
        -0.0009, -0.0014, -0.0007, -0.0017, -0.0012, -0.0032, -0.0004, -0.0018,
        -0.0030, -0.0026, -0.0015, -0.0044, -0.0037,  0.0026,  0.0036,  0.0028,
         0.0011,  0.0047,  0.0062,  0.0098,  0.0125,  0.0137,  0.0118,  0.0128,
         0.0123,  0.0127,  0.0140,  0.0150,  0.0164,  0.0170,  0.0141,  0.0145,
         0.0145,  0.0143,  0.0137,  0.0127,  0.0106,  0.0104,  0.0102,  0.0097,
         0.0105,  0.0093,  0.0098,  0.0142,  0.0171,  0.0122,  0.0105,  0.0097,
         0.0112,  0.0136,  0.0146,  0.0120,  0.0149,  0.0127,  0.0099,  0.0112,
         0.0141,  0.0127,  0.0131,  0.0144,  0.0168,  0.0155,  0.0150,  0.0154,
         0.0166,  0.0176,  0.0194,  0.0210,  0.0204,  0.0191,  0.0198,  0.0192,
         0.0181,  0.0187,  0.0184,  0.0182,  0.0171,  0.0164,  0.0198,  0.0186,
         0.0192,  0.0224,  0.0244,  0.0238,  0.0229,  0.0240,  0.0236,  0.0228,
         0.0244,  0.0246,  0.0238,  0.0244,  0.0265,  0.0280,  0.0288,  0.0282,
         0.0293,  0.0319,  0.0307,  0.0297,  0.0281,  0.0274,  0.0255,  0.0265,
         0.0275,  0.0269,  0.0260,  0.0292,  0.0286,  0.0262,  0.0248,  0.0228,
         0.0228,  0.0264,  0.0301,  0.0327,  0.0310,  0.0298,  0.0282,  0.0345,
         0.0328,  0.0342,  0.0365,  0.0377,  0.0333,  0.0330,  0.0287,  0.0278,
         0.0242,  0.0248,  0.0250,  0.0268,  0.0292,  0.0270,  0.0238,  0.0255,
         0.0253,  0.0270,  0.0277,  0.0272,  0.0253,  0.0254,  0.0239,  0.0256,
         0.0272,  0.0258,  0.0276,  0.0276,  0.0275,  0.0277,  0.0279,  0.0297,
         0.0288,  0.0315,  0.0365,  0.0374,  0.0336,  0.0333,  0.0315,  0.0336,
         0.0296,  0.0300,  0.0300,  0.0261,  0.0309,  0.0288,  0.0281,  0.0311,
         0.0316,  0.0304,  0.0290,  0.0317,  0.0341,  0.0302,  0.0274,  0.0297,
         0.0307,  0.0339,  0.0314,  0.0290,  0.0331,  0.0300,  0.0330,  0.0313,
         0.0340,  0.0368,  0.0304,  0.0369,  0.0348,  0.0340,  0.0312,  0.0357,
         0.0423,  0.0431,  0.0407,  0.0398,  0.0361,  0.0349,  0.0331,  0.0328,
         0.0342,  0.0355,  0.0335,  0.0309,  0.0325,  0.0238,  0.0229,  0.0235,
         0.0315,  0.0349,  0.0361,  0.0355,  0.0294,  0.0309,  0.0364,  0.0372,
         0.0371,  0.0376,  0.0371,  0.0396,  0.0362,  0.0353,  0.0407,  0.0390,
         0.0363,  0.0376,  0.0370,  0.0385,  0.0390,  0.0385,  0.0398,  0.0390,
         0.0453,  0.0444,  0.0491,  0.0482,  0.0483,  0.0415,  0.0465,  0.0500,
         0.0529,  0.0537,  0.0526,  0.0501,  0.0510,  0.0583,  0.0648,  0.0685,
         0.0703,  0.0712,  0.0700,  0.0697,  0.0711,  0.0721,  0.0790,  0.0797,
         0.0794,  0.0801,  0.0803,  0.0815,  0.0811,  0.0836,  0.0874,  0.0879,
         0.0948,  0.0977,  0.1013,  0.0971,  0.1013,  0.1008,  0.1023,  0.0965,
         0.0968,  0.0892,  0.0904,  0.0859,  0.0858,  0.0897,  0.0950,  0.0958,
         0.0850,  0.0820,  0.0766,  0.0812,  0.0822,  0.0752,  0.0791,  0.0766,
         0.0752,  0.0732,  0.0698,  0.0665,  0.0669,  0.0680,  0.0692,  0.0690,
         0.0643,  0.0653,  0.0596,  0.0621,  0.0633,  0.0587,  0.0632,  0.0666,
         0.0585,  0.0570,  0.0513,  0.0478,  0.0452,  0.0416,  0.0420,  0.0410,
         0.0401,  0.0386,  0.0405,  0.0436,  0.0456,  0.0469,  0.0408,  0.0400,
         0.0396,  0.0361,  0.0311,  0.0275,  0.0287,  0.0304],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0479, 0.0490, 0.0537, 0.0538, 0.0540, 0.0523, 0.0512, 0.0467, 0.0469,
        0.0492, 0.0528, 0.0549, 0.0548, 0.0532, 0.0494, 0.0505, 0.0507, 0.0496,
        0.0488, 0.0483, 0.0516, 0.0525, 0.0552, 0.0578, 0.0566, 0.0583, 0.0537,
        0.0525, 0.0517, 0.0490, 0.0499, 0.0532, 0.0525, 0.0507, 0.0487, 0.0512,
        0.0545, 0.0507, 0.0518, 0.0505, 0.0497, 0.0490, 0.0496, 0.0492, 0.0503,
        0.0519, 0.0518, 0.0508, 0.0490, 0.0491, 0.0501, 0.0503, 0.0514, 0.0492,
        0.0506, 0.0518, 0.0515, 0.0511, 0.0501, 0.0520, 0.0520, 0.0505, 0.0518,
        0.0511, 0.0517, 0.0510, 0.0510, 0.0517, 0.0507, 0.0500, 0.0508, 0.0494,
        0.0490, 0.0481, 0.0467, 0.0468, 0.0469, 0.0494, 0.0499, 0.0507, 0.0505,
        0.0498, 0.0513, 0.0504, 0.0511, 0.0529, 0.0525, 0.0521, 0.0515, 0.0512,
        0.0516, 0.0514, 0.0506, 0.0488, 0.0486, 0.0499, 0.0511, 0.0510, 0.0499,
        0.0491, 0.0487, 0.0485, 0.0505, 0.0499, 0.0470, 0.0468, 0.0486, 0.0499,
        0.0488, 0.0473, 0.0478, 0.0471, 0.0477, 0.0468, 0.0461, 0.0454, 0.0459,
        0.0455, 0.0461, 0.0475, 0.0463, 0.0440, 0.0485, 0.0473, 0.0465, 0.0461,
        0.0457, 0.0450, 0.0453, 0.0452, 0.0438, 0.0438, 0.0440, 0.0447, 0.0448,
        0.0444, 0.0455, 0.0459, 0.0462, 0.0491, 0.0481, 0.0467, 0.0469, 0.0483,
        0.0480, 0.0464, 0.0447, 0.0443, 0.0451, 0.0463, 0.0474, 0.0482, 0.0465,
        0.0458, 0.0463, 0.0469, 0.0458, 0.0441, 0.0435, 0.0445, 0.0453, 0.0444,
        0.0449, 0.0448, 0.0450, 0.0435, 0.0421, 0.0429, 0.0434, 0.0430, 0.0422,
        0.0428, 0.0429, 0.0432, 0.0436, 0.0479, 0.0444, 0.0432, 0.0444, 0.0454,
        0.0425, 0.0405, 0.0421, 0.0424, 0.0411, 0.0406, 0.0411, 0.0414, 0.0413,
        0.0407, 0.0420, 0.0433, 0.0438, 0.0420, 0.0429, 0.0428, 0.0453, 0.0510,
        0.0507, 0.0494, 0.0489, 0.0460, 0.0465, 0.0506, 0.0553, 0.0489, 0.0511,
        0.0541, 0.0539, 0.0545, 0.0542, 0.0547, 0.0542, 0.0672, 0.0728, 0.0789,
        0.0801, 0.0756, 0.0796, 0.0790, 0.0816, 0.0883, 0.0883, 0.0884, 0.0925,
        0.0964, 0.0922, 0.0912, 0.0915, 0.0891, 0.0890, 0.0946, 0.1037, 0.1110,
        0.1037, 0.1109, 0.1150, 0.1242, 0.1341, 0.1325, 0.1280, 0.1327, 0.1332,
        0.1364, 0.1311, 0.1460, 0.1481, 0.1514, 0.1682, 0.1949, 0.1867, 0.1909,
        0.1694, 0.1661, 0.1548, 0.1600, 0.1492, 0.1403, 0.1306, 0.1229, 0.1317,
        0.1316, 0.1146, 0.1255, 0.1220, 0.1245, 0.1152, 0.1073, 0.0993, 0.0972,
        0.0931, 0.0910, 0.0900, 0.0871, 0.0860, 0.0823, 0.0809, 0.0782, 0.0773,
        0.0746, 0.0759, 0.0768, 0.0735, 0.0723, 0.0674, 0.0607, 0.0579, 0.0560,
        0.0524, 0.0518, 0.0537, 0.0498, 0.0469, 0.0460, 0.0441, 0.0410, 0.0384,
        0.0372, 0.0384, 0.0394, 0.0401, 0.0426, 0.0446, 0.0423, 0.0391, 0.0381,
        0.0382, 0.0415, 0.0424, 0.0396, 0.0447, 0.0444, 0.0492, 0.0479, 0.0488,
        0.0542, 0.0538, 0.0536, 0.0557, 0.0491, 0.0489, 0.0450, 0.0432, 0.0438,
        0.0456, 0.0436, 0.0424, 0.0458, 0.0355, 0.0463, 0.0494, 0.0473, 0.0461,
        0.0451, 0.0457, 0.0458, 0.0447, 0.0439, 0.0439, 0.0442, 0.0442, 0.0403,
        0.0423, 0.0416, 0.0401, 0.0407, 0.0420, 0.0418, 0.0500, 0.0578],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0456, 0.0465, 0.0504, 0.0501, 0.0497, 0.0488, 0.0474, 0.0439, 0.0435,
        0.0455, 0.0487, 0.0505, 0.0502, 0.0485, 0.0450, 0.0462, 0.0466, 0.0459,
        0.0449, 0.0445, 0.0469, 0.0480, 0.0503, 0.0527, 0.0517, 0.0531, 0.0493,
        0.0481, 0.0474, 0.0447, 0.0457, 0.0485, 0.0478, 0.0463, 0.0446, 0.0472,
        0.0500, 0.0461, 0.0474, 0.0463, 0.0456, 0.0450, 0.0455, 0.0451, 0.0460,
        0.0475, 0.0475, 0.0467, 0.0450, 0.0450, 0.0459, 0.0461, 0.0473, 0.0449,
        0.0465, 0.0477, 0.0472, 0.0469, 0.0460, 0.0478, 0.0477, 0.0462, 0.0476,
        0.0468, 0.0475, 0.0467, 0.0467, 0.0474, 0.0463, 0.0456, 0.0465, 0.0451,
        0.0448, 0.0439, 0.0426, 0.0428, 0.0429, 0.0451, 0.0458, 0.0465, 0.0463,
        0.0457, 0.0472, 0.0462, 0.0467, 0.0483, 0.0481, 0.0477, 0.0472, 0.0467,
        0.0472, 0.0471, 0.0463, 0.0446, 0.0444, 0.0458, 0.0468, 0.0467, 0.0457,
        0.0450, 0.0446, 0.0445, 0.0466, 0.0460, 0.0430, 0.0429, 0.0445, 0.0457,
        0.0446, 0.0433, 0.0438, 0.0432, 0.0438, 0.0429, 0.0424, 0.0418, 0.0422,
        0.0418, 0.0424, 0.0437, 0.0424, 0.0399, 0.0446, 0.0433, 0.0427, 0.0424,
        0.0421, 0.0413, 0.0417, 0.0416, 0.0403, 0.0402, 0.0404, 0.0410, 0.0412,
        0.0411, 0.0421, 0.0424, 0.0424, 0.0450, 0.0442, 0.0429, 0.0432, 0.0440,
        0.0438, 0.0424, 0.0408, 0.0405, 0.0414, 0.0426, 0.0435, 0.0444, 0.0429,
        0.0423, 0.0429, 0.0434, 0.0423, 0.0410, 0.0402, 0.0413, 0.0419, 0.0411,
        0.0416, 0.0415, 0.0418, 0.0404, 0.0389, 0.0399, 0.0404, 0.0398, 0.0391,
        0.0395, 0.0395, 0.0397, 0.0402, 0.0446, 0.0414, 0.0400, 0.0409, 0.0423,
        0.0396, 0.0377, 0.0391, 0.0392, 0.0378, 0.0375, 0.0381, 0.0384, 0.0382,
        0.0375, 0.0388, 0.0401, 0.0408, 0.0390, 0.0397, 0.0393, 0.0417, 0.0477,
        0.0474, 0.0462, 0.0458, 0.0429, 0.0433, 0.0474, 0.0522, 0.0451, 0.0480,
        0.0499, 0.0504, 0.0512, 0.0505, 0.0515, 0.0508, 0.0642, 0.0706, 0.0765,
        0.0773, 0.0724, 0.0763, 0.0756, 0.0786, 0.0859, 0.0858, 0.0863, 0.0901,
        0.0936, 0.0893, 0.0881, 0.0884, 0.0858, 0.0859, 0.0922, 0.1019, 0.1097,
        0.1032, 0.1111, 0.1164, 0.1255, 0.1339, 0.1316, 0.1271, 0.1320, 0.1328,
        0.1360, 0.1318, 0.1488, 0.1547, 0.1569, 0.1835, 0.2065, 0.1858, 0.1840,
        0.1666, 0.1606, 0.1467, 0.1477, 0.1360, 0.1298, 0.1242, 0.1171, 0.1255,
        0.1279, 0.1129, 0.1256, 0.1212, 0.1246, 0.1141, 0.1050, 0.0955, 0.0920,
        0.0875, 0.0855, 0.0849, 0.0818, 0.0806, 0.0769, 0.0759, 0.0732, 0.0718,
        0.0690, 0.0701, 0.0708, 0.0675, 0.0666, 0.0623, 0.0566, 0.0540, 0.0526,
        0.0495, 0.0488, 0.0507, 0.0470, 0.0442, 0.0435, 0.0417, 0.0389, 0.0365,
        0.0352, 0.0361, 0.0368, 0.0374, 0.0400, 0.0420, 0.0400, 0.0369, 0.0360,
        0.0360, 0.0391, 0.0400, 0.0372, 0.0424, 0.0418, 0.0460, 0.0447, 0.0455,
        0.0505, 0.0500, 0.0498, 0.0519, 0.0458, 0.0457, 0.0419, 0.0402, 0.0406,
        0.0424, 0.0406, 0.0395, 0.0428, 0.0330, 0.0434, 0.0461, 0.0438, 0.0426,
        0.0413, 0.0417, 0.0418, 0.0409, 0.0403, 0.0408, 0.0412, 0.0413, 0.0375,
        0.0395, 0.0389, 0.0372, 0.0377, 0.0390, 0.0388, 0.0464, 0.0535],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0580, 0.0617, 0.0648, 0.0662, 0.0641, 0.0659, 0.0632, 0.0625, 0.0595,
        0.0603, 0.0649, 0.0677, 0.0670, 0.0634, 0.0584, 0.0600, 0.0613, 0.0599,
        0.0584, 0.0571, 0.0612, 0.0619, 0.0652, 0.0691, 0.0689, 0.0717, 0.0662,
        0.0654, 0.0651, 0.0613, 0.0613, 0.0648, 0.0631, 0.0606, 0.0583, 0.0601,
        0.0642, 0.0594, 0.0599, 0.0583, 0.0573, 0.0566, 0.0571, 0.0564, 0.0573,
        0.0586, 0.0588, 0.0580, 0.0561, 0.0561, 0.0572, 0.0577, 0.0592, 0.0572,
        0.0583, 0.0593, 0.0586, 0.0581, 0.0569, 0.0591, 0.0593, 0.0577, 0.0588,
        0.0582, 0.0591, 0.0585, 0.0585, 0.0592, 0.0582, 0.0579, 0.0593, 0.0579,
        0.0572, 0.0558, 0.0543, 0.0547, 0.0551, 0.0582, 0.0583, 0.0584, 0.0578,
        0.0571, 0.0586, 0.0574, 0.0578, 0.0599, 0.0594, 0.0592, 0.0586, 0.0584,
        0.0587, 0.0584, 0.0575, 0.0554, 0.0556, 0.0565, 0.0578, 0.0578, 0.0567,
        0.0557, 0.0552, 0.0549, 0.0575, 0.0570, 0.0535, 0.0530, 0.0550, 0.0568,
        0.0555, 0.0537, 0.0541, 0.0532, 0.0537, 0.0527, 0.0520, 0.0511, 0.0518,
        0.0516, 0.0523, 0.0536, 0.0525, 0.0502, 0.0549, 0.0536, 0.0524, 0.0519,
        0.0518, 0.0510, 0.0513, 0.0509, 0.0492, 0.0490, 0.0493, 0.0503, 0.0506,
        0.0501, 0.0515, 0.0520, 0.0526, 0.0551, 0.0538, 0.0524, 0.0526, 0.0544,
        0.0540, 0.0518, 0.0501, 0.0500, 0.0510, 0.0525, 0.0538, 0.0546, 0.0523,
        0.0515, 0.0522, 0.0528, 0.0515, 0.0493, 0.0489, 0.0503, 0.0511, 0.0505,
        0.0513, 0.0513, 0.0520, 0.0497, 0.0478, 0.0487, 0.0493, 0.0489, 0.0478,
        0.0484, 0.0484, 0.0488, 0.0497, 0.0565, 0.0523, 0.0503, 0.0503, 0.0506,
        0.0472, 0.0449, 0.0460, 0.0465, 0.0461, 0.0461, 0.0472, 0.0481, 0.0471,
        0.0455, 0.0462, 0.0469, 0.0478, 0.0462, 0.0479, 0.0473, 0.0499, 0.0600,
        0.0593, 0.0567, 0.0569, 0.0532, 0.0549, 0.0593, 0.0615, 0.0531, 0.0571,
        0.0551, 0.0603, 0.0629, 0.0638, 0.0629, 0.0633, 0.0791, 0.0900, 0.0961,
        0.0924, 0.0865, 0.0879, 0.0873, 0.0929, 0.1031, 0.1033, 0.1040, 0.1081,
        0.1116, 0.1068, 0.1035, 0.1030, 0.0993, 0.0998, 0.1066, 0.1173, 0.1268,
        0.1213, 0.1290, 0.1333, 0.1397, 0.1489, 0.1463, 0.1402, 0.1433, 0.1444,
        0.1523, 0.1575, 0.1842, 0.2033, 0.2040, 0.2083, 0.2125, 0.1948, 0.1659,
        0.1680, 0.1483, 0.1281, 0.1220, 0.1192, 0.1225, 0.1264, 0.1253, 0.1379,
        0.1482, 0.1276, 0.1362, 0.1303, 0.1292, 0.1147, 0.1017, 0.0912, 0.0881,
        0.0845, 0.0836, 0.0844, 0.0819, 0.0798, 0.0767, 0.0757, 0.0730, 0.0715,
        0.0688, 0.0701, 0.0706, 0.0678, 0.0674, 0.0638, 0.0583, 0.0551, 0.0530,
        0.0508, 0.0513, 0.0534, 0.0496, 0.0468, 0.0461, 0.0450, 0.0424, 0.0403,
        0.0395, 0.0408, 0.0419, 0.0422, 0.0450, 0.0473, 0.0451, 0.0426, 0.0415,
        0.0410, 0.0443, 0.0454, 0.0426, 0.0513, 0.0502, 0.0545, 0.0523, 0.0536,
        0.0596, 0.0585, 0.0589, 0.0612, 0.0541, 0.0540, 0.0489, 0.0463, 0.0469,
        0.0483, 0.0464, 0.0450, 0.0497, 0.0384, 0.0520, 0.0544, 0.0515, 0.0501,
        0.0487, 0.0486, 0.0490, 0.0483, 0.0468, 0.0467, 0.0480, 0.0483, 0.0443,
        0.0470, 0.0460, 0.0434, 0.0443, 0.0462, 0.0454, 0.0533, 0.0634],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0479, 0.0490, 0.0536, 0.0536, 0.0537, 0.0522, 0.0510, 0.0467, 0.0467,
        0.0490, 0.0525, 0.0546, 0.0545, 0.0528, 0.0491, 0.0502, 0.0504, 0.0494,
        0.0485, 0.0480, 0.0512, 0.0521, 0.0548, 0.0574, 0.0562, 0.0579, 0.0534,
        0.0522, 0.0514, 0.0487, 0.0496, 0.0529, 0.0521, 0.0503, 0.0484, 0.0509,
        0.0541, 0.0503, 0.0514, 0.0501, 0.0493, 0.0487, 0.0492, 0.0489, 0.0499,
        0.0515, 0.0514, 0.0505, 0.0487, 0.0488, 0.0497, 0.0499, 0.0511, 0.0488,
        0.0502, 0.0515, 0.0511, 0.0507, 0.0497, 0.0516, 0.0516, 0.0501, 0.0514,
        0.0507, 0.0513, 0.0506, 0.0506, 0.0514, 0.0503, 0.0496, 0.0505, 0.0490,
        0.0487, 0.0477, 0.0463, 0.0465, 0.0466, 0.0490, 0.0496, 0.0503, 0.0501,
        0.0495, 0.0510, 0.0500, 0.0507, 0.0524, 0.0521, 0.0517, 0.0511, 0.0508,
        0.0512, 0.0510, 0.0502, 0.0484, 0.0482, 0.0495, 0.0507, 0.0506, 0.0495,
        0.0487, 0.0483, 0.0481, 0.0502, 0.0496, 0.0467, 0.0464, 0.0483, 0.0495,
        0.0484, 0.0469, 0.0474, 0.0467, 0.0473, 0.0464, 0.0458, 0.0451, 0.0456,
        0.0452, 0.0458, 0.0471, 0.0459, 0.0436, 0.0482, 0.0469, 0.0462, 0.0458,
        0.0454, 0.0447, 0.0450, 0.0448, 0.0435, 0.0435, 0.0437, 0.0443, 0.0445,
        0.0441, 0.0452, 0.0456, 0.0459, 0.0487, 0.0478, 0.0463, 0.0466, 0.0479,
        0.0476, 0.0460, 0.0443, 0.0440, 0.0448, 0.0460, 0.0471, 0.0479, 0.0462,
        0.0455, 0.0460, 0.0466, 0.0455, 0.0438, 0.0432, 0.0442, 0.0450, 0.0441,
        0.0446, 0.0446, 0.0447, 0.0433, 0.0418, 0.0427, 0.0432, 0.0427, 0.0420,
        0.0426, 0.0426, 0.0429, 0.0433, 0.0476, 0.0442, 0.0429, 0.0441, 0.0451,
        0.0422, 0.0403, 0.0418, 0.0421, 0.0408, 0.0404, 0.0409, 0.0412, 0.0411,
        0.0404, 0.0417, 0.0430, 0.0435, 0.0417, 0.0426, 0.0425, 0.0449, 0.0508,
        0.0505, 0.0491, 0.0487, 0.0458, 0.0463, 0.0503, 0.0551, 0.0485, 0.0509,
        0.0536, 0.0536, 0.0543, 0.0539, 0.0545, 0.0540, 0.0671, 0.0728, 0.0789,
        0.0799, 0.0754, 0.0794, 0.0787, 0.0814, 0.0883, 0.0882, 0.0885, 0.0925,
        0.0963, 0.0921, 0.0911, 0.0913, 0.0889, 0.0888, 0.0945, 0.1037, 0.1111,
        0.1039, 0.1112, 0.1155, 0.1246, 0.1344, 0.1327, 0.1281, 0.1328, 0.1333,
        0.1367, 0.1315, 0.1468, 0.1494, 0.1523, 0.1712, 0.1974, 0.1865, 0.1899,
        0.1687, 0.1649, 0.1532, 0.1577, 0.1465, 0.1380, 0.1295, 0.1219, 0.1310,
        0.1312, 0.1146, 0.1258, 0.1221, 0.1246, 0.1151, 0.1070, 0.0988, 0.0964,
        0.0923, 0.0902, 0.0892, 0.0863, 0.0852, 0.0815, 0.0802, 0.0775, 0.0765,
        0.0738, 0.0751, 0.0759, 0.0727, 0.0716, 0.0667, 0.0601, 0.0573, 0.0555,
        0.0520, 0.0514, 0.0533, 0.0494, 0.0466, 0.0457, 0.0438, 0.0408, 0.0382,
        0.0370, 0.0382, 0.0392, 0.0398, 0.0423, 0.0443, 0.0420, 0.0389, 0.0379,
        0.0380, 0.0413, 0.0421, 0.0394, 0.0446, 0.0442, 0.0490, 0.0476, 0.0485,
        0.0539, 0.0535, 0.0532, 0.0553, 0.0488, 0.0486, 0.0447, 0.0429, 0.0435,
        0.0452, 0.0433, 0.0421, 0.0455, 0.0352, 0.0461, 0.0491, 0.0470, 0.0458,
        0.0447, 0.0453, 0.0454, 0.0443, 0.0436, 0.0436, 0.0439, 0.0439, 0.0400,
        0.0420, 0.0414, 0.0398, 0.0404, 0.0417, 0.0415, 0.0496, 0.0574],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.1166, 0.1208, 0.1342, 0.1429, 0.1443, 0.1470, 0.1427, 0.1404, 0.1310,
        0.1330, 0.1424, 0.1516, 0.1531, 0.1460, 0.1358, 0.1380, 0.1402, 0.1346,
        0.1320, 0.1295, 0.1427, 0.1453, 0.1551, 0.1612, 0.1618, 0.1660, 0.1600,
        0.1635, 0.1687, 0.1668, 0.1684, 0.1819, 0.1748, 0.1571, 0.1515, 0.1530,
        0.1650, 0.1563, 0.1444, 0.1342, 0.1301, 0.1267, 0.1250, 0.1239, 0.1250,
        0.1252, 0.1258, 0.1229, 0.1199, 0.1214, 0.1239, 0.1254, 0.1258, 0.1228,
        0.1215, 0.1238, 0.1228, 0.1213, 0.1194, 0.1228, 0.1242, 0.1228, 0.1226,
        0.1229, 0.1242, 0.1248, 0.1263, 0.1277, 0.1277, 0.1290, 0.1318, 0.1308,
        0.1294, 0.1284, 0.1276, 0.1290, 0.1299, 0.1360, 0.1337, 0.1312, 0.1272,
        0.1251, 0.1244, 0.1232, 0.1245, 0.1271, 0.1251, 0.1249, 0.1238, 0.1258,
        0.1260, 0.1248, 0.1229, 0.1192, 0.1211, 0.1201, 0.1223, 0.1229, 0.1211,
        0.1186, 0.1173, 0.1156, 0.1180, 0.1175, 0.1135, 0.1125, 0.1162, 0.1203,
        0.1185, 0.1154, 0.1147, 0.1132, 0.1139, 0.1114, 0.1090, 0.1069, 0.1077,
        0.1075, 0.1090, 0.1103, 0.1091, 0.1040, 0.1046, 0.1050, 0.1031, 0.1035,
        0.1036, 0.1034, 0.1036, 0.1022, 0.1002, 0.1026, 0.1036, 0.1053, 0.1060,
        0.1017, 0.1044, 0.1043, 0.1056, 0.1067, 0.1044, 0.1044, 0.1069, 0.1119,
        0.1078, 0.1032, 0.1014, 0.1016, 0.1013, 0.1053, 0.1104, 0.1175, 0.1030,
        0.1013, 0.1021, 0.1037, 0.1004, 0.0924, 0.0947, 0.0970, 0.0980, 0.0981,
        0.0978, 0.0997, 0.0989, 0.0939, 0.0924, 0.0922, 0.0929, 0.0942, 0.0932,
        0.0950, 0.0954, 0.0963, 0.0989, 0.1104, 0.0974, 0.0932, 0.0967, 0.0892,
        0.0830, 0.0794, 0.0816, 0.0852, 0.0874, 0.0874, 0.0887, 0.0902, 0.0876,
        0.0836, 0.0805, 0.0812, 0.0799, 0.0785, 0.0855, 0.0859, 0.0852, 0.0990,
        0.1012, 0.0958, 0.0955, 0.0900, 0.0937, 0.0987, 0.0985, 0.0933, 0.0933,
        0.0956, 0.1022, 0.1053, 0.1161, 0.1114, 0.1099, 0.1236, 0.1336, 0.1398,
        0.1329, 0.1297, 0.1326, 0.1338, 0.1376, 0.1461, 0.1486, 0.1476, 0.1552,
        0.1599, 0.1579, 0.1516, 0.1499, 0.1445, 0.1454, 0.1467, 0.1543, 0.1610,
        0.1535, 0.1621, 0.1629, 0.1675, 0.1846, 0.1852, 0.1784, 0.1775, 0.1707,
        0.1715, 0.1672, 0.1806, 0.1812, 0.1843, 0.2105, 0.2295, 0.1923, 0.1825,
        0.1926, 0.1877, 0.1740, 0.1691, 0.1694, 0.1713, 0.1666, 0.1670, 0.1823,
        0.1854, 0.1542, 0.1666, 0.1672, 0.1601, 0.1496, 0.1418, 0.1342, 0.1358,
        0.1341, 0.1325, 0.1321, 0.1335, 0.1299, 0.1235, 0.1158, 0.1127, 0.1128,
        0.1122, 0.1153, 0.1163, 0.1140, 0.1130, 0.1094, 0.0999, 0.0929, 0.0868,
        0.0805, 0.0847, 0.0882, 0.0814, 0.0776, 0.0767, 0.0757, 0.0716, 0.0691,
        0.0717, 0.0764, 0.0816, 0.0841, 0.0853, 0.0827, 0.0752, 0.0730, 0.0731,
        0.0720, 0.0763, 0.0813, 0.0774, 0.1007, 0.0977, 0.1011, 0.0964, 0.0988,
        0.1107, 0.1122, 0.1107, 0.1183, 0.1016, 0.1007, 0.0924, 0.0874, 0.0900,
        0.0909, 0.0871, 0.0831, 0.0912, 0.0783, 0.1020, 0.1047, 0.0978, 0.0988,
        0.1042, 0.1015, 0.0988, 0.1011, 0.0986, 0.0903, 0.0892, 0.0902, 0.0829,
        0.0881, 0.0859, 0.0803, 0.0844, 0.0920, 0.0813, 0.0998, 0.1171],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0140, 0.0158, 0.0181, 0.0200, 0.0266, 0.0298, 0.0306, 0.0321, 0.0329,
        0.0337, 0.0348, 0.0348, 0.0348, 0.0364, 0.0359, 0.0356, 0.0345, 0.0340,
        0.0338, 0.0375, 0.0334, 0.0348, 0.0368, 0.0369, 0.0354, 0.0377, 0.0363,
        0.0353, 0.0348, 0.0333, 0.0351, 0.0384, 0.0386, 0.0374, 0.0379, 0.0401,
        0.0416, 0.0393, 0.0409, 0.0399, 0.0391, 0.0385, 0.0390, 0.0397, 0.0415,
        0.0436, 0.0432, 0.0416, 0.0395, 0.0399, 0.0406, 0.0402, 0.0404, 0.0384,
        0.0403, 0.0421, 0.0425, 0.0422, 0.0417, 0.0427, 0.0425, 0.0413, 0.0429,
        0.0420, 0.0416, 0.0407, 0.0409, 0.0416, 0.0402, 0.0385, 0.0380, 0.0367,
        0.0373, 0.0377, 0.0369, 0.0364, 0.0361, 0.0372, 0.0389, 0.0408, 0.0411,
        0.0406, 0.0417, 0.0414, 0.0431, 0.0443, 0.0440, 0.0431, 0.0423, 0.0418,
        0.0425, 0.0426, 0.0418, 0.0403, 0.0403, 0.0420, 0.0432, 0.0426, 0.0414,
        0.0410, 0.0408, 0.0406, 0.0409, 0.0397, 0.0381, 0.0389, 0.0406, 0.0410,
        0.0399, 0.0390, 0.0394, 0.0392, 0.0402, 0.0394, 0.0389, 0.0385, 0.0387,
        0.0376, 0.0382, 0.0398, 0.0390, 0.0371, 0.0401, 0.0389, 0.0387, 0.0385,
        0.0378, 0.0370, 0.0373, 0.0373, 0.0365, 0.0370, 0.0372, 0.0374, 0.0372,
        0.0371, 0.0377, 0.0377, 0.0382, 0.0415, 0.0410, 0.0399, 0.0403, 0.0415,
        0.0411, 0.0400, 0.0383, 0.0378, 0.0372, 0.0384, 0.0391, 0.0400, 0.0387,
        0.0379, 0.0379, 0.0383, 0.0379, 0.0378, 0.0369, 0.0379, 0.0415, 0.0407,
        0.0384, 0.0380, 0.0352, 0.0359, 0.0343, 0.0342, 0.0345, 0.0339, 0.0338,
        0.0344, 0.0346, 0.0349, 0.0352, 0.0372, 0.0349, 0.0336, 0.0376, 0.0391,
        0.0366, 0.0357, 0.0402, 0.0419, 0.0376, 0.0379, 0.0359, 0.0328, 0.0354,
        0.0353, 0.0376, 0.0411, 0.0395, 0.0371, 0.0359, 0.0369, 0.0419, 0.0426,
        0.0411, 0.0403, 0.0392, 0.0388, 0.0390, 0.0437, 0.0495, 0.0439, 0.0509,
        0.0523, 0.0492, 0.0486, 0.0450, 0.0476, 0.0470, 0.0511, 0.0506, 0.0581,
        0.0662, 0.0604, 0.0631, 0.0631, 0.0630, 0.0641, 0.0620, 0.0613, 0.0663,
        0.0712, 0.0656, 0.0675, 0.0693, 0.0714, 0.0704, 0.0728, 0.0782, 0.0818,
        0.0728, 0.0780, 0.0788, 0.0876, 0.0980, 0.0995, 0.1001, 0.1093, 0.1105,
        0.1108, 0.1019, 0.1084, 0.1088, 0.1159, 0.1133, 0.1149, 0.1126, 0.1241,
        0.1340, 0.1483, 0.1479, 0.1411, 0.1355, 0.1288, 0.1136, 0.1119, 0.1079,
        0.0999, 0.1009, 0.1113, 0.1152, 0.1276, 0.1411, 0.1552, 0.1400, 0.1416,
        0.1375, 0.1349, 0.1350, 0.1323, 0.1321, 0.1113, 0.1009, 0.0963, 0.0964,
        0.0946, 0.0957, 0.0965, 0.0922, 0.0871, 0.0811, 0.0753, 0.0752, 0.0732,
        0.0682, 0.0665, 0.0685, 0.0650, 0.0639, 0.0623, 0.0556, 0.0471, 0.0416,
        0.0386, 0.0399, 0.0402, 0.0418, 0.0443, 0.0458, 0.0434, 0.0388, 0.0375,
        0.0376, 0.0406, 0.0406, 0.0348, 0.0349, 0.0362, 0.0410, 0.0427, 0.0441,
        0.0470, 0.0471, 0.0470, 0.0469, 0.0430, 0.0417, 0.0399, 0.0395, 0.0408,
        0.0424, 0.0405, 0.0394, 0.0408, 0.0352, 0.0403, 0.0431, 0.0420, 0.0407,
        0.0414, 0.0433, 0.0418, 0.0399, 0.0396, 0.0389, 0.0383, 0.0389, 0.0352,
        0.0349, 0.0348, 0.0355, 0.0366, 0.0376, 0.0380, 0.0427, 0.0466],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0395, 0.0392, 0.0440, 0.0436, 0.0440, 0.0414, 0.0410, 0.0379, 0.0384,
        0.0400, 0.0426, 0.0441, 0.0439, 0.0430, 0.0401, 0.0413, 0.0415, 0.0410,
        0.0402, 0.0399, 0.0417, 0.0429, 0.0449, 0.0467, 0.0455, 0.0465, 0.0434,
        0.0423, 0.0416, 0.0393, 0.0405, 0.0432, 0.0427, 0.0414, 0.0400, 0.0427,
        0.0450, 0.0415, 0.0429, 0.0419, 0.0414, 0.0407, 0.0412, 0.0409, 0.0418,
        0.0434, 0.0432, 0.0425, 0.0408, 0.0409, 0.0418, 0.0418, 0.0429, 0.0404,
        0.0421, 0.0433, 0.0430, 0.0428, 0.0419, 0.0435, 0.0433, 0.0419, 0.0433,
        0.0426, 0.0431, 0.0424, 0.0424, 0.0430, 0.0419, 0.0411, 0.0418, 0.0405,
        0.0404, 0.0397, 0.0384, 0.0386, 0.0386, 0.0404, 0.0413, 0.0423, 0.0422,
        0.0415, 0.0430, 0.0420, 0.0425, 0.0439, 0.0438, 0.0434, 0.0429, 0.0423,
        0.0429, 0.0429, 0.0422, 0.0406, 0.0402, 0.0418, 0.0427, 0.0426, 0.0416,
        0.0410, 0.0406, 0.0406, 0.0425, 0.0419, 0.0391, 0.0391, 0.0406, 0.0415,
        0.0406, 0.0394, 0.0400, 0.0395, 0.0401, 0.0393, 0.0389, 0.0383, 0.0387,
        0.0381, 0.0387, 0.0400, 0.0387, 0.0361, 0.0406, 0.0393, 0.0390, 0.0388,
        0.0384, 0.0377, 0.0380, 0.0381, 0.0370, 0.0369, 0.0370, 0.0376, 0.0377,
        0.0377, 0.0385, 0.0388, 0.0386, 0.0412, 0.0406, 0.0392, 0.0396, 0.0400,
        0.0398, 0.0388, 0.0372, 0.0369, 0.0377, 0.0389, 0.0397, 0.0406, 0.0394,
        0.0389, 0.0394, 0.0399, 0.0388, 0.0378, 0.0370, 0.0379, 0.0386, 0.0376,
        0.0380, 0.0378, 0.0380, 0.0371, 0.0355, 0.0366, 0.0371, 0.0364, 0.0358,
        0.0362, 0.0361, 0.0363, 0.0367, 0.0404, 0.0375, 0.0362, 0.0374, 0.0390,
        0.0367, 0.0349, 0.0364, 0.0363, 0.0346, 0.0343, 0.0347, 0.0347, 0.0349,
        0.0343, 0.0358, 0.0373, 0.0379, 0.0361, 0.0365, 0.0362, 0.0385, 0.0432,
        0.0432, 0.0423, 0.0417, 0.0391, 0.0392, 0.0431, 0.0486, 0.0418, 0.0448,
        0.0476, 0.0469, 0.0473, 0.0459, 0.0474, 0.0465, 0.0590, 0.0641, 0.0700,
        0.0720, 0.0667, 0.0712, 0.0708, 0.0731, 0.0794, 0.0791, 0.0798, 0.0835,
        0.0869, 0.0826, 0.0821, 0.0826, 0.0806, 0.0805, 0.0865, 0.0960, 0.1032,
        0.0968, 0.1048, 0.1104, 0.1205, 0.1286, 0.1261, 0.1219, 0.1272, 0.1281,
        0.1303, 0.1247, 0.1396, 0.1430, 0.1483, 0.1656, 0.1938, 0.1883, 0.1937,
        0.1700, 0.1659, 0.1535, 0.1579, 0.1474, 0.1370, 0.1247, 0.1158, 0.1232,
        0.1225, 0.1085, 0.1207, 0.1168, 0.1221, 0.1134, 0.1058, 0.0964, 0.0929,
        0.0882, 0.0859, 0.0850, 0.0819, 0.0807, 0.0765, 0.0756, 0.0730, 0.0717,
        0.0689, 0.0699, 0.0707, 0.0673, 0.0662, 0.0617, 0.0561, 0.0539, 0.0526,
        0.0492, 0.0482, 0.0500, 0.0463, 0.0435, 0.0427, 0.0406, 0.0377, 0.0351,
        0.0337, 0.0344, 0.0350, 0.0357, 0.0381, 0.0399, 0.0380, 0.0348, 0.0339,
        0.0341, 0.0371, 0.0380, 0.0351, 0.0391, 0.0387, 0.0428, 0.0419, 0.0426,
        0.0471, 0.0468, 0.0465, 0.0483, 0.0427, 0.0425, 0.0391, 0.0377, 0.0380,
        0.0399, 0.0381, 0.0372, 0.0401, 0.0311, 0.0404, 0.0430, 0.0409, 0.0397,
        0.0385, 0.0389, 0.0390, 0.0381, 0.0378, 0.0383, 0.0386, 0.0387, 0.0349,
        0.0366, 0.0361, 0.0348, 0.0352, 0.0363, 0.0364, 0.0435, 0.0496],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0464, 0.0472, 0.0517, 0.0516, 0.0519, 0.0499, 0.0489, 0.0448, 0.0451,
        0.0472, 0.0506, 0.0526, 0.0524, 0.0510, 0.0474, 0.0485, 0.0487, 0.0477,
        0.0469, 0.0465, 0.0494, 0.0504, 0.0529, 0.0553, 0.0541, 0.0557, 0.0514,
        0.0502, 0.0494, 0.0468, 0.0478, 0.0510, 0.0503, 0.0486, 0.0468, 0.0493,
        0.0524, 0.0486, 0.0499, 0.0486, 0.0479, 0.0472, 0.0477, 0.0474, 0.0484,
        0.0500, 0.0500, 0.0490, 0.0472, 0.0473, 0.0483, 0.0484, 0.0496, 0.0473,
        0.0488, 0.0500, 0.0496, 0.0493, 0.0483, 0.0501, 0.0501, 0.0486, 0.0500,
        0.0493, 0.0498, 0.0491, 0.0491, 0.0498, 0.0487, 0.0481, 0.0488, 0.0474,
        0.0471, 0.0462, 0.0449, 0.0450, 0.0451, 0.0474, 0.0480, 0.0488, 0.0486,
        0.0480, 0.0495, 0.0486, 0.0493, 0.0509, 0.0506, 0.0502, 0.0496, 0.0492,
        0.0497, 0.0496, 0.0488, 0.0470, 0.0468, 0.0481, 0.0492, 0.0491, 0.0481,
        0.0473, 0.0469, 0.0467, 0.0488, 0.0482, 0.0453, 0.0451, 0.0469, 0.0481,
        0.0470, 0.0456, 0.0461, 0.0454, 0.0461, 0.0451, 0.0446, 0.0439, 0.0444,
        0.0439, 0.0445, 0.0458, 0.0446, 0.0423, 0.0468, 0.0455, 0.0449, 0.0445,
        0.0441, 0.0434, 0.0437, 0.0436, 0.0423, 0.0423, 0.0425, 0.0431, 0.0433,
        0.0430, 0.0440, 0.0444, 0.0446, 0.0474, 0.0465, 0.0451, 0.0453, 0.0465,
        0.0463, 0.0447, 0.0430, 0.0427, 0.0435, 0.0447, 0.0457, 0.0466, 0.0450,
        0.0443, 0.0448, 0.0454, 0.0443, 0.0428, 0.0421, 0.0431, 0.0439, 0.0429,
        0.0434, 0.0433, 0.0435, 0.0421, 0.0407, 0.0416, 0.0421, 0.0415, 0.0408,
        0.0414, 0.0414, 0.0417, 0.0421, 0.0463, 0.0429, 0.0417, 0.0429, 0.0440,
        0.0413, 0.0393, 0.0409, 0.0411, 0.0397, 0.0393, 0.0397, 0.0400, 0.0399,
        0.0393, 0.0407, 0.0420, 0.0425, 0.0407, 0.0415, 0.0414, 0.0438, 0.0494,
        0.0491, 0.0479, 0.0474, 0.0445, 0.0450, 0.0490, 0.0539, 0.0474, 0.0498,
        0.0527, 0.0524, 0.0530, 0.0524, 0.0532, 0.0526, 0.0655, 0.0709, 0.0770,
        0.0784, 0.0737, 0.0779, 0.0772, 0.0798, 0.0864, 0.0863, 0.0865, 0.0906,
        0.0943, 0.0901, 0.0892, 0.0896, 0.0873, 0.0872, 0.0928, 0.1020, 0.1093,
        0.1022, 0.1095, 0.1139, 0.1232, 0.1328, 0.1310, 0.1265, 0.1314, 0.1321,
        0.1352, 0.1298, 0.1447, 0.1471, 0.1507, 0.1675, 0.1944, 0.1869, 0.1914,
        0.1692, 0.1660, 0.1546, 0.1597, 0.1488, 0.1397, 0.1295, 0.1214, 0.1300,
        0.1297, 0.1133, 0.1245, 0.1208, 0.1239, 0.1147, 0.1069, 0.0986, 0.0962,
        0.0920, 0.0899, 0.0889, 0.0859, 0.0849, 0.0811, 0.0798, 0.0771, 0.0761,
        0.0734, 0.0747, 0.0755, 0.0722, 0.0710, 0.0662, 0.0597, 0.0570, 0.0553,
        0.0518, 0.0510, 0.0529, 0.0491, 0.0462, 0.0453, 0.0434, 0.0403, 0.0378,
        0.0365, 0.0376, 0.0385, 0.0391, 0.0416, 0.0436, 0.0414, 0.0382, 0.0372,
        0.0374, 0.0406, 0.0414, 0.0387, 0.0435, 0.0432, 0.0479, 0.0466, 0.0475,
        0.0527, 0.0523, 0.0520, 0.0541, 0.0477, 0.0475, 0.0437, 0.0420, 0.0426,
        0.0443, 0.0425, 0.0413, 0.0446, 0.0346, 0.0451, 0.0480, 0.0459, 0.0447,
        0.0436, 0.0442, 0.0443, 0.0433, 0.0426, 0.0427, 0.0430, 0.0430, 0.0391,
        0.0410, 0.0405, 0.0390, 0.0395, 0.0408, 0.0406, 0.0486, 0.0561],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0682, 0.0726, 0.0771, 0.0804, 0.0782, 0.0807, 0.0777, 0.0769, 0.0725,
        0.0733, 0.0788, 0.0825, 0.0819, 0.0777, 0.0720, 0.0738, 0.0753, 0.0730,
        0.0712, 0.0694, 0.0757, 0.0762, 0.0808, 0.0856, 0.0856, 0.0896, 0.0824,
        0.0819, 0.0820, 0.0778, 0.0773, 0.0825, 0.0799, 0.0756, 0.0724, 0.0733,
        0.0790, 0.0738, 0.0731, 0.0707, 0.0691, 0.0684, 0.0689, 0.0682, 0.0693,
        0.0705, 0.0708, 0.0696, 0.0675, 0.0677, 0.0690, 0.0697, 0.0713, 0.0692,
        0.0699, 0.0710, 0.0702, 0.0696, 0.0681, 0.0708, 0.0712, 0.0697, 0.0705,
        0.0700, 0.0710, 0.0705, 0.0706, 0.0715, 0.0708, 0.0706, 0.0723, 0.0709,
        0.0700, 0.0685, 0.0669, 0.0672, 0.0676, 0.0715, 0.0710, 0.0707, 0.0696,
        0.0689, 0.0702, 0.0691, 0.0698, 0.0722, 0.0714, 0.0712, 0.0704, 0.0707,
        0.0708, 0.0704, 0.0693, 0.0669, 0.0674, 0.0679, 0.0694, 0.0694, 0.0682,
        0.0669, 0.0663, 0.0657, 0.0686, 0.0682, 0.0645, 0.0639, 0.0662, 0.0685,
        0.0669, 0.0648, 0.0650, 0.0639, 0.0645, 0.0632, 0.0622, 0.0611, 0.0619,
        0.0618, 0.0626, 0.0639, 0.0629, 0.0607, 0.0650, 0.0639, 0.0623, 0.0618,
        0.0617, 0.0610, 0.0612, 0.0605, 0.0586, 0.0587, 0.0591, 0.0602, 0.0607,
        0.0594, 0.0611, 0.0617, 0.0628, 0.0654, 0.0637, 0.0623, 0.0625, 0.0653,
        0.0646, 0.0618, 0.0601, 0.0600, 0.0605, 0.0624, 0.0643, 0.0652, 0.0619,
        0.0609, 0.0616, 0.0624, 0.0610, 0.0575, 0.0575, 0.0591, 0.0601, 0.0594,
        0.0604, 0.0607, 0.0613, 0.0584, 0.0565, 0.0571, 0.0577, 0.0577, 0.0565,
        0.0574, 0.0575, 0.0580, 0.0590, 0.0666, 0.0611, 0.0590, 0.0595, 0.0584,
        0.0544, 0.0518, 0.0530, 0.0541, 0.0543, 0.0544, 0.0555, 0.0567, 0.0554,
        0.0535, 0.0535, 0.0542, 0.0549, 0.0532, 0.0558, 0.0556, 0.0578, 0.0687,
        0.0682, 0.0650, 0.0653, 0.0613, 0.0633, 0.0678, 0.0692, 0.0615, 0.0649,
        0.0638, 0.0694, 0.0718, 0.0744, 0.0722, 0.0727, 0.0885, 0.0994, 0.1059,
        0.1011, 0.0953, 0.0968, 0.0966, 0.1021, 0.1121, 0.1128, 0.1130, 0.1179,
        0.1219, 0.1175, 0.1139, 0.1131, 0.1092, 0.1095, 0.1149, 0.1246, 0.1334,
        0.1272, 0.1345, 0.1375, 0.1438, 0.1553, 0.1539, 0.1474, 0.1492, 0.1484,
        0.1554, 0.1589, 0.1819, 0.1972, 0.1993, 0.2118, 0.2149, 0.1945, 0.1683,
        0.1729, 0.1564, 0.1364, 0.1303, 0.1284, 0.1307, 0.1341, 0.1334, 0.1466,
        0.1562, 0.1330, 0.1408, 0.1362, 0.1334, 0.1198, 0.1082, 0.0987, 0.0966,
        0.0936, 0.0926, 0.0932, 0.0916, 0.0892, 0.0856, 0.0836, 0.0810, 0.0801,
        0.0776, 0.0793, 0.0800, 0.0772, 0.0767, 0.0727, 0.0660, 0.0618, 0.0588,
        0.0561, 0.0574, 0.0597, 0.0554, 0.0524, 0.0516, 0.0505, 0.0477, 0.0453,
        0.0449, 0.0468, 0.0485, 0.0489, 0.0516, 0.0537, 0.0507, 0.0482, 0.0471,
        0.0466, 0.0502, 0.0517, 0.0490, 0.0591, 0.0582, 0.0631, 0.0606, 0.0621,
        0.0693, 0.0684, 0.0689, 0.0716, 0.0629, 0.0628, 0.0569, 0.0538, 0.0548,
        0.0560, 0.0539, 0.0520, 0.0574, 0.0450, 0.0606, 0.0633, 0.0602, 0.0592,
        0.0584, 0.0583, 0.0587, 0.0582, 0.0563, 0.0546, 0.0556, 0.0560, 0.0516,
        0.0547, 0.0535, 0.0506, 0.0518, 0.0543, 0.0524, 0.0618, 0.0738],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.0602, 0.0631, 0.0687, 0.0709, 0.0706, 0.0722, 0.0696, 0.0645, 0.0622,
        0.0649, 0.0699, 0.0731, 0.0728, 0.0700, 0.0652, 0.0665, 0.0670, 0.0648,
        0.0636, 0.0624, 0.0682, 0.0689, 0.0731, 0.0770, 0.0761, 0.0792, 0.0725,
        0.0714, 0.0709, 0.0672, 0.0675, 0.0723, 0.0707, 0.0673, 0.0641, 0.0659,
        0.0709, 0.0665, 0.0667, 0.0646, 0.0633, 0.0626, 0.0632, 0.0627, 0.0638,
        0.0653, 0.0655, 0.0643, 0.0622, 0.0624, 0.0636, 0.0641, 0.0654, 0.0632,
        0.0642, 0.0654, 0.0649, 0.0643, 0.0630, 0.0654, 0.0658, 0.0643, 0.0653,
        0.0647, 0.0655, 0.0649, 0.0650, 0.0659, 0.0650, 0.0646, 0.0659, 0.0643,
        0.0635, 0.0622, 0.0607, 0.0609, 0.0611, 0.0644, 0.0643, 0.0645, 0.0639,
        0.0633, 0.0647, 0.0638, 0.0647, 0.0670, 0.0662, 0.0659, 0.0651, 0.0653,
        0.0654, 0.0651, 0.0641, 0.0619, 0.0620, 0.0628, 0.0643, 0.0643, 0.0631,
        0.0620, 0.0614, 0.0609, 0.0634, 0.0629, 0.0596, 0.0591, 0.0613, 0.0632,
        0.0618, 0.0599, 0.0601, 0.0592, 0.0598, 0.0586, 0.0577, 0.0567, 0.0574,
        0.0572, 0.0579, 0.0592, 0.0582, 0.0561, 0.0604, 0.0593, 0.0581, 0.0575,
        0.0572, 0.0565, 0.0567, 0.0562, 0.0545, 0.0547, 0.0549, 0.0558, 0.0563,
        0.0551, 0.0565, 0.0571, 0.0581, 0.0610, 0.0596, 0.0581, 0.0583, 0.0608,
        0.0604, 0.0578, 0.0561, 0.0558, 0.0564, 0.0579, 0.0595, 0.0604, 0.0575,
        0.0566, 0.0572, 0.0579, 0.0567, 0.0537, 0.0535, 0.0547, 0.0557, 0.0548,
        0.0557, 0.0559, 0.0561, 0.0538, 0.0523, 0.0528, 0.0534, 0.0533, 0.0524,
        0.0532, 0.0534, 0.0538, 0.0544, 0.0601, 0.0554, 0.0540, 0.0552, 0.0548,
        0.0511, 0.0487, 0.0502, 0.0511, 0.0506, 0.0503, 0.0511, 0.0519, 0.0512,
        0.0500, 0.0507, 0.0517, 0.0521, 0.0503, 0.0523, 0.0525, 0.0545, 0.0624,
        0.0622, 0.0600, 0.0599, 0.0563, 0.0576, 0.0617, 0.0649, 0.0583, 0.0604,
        0.0624, 0.0643, 0.0655, 0.0672, 0.0660, 0.0660, 0.0805, 0.0880, 0.0943,
        0.0929, 0.0888, 0.0919, 0.0913, 0.0949, 0.1030, 0.1035, 0.1033, 0.1081,
        0.1122, 0.1082, 0.1060, 0.1057, 0.1026, 0.1026, 0.1076, 0.1165, 0.1241,
        0.1166, 0.1236, 0.1268, 0.1344, 0.1465, 0.1458, 0.1402, 0.1431, 0.1420,
        0.1463, 0.1428, 0.1590, 0.1619, 0.1619, 0.1878, 0.2080, 0.1853, 0.1755,
        0.1739, 0.1671, 0.1503, 0.1506, 0.1429, 0.1389, 0.1357, 0.1312, 0.1420,
        0.1453, 0.1248, 0.1351, 0.1318, 0.1306, 0.1195, 0.1104, 0.1029, 0.1017,
        0.0984, 0.0966, 0.0959, 0.0937, 0.0918, 0.0885, 0.0864, 0.0836, 0.0829,
        0.0804, 0.0821, 0.0830, 0.0802, 0.0792, 0.0742, 0.0664, 0.0625, 0.0596,
        0.0561, 0.0566, 0.0587, 0.0545, 0.0514, 0.0505, 0.0489, 0.0458, 0.0433,
        0.0427, 0.0446, 0.0463, 0.0468, 0.0492, 0.0512, 0.0483, 0.0454, 0.0443,
        0.0442, 0.0478, 0.0491, 0.0466, 0.0542, 0.0538, 0.0592, 0.0572, 0.0584,
        0.0652, 0.0648, 0.0648, 0.0672, 0.0590, 0.0588, 0.0539, 0.0514, 0.0525,
        0.0539, 0.0517, 0.0499, 0.0545, 0.0422, 0.0559, 0.0594, 0.0570, 0.0561,
        0.0554, 0.0560, 0.0563, 0.0552, 0.0536, 0.0524, 0.0528, 0.0528, 0.0484,
        0.0511, 0.0501, 0.0479, 0.0489, 0.0508, 0.0496, 0.0590, 0.0696],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([-1.2682e-03, -5.2014e-03, -9.4581e-03, -7.2245e-03,  5.1076e-04,
         1.8320e-03,  2.0263e-04, -1.8270e-03, -2.0686e-03, -1.2485e-03,
        -1.1237e-03, -2.9800e-03, -3.0903e-03,  3.8357e-04,  3.4518e-03,
         2.1780e-03,  2.3902e-03,  4.4475e-03,  2.1266e-03,  2.6336e-03,
        -2.5070e-03, -1.8793e-03,  1.8224e-03, -1.9250e-03, -2.5064e-03,
        -4.9064e-03, -3.0122e-03, -4.1620e-03, -4.3255e-03, -7.2082e-03,
        -2.8099e-03, -4.6252e-03, -6.1800e-03, -6.0611e-03, -4.6310e-03,
        -7.0759e-03, -6.7309e-03, -1.1620e-03, -6.7833e-04, -1.4765e-03,
        -2.7569e-03,  6.9886e-06,  5.0674e-04,  3.8326e-03,  6.1731e-03,
         7.1060e-03,  4.7405e-03,  5.6688e-03,  5.2448e-03,  5.4395e-03,
         6.6600e-03,  7.8003e-03,  9.1029e-03,  9.5545e-03,  5.9564e-03,
         6.7544e-03,  6.6479e-03,  6.6615e-03,  6.3387e-03,  5.7497e-03,
         3.7969e-03,  3.3755e-03,  3.6697e-03,  3.2416e-03,  4.2152e-03,
         3.0567e-03,  3.3842e-03,  7.2948e-03,  1.0010e-02,  5.1251e-03,
         4.2622e-03,  3.3778e-03,  4.6963e-03,  6.6570e-03,  7.4282e-03,
         4.6354e-03,  8.0718e-03,  5.5268e-03,  2.0885e-03,  3.9951e-03,
         7.1500e-03,  5.3047e-03,  6.0548e-03,  7.1645e-03,  9.1737e-03,
         7.1093e-03,  6.9303e-03,  7.4901e-03,  8.6867e-03,  9.1660e-03,
         1.0627e-02,  1.1782e-02,  1.1285e-02,  1.0354e-02,  1.1228e-02,
         1.0439e-02,  9.1745e-03,  9.9013e-03,  9.8696e-03,  9.7576e-03,
         8.4246e-03,  8.1251e-03,  1.1834e-02,  1.0426e-02,  1.1122e-02,
         1.4247e-02,  1.5937e-02,  1.5035e-02,  1.3790e-02,  1.5003e-02,
         1.4661e-02,  1.3649e-02,  1.5368e-02,  1.5674e-02,  1.5036e-02,
         1.5699e-02,  1.7357e-02,  1.8788e-02,  1.9484e-02,  1.8689e-02,
         1.9866e-02,  2.3749e-02,  2.1131e-02,  1.9791e-02,  1.8489e-02,
         1.8584e-02,  1.7063e-02,  1.8079e-02,  1.8868e-02,  1.8223e-02,
         1.7137e-02,  1.9924e-02,  1.9307e-02,  1.6951e-02,  1.5524e-02,
         1.4082e-02,  1.3909e-02,  1.7478e-02,  2.2009e-02,  2.4080e-02,
         2.1350e-02,  2.0149e-02,  1.8650e-02,  2.6120e-02,  2.1926e-02,
         2.3859e-02,  2.6364e-02,  2.7653e-02,  2.3277e-02,  2.3325e-02,
         1.9138e-02,  1.8483e-02,  1.5427e-02,  1.5618e-02,  1.5755e-02,
         1.7378e-02,  2.0067e-02,  1.8738e-02,  1.5242e-02,  1.7080e-02,
         1.6833e-02,  1.8323e-02,  1.9660e-02,  1.9179e-02,  1.7488e-02,
         1.7848e-02,  1.6331e-02,  1.7979e-02,  1.9578e-02,  1.7778e-02,
         1.9752e-02,  1.9580e-02,  1.9216e-02,  1.9322e-02,  1.9204e-02,
         2.1050e-02,  2.0176e-02,  2.2861e-02,  2.7804e-02,  2.9497e-02,
         2.6800e-02,  2.7191e-02,  2.5639e-02,  2.7476e-02,  2.3608e-02,
         2.3821e-02,  2.3584e-02,  1.9477e-02,  2.4292e-02,  2.1808e-02,
         2.1290e-02,  2.5343e-02,  2.6117e-02,  2.4678e-02,  2.3081e-02,
         2.5229e-02,  2.7749e-02,  2.3763e-02,  2.0842e-02,  2.3085e-02,
         2.3843e-02,  2.7485e-02,  2.4614e-02,  2.2821e-02,  2.7207e-02,
         2.4378e-02,  2.7350e-02,  2.6359e-02,  2.9142e-02,  3.1564e-02,
         2.4944e-02,  3.1915e-02,  2.9991e-02,  2.9423e-02,  2.6864e-02,
         3.2227e-02,  3.7526e-02,  3.8248e-02,  3.6112e-02,  3.5329e-02,
         3.1778e-02,  3.0582e-02,  2.8715e-02,  2.8233e-02,  2.9904e-02,
         3.1217e-02,  2.9217e-02,  2.6444e-02,  2.8169e-02,  2.0304e-02,
         1.9697e-02,  2.0367e-02,  2.8090e-02,  3.1462e-02,  3.2759e-02,
         3.2509e-02,  2.6564e-02,  2.7760e-02,  3.3002e-02,  3.3878e-02,
         3.3989e-02,  3.4363e-02,  3.3995e-02,  3.6649e-02,  3.3503e-02,
         3.2667e-02,  3.8135e-02,  3.6284e-02,  3.3639e-02,  3.4894e-02,
         3.4204e-02,  3.5449e-02,  3.5944e-02,  3.5364e-02,  3.6682e-02,
         3.5725e-02,  4.1865e-02,  4.1073e-02,  4.5429e-02,  4.4815e-02,
         4.5027e-02,  3.8647e-02,  4.3497e-02,  4.7016e-02,  4.9763e-02,
         5.0616e-02,  4.9466e-02,  4.6820e-02,  4.7226e-02,  5.3151e-02,
         5.8603e-02,  6.2197e-02,  6.3993e-02,  6.4802e-02,  6.4125e-02,
         6.3693e-02,  6.4366e-02,  6.5152e-02,  7.1679e-02,  7.2683e-02,
         7.2432e-02,  7.3301e-02,  7.3998e-02,  7.4803e-02,  7.4487e-02,
         7.6785e-02,  8.0260e-02,  8.0884e-02,  8.6982e-02,  8.9646e-02,
         9.2570e-02,  8.8639e-02,  9.2793e-02,  9.3828e-02,  9.7385e-02,
         9.3832e-02,  9.4836e-02,  8.8470e-02,  9.0127e-02,  8.5868e-02,
         8.6086e-02,  9.0105e-02,  9.4895e-02,  9.5305e-02,  8.5070e-02,
         8.2493e-02,  7.7315e-02,  8.1870e-02,  8.2559e-02,  7.5121e-02,
         7.9823e-02,  7.7596e-02,  7.6368e-02,  7.3876e-02,  6.9672e-02,
         6.5642e-02,  6.6412e-02,  6.7251e-02,  6.8635e-02,  6.8119e-02,
         6.3981e-02,  6.5351e-02,  5.9956e-02,  6.2311e-02,  6.3276e-02,
         5.8675e-02,  6.3358e-02,  6.6256e-02,  5.7378e-02,  5.6049e-02,
         5.0477e-02,  4.7350e-02,  4.5327e-02,  4.2044e-02,  4.2634e-02,
         4.1374e-02,  3.9772e-02,  3.7337e-02,  3.8555e-02,  4.1067e-02,
         4.3099e-02,  4.6238e-02,  4.1434e-02,  4.1504e-02,  4.1157e-02,
         3.7587e-02,  3.2458e-02,  2.8219e-02,  2.9176e-02,  2.8557e-02],
       grad_fn=<SelectBackward>)
warning: file exists
torch.Size([350, 10, 5])
tensor([0.1052, 0.1094, 0.1231, 0.1327, 0.1365, 0.1409, 0.1366, 0.1287, 0.1208,
        0.1247, 0.1339, 0.1421, 0.1433, 0.1377, 0.1290, 0.1314, 0.1329, 0.1267,
        0.1245, 0.1217, 0.1351, 0.1379, 0.1481, 0.1542, 0.1538, 0.1575, 0.1508,
        0.1537, 0.1581, 0.1555, 0.1569, 0.1706, 0.1641, 0.1478, 0.1408, 0.1421,
        0.1540, 0.1464, 0.1357, 0.1264, 0.1226, 0.1198, 0.1187, 0.1180, 0.1191,
        0.1195, 0.1201, 0.1174, 0.1144, 0.1157, 0.1183, 0.1196, 0.1201, 0.1171,
        0.1160, 0.1181, 0.1173, 0.1159, 0.1141, 0.1174, 0.1188, 0.1175, 0.1174,
        0.1177, 0.1189, 0.1194, 0.1208, 0.1222, 0.1222, 0.1232, 0.1257, 0.1245,
        0.1229, 0.1218, 0.1208, 0.1218, 0.1223, 0.1281, 0.1261, 0.1240, 0.1205,
        0.1186, 0.1184, 0.1174, 0.1190, 0.1218, 0.1198, 0.1196, 0.1185, 0.1204,
        0.1205, 0.1194, 0.1176, 0.1140, 0.1156, 0.1146, 0.1169, 0.1175, 0.1157,
        0.1133, 0.1121, 0.1105, 0.1129, 0.1125, 0.1086, 0.1076, 0.1111, 0.1151,
        0.1133, 0.1102, 0.1095, 0.1080, 0.1086, 0.1063, 0.1039, 0.1018, 0.1026,
        0.1026, 0.1040, 0.1053, 0.1042, 0.0997, 0.1009, 0.1012, 0.0994, 0.0995,
        0.0996, 0.0992, 0.0993, 0.0979, 0.0958, 0.0982, 0.0991, 0.1008, 0.1016,
        0.0967, 0.0993, 0.0995, 0.1011, 0.1028, 0.1006, 0.1003, 0.1024, 0.1074,
        0.1042, 0.0997, 0.0979, 0.0980, 0.0977, 0.1012, 0.1057, 0.1114, 0.0987,
        0.0970, 0.0979, 0.0992, 0.0963, 0.0884, 0.0903, 0.0923, 0.0934, 0.0935,
        0.0935, 0.0954, 0.0944, 0.0899, 0.0885, 0.0881, 0.0889, 0.0901, 0.0892,
        0.0910, 0.0914, 0.0921, 0.0941, 0.1039, 0.0925, 0.0893, 0.0929, 0.0864,
        0.0802, 0.0767, 0.0788, 0.0823, 0.0840, 0.0838, 0.0848, 0.0862, 0.0841,
        0.0809, 0.0786, 0.0792, 0.0780, 0.0764, 0.0828, 0.0835, 0.0826, 0.0943,
        0.0964, 0.0918, 0.0916, 0.0863, 0.0896, 0.0940, 0.0948, 0.0899, 0.0896,
        0.0926, 0.0974, 0.0999, 0.1095, 0.1046, 0.1045, 0.1186, 0.1276, 0.1338,
        0.1283, 0.1257, 0.1289, 0.1299, 0.1333, 0.1413, 0.1439, 0.1425, 0.1499,
        0.1544, 0.1524, 0.1467, 0.1452, 0.1404, 0.1411, 0.1427, 0.1500, 0.1566,
        0.1488, 0.1571, 0.1579, 0.1630, 0.1803, 0.1813, 0.1746, 0.1740, 0.1674,
        0.1678, 0.1622, 0.1744, 0.1724, 0.1780, 0.1889, 0.2205, 0.1904, 0.1907,
        0.1901, 0.1884, 0.1754, 0.1743, 0.1767, 0.1758, 0.1671, 0.1668, 0.1816,
        0.1813, 0.1505, 0.1632, 0.1641, 0.1572, 0.1472, 0.1396, 0.1333, 0.1358,
        0.1343, 0.1325, 0.1316, 0.1322, 0.1281, 0.1228, 0.1156, 0.1123, 0.1122,
        0.1116, 0.1148, 0.1159, 0.1138, 0.1126, 0.1087, 0.0981, 0.0909, 0.0847,
        0.0780, 0.0819, 0.0854, 0.0788, 0.0750, 0.0740, 0.0728, 0.0686, 0.0661,
        0.0687, 0.0735, 0.0787, 0.0808, 0.0815, 0.0793, 0.0722, 0.0698, 0.0697,
        0.0689, 0.0733, 0.0780, 0.0747, 0.0954, 0.0931, 0.0973, 0.0929, 0.0950,
        0.1068, 0.1084, 0.1068, 0.1138, 0.0977, 0.0968, 0.0892, 0.0847, 0.0877,
        0.0885, 0.0845, 0.0805, 0.0883, 0.0741, 0.0963, 0.1000, 0.0943, 0.0954,
        0.1006, 0.0987, 0.0966, 0.0984, 0.0957, 0.0875, 0.0861, 0.0866, 0.0795,
        0.0843, 0.0823, 0.0773, 0.0807, 0.0875, 0.0779, 0.0944, 0.1117],
       grad_fn=<SelectBackward>)
