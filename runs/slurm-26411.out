Loading parflow-ml/latest
  Loading requirement: openmpi/gcc/4.1.0 parflow/3.9.0 gdal/3.2.1
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  warnings.warn(
/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/05_utils/sbiutils.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y_out)
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 25/5000 [00:00<00:19, 249.77it/s]Running 5000 simulations.:   1%|          | 51/5000 [00:00<00:19, 249.90it/s]Running 5000 simulations.:   2%|▏         | 77/5000 [00:00<00:19, 250.51it/s]Running 5000 simulations.:   2%|▏         | 103/5000 [00:00<00:19, 250.74it/s]Running 5000 simulations.:   3%|▎         | 128/5000 [00:00<00:19, 250.48it/s]Running 5000 simulations.:   3%|▎         | 154/5000 [00:00<00:19, 250.97it/s]Running 5000 simulations.:   4%|▎         | 180/5000 [00:00<00:19, 251.41it/s]Running 5000 simulations.:   4%|▍         | 206/5000 [00:00<00:19, 251.47it/s]Running 5000 simulations.:   5%|▍         | 232/5000 [00:00<00:18, 251.34it/s]Running 5000 simulations.:   5%|▌         | 257/5000 [00:01<00:18, 250.28it/s]Running 5000 simulations.:   6%|▌         | 282/5000 [00:01<00:18, 250.10it/s]Running 5000 simulations.:   6%|▌         | 307/5000 [00:01<00:18, 249.88it/s]Running 5000 simulations.:   7%|▋         | 333/5000 [00:01<00:18, 250.08it/s]Running 5000 simulations.:   7%|▋         | 359/5000 [00:01<00:18, 250.43it/s]Running 5000 simulations.:   8%|▊         | 384/5000 [00:01<00:18, 250.23it/s]Running 5000 simulations.:   8%|▊         | 409/5000 [00:01<00:18, 250.16it/s]Running 5000 simulations.:   9%|▊         | 434/5000 [00:01<00:18, 246.37it/s]Running 5000 simulations.:   9%|▉         | 459/5000 [00:01<00:18, 247.34it/s]Running 5000 simulations.:  10%|▉         | 484/5000 [00:01<00:18, 247.30it/s]Running 5000 simulations.:  10%|█         | 509/5000 [00:02<00:18, 246.48it/s]Running 5000 simulations.:  11%|█         | 534/5000 [00:02<00:18, 246.76it/s]Running 5000 simulations.:  11%|█         | 559/5000 [00:02<00:17, 247.47it/s]Running 5000 simulations.:  12%|█▏        | 584/5000 [00:02<00:18, 240.85it/s]Running 5000 simulations.:  12%|█▏        | 609/5000 [00:02<00:18, 233.63it/s]Running 5000 simulations.:  13%|█▎        | 633/5000 [00:02<00:19, 229.13it/s]Running 5000 simulations.:  13%|█▎        | 656/5000 [00:02<00:19, 225.01it/s]Running 5000 simulations.:  14%|█▎        | 679/5000 [00:02<00:19, 223.28it/s]Running 5000 simulations.:  14%|█▍        | 702/5000 [00:02<00:19, 222.21it/s]Running 5000 simulations.:  14%|█▍        | 725/5000 [00:02<00:19, 221.34it/s]Running 5000 simulations.:  15%|█▍        | 748/5000 [00:03<00:19, 220.43it/s]Running 5000 simulations.:  15%|█▌        | 771/5000 [00:03<00:19, 219.66it/s]Running 5000 simulations.:  16%|█▌        | 793/5000 [00:03<00:19, 219.43it/s]Running 5000 simulations.:  16%|█▋        | 815/5000 [00:03<00:19, 218.81it/s]Running 5000 simulations.:  17%|█▋        | 837/5000 [00:03<00:19, 218.64it/s]Running 5000 simulations.:  17%|█▋        | 859/5000 [00:03<00:18, 218.85it/s]Running 5000 simulations.:  18%|█▊        | 881/5000 [00:03<00:18, 217.53it/s]Running 5000 simulations.:  18%|█▊        | 903/5000 [00:03<00:18, 215.70it/s]Running 5000 simulations.:  18%|█▊        | 925/5000 [00:03<00:18, 215.85it/s]Running 5000 simulations.:  19%|█▉        | 947/5000 [00:04<00:18, 216.43it/s]Running 5000 simulations.:  19%|█▉        | 969/5000 [00:04<00:18, 216.41it/s]Running 5000 simulations.:  20%|█▉        | 991/5000 [00:04<00:18, 216.36it/s]Running 5000 simulations.:  20%|██        | 1013/5000 [00:04<00:18, 216.18it/s]Running 5000 simulations.:  21%|██        | 1035/5000 [00:04<00:18, 215.36it/s]Running 5000 simulations.:  21%|██        | 1057/5000 [00:04<00:18, 215.93it/s]Running 5000 simulations.:  22%|██▏       | 1079/5000 [00:04<00:18, 216.08it/s]Running 5000 simulations.:  22%|██▏       | 1101/5000 [00:04<00:18, 215.69it/s]Running 5000 simulations.:  22%|██▏       | 1123/5000 [00:04<00:17, 215.62it/s]Running 5000 simulations.:  23%|██▎       | 1145/5000 [00:04<00:17, 215.81it/s]Running 5000 simulations.:  23%|██▎       | 1167/5000 [00:05<00:17, 216.16it/s]Running 5000 simulations.:  24%|██▍       | 1189/5000 [00:05<00:17, 216.52it/s]Running 5000 simulations.:  24%|██▍       | 1211/5000 [00:05<00:17, 216.02it/s]Running 5000 simulations.:  25%|██▍       | 1233/5000 [00:05<00:17, 215.52it/s]Running 5000 simulations.:  25%|██▌       | 1255/5000 [00:05<00:17, 215.51it/s]Running 5000 simulations.:  26%|██▌       | 1277/5000 [00:05<00:17, 215.40it/s]Running 5000 simulations.:  26%|██▌       | 1299/5000 [00:05<00:17, 215.36it/s]Running 5000 simulations.:  26%|██▋       | 1321/5000 [00:05<00:17, 215.95it/s]Running 5000 simulations.:  27%|██▋       | 1343/5000 [00:05<00:16, 215.43it/s]Running 5000 simulations.:  27%|██▋       | 1365/5000 [00:05<00:16, 215.92it/s]Running 5000 simulations.:  28%|██▊       | 1387/5000 [00:06<00:16, 215.65it/s]Running 5000 simulations.:  28%|██▊       | 1409/5000 [00:06<00:16, 215.53it/s]Running 5000 simulations.:  29%|██▊       | 1431/5000 [00:06<00:16, 215.51it/s]Running 5000 simulations.:  29%|██▉       | 1453/5000 [00:06<00:16, 215.04it/s]Running 5000 simulations.:  30%|██▉       | 1475/5000 [00:06<00:16, 214.05it/s]Running 5000 simulations.:  30%|██▉       | 1497/5000 [00:06<00:16, 213.57it/s]Running 5000 simulations.:  30%|███       | 1520/5000 [00:06<00:16, 216.12it/s]Running 5000 simulations.:  31%|███       | 1545/5000 [00:06<00:15, 223.58it/s]Running 5000 simulations.:  31%|███▏      | 1570/5000 [00:06<00:15, 228.37it/s]Running 5000 simulations.:  32%|███▏      | 1595/5000 [00:06<00:14, 232.78it/s]Running 5000 simulations.:  32%|███▏      | 1620/5000 [00:07<00:14, 236.03it/s]Running 5000 simulations.:  33%|███▎      | 1645/5000 [00:07<00:14, 238.61it/s]Running 5000 simulations.:  33%|███▎      | 1670/5000 [00:07<00:13, 239.80it/s]Running 5000 simulations.:  34%|███▍      | 1695/5000 [00:07<00:13, 240.72it/s]Running 5000 simulations.:  34%|███▍      | 1720/5000 [00:07<00:13, 240.41it/s]Running 5000 simulations.:  35%|███▍      | 1745/5000 [00:07<00:13, 240.42it/s]Running 5000 simulations.:  35%|███▌      | 1770/5000 [00:07<00:13, 241.09it/s]Running 5000 simulations.:  36%|███▌      | 1795/5000 [00:07<00:13, 242.05it/s]Running 5000 simulations.:  36%|███▋      | 1820/5000 [00:07<00:13, 242.79it/s]Running 5000 simulations.:  37%|███▋      | 1845/5000 [00:08<00:12, 243.22it/s]Running 5000 simulations.:  37%|███▋      | 1870/5000 [00:08<00:12, 243.43it/s]Running 5000 simulations.:  38%|███▊      | 1895/5000 [00:08<00:12, 243.64it/s]Running 5000 simulations.:  38%|███▊      | 1920/5000 [00:08<00:12, 243.79it/s]Running 5000 simulations.:  39%|███▉      | 1945/5000 [00:08<00:12, 243.75it/s]Running 5000 simulations.:  39%|███▉      | 1970/5000 [00:08<00:12, 243.85it/s]Running 5000 simulations.:  40%|███▉      | 1995/5000 [00:08<00:12, 243.45it/s]Running 5000 simulations.:  40%|████      | 2020/5000 [00:08<00:12, 241.98it/s]Running 5000 simulations.:  41%|████      | 2045/5000 [00:08<00:12, 241.52it/s]Running 5000 simulations.:  41%|████▏     | 2070/5000 [00:08<00:12, 241.83it/s]Running 5000 simulations.:  42%|████▏     | 2095/5000 [00:09<00:12, 241.37it/s]Running 5000 simulations.:  42%|████▏     | 2120/5000 [00:09<00:11, 241.34it/s]Running 5000 simulations.:  43%|████▎     | 2145/5000 [00:09<00:11, 240.82it/s]Running 5000 simulations.:  43%|████▎     | 2170/5000 [00:09<00:11, 240.37it/s]Running 5000 simulations.:  44%|████▍     | 2195/5000 [00:09<00:11, 240.18it/s]Running 5000 simulations.:  44%|████▍     | 2220/5000 [00:09<00:11, 239.62it/s]Running 5000 simulations.:  45%|████▍     | 2245/5000 [00:09<00:11, 240.32it/s]Running 5000 simulations.:  45%|████▌     | 2270/5000 [00:09<00:11, 241.26it/s]Running 5000 simulations.:  46%|████▌     | 2295/5000 [00:09<00:11, 239.82it/s]Running 5000 simulations.:  46%|████▋     | 2319/5000 [00:09<00:11, 239.38it/s]Running 5000 simulations.:  47%|████▋     | 2343/5000 [00:10<00:11, 239.48it/s]Running 5000 simulations.:  47%|████▋     | 2368/5000 [00:10<00:10, 240.56it/s]Running 5000 simulations.:  48%|████▊     | 2393/5000 [00:10<00:10, 240.51it/s]Running 5000 simulations.:  48%|████▊     | 2418/5000 [00:10<00:10, 239.58it/s]Running 5000 simulations.:  49%|████▉     | 2442/5000 [00:10<00:10, 239.05it/s]Running 5000 simulations.:  49%|████▉     | 2466/5000 [00:10<00:10, 238.26it/s]Running 5000 simulations.:  50%|████▉     | 2490/5000 [00:10<00:10, 238.10it/s]Running 5000 simulations.:  50%|█████     | 2515/5000 [00:10<00:10, 238.75it/s]Running 5000 simulations.:  51%|█████     | 2539/5000 [00:10<00:10, 238.94it/s]Running 5000 simulations.:  51%|█████▏    | 2563/5000 [00:11<00:10, 238.51it/s]Running 5000 simulations.:  52%|█████▏    | 2587/5000 [00:11<00:10, 238.82it/s]Running 5000 simulations.:  52%|█████▏    | 2612/5000 [00:11<00:09, 239.72it/s]Running 5000 simulations.:  53%|█████▎    | 2637/5000 [00:11<00:09, 240.09it/s]Running 5000 simulations.:  53%|█████▎    | 2662/5000 [00:11<00:09, 240.62it/s]Running 5000 simulations.:  54%|█████▎    | 2687/5000 [00:11<00:09, 241.03it/s]Running 5000 simulations.:  54%|█████▍    | 2712/5000 [00:11<00:09, 241.17it/s]Running 5000 simulations.:  55%|█████▍    | 2737/5000 [00:11<00:09, 241.36it/s]Running 5000 simulations.:  55%|█████▌    | 2762/5000 [00:11<00:09, 241.25it/s]Running 5000 simulations.:  56%|█████▌    | 2787/5000 [00:11<00:09, 240.55it/s]Running 5000 simulations.:  56%|█████▌    | 2812/5000 [00:12<00:09, 239.50it/s]Running 5000 simulations.:  57%|█████▋    | 2836/5000 [00:12<00:09, 238.15it/s]Running 5000 simulations.:  57%|█████▋    | 2860/5000 [00:12<00:09, 236.29it/s]Running 5000 simulations.:  58%|█████▊    | 2884/5000 [00:12<00:08, 235.54it/s]Running 5000 simulations.:  58%|█████▊    | 2908/5000 [00:12<00:08, 234.56it/s]Running 5000 simulations.:  59%|█████▊    | 2932/5000 [00:12<00:08, 233.24it/s]Running 5000 simulations.:  59%|█████▉    | 2956/5000 [00:12<00:08, 232.57it/s]Running 5000 simulations.:  60%|█████▉    | 2980/5000 [00:12<00:08, 232.12it/s]Running 5000 simulations.:  60%|██████    | 3004/5000 [00:12<00:08, 232.58it/s]Running 5000 simulations.:  61%|██████    | 3028/5000 [00:12<00:08, 233.46it/s]Running 5000 simulations.:  61%|██████    | 3052/5000 [00:13<00:08, 234.16it/s]Running 5000 simulations.:  62%|██████▏   | 3076/5000 [00:13<00:08, 234.79it/s]Running 5000 simulations.:  62%|██████▏   | 3100/5000 [00:13<00:08, 235.07it/s]Running 5000 simulations.:  62%|██████▏   | 3124/5000 [00:13<00:07, 235.51it/s]Running 5000 simulations.:  63%|██████▎   | 3148/5000 [00:13<00:07, 235.67it/s]Running 5000 simulations.:  63%|██████▎   | 3172/5000 [00:13<00:07, 235.84it/s]Running 5000 simulations.:  64%|██████▍   | 3196/5000 [00:13<00:07, 235.58it/s]Running 5000 simulations.:  64%|██████▍   | 3220/5000 [00:13<00:07, 235.41it/s]Running 5000 simulations.:  65%|██████▍   | 3244/5000 [00:13<00:07, 234.78it/s]Running 5000 simulations.:  65%|██████▌   | 3268/5000 [00:13<00:07, 233.71it/s]Running 5000 simulations.:  66%|██████▌   | 3292/5000 [00:14<00:07, 233.66it/s]Running 5000 simulations.:  66%|██████▋   | 3316/5000 [00:14<00:07, 233.75it/s]Running 5000 simulations.:  67%|██████▋   | 3340/5000 [00:14<00:07, 233.67it/s]Running 5000 simulations.:  67%|██████▋   | 3364/5000 [00:14<00:06, 233.91it/s]Running 5000 simulations.:  68%|██████▊   | 3388/5000 [00:14<00:06, 234.43it/s]Running 5000 simulations.:  68%|██████▊   | 3412/5000 [00:14<00:06, 234.56it/s]Running 5000 simulations.:  69%|██████▊   | 3436/5000 [00:14<00:06, 234.82it/s]Running 5000 simulations.:  69%|██████▉   | 3460/5000 [00:14<00:06, 234.94it/s]Running 5000 simulations.:  70%|██████▉   | 3484/5000 [00:14<00:06, 235.03it/s]Running 5000 simulations.:  70%|███████   | 3508/5000 [00:15<00:06, 235.01it/s]Running 5000 simulations.:  71%|███████   | 3532/5000 [00:15<00:06, 234.41it/s]Running 5000 simulations.:  71%|███████   | 3556/5000 [00:15<00:06, 234.17it/s]Running 5000 simulations.:  72%|███████▏  | 3580/5000 [00:15<00:06, 234.25it/s]Running 5000 simulations.:  72%|███████▏  | 3604/5000 [00:15<00:05, 234.41it/s]Running 5000 simulations.:  73%|███████▎  | 3628/5000 [00:15<00:05, 234.36it/s]Running 5000 simulations.:  73%|███████▎  | 3652/5000 [00:15<00:05, 234.16it/s]Running 5000 simulations.:  74%|███████▎  | 3676/5000 [00:15<00:05, 233.23it/s]Running 5000 simulations.:  74%|███████▍  | 3700/5000 [00:15<00:05, 232.48it/s]Running 5000 simulations.:  74%|███████▍  | 3724/5000 [00:15<00:05, 231.96it/s]Running 5000 simulations.:  75%|███████▍  | 3748/5000 [00:16<00:05, 231.58it/s]Running 5000 simulations.:  75%|███████▌  | 3772/5000 [00:16<00:05, 231.29it/s]Running 5000 simulations.:  76%|███████▌  | 3796/5000 [00:16<00:05, 231.33it/s]Running 5000 simulations.:  76%|███████▋  | 3820/5000 [00:16<00:05, 231.90it/s]Running 5000 simulations.:  77%|███████▋  | 3844/5000 [00:16<00:04, 232.90it/s]Running 5000 simulations.:  77%|███████▋  | 3868/5000 [00:16<00:04, 233.38it/s]Running 5000 simulations.:  78%|███████▊  | 3892/5000 [00:16<00:04, 233.40it/s]Running 5000 simulations.:  78%|███████▊  | 3916/5000 [00:16<00:04, 233.72it/s]Running 5000 simulations.:  79%|███████▉  | 3940/5000 [00:16<00:04, 234.10it/s]Running 5000 simulations.:  79%|███████▉  | 3964/5000 [00:16<00:04, 232.76it/s]Running 5000 simulations.:  80%|███████▉  | 3988/5000 [00:17<00:04, 228.02it/s]Running 5000 simulations.:  80%|████████  | 4011/5000 [00:17<00:04, 222.11it/s]Running 5000 simulations.:  81%|████████  | 4034/5000 [00:17<00:04, 218.17it/s]Running 5000 simulations.:  81%|████████  | 4056/5000 [00:17<00:04, 216.42it/s]Running 5000 simulations.:  82%|████████▏ | 4078/5000 [00:17<00:04, 215.33it/s]Running 5000 simulations.:  82%|████████▏ | 4100/5000 [00:17<00:04, 215.06it/s]Running 5000 simulations.:  82%|████████▏ | 4122/5000 [00:17<00:04, 214.75it/s]Running 5000 simulations.:  83%|████████▎ | 4144/5000 [00:17<00:03, 214.22it/s]Running 5000 simulations.:  83%|████████▎ | 4166/5000 [00:17<00:03, 214.07it/s]Running 5000 simulations.:  84%|████████▍ | 4188/5000 [00:18<00:03, 210.05it/s]Running 5000 simulations.:  84%|████████▍ | 4210/5000 [00:18<00:03, 208.81it/s]Running 5000 simulations.:  85%|████████▍ | 4232/5000 [00:18<00:03, 210.12it/s]Running 5000 simulations.:  85%|████████▌ | 4254/5000 [00:18<00:03, 211.31it/s]Running 5000 simulations.:  86%|████████▌ | 4276/5000 [00:18<00:03, 211.53it/s]Running 5000 simulations.:  86%|████████▌ | 4298/5000 [00:18<00:03, 212.23it/s]Running 5000 simulations.:  86%|████████▋ | 4323/5000 [00:18<00:03, 221.43it/s]Running 5000 simulations.:  87%|████████▋ | 4349/5000 [00:18<00:02, 230.79it/s]Running 5000 simulations.:  88%|████████▊ | 4375/5000 [00:18<00:02, 238.48it/s]Running 5000 simulations.:  88%|████████▊ | 4401/5000 [00:18<00:02, 244.01it/s]Running 5000 simulations.:  89%|████████▊ | 4427/5000 [00:19<00:02, 247.72it/s]Running 5000 simulations.:  89%|████████▉ | 4454/5000 [00:19<00:02, 251.36it/s]Running 5000 simulations.:  90%|████████▉ | 4480/5000 [00:19<00:02, 253.35it/s]Running 5000 simulations.:  90%|█████████ | 4507/5000 [00:19<00:01, 255.47it/s]Running 5000 simulations.:  91%|█████████ | 4534/5000 [00:19<00:01, 256.79it/s]Running 5000 simulations.:  91%|█████████ | 4560/5000 [00:19<00:01, 257.51it/s]Running 5000 simulations.:  92%|█████████▏| 4587/5000 [00:19<00:01, 258.88it/s]Running 5000 simulations.:  92%|█████████▏| 4613/5000 [00:19<00:01, 258.52it/s]Running 5000 simulations.:  93%|█████████▎| 4640/5000 [00:19<00:01, 259.82it/s]Running 5000 simulations.:  93%|█████████▎| 4667/5000 [00:19<00:01, 260.28it/s]Running 5000 simulations.:  94%|█████████▍| 4694/5000 [00:20<00:01, 260.84it/s]Running 5000 simulations.:  94%|█████████▍| 4721/5000 [00:20<00:01, 260.23it/s]Running 5000 simulations.:  95%|█████████▍| 4748/5000 [00:20<00:00, 261.22it/s]Running 5000 simulations.:  96%|█████████▌| 4775/5000 [00:20<00:00, 262.02it/s]Running 5000 simulations.:  96%|█████████▌| 4802/5000 [00:20<00:00, 262.27it/s]Running 5000 simulations.:  97%|█████████▋| 4829/5000 [00:20<00:00, 263.07it/s]Running 5000 simulations.:  97%|█████████▋| 4856/5000 [00:20<00:00, 263.33it/s]Running 5000 simulations.:  98%|█████████▊| 4883/5000 [00:20<00:00, 264.05it/s]Running 5000 simulations.:  98%|█████████▊| 4910/5000 [00:20<00:00, 263.20it/s]Running 5000 simulations.:  99%|█████████▊| 4937/5000 [00:20<00:00, 262.59it/s]Running 5000 simulations.:  99%|█████████▉| 4964/5000 [00:21<00:00, 261.17it/s]Running 5000 simulations.: 100%|█████████▉| 4991/5000 [00:21<00:00, 261.08it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:21<00:00, 235.38it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:32, 152.08it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:32, 153.07it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:32, 153.08it/s]Running 5000 simulations.:   1%|▏         | 64/5000 [00:00<00:32, 153.42it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:32, 152.47it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:32, 151.97it/s]Running 5000 simulations.:   2%|▏         | 112/5000 [00:00<00:32, 151.73it/s]Running 5000 simulations.:   3%|▎         | 127/5000 [00:00<00:32, 150.97it/s]Running 5000 simulations.:   3%|▎         | 143/5000 [00:00<00:32, 150.76it/s]Running 5000 simulations.:   3%|▎         | 159/5000 [00:01<00:32, 150.69it/s]Running 5000 simulations.:   4%|▎         | 175/5000 [00:01<00:32, 150.65it/s]Running 5000 simulations.:   4%|▍         | 191/5000 [00:01<00:31, 152.27it/s]Running 5000 simulations.:   4%|▍         | 207/5000 [00:01<00:31, 153.07it/s]Running 5000 simulations.:   4%|▍         | 223/5000 [00:01<00:31, 153.72it/s]Running 5000 simulations.:   5%|▍         | 239/5000 [00:01<00:31, 153.23it/s]Running 5000 simulations.:   5%|▌         | 255/5000 [00:01<00:30, 153.11it/s]Running 5000 simulations.:   5%|▌         | 271/5000 [00:01<00:30, 153.13it/s]Running 5000 simulations.:   6%|▌         | 287/5000 [00:01<00:30, 152.73it/s]Running 5000 simulations.:   6%|▌         | 303/5000 [00:01<00:30, 153.38it/s]Running 5000 simulations.:   6%|▋         | 319/5000 [00:02<00:30, 153.55it/s]Running 5000 simulations.:   7%|▋         | 335/5000 [00:02<00:30, 152.86it/s]Running 5000 simulations.:   7%|▋         | 351/5000 [00:02<00:30, 151.80it/s]Running 5000 simulations.:   7%|▋         | 367/5000 [00:02<00:30, 150.61it/s]Running 5000 simulations.:   8%|▊         | 383/5000 [00:02<00:30, 149.70it/s]Running 5000 simulations.:   8%|▊         | 398/5000 [00:02<00:30, 148.81it/s]Running 5000 simulations.:   8%|▊         | 413/5000 [00:02<00:30, 148.81it/s]Running 5000 simulations.:   9%|▊         | 428/5000 [00:02<00:30, 148.54it/s]Running 5000 simulations.:   9%|▉         | 443/5000 [00:02<00:30, 148.76it/s]Running 5000 simulations.:   9%|▉         | 458/5000 [00:03<00:30, 148.73it/s]Running 5000 simulations.:   9%|▉         | 473/5000 [00:03<00:30, 148.61it/s]Running 5000 simulations.:  10%|▉         | 488/5000 [00:03<00:30, 148.56it/s]Running 5000 simulations.:  10%|█         | 503/5000 [00:03<00:30, 148.87it/s]Running 5000 simulations.:  10%|█         | 518/5000 [00:03<00:30, 149.14it/s]Running 5000 simulations.:  11%|█         | 533/5000 [00:03<00:30, 148.61it/s]Running 5000 simulations.:  11%|█         | 548/5000 [00:03<00:29, 148.91it/s]Running 5000 simulations.:  11%|█▏        | 563/5000 [00:03<00:29, 148.76it/s]Running 5000 simulations.:  12%|█▏        | 578/5000 [00:03<00:29, 148.55it/s]Running 5000 simulations.:  12%|█▏        | 593/5000 [00:03<00:29, 148.48it/s]Running 5000 simulations.:  12%|█▏        | 608/5000 [00:04<00:29, 148.23it/s]Running 5000 simulations.:  12%|█▏        | 623/5000 [00:04<00:29, 148.08it/s]Running 5000 simulations.:  13%|█▎        | 638/5000 [00:04<00:29, 148.36it/s]Running 5000 simulations.:  13%|█▎        | 653/5000 [00:04<00:29, 148.54it/s]Running 5000 simulations.:  13%|█▎        | 668/5000 [00:04<00:29, 148.04it/s]Running 5000 simulations.:  14%|█▎        | 683/5000 [00:04<00:29, 147.88it/s]Running 5000 simulations.:  14%|█▍        | 698/5000 [00:04<00:29, 147.68it/s]Running 5000 simulations.:  14%|█▍        | 713/5000 [00:04<00:29, 147.49it/s]Running 5000 simulations.:  15%|█▍        | 728/5000 [00:04<00:28, 147.65it/s]Running 5000 simulations.:  15%|█▍        | 743/5000 [00:04<00:28, 147.84it/s]Running 5000 simulations.:  15%|█▌        | 758/5000 [00:05<00:28, 147.66it/s]Running 5000 simulations.:  15%|█▌        | 773/5000 [00:05<00:28, 147.92it/s]Running 5000 simulations.:  16%|█▌        | 788/5000 [00:05<00:28, 147.84it/s]Running 5000 simulations.:  16%|█▌        | 803/5000 [00:05<00:28, 147.89it/s]Running 5000 simulations.:  16%|█▋        | 818/5000 [00:05<00:28, 147.49it/s]Running 5000 simulations.:  17%|█▋        | 833/5000 [00:05<00:28, 147.17it/s]Running 5000 simulations.:  17%|█▋        | 848/5000 [00:05<00:28, 147.06it/s]Running 5000 simulations.:  17%|█▋        | 863/5000 [00:05<00:28, 146.78it/s]Running 5000 simulations.:  18%|█▊        | 878/5000 [00:05<00:28, 147.11it/s]Running 5000 simulations.:  18%|█▊        | 893/5000 [00:05<00:27, 147.50it/s]Running 5000 simulations.:  18%|█▊        | 908/5000 [00:06<00:27, 147.59it/s]Running 5000 simulations.:  18%|█▊        | 923/5000 [00:06<00:27, 147.84it/s]Running 5000 simulations.:  19%|█▉        | 938/5000 [00:06<00:27, 147.87it/s]Running 5000 simulations.:  19%|█▉        | 953/5000 [00:06<00:27, 147.55it/s]Running 5000 simulations.:  19%|█▉        | 968/5000 [00:06<00:27, 147.75it/s]Running 5000 simulations.:  20%|█▉        | 983/5000 [00:06<00:27, 147.93it/s]Running 5000 simulations.:  20%|█▉        | 998/5000 [00:06<00:27, 148.13it/s]Running 5000 simulations.:  20%|██        | 1013/5000 [00:06<00:26, 148.15it/s]Running 5000 simulations.:  21%|██        | 1028/5000 [00:06<00:26, 147.90it/s]Running 5000 simulations.:  21%|██        | 1043/5000 [00:06<00:26, 148.05it/s]Running 5000 simulations.:  21%|██        | 1058/5000 [00:07<00:26, 148.07it/s]Running 5000 simulations.:  21%|██▏       | 1073/5000 [00:07<00:26, 147.88it/s]Running 5000 simulations.:  22%|██▏       | 1088/5000 [00:07<00:26, 147.32it/s]Running 5000 simulations.:  22%|██▏       | 1103/5000 [00:07<00:26, 147.14it/s]Running 5000 simulations.:  22%|██▏       | 1118/5000 [00:07<00:26, 147.26it/s]Running 5000 simulations.:  23%|██▎       | 1133/5000 [00:07<00:26, 147.46it/s]Running 5000 simulations.:  23%|██▎       | 1148/5000 [00:07<00:26, 147.53it/s]Running 5000 simulations.:  23%|██▎       | 1163/5000 [00:07<00:26, 147.50it/s]Running 5000 simulations.:  24%|██▎       | 1178/5000 [00:07<00:25, 147.18it/s]Running 5000 simulations.:  24%|██▍       | 1193/5000 [00:07<00:25, 147.18it/s]Running 5000 simulations.:  24%|██▍       | 1208/5000 [00:08<00:25, 147.20it/s]Running 5000 simulations.:  24%|██▍       | 1223/5000 [00:08<00:25, 147.01it/s]Running 5000 simulations.:  25%|██▍       | 1238/5000 [00:08<00:25, 147.22it/s]Running 5000 simulations.:  25%|██▌       | 1253/5000 [00:08<00:25, 147.18it/s]Running 5000 simulations.:  25%|██▌       | 1268/5000 [00:08<00:25, 147.39it/s]Running 5000 simulations.:  26%|██▌       | 1283/5000 [00:08<00:25, 147.54it/s]Running 5000 simulations.:  26%|██▌       | 1298/5000 [00:08<00:25, 147.16it/s]Running 5000 simulations.:  26%|██▋       | 1313/5000 [00:08<00:25, 147.00it/s]Running 5000 simulations.:  27%|██▋       | 1328/5000 [00:08<00:25, 146.53it/s]Running 5000 simulations.:  27%|██▋       | 1343/5000 [00:09<00:24, 146.60it/s]Running 5000 simulations.:  27%|██▋       | 1358/5000 [00:09<00:24, 146.81it/s]Running 5000 simulations.:  27%|██▋       | 1373/5000 [00:09<00:24, 146.85it/s]Running 5000 simulations.:  28%|██▊       | 1388/5000 [00:09<00:24, 146.99it/s]Running 5000 simulations.:  28%|██▊       | 1403/5000 [00:09<00:24, 146.61it/s]Running 5000 simulations.:  28%|██▊       | 1418/5000 [00:09<00:24, 146.41it/s]Running 5000 simulations.:  29%|██▊       | 1433/5000 [00:09<00:24, 147.38it/s]Running 5000 simulations.:  29%|██▉       | 1448/5000 [00:09<00:23, 148.05it/s]Running 5000 simulations.:  29%|██▉       | 1463/5000 [00:09<00:23, 148.57it/s]Running 5000 simulations.:  30%|██▉       | 1478/5000 [00:09<00:23, 148.89it/s]Running 5000 simulations.:  30%|██▉       | 1494/5000 [00:10<00:23, 149.29it/s]Running 5000 simulations.:  30%|███       | 1510/5000 [00:10<00:23, 149.57it/s]Running 5000 simulations.:  31%|███       | 1526/5000 [00:10<00:23, 150.16it/s]Running 5000 simulations.:  31%|███       | 1542/5000 [00:10<00:23, 150.24it/s]Running 5000 simulations.:  31%|███       | 1558/5000 [00:10<00:22, 149.81it/s]Running 5000 simulations.:  31%|███▏      | 1573/5000 [00:10<00:22, 149.69it/s]Running 5000 simulations.:  32%|███▏      | 1588/5000 [00:10<00:22, 148.92it/s]Running 5000 simulations.:  32%|███▏      | 1603/5000 [00:10<00:22, 148.30it/s]Running 5000 simulations.:  32%|███▏      | 1618/5000 [00:10<00:22, 147.69it/s]Running 5000 simulations.:  33%|███▎      | 1633/5000 [00:10<00:22, 147.36it/s]Running 5000 simulations.:  33%|███▎      | 1648/5000 [00:11<00:22, 147.76it/s]Running 5000 simulations.:  33%|███▎      | 1663/5000 [00:11<00:22, 147.64it/s]Running 5000 simulations.:  34%|███▎      | 1678/5000 [00:11<00:22, 147.68it/s]Running 5000 simulations.:  34%|███▍      | 1693/5000 [00:11<00:22, 147.99it/s]Running 5000 simulations.:  34%|███▍      | 1708/5000 [00:11<00:22, 147.80it/s]Running 5000 simulations.:  34%|███▍      | 1723/5000 [00:11<00:22, 147.75it/s]Running 5000 simulations.:  35%|███▍      | 1738/5000 [00:11<00:22, 147.80it/s]Running 5000 simulations.:  35%|███▌      | 1753/5000 [00:11<00:21, 147.97it/s]Running 5000 simulations.:  35%|███▌      | 1768/5000 [00:11<00:21, 147.73it/s]Running 5000 simulations.:  36%|███▌      | 1783/5000 [00:11<00:21, 148.24it/s]Running 5000 simulations.:  36%|███▌      | 1798/5000 [00:12<00:21, 148.01it/s]Running 5000 simulations.:  36%|███▋      | 1813/5000 [00:12<00:21, 147.86it/s]Running 5000 simulations.:  37%|███▋      | 1828/5000 [00:12<00:21, 147.93it/s]Running 5000 simulations.:  37%|███▋      | 1843/5000 [00:12<00:21, 147.38it/s]Running 5000 simulations.:  37%|███▋      | 1858/5000 [00:12<00:21, 147.51it/s]Running 5000 simulations.:  37%|███▋      | 1873/5000 [00:12<00:21, 147.33it/s]Running 5000 simulations.:  38%|███▊      | 1888/5000 [00:12<00:21, 147.61it/s]Running 5000 simulations.:  38%|███▊      | 1903/5000 [00:12<00:20, 147.92it/s]Running 5000 simulations.:  38%|███▊      | 1918/5000 [00:12<00:20, 147.96it/s]Running 5000 simulations.:  39%|███▊      | 1933/5000 [00:13<00:20, 147.78it/s]Running 5000 simulations.:  39%|███▉      | 1948/5000 [00:13<00:20, 147.78it/s]Running 5000 simulations.:  39%|███▉      | 1963/5000 [00:13<00:20, 147.81it/s]Running 5000 simulations.:  40%|███▉      | 1978/5000 [00:13<00:20, 147.85it/s]Running 5000 simulations.:  40%|███▉      | 1993/5000 [00:13<00:20, 147.83it/s]Running 5000 simulations.:  40%|████      | 2008/5000 [00:13<00:20, 147.73it/s]Running 5000 simulations.:  40%|████      | 2023/5000 [00:13<00:20, 147.57it/s]Running 5000 simulations.:  41%|████      | 2038/5000 [00:13<00:20, 147.15it/s]Running 5000 simulations.:  41%|████      | 2053/5000 [00:13<00:20, 147.06it/s]Running 5000 simulations.:  41%|████▏     | 2068/5000 [00:13<00:19, 146.96it/s]Running 5000 simulations.:  42%|████▏     | 2083/5000 [00:14<00:19, 147.29it/s]Running 5000 simulations.:  42%|████▏     | 2098/5000 [00:14<00:19, 147.32it/s]Running 5000 simulations.:  42%|████▏     | 2113/5000 [00:14<00:19, 147.50it/s]Running 5000 simulations.:  43%|████▎     | 2128/5000 [00:14<00:19, 147.91it/s]Running 5000 simulations.:  43%|████▎     | 2143/5000 [00:14<00:19, 147.82it/s]Running 5000 simulations.:  43%|████▎     | 2158/5000 [00:14<00:19, 147.33it/s]Running 5000 simulations.:  43%|████▎     | 2173/5000 [00:14<00:19, 147.77it/s]Running 5000 simulations.:  44%|████▍     | 2188/5000 [00:14<00:18, 148.03it/s]Running 5000 simulations.:  44%|████▍     | 2203/5000 [00:14<00:18, 148.13it/s]Running 5000 simulations.:  44%|████▍     | 2218/5000 [00:14<00:18, 148.04it/s]Running 5000 simulations.:  45%|████▍     | 2233/5000 [00:15<00:18, 148.28it/s]Running 5000 simulations.:  45%|████▍     | 2248/5000 [00:15<00:18, 147.65it/s]Running 5000 simulations.:  45%|████▌     | 2263/5000 [00:15<00:18, 147.30it/s]Running 5000 simulations.:  46%|████▌     | 2278/5000 [00:15<00:18, 146.87it/s]Running 5000 simulations.:  46%|████▌     | 2293/5000 [00:15<00:18, 147.21it/s]Running 5000 simulations.:  46%|████▌     | 2308/5000 [00:15<00:18, 147.06it/s]Running 5000 simulations.:  46%|████▋     | 2323/5000 [00:15<00:18, 147.23it/s]Running 5000 simulations.:  47%|████▋     | 2339/5000 [00:15<00:17, 148.26it/s]Running 5000 simulations.:  47%|████▋     | 2354/5000 [00:15<00:17, 147.88it/s]Running 5000 simulations.:  47%|████▋     | 2369/5000 [00:15<00:17, 147.71it/s]Running 5000 simulations.:  48%|████▊     | 2384/5000 [00:16<00:17, 147.58it/s]Running 5000 simulations.:  48%|████▊     | 2399/5000 [00:16<00:17, 147.46it/s]Running 5000 simulations.:  48%|████▊     | 2414/5000 [00:16<00:17, 147.87it/s]Running 5000 simulations.:  49%|████▊     | 2429/5000 [00:16<00:17, 147.96it/s]Running 5000 simulations.:  49%|████▉     | 2444/5000 [00:16<00:17, 147.38it/s]Running 5000 simulations.:  49%|████▉     | 2459/5000 [00:16<00:17, 147.32it/s]Running 5000 simulations.:  49%|████▉     | 2474/5000 [00:16<00:17, 147.01it/s]Running 5000 simulations.:  50%|████▉     | 2489/5000 [00:16<00:17, 146.88it/s]Running 5000 simulations.:  50%|█████     | 2504/5000 [00:16<00:16, 147.17it/s]Running 5000 simulations.:  50%|█████     | 2519/5000 [00:16<00:16, 146.89it/s]Running 5000 simulations.:  51%|█████     | 2534/5000 [00:17<00:16, 147.17it/s]Running 5000 simulations.:  51%|█████     | 2549/5000 [00:17<00:16, 147.63it/s]Running 5000 simulations.:  51%|█████▏    | 2564/5000 [00:17<00:16, 147.53it/s]Running 5000 simulations.:  52%|█████▏    | 2579/5000 [00:17<00:16, 147.71it/s]Running 5000 simulations.:  52%|█████▏    | 2594/5000 [00:17<00:16, 147.36it/s]Running 5000 simulations.:  52%|█████▏    | 2609/5000 [00:17<00:16, 147.19it/s]Running 5000 simulations.:  52%|█████▏    | 2624/5000 [00:17<00:16, 147.28it/s]Running 5000 simulations.:  53%|█████▎    | 2639/5000 [00:17<00:16, 145.31it/s]Running 5000 simulations.:  53%|█████▎    | 2654/5000 [00:17<00:16, 143.57it/s]Running 5000 simulations.:  53%|█████▎    | 2669/5000 [00:18<00:16, 142.51it/s]Running 5000 simulations.:  54%|█████▎    | 2684/5000 [00:18<00:16, 141.90it/s]Running 5000 simulations.:  54%|█████▍    | 2699/5000 [00:18<00:16, 141.44it/s]Running 5000 simulations.:  54%|█████▍    | 2714/5000 [00:18<00:16, 140.93it/s]Running 5000 simulations.:  55%|█████▍    | 2729/5000 [00:18<00:16, 140.33it/s]Running 5000 simulations.:  55%|█████▍    | 2744/5000 [00:18<00:16, 140.06it/s]Running 5000 simulations.:  55%|█████▌    | 2759/5000 [00:18<00:15, 140.16it/s]Running 5000 simulations.:  55%|█████▌    | 2774/5000 [00:18<00:15, 140.18it/s]Running 5000 simulations.:  56%|█████▌    | 2789/5000 [00:18<00:15, 139.92it/s]Running 5000 simulations.:  56%|█████▌    | 2803/5000 [00:18<00:15, 139.63it/s]Running 5000 simulations.:  56%|█████▋    | 2817/5000 [00:19<00:15, 139.35it/s]Running 5000 simulations.:  57%|█████▋    | 2832/5000 [00:19<00:15, 139.64it/s]Running 5000 simulations.:  57%|█████▋    | 2846/5000 [00:19<00:15, 139.72it/s]Running 5000 simulations.:  57%|█████▋    | 2861/5000 [00:19<00:15, 140.05it/s]Running 5000 simulations.:  58%|█████▊    | 2876/5000 [00:19<00:14, 142.28it/s]Running 5000 simulations.:  58%|█████▊    | 2891/5000 [00:19<00:14, 143.62it/s]Running 5000 simulations.:  58%|█████▊    | 2906/5000 [00:19<00:14, 144.40it/s]Running 5000 simulations.:  58%|█████▊    | 2921/5000 [00:19<00:14, 145.47it/s]Running 5000 simulations.:  59%|█████▊    | 2936/5000 [00:19<00:14, 146.40it/s]Running 5000 simulations.:  59%|█████▉    | 2951/5000 [00:19<00:13, 146.45it/s]Running 5000 simulations.:  59%|█████▉    | 2966/5000 [00:20<00:13, 146.66it/s]Running 5000 simulations.:  60%|█████▉    | 2981/5000 [00:20<00:13, 147.04it/s]Running 5000 simulations.:  60%|█████▉    | 2996/5000 [00:20<00:13, 147.09it/s]Running 5000 simulations.:  60%|██████    | 3011/5000 [00:20<00:13, 146.76it/s]Running 5000 simulations.:  61%|██████    | 3026/5000 [00:20<00:14, 139.33it/s]Running 5000 simulations.:  61%|██████    | 3041/5000 [00:20<00:13, 141.69it/s]Running 5000 simulations.:  61%|██████    | 3056/5000 [00:20<00:13, 143.23it/s]Running 5000 simulations.:  61%|██████▏   | 3071/5000 [00:20<00:13, 144.30it/s]Running 5000 simulations.:  62%|██████▏   | 3086/5000 [00:20<00:13, 145.36it/s]Running 5000 simulations.:  62%|██████▏   | 3101/5000 [00:21<00:12, 146.14it/s]Running 5000 simulations.:  62%|██████▏   | 3116/5000 [00:21<00:12, 146.09it/s]Running 5000 simulations.:  63%|██████▎   | 3131/5000 [00:21<00:12, 146.65it/s]Running 5000 simulations.:  63%|██████▎   | 3146/5000 [00:21<00:12, 146.79it/s]Running 5000 simulations.:  63%|██████▎   | 3161/5000 [00:21<00:12, 146.90it/s]Running 5000 simulations.:  64%|██████▎   | 3176/5000 [00:21<00:12, 146.78it/s]Running 5000 simulations.:  64%|██████▍   | 3191/5000 [00:21<00:12, 146.57it/s]Running 5000 simulations.:  64%|██████▍   | 3206/5000 [00:21<00:12, 146.46it/s]Running 5000 simulations.:  64%|██████▍   | 3221/5000 [00:21<00:12, 146.69it/s]Running 5000 simulations.:  65%|██████▍   | 3236/5000 [00:21<00:12, 146.97it/s]Running 5000 simulations.:  65%|██████▌   | 3251/5000 [00:22<00:11, 147.14it/s]Running 5000 simulations.:  65%|██████▌   | 3266/5000 [00:22<00:11, 147.37it/s]Running 5000 simulations.:  66%|██████▌   | 3281/5000 [00:22<00:11, 147.69it/s]Running 5000 simulations.:  66%|██████▌   | 3296/5000 [00:22<00:11, 147.37it/s]Running 5000 simulations.:  66%|██████▌   | 3311/5000 [00:22<00:11, 147.45it/s]Running 5000 simulations.:  67%|██████▋   | 3326/5000 [00:22<00:11, 147.43it/s]Running 5000 simulations.:  67%|██████▋   | 3341/5000 [00:22<00:11, 147.09it/s]Running 5000 simulations.:  67%|██████▋   | 3356/5000 [00:22<00:11, 147.08it/s]Running 5000 simulations.:  67%|██████▋   | 3371/5000 [00:22<00:11, 147.11it/s]Running 5000 simulations.:  68%|██████▊   | 3386/5000 [00:22<00:10, 147.22it/s]Running 5000 simulations.:  68%|██████▊   | 3401/5000 [00:23<00:10, 147.27it/s]Running 5000 simulations.:  68%|██████▊   | 3416/5000 [00:23<00:10, 147.77it/s]Running 5000 simulations.:  69%|██████▊   | 3431/5000 [00:23<00:10, 147.77it/s]Running 5000 simulations.:  69%|██████▉   | 3446/5000 [00:23<00:10, 147.64it/s]Running 5000 simulations.:  69%|██████▉   | 3461/5000 [00:23<00:10, 147.53it/s]Running 5000 simulations.:  70%|██████▉   | 3476/5000 [00:23<00:10, 147.47it/s]Running 5000 simulations.:  70%|██████▉   | 3491/5000 [00:23<00:10, 147.49it/s]Running 5000 simulations.:  70%|███████   | 3506/5000 [00:23<00:10, 147.47it/s]Running 5000 simulations.:  70%|███████   | 3521/5000 [00:23<00:10, 147.40it/s]Running 5000 simulations.:  71%|███████   | 3536/5000 [00:23<00:09, 147.37it/s]Running 5000 simulations.:  71%|███████   | 3551/5000 [00:24<00:09, 147.72it/s]Running 5000 simulations.:  71%|███████▏  | 3566/5000 [00:24<00:09, 147.73it/s]Running 5000 simulations.:  72%|███████▏  | 3581/5000 [00:24<00:09, 147.42it/s]Running 5000 simulations.:  72%|███████▏  | 3596/5000 [00:24<00:09, 147.67it/s]Running 5000 simulations.:  72%|███████▏  | 3611/5000 [00:24<00:09, 147.63it/s]Running 5000 simulations.:  73%|███████▎  | 3626/5000 [00:24<00:09, 147.74it/s]Running 5000 simulations.:  73%|███████▎  | 3641/5000 [00:24<00:09, 147.63it/s]Running 5000 simulations.:  73%|███████▎  | 3656/5000 [00:24<00:09, 147.72it/s]Running 5000 simulations.:  73%|███████▎  | 3671/5000 [00:24<00:09, 147.22it/s]Running 5000 simulations.:  74%|███████▎  | 3686/5000 [00:25<00:08, 146.82it/s]Running 5000 simulations.:  74%|███████▍  | 3701/5000 [00:25<00:08, 146.51it/s]Running 5000 simulations.:  74%|███████▍  | 3716/5000 [00:25<00:08, 146.63it/s]Running 5000 simulations.:  75%|███████▍  | 3731/5000 [00:25<00:08, 146.98it/s]Running 5000 simulations.:  75%|███████▍  | 3746/5000 [00:25<00:08, 147.25it/s]Running 5000 simulations.:  75%|███████▌  | 3761/5000 [00:25<00:08, 147.50it/s]Running 5000 simulations.:  76%|███████▌  | 3776/5000 [00:25<00:08, 147.45it/s]Running 5000 simulations.:  76%|███████▌  | 3791/5000 [00:25<00:08, 147.88it/s]Running 5000 simulations.:  76%|███████▌  | 3806/5000 [00:25<00:08, 147.58it/s]Running 5000 simulations.:  76%|███████▋  | 3821/5000 [00:25<00:07, 147.53it/s]Running 5000 simulations.:  77%|███████▋  | 3836/5000 [00:26<00:07, 147.75it/s]Running 5000 simulations.:  77%|███████▋  | 3851/5000 [00:26<00:07, 147.22it/s]Running 5000 simulations.:  77%|███████▋  | 3866/5000 [00:26<00:07, 146.88it/s]Running 5000 simulations.:  78%|███████▊  | 3881/5000 [00:26<00:07, 147.16it/s]Running 5000 simulations.:  78%|███████▊  | 3896/5000 [00:26<00:07, 147.19it/s]Running 5000 simulations.:  78%|███████▊  | 3911/5000 [00:26<00:07, 147.20it/s]Running 5000 simulations.:  79%|███████▊  | 3926/5000 [00:26<00:07, 147.52it/s]Running 5000 simulations.:  79%|███████▉  | 3941/5000 [00:26<00:07, 147.65it/s]Running 5000 simulations.:  79%|███████▉  | 3956/5000 [00:26<00:07, 148.16it/s]Running 5000 simulations.:  79%|███████▉  | 3971/5000 [00:26<00:06, 148.31it/s]Running 5000 simulations.:  80%|███████▉  | 3986/5000 [00:27<00:06, 148.16it/s]Running 5000 simulations.:  80%|████████  | 4001/5000 [00:27<00:06, 147.79it/s]Running 5000 simulations.:  80%|████████  | 4016/5000 [00:27<00:06, 147.83it/s]Running 5000 simulations.:  81%|████████  | 4031/5000 [00:27<00:06, 147.57it/s]Running 5000 simulations.:  81%|████████  | 4046/5000 [00:27<00:06, 147.80it/s]Running 5000 simulations.:  81%|████████  | 4061/5000 [00:27<00:06, 148.35it/s]Running 5000 simulations.:  82%|████████▏ | 4076/5000 [00:27<00:06, 148.51it/s]Running 5000 simulations.:  82%|████████▏ | 4091/5000 [00:27<00:06, 148.70it/s]Running 5000 simulations.:  82%|████████▏ | 4106/5000 [00:27<00:06, 148.59it/s]Running 5000 simulations.:  82%|████████▏ | 4121/5000 [00:27<00:05, 148.34it/s]Running 5000 simulations.:  83%|████████▎ | 4136/5000 [00:28<00:05, 147.75it/s]Running 5000 simulations.:  83%|████████▎ | 4151/5000 [00:28<00:05, 147.53it/s]Running 5000 simulations.:  83%|████████▎ | 4166/5000 [00:28<00:05, 147.17it/s]Running 5000 simulations.:  84%|████████▎ | 4181/5000 [00:28<00:05, 147.02it/s]Running 5000 simulations.:  84%|████████▍ | 4196/5000 [00:28<00:05, 147.01it/s]Running 5000 simulations.:  84%|████████▍ | 4211/5000 [00:28<00:05, 146.98it/s]Running 5000 simulations.:  85%|████████▍ | 4226/5000 [00:28<00:05, 147.13it/s]Running 5000 simulations.:  85%|████████▍ | 4241/5000 [00:28<00:05, 147.57it/s]Running 5000 simulations.:  85%|████████▌ | 4256/5000 [00:28<00:05, 147.58it/s]Running 5000 simulations.:  85%|████████▌ | 4271/5000 [00:28<00:04, 146.79it/s]Running 5000 simulations.:  86%|████████▌ | 4286/5000 [00:29<00:04, 143.95it/s]Running 5000 simulations.:  86%|████████▌ | 4301/5000 [00:29<00:04, 141.64it/s]Running 5000 simulations.:  86%|████████▋ | 4316/5000 [00:29<00:04, 140.18it/s]Running 5000 simulations.:  87%|████████▋ | 4331/5000 [00:29<00:04, 138.93it/s]Running 5000 simulations.:  87%|████████▋ | 4345/5000 [00:29<00:04, 138.21it/s]Running 5000 simulations.:  87%|████████▋ | 4359/5000 [00:29<00:04, 137.74it/s]Running 5000 simulations.:  87%|████████▋ | 4373/5000 [00:29<00:04, 137.78it/s]Running 5000 simulations.:  88%|████████▊ | 4387/5000 [00:29<00:04, 137.81it/s]Running 5000 simulations.:  88%|████████▊ | 4401/5000 [00:29<00:04, 137.64it/s]Running 5000 simulations.:  88%|████████▊ | 4415/5000 [00:30<00:04, 137.93it/s]Running 5000 simulations.:  89%|████████▊ | 4429/5000 [00:30<00:04, 138.30it/s]Running 5000 simulations.:  89%|████████▉ | 4443/5000 [00:30<00:04, 137.95it/s]Running 5000 simulations.:  89%|████████▉ | 4457/5000 [00:30<00:03, 137.86it/s]Running 5000 simulations.:  89%|████████▉ | 4471/5000 [00:30<00:03, 137.93it/s]Running 5000 simulations.:  90%|████████▉ | 4485/5000 [00:30<00:03, 138.13it/s]Running 5000 simulations.:  90%|████████▉ | 4499/5000 [00:30<00:03, 138.08it/s]Running 5000 simulations.:  90%|█████████ | 4513/5000 [00:30<00:03, 137.99it/s]Running 5000 simulations.:  91%|█████████ | 4527/5000 [00:30<00:03, 138.22it/s]Running 5000 simulations.:  91%|█████████ | 4541/5000 [00:30<00:03, 138.22it/s]Running 5000 simulations.:  91%|█████████ | 4555/5000 [00:31<00:03, 138.26it/s]Running 5000 simulations.:  91%|█████████▏| 4569/5000 [00:31<00:03, 137.98it/s]Running 5000 simulations.:  92%|█████████▏| 4583/5000 [00:31<00:03, 137.63it/s]Running 5000 simulations.:  92%|█████████▏| 4597/5000 [00:31<00:02, 137.59it/s]Running 5000 simulations.:  92%|█████████▏| 4611/5000 [00:31<00:02, 137.59it/s]Running 5000 simulations.:  92%|█████████▎| 4625/5000 [00:31<00:02, 137.71it/s]Running 5000 simulations.:  93%|█████████▎| 4639/5000 [00:31<00:02, 138.14it/s]Running 5000 simulations.:  93%|█████████▎| 4653/5000 [00:31<00:02, 138.37it/s]Running 5000 simulations.:  93%|█████████▎| 4667/5000 [00:31<00:02, 138.25it/s]Running 5000 simulations.:  94%|█████████▎| 4681/5000 [00:31<00:02, 138.04it/s]Running 5000 simulations.:  94%|█████████▍| 4695/5000 [00:32<00:02, 137.64it/s]Running 5000 simulations.:  94%|█████████▍| 4709/5000 [00:32<00:02, 137.61it/s]Running 5000 simulations.:  94%|█████████▍| 4723/5000 [00:32<00:02, 137.75it/s]Running 5000 simulations.:  95%|█████████▍| 4737/5000 [00:32<00:01, 138.02it/s]Running 5000 simulations.:  95%|█████████▌| 4751/5000 [00:32<00:01, 137.82it/s]Running 5000 simulations.:  95%|█████████▌| 4765/5000 [00:32<00:01, 138.12it/s]Running 5000 simulations.:  96%|█████████▌| 4779/5000 [00:32<00:01, 138.22it/s]Running 5000 simulations.:  96%|█████████▌| 4793/5000 [00:32<00:01, 138.14it/s]Running 5000 simulations.:  96%|█████████▌| 4807/5000 [00:32<00:01, 138.10it/s]Running 5000 simulations.:  96%|█████████▋| 4821/5000 [00:32<00:01, 138.04it/s]Running 5000 simulations.:  97%|█████████▋| 4836/5000 [00:33<00:01, 139.55it/s]Running 5000 simulations.:  97%|█████████▋| 4852/5000 [00:33<00:01, 142.71it/s]Running 5000 simulations.:  97%|█████████▋| 4868/5000 [00:33<00:00, 144.90it/s]Running 5000 simulations.:  98%|█████████▊| 4884/5000 [00:33<00:00, 146.45it/s]Running 5000 simulations.:  98%|█████████▊| 4899/5000 [00:33<00:00, 147.07it/s]Running 5000 simulations.:  98%|█████████▊| 4914/5000 [00:33<00:00, 147.85it/s]Running 5000 simulations.:  99%|█████████▊| 4929/5000 [00:33<00:00, 148.07it/s]Running 5000 simulations.:  99%|█████████▉| 4944/5000 [00:33<00:00, 148.39it/s]Running 5000 simulations.:  99%|█████████▉| 4960/5000 [00:33<00:00, 149.16it/s]Running 5000 simulations.: 100%|█████████▉| 4975/5000 [00:33<00:00, 149.21it/s]Running 5000 simulations.: 100%|█████████▉| 4991/5000 [00:34<00:00, 149.97it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:34<00:00, 146.39it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:32, 154.67it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:32, 154.99it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:32, 154.27it/s]Running 5000 simulations.:   1%|▏         | 64/5000 [00:00<00:32, 154.02it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:32, 153.69it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:31, 153.56it/s]Running 5000 simulations.:   2%|▏         | 112/5000 [00:00<00:31, 153.51it/s]Running 5000 simulations.:   3%|▎         | 128/5000 [00:00<00:31, 153.32it/s]Running 5000 simulations.:   3%|▎         | 144/5000 [00:00<00:31, 152.73it/s]Running 5000 simulations.:   3%|▎         | 160/5000 [00:01<00:31, 152.04it/s]Running 5000 simulations.:   4%|▎         | 176/5000 [00:01<00:31, 151.62it/s]Running 5000 simulations.:   4%|▍         | 192/5000 [00:01<00:31, 151.75it/s]Running 5000 simulations.:   4%|▍         | 208/5000 [00:01<00:31, 151.85it/s]Running 5000 simulations.:   4%|▍         | 224/5000 [00:01<00:31, 151.32it/s]Running 5000 simulations.:   5%|▍         | 240/5000 [00:01<00:31, 151.12it/s]Running 5000 simulations.:   5%|▌         | 256/5000 [00:01<00:31, 151.26it/s]Running 5000 simulations.:   5%|▌         | 272/5000 [00:01<00:31, 151.89it/s]Running 5000 simulations.:   6%|▌         | 288/5000 [00:01<00:30, 152.01it/s]Running 5000 simulations.:   6%|▌         | 304/5000 [00:01<00:30, 151.59it/s]Running 5000 simulations.:   6%|▋         | 320/5000 [00:02<00:30, 151.21it/s]Running 5000 simulations.:   7%|▋         | 336/5000 [00:02<00:30, 151.28it/s]Running 5000 simulations.:   7%|▋         | 352/5000 [00:02<00:30, 150.82it/s]Running 5000 simulations.:   7%|▋         | 368/5000 [00:02<00:31, 148.01it/s]Running 5000 simulations.:   8%|▊         | 383/5000 [00:02<00:31, 145.67it/s]Running 5000 simulations.:   8%|▊         | 398/5000 [00:02<00:31, 144.20it/s]Running 5000 simulations.:   8%|▊         | 413/5000 [00:02<00:31, 143.62it/s]Running 5000 simulations.:   9%|▊         | 428/5000 [00:02<00:31, 142.94it/s]Running 5000 simulations.:   9%|▉         | 443/5000 [00:02<00:31, 142.49it/s]Running 5000 simulations.:   9%|▉         | 458/5000 [00:03<00:32, 141.83it/s]Running 5000 simulations.:   9%|▉         | 473/5000 [00:03<00:31, 141.80it/s]Running 5000 simulations.:  10%|▉         | 488/5000 [00:03<00:31, 142.05it/s]Running 5000 simulations.:  10%|█         | 503/5000 [00:03<00:31, 141.51it/s]Running 5000 simulations.:  10%|█         | 518/5000 [00:03<00:31, 140.90it/s]Running 5000 simulations.:  11%|█         | 533/5000 [00:03<00:31, 140.70it/s]Running 5000 simulations.:  11%|█         | 548/5000 [00:03<00:31, 141.01it/s]Running 5000 simulations.:  11%|█▏        | 563/5000 [00:03<00:31, 140.93it/s]Running 5000 simulations.:  12%|█▏        | 578/5000 [00:03<00:31, 140.52it/s]Running 5000 simulations.:  12%|█▏        | 593/5000 [00:04<00:31, 140.70it/s]Running 5000 simulations.:  12%|█▏        | 608/5000 [00:04<00:31, 140.90it/s]Running 5000 simulations.:  12%|█▏        | 623/5000 [00:04<00:31, 140.44it/s]Running 5000 simulations.:  13%|█▎        | 638/5000 [00:04<00:31, 140.36it/s]Running 5000 simulations.:  13%|█▎        | 653/5000 [00:04<00:30, 140.41it/s]Running 5000 simulations.:  13%|█▎        | 668/5000 [00:04<00:30, 140.44it/s]Running 5000 simulations.:  14%|█▎        | 683/5000 [00:04<00:30, 140.39it/s]Running 5000 simulations.:  14%|█▍        | 698/5000 [00:04<00:30, 140.14it/s]Running 5000 simulations.:  14%|█▍        | 713/5000 [00:04<00:30, 139.71it/s]Running 5000 simulations.:  15%|█▍        | 727/5000 [00:04<00:30, 139.72it/s]Running 5000 simulations.:  15%|█▍        | 742/5000 [00:05<00:30, 139.78it/s]Running 5000 simulations.:  15%|█▌        | 756/5000 [00:05<00:30, 139.41it/s]Running 5000 simulations.:  15%|█▌        | 770/5000 [00:05<00:30, 138.95it/s]Running 5000 simulations.:  16%|█▌        | 784/5000 [00:05<00:30, 138.93it/s]Running 5000 simulations.:  16%|█▌        | 798/5000 [00:05<00:30, 138.93it/s]Running 5000 simulations.:  16%|█▋        | 813/5000 [00:05<00:30, 139.50it/s]Running 5000 simulations.:  17%|█▋        | 827/5000 [00:05<00:29, 139.33it/s]Running 5000 simulations.:  17%|█▋        | 841/5000 [00:05<00:29, 139.20it/s]Running 5000 simulations.:  17%|█▋        | 855/5000 [00:05<00:29, 139.27it/s]Running 5000 simulations.:  17%|█▋        | 870/5000 [00:06<00:29, 139.77it/s]Running 5000 simulations.:  18%|█▊        | 885/5000 [00:06<00:29, 139.83it/s]Running 5000 simulations.:  18%|█▊        | 899/5000 [00:06<00:29, 139.44it/s]Running 5000 simulations.:  18%|█▊        | 913/5000 [00:06<00:29, 139.05it/s]Running 5000 simulations.:  19%|█▊        | 927/5000 [00:06<00:29, 138.97it/s]Running 5000 simulations.:  19%|█▉        | 941/5000 [00:06<00:30, 135.00it/s]Running 5000 simulations.:  19%|█▉        | 956/5000 [00:06<00:29, 138.89it/s]Running 5000 simulations.:  19%|█▉        | 971/5000 [00:06<00:28, 141.70it/s]Running 5000 simulations.:  20%|█▉        | 986/5000 [00:06<00:27, 143.90it/s]Running 5000 simulations.:  20%|██        | 1001/5000 [00:06<00:27, 145.53it/s]Running 5000 simulations.:  20%|██        | 1016/5000 [00:07<00:27, 146.63it/s]Running 5000 simulations.:  21%|██        | 1031/5000 [00:07<00:26, 147.29it/s]Running 5000 simulations.:  21%|██        | 1047/5000 [00:07<00:26, 148.16it/s]Running 5000 simulations.:  21%|██▏       | 1063/5000 [00:07<00:26, 148.78it/s]Running 5000 simulations.:  22%|██▏       | 1079/5000 [00:07<00:26, 149.13it/s]Running 5000 simulations.:  22%|██▏       | 1095/5000 [00:07<00:26, 149.75it/s]Running 5000 simulations.:  22%|██▏       | 1110/5000 [00:07<00:26, 148.77it/s]Running 5000 simulations.:  22%|██▎       | 1125/5000 [00:07<00:26, 147.89it/s]Running 5000 simulations.:  23%|██▎       | 1140/5000 [00:07<00:26, 147.66it/s]Running 5000 simulations.:  23%|██▎       | 1155/5000 [00:07<00:26, 147.68it/s]Running 5000 simulations.:  23%|██▎       | 1170/5000 [00:08<00:26, 147.22it/s]Running 5000 simulations.:  24%|██▎       | 1185/5000 [00:08<00:25, 146.87it/s]Running 5000 simulations.:  24%|██▍       | 1200/5000 [00:08<00:25, 146.89it/s]Running 5000 simulations.:  24%|██▍       | 1215/5000 [00:08<00:25, 146.60it/s]Running 5000 simulations.:  25%|██▍       | 1230/5000 [00:08<00:25, 146.70it/s]Running 5000 simulations.:  25%|██▍       | 1245/5000 [00:08<00:25, 146.81it/s]Running 5000 simulations.:  25%|██▌       | 1260/5000 [00:08<00:25, 147.12it/s]Running 5000 simulations.:  26%|██▌       | 1275/5000 [00:08<00:25, 147.33it/s]Running 5000 simulations.:  26%|██▌       | 1290/5000 [00:08<00:25, 146.91it/s]Running 5000 simulations.:  26%|██▌       | 1305/5000 [00:08<00:25, 146.77it/s]Running 5000 simulations.:  26%|██▋       | 1320/5000 [00:09<00:25, 146.64it/s]Running 5000 simulations.:  27%|██▋       | 1335/5000 [00:09<00:24, 146.61it/s]Running 5000 simulations.:  27%|██▋       | 1350/5000 [00:09<00:24, 146.79it/s]Running 5000 simulations.:  27%|██▋       | 1365/5000 [00:09<00:24, 146.63it/s]Running 5000 simulations.:  28%|██▊       | 1380/5000 [00:09<00:24, 146.50it/s]Running 5000 simulations.:  28%|██▊       | 1395/5000 [00:09<00:24, 147.33it/s]Running 5000 simulations.:  28%|██▊       | 1410/5000 [00:09<00:24, 146.89it/s]Running 5000 simulations.:  28%|██▊       | 1425/5000 [00:09<00:24, 146.98it/s]Running 5000 simulations.:  29%|██▉       | 1440/5000 [00:09<00:24, 146.92it/s]Running 5000 simulations.:  29%|██▉       | 1455/5000 [00:10<00:24, 146.80it/s]Running 5000 simulations.:  29%|██▉       | 1470/5000 [00:10<00:24, 146.77it/s]Running 5000 simulations.:  30%|██▉       | 1485/5000 [00:10<00:23, 146.87it/s]Running 5000 simulations.:  30%|███       | 1500/5000 [00:10<00:23, 146.97it/s]Running 5000 simulations.:  30%|███       | 1515/5000 [00:10<00:23, 146.75it/s]Running 5000 simulations.:  31%|███       | 1530/5000 [00:10<00:23, 146.61it/s]Running 5000 simulations.:  31%|███       | 1545/5000 [00:10<00:23, 146.81it/s]Running 5000 simulations.:  31%|███       | 1560/5000 [00:10<00:23, 147.07it/s]Running 5000 simulations.:  32%|███▏      | 1575/5000 [00:10<00:23, 147.24it/s]Running 5000 simulations.:  32%|███▏      | 1590/5000 [00:10<00:23, 147.46it/s]Running 5000 simulations.:  32%|███▏      | 1605/5000 [00:11<00:22, 147.73it/s]Running 5000 simulations.:  32%|███▏      | 1620/5000 [00:11<00:22, 147.38it/s]Running 5000 simulations.:  33%|███▎      | 1635/5000 [00:11<00:22, 147.33it/s]Running 5000 simulations.:  33%|███▎      | 1650/5000 [00:11<00:22, 147.14it/s]Running 5000 simulations.:  33%|███▎      | 1665/5000 [00:11<00:22, 147.53it/s]Running 5000 simulations.:  34%|███▎      | 1680/5000 [00:11<00:22, 147.25it/s]Running 5000 simulations.:  34%|███▍      | 1695/5000 [00:11<00:22, 147.03it/s]Running 5000 simulations.:  34%|███▍      | 1710/5000 [00:11<00:22, 146.93it/s]Running 5000 simulations.:  34%|███▍      | 1725/5000 [00:11<00:22, 145.18it/s]Running 5000 simulations.:  35%|███▍      | 1740/5000 [00:11<00:22, 143.32it/s]Running 5000 simulations.:  35%|███▌      | 1755/5000 [00:12<00:22, 141.35it/s]Running 5000 simulations.:  35%|███▌      | 1770/5000 [00:12<00:23, 140.07it/s]Running 5000 simulations.:  36%|███▌      | 1785/5000 [00:12<00:23, 139.27it/s]Running 5000 simulations.:  36%|███▌      | 1799/5000 [00:12<00:23, 138.65it/s]Running 5000 simulations.:  36%|███▋      | 1813/5000 [00:12<00:22, 138.81it/s]Running 5000 simulations.:  37%|███▋      | 1827/5000 [00:12<00:22, 138.99it/s]Running 5000 simulations.:  37%|███▋      | 1841/5000 [00:12<00:22, 139.09it/s]Running 5000 simulations.:  37%|███▋      | 1855/5000 [00:12<00:22, 138.78it/s]Running 5000 simulations.:  37%|███▋      | 1870/5000 [00:12<00:22, 140.56it/s]Running 5000 simulations.:  38%|███▊      | 1885/5000 [00:12<00:21, 142.80it/s]Running 5000 simulations.:  38%|███▊      | 1900/5000 [00:13<00:21, 144.31it/s]Running 5000 simulations.:  38%|███▊      | 1915/5000 [00:13<00:21, 145.29it/s]Running 5000 simulations.:  39%|███▊      | 1930/5000 [00:13<00:21, 146.06it/s]Running 5000 simulations.:  39%|███▉      | 1945/5000 [00:13<00:20, 147.14it/s]Running 5000 simulations.:  39%|███▉      | 1960/5000 [00:13<00:20, 146.63it/s]Running 5000 simulations.:  40%|███▉      | 1975/5000 [00:13<00:20, 146.60it/s]Running 5000 simulations.:  40%|███▉      | 1990/5000 [00:13<00:20, 147.16it/s]Running 5000 simulations.:  40%|████      | 2005/5000 [00:13<00:20, 147.45it/s]Running 5000 simulations.:  40%|████      | 2020/5000 [00:13<00:20, 147.89it/s]Running 5000 simulations.:  41%|████      | 2035/5000 [00:14<00:20, 148.10it/s]Running 5000 simulations.:  41%|████      | 2050/5000 [00:14<00:19, 147.80it/s]Running 5000 simulations.:  41%|████▏     | 2065/5000 [00:14<00:19, 148.00it/s]Running 5000 simulations.:  42%|████▏     | 2080/5000 [00:14<00:19, 147.76it/s]Running 5000 simulations.:  42%|████▏     | 2095/5000 [00:14<00:19, 147.61it/s]Running 5000 simulations.:  42%|████▏     | 2110/5000 [00:14<00:19, 147.93it/s]Running 5000 simulations.:  42%|████▎     | 2125/5000 [00:14<00:19, 148.10it/s]Running 5000 simulations.:  43%|████▎     | 2140/5000 [00:14<00:19, 148.49it/s]Running 5000 simulations.:  43%|████▎     | 2155/5000 [00:14<00:19, 148.29it/s]Running 5000 simulations.:  43%|████▎     | 2170/5000 [00:14<00:19, 148.04it/s]Running 5000 simulations.:  44%|████▎     | 2185/5000 [00:15<00:18, 148.39it/s]Running 5000 simulations.:  44%|████▍     | 2200/5000 [00:15<00:18, 148.07it/s]Running 5000 simulations.:  44%|████▍     | 2216/5000 [00:15<00:18, 148.69it/s]Running 5000 simulations.:  45%|████▍     | 2232/5000 [00:15<00:18, 149.38it/s]Running 5000 simulations.:  45%|████▍     | 2247/5000 [00:15<00:18, 149.52it/s]Running 5000 simulations.:  45%|████▌     | 2262/5000 [00:15<00:18, 148.59it/s]Running 5000 simulations.:  46%|████▌     | 2277/5000 [00:15<00:18, 147.55it/s]Running 5000 simulations.:  46%|████▌     | 2292/5000 [00:15<00:18, 146.85it/s]Running 5000 simulations.:  46%|████▌     | 2307/5000 [00:15<00:18, 146.48it/s]Running 5000 simulations.:  46%|████▋     | 2322/5000 [00:15<00:18, 146.29it/s]Running 5000 simulations.:  47%|████▋     | 2337/5000 [00:16<00:18, 146.06it/s]Running 5000 simulations.:  47%|████▋     | 2352/5000 [00:16<00:18, 146.31it/s]Running 5000 simulations.:  47%|████▋     | 2367/5000 [00:16<00:18, 145.89it/s]Running 5000 simulations.:  48%|████▊     | 2382/5000 [00:16<00:17, 145.50it/s]Running 5000 simulations.:  48%|████▊     | 2397/5000 [00:16<00:17, 145.40it/s]Running 5000 simulations.:  48%|████▊     | 2412/5000 [00:16<00:17, 145.81it/s]Running 5000 simulations.:  49%|████▊     | 2427/5000 [00:16<00:17, 145.85it/s]Running 5000 simulations.:  49%|████▉     | 2442/5000 [00:16<00:17, 145.77it/s]Running 5000 simulations.:  49%|████▉     | 2457/5000 [00:16<00:17, 145.78it/s]Running 5000 simulations.:  49%|████▉     | 2472/5000 [00:16<00:17, 145.89it/s]Running 5000 simulations.:  50%|████▉     | 2487/5000 [00:17<00:17, 144.86it/s]Running 5000 simulations.:  50%|█████     | 2502/5000 [00:17<00:17, 143.25it/s]Running 5000 simulations.:  50%|█████     | 2517/5000 [00:17<00:17, 141.71it/s]Running 5000 simulations.:  51%|█████     | 2532/5000 [00:17<00:17, 140.68it/s]Running 5000 simulations.:  51%|█████     | 2547/5000 [00:17<00:17, 139.68it/s]Running 5000 simulations.:  51%|█████     | 2561/5000 [00:17<00:17, 139.08it/s]Running 5000 simulations.:  52%|█████▏    | 2575/5000 [00:17<00:17, 138.64it/s]Running 5000 simulations.:  52%|█████▏    | 2589/5000 [00:17<00:17, 138.57it/s]Running 5000 simulations.:  52%|█████▏    | 2603/5000 [00:17<00:17, 138.43it/s]Running 5000 simulations.:  52%|█████▏    | 2617/5000 [00:18<00:17, 138.24it/s]Running 5000 simulations.:  53%|█████▎    | 2631/5000 [00:18<00:17, 138.39it/s]Running 5000 simulations.:  53%|█████▎    | 2646/5000 [00:18<00:16, 140.20it/s]Running 5000 simulations.:  53%|█████▎    | 2661/5000 [00:18<00:16, 141.78it/s]Running 5000 simulations.:  54%|█████▎    | 2676/5000 [00:18<00:16, 142.80it/s]Running 5000 simulations.:  54%|█████▍    | 2691/5000 [00:18<00:16, 143.50it/s]Running 5000 simulations.:  54%|█████▍    | 2706/5000 [00:18<00:15, 144.44it/s]Running 5000 simulations.:  54%|█████▍    | 2721/5000 [00:18<00:15, 145.04it/s]Running 5000 simulations.:  55%|█████▍    | 2736/5000 [00:18<00:15, 145.08it/s]Running 5000 simulations.:  55%|█████▌    | 2751/5000 [00:18<00:15, 145.40it/s]Running 5000 simulations.:  55%|█████▌    | 2766/5000 [00:19<00:15, 145.22it/s]Running 5000 simulations.:  56%|█████▌    | 2781/5000 [00:19<00:15, 144.86it/s]Running 5000 simulations.:  56%|█████▌    | 2796/5000 [00:19<00:15, 144.61it/s]Running 5000 simulations.:  56%|█████▌    | 2811/5000 [00:19<00:15, 144.79it/s]Running 5000 simulations.:  57%|█████▋    | 2826/5000 [00:19<00:15, 144.72it/s]Running 5000 simulations.:  57%|█████▋    | 2841/5000 [00:19<00:14, 145.10it/s]Running 5000 simulations.:  57%|█████▋    | 2856/5000 [00:19<00:14, 145.16it/s]Running 5000 simulations.:  57%|█████▋    | 2871/5000 [00:19<00:14, 142.62it/s]Running 5000 simulations.:  58%|█████▊    | 2886/5000 [00:19<00:15, 140.48it/s]Running 5000 simulations.:  58%|█████▊    | 2901/5000 [00:20<00:15, 139.52it/s]Running 5000 simulations.:  58%|█████▊    | 2915/5000 [00:20<00:15, 138.61it/s]Running 5000 simulations.:  59%|█████▊    | 2929/5000 [00:20<00:15, 137.83it/s]Running 5000 simulations.:  59%|█████▉    | 2943/5000 [00:20<00:14, 137.35it/s]Running 5000 simulations.:  59%|█████▉    | 2957/5000 [00:20<00:14, 137.05it/s]Running 5000 simulations.:  59%|█████▉    | 2971/5000 [00:20<00:14, 137.29it/s]Running 5000 simulations.:  60%|█████▉    | 2985/5000 [00:20<00:14, 136.96it/s]Running 5000 simulations.:  60%|█████▉    | 2999/5000 [00:20<00:14, 136.84it/s]Running 5000 simulations.:  60%|██████    | 3014/5000 [00:20<00:14, 139.66it/s]Running 5000 simulations.:  61%|██████    | 3029/5000 [00:20<00:13, 142.42it/s]Running 5000 simulations.:  61%|██████    | 3044/5000 [00:21<00:13, 144.14it/s]Running 5000 simulations.:  61%|██████    | 3059/5000 [00:21<00:13, 145.25it/s]Running 5000 simulations.:  61%|██████▏   | 3074/5000 [00:21<00:13, 146.00it/s]Running 5000 simulations.:  62%|██████▏   | 3089/5000 [00:21<00:13, 146.48it/s]Running 5000 simulations.:  62%|██████▏   | 3104/5000 [00:21<00:12, 147.11it/s]Running 5000 simulations.:  62%|██████▏   | 3119/5000 [00:21<00:12, 147.27it/s]Running 5000 simulations.:  63%|██████▎   | 3134/5000 [00:21<00:12, 147.18it/s]Running 5000 simulations.:  63%|██████▎   | 3149/5000 [00:21<00:12, 146.81it/s]Running 5000 simulations.:  63%|██████▎   | 3164/5000 [00:21<00:12, 146.53it/s]Running 5000 simulations.:  64%|██████▎   | 3179/5000 [00:21<00:12, 146.99it/s]Running 5000 simulations.:  64%|██████▍   | 3194/5000 [00:22<00:12, 147.21it/s]Running 5000 simulations.:  64%|██████▍   | 3209/5000 [00:22<00:12, 147.33it/s]Running 5000 simulations.:  64%|██████▍   | 3224/5000 [00:22<00:12, 147.51it/s]Running 5000 simulations.:  65%|██████▍   | 3239/5000 [00:22<00:11, 147.28it/s]Running 5000 simulations.:  65%|██████▌   | 3254/5000 [00:22<00:11, 147.03it/s]Running 5000 simulations.:  65%|██████▌   | 3269/5000 [00:22<00:11, 147.25it/s]Running 5000 simulations.:  66%|██████▌   | 3284/5000 [00:22<00:11, 147.31it/s]Running 5000 simulations.:  66%|██████▌   | 3299/5000 [00:22<00:11, 147.58it/s]Running 5000 simulations.:  66%|██████▋   | 3314/5000 [00:22<00:11, 147.35it/s]Running 5000 simulations.:  67%|██████▋   | 3329/5000 [00:22<00:11, 147.47it/s]Running 5000 simulations.:  67%|██████▋   | 3344/5000 [00:23<00:11, 147.71it/s]Running 5000 simulations.:  67%|██████▋   | 3359/5000 [00:23<00:11, 147.86it/s]Running 5000 simulations.:  67%|██████▋   | 3374/5000 [00:23<00:10, 148.11it/s]Running 5000 simulations.:  68%|██████▊   | 3389/5000 [00:23<00:10, 148.27it/s]Running 5000 simulations.:  68%|██████▊   | 3404/5000 [00:23<00:10, 147.88it/s]Running 5000 simulations.:  68%|██████▊   | 3419/5000 [00:23<00:10, 147.70it/s]Running 5000 simulations.:  69%|██████▊   | 3434/5000 [00:23<00:10, 147.66it/s]Running 5000 simulations.:  69%|██████▉   | 3449/5000 [00:23<00:10, 147.52it/s]Running 5000 simulations.:  69%|██████▉   | 3464/5000 [00:23<00:10, 147.35it/s]Running 5000 simulations.:  70%|██████▉   | 3479/5000 [00:23<00:10, 147.03it/s]Running 5000 simulations.:  70%|██████▉   | 3494/5000 [00:24<00:10, 147.03it/s]Running 5000 simulations.:  70%|███████   | 3509/5000 [00:24<00:10, 147.04it/s]Running 5000 simulations.:  70%|███████   | 3524/5000 [00:24<00:10, 147.34it/s]Running 5000 simulations.:  71%|███████   | 3539/5000 [00:24<00:09, 147.71it/s]Running 5000 simulations.:  71%|███████   | 3554/5000 [00:24<00:09, 147.76it/s]Running 5000 simulations.:  71%|███████▏  | 3569/5000 [00:24<00:09, 147.57it/s]Running 5000 simulations.:  72%|███████▏  | 3584/5000 [00:24<00:09, 147.36it/s]Running 5000 simulations.:  72%|███████▏  | 3599/5000 [00:24<00:09, 147.22it/s]Running 5000 simulations.:  72%|███████▏  | 3614/5000 [00:24<00:09, 147.36it/s]Running 5000 simulations.:  73%|███████▎  | 3629/5000 [00:24<00:09, 147.09it/s]Running 5000 simulations.:  73%|███████▎  | 3644/5000 [00:25<00:09, 147.47it/s]Running 5000 simulations.:  73%|███████▎  | 3659/5000 [00:25<00:09, 147.96it/s]Running 5000 simulations.:  73%|███████▎  | 3674/5000 [00:25<00:08, 148.30it/s]Running 5000 simulations.:  74%|███████▍  | 3689/5000 [00:25<00:08, 148.25it/s]Running 5000 simulations.:  74%|███████▍  | 3704/5000 [00:25<00:08, 147.97it/s]Running 5000 simulations.:  74%|███████▍  | 3719/5000 [00:25<00:08, 147.57it/s]Running 5000 simulations.:  75%|███████▍  | 3734/5000 [00:25<00:08, 148.02it/s]Running 5000 simulations.:  75%|███████▍  | 3749/5000 [00:25<00:08, 148.12it/s]Running 5000 simulations.:  75%|███████▌  | 3764/5000 [00:25<00:08, 147.95it/s]Running 5000 simulations.:  76%|███████▌  | 3779/5000 [00:26<00:08, 148.24it/s]Running 5000 simulations.:  76%|███████▌  | 3794/5000 [00:26<00:08, 148.08it/s]Running 5000 simulations.:  76%|███████▌  | 3809/5000 [00:26<00:08, 147.65it/s]Running 5000 simulations.:  76%|███████▋  | 3824/5000 [00:26<00:07, 147.53it/s]Running 5000 simulations.:  77%|███████▋  | 3839/5000 [00:26<00:07, 147.41it/s]Running 5000 simulations.:  77%|███████▋  | 3854/5000 [00:26<00:07, 147.32it/s]Running 5000 simulations.:  77%|███████▋  | 3869/5000 [00:26<00:07, 147.24it/s]Running 5000 simulations.:  78%|███████▊  | 3884/5000 [00:26<00:07, 147.25it/s]Running 5000 simulations.:  78%|███████▊  | 3899/5000 [00:26<00:07, 147.27it/s]Running 5000 simulations.:  78%|███████▊  | 3914/5000 [00:26<00:07, 146.79it/s]Running 5000 simulations.:  79%|███████▊  | 3929/5000 [00:27<00:07, 146.54it/s]Running 5000 simulations.:  79%|███████▉  | 3944/5000 [00:27<00:07, 146.31it/s]Running 5000 simulations.:  79%|███████▉  | 3959/5000 [00:27<00:07, 146.10it/s]Running 5000 simulations.:  79%|███████▉  | 3974/5000 [00:27<00:07, 145.79it/s]Running 5000 simulations.:  80%|███████▉  | 3989/5000 [00:27<00:06, 146.23it/s]Running 5000 simulations.:  80%|████████  | 4004/5000 [00:27<00:06, 145.65it/s]Running 5000 simulations.:  80%|████████  | 4019/5000 [00:27<00:06, 145.63it/s]Running 5000 simulations.:  81%|████████  | 4034/5000 [00:27<00:06, 145.79it/s]Running 5000 simulations.:  81%|████████  | 4049/5000 [00:27<00:06, 145.42it/s]Running 5000 simulations.:  81%|████████▏ | 4064/5000 [00:27<00:06, 145.06it/s]Running 5000 simulations.:  82%|████████▏ | 4079/5000 [00:28<00:06, 144.74it/s]Running 5000 simulations.:  82%|████████▏ | 4094/5000 [00:28<00:06, 144.79it/s]Running 5000 simulations.:  82%|████████▏ | 4109/5000 [00:28<00:06, 145.14it/s]Running 5000 simulations.:  82%|████████▏ | 4124/5000 [00:28<00:06, 145.41it/s]Running 5000 simulations.:  83%|████████▎ | 4139/5000 [00:28<00:05, 145.77it/s]Running 5000 simulations.:  83%|████████▎ | 4154/5000 [00:28<00:05, 145.78it/s]Running 5000 simulations.:  83%|████████▎ | 4169/5000 [00:28<00:05, 146.25it/s]Running 5000 simulations.:  84%|████████▎ | 4186/5000 [00:28<00:05, 150.83it/s]Running 5000 simulations.:  84%|████████▍ | 4202/5000 [00:28<00:05, 151.17it/s]Running 5000 simulations.:  84%|████████▍ | 4218/5000 [00:28<00:05, 149.33it/s]Running 5000 simulations.:  85%|████████▍ | 4233/5000 [00:29<00:05, 148.25it/s]Running 5000 simulations.:  85%|████████▍ | 4248/5000 [00:29<00:05, 147.69it/s]Running 5000 simulations.:  85%|████████▌ | 4263/5000 [00:29<00:05, 147.09it/s]Running 5000 simulations.:  86%|████████▌ | 4278/5000 [00:29<00:04, 146.54it/s]Running 5000 simulations.:  86%|████████▌ | 4293/5000 [00:29<00:04, 145.87it/s]Running 5000 simulations.:  86%|████████▌ | 4308/5000 [00:29<00:04, 146.16it/s]Running 5000 simulations.:  86%|████████▋ | 4324/5000 [00:29<00:04, 147.36it/s]Running 5000 simulations.:  87%|████████▋ | 4339/5000 [00:29<00:04, 147.88it/s]Running 5000 simulations.:  87%|████████▋ | 4354/5000 [00:29<00:04, 148.23it/s]Running 5000 simulations.:  87%|████████▋ | 4369/5000 [00:30<00:04, 145.20it/s]Running 5000 simulations.:  88%|████████▊ | 4384/5000 [00:30<00:04, 141.43it/s]Running 5000 simulations.:  88%|████████▊ | 4399/5000 [00:30<00:04, 139.23it/s]Running 5000 simulations.:  88%|████████▊ | 4413/5000 [00:30<00:04, 137.29it/s]Running 5000 simulations.:  89%|████████▊ | 4427/5000 [00:30<00:04, 136.10it/s]Running 5000 simulations.:  89%|████████▉ | 4441/5000 [00:30<00:04, 135.74it/s]Running 5000 simulations.:  89%|████████▉ | 4455/5000 [00:30<00:04, 135.35it/s]Running 5000 simulations.:  89%|████████▉ | 4470/5000 [00:30<00:03, 137.17it/s]Running 5000 simulations.:  90%|████████▉ | 4485/5000 [00:30<00:03, 138.75it/s]Running 5000 simulations.:  90%|█████████ | 4500/5000 [00:30<00:03, 139.92it/s]Running 5000 simulations.:  90%|█████████ | 4515/5000 [00:31<00:03, 140.47it/s]Running 5000 simulations.:  91%|█████████ | 4530/5000 [00:31<00:03, 141.22it/s]Running 5000 simulations.:  91%|█████████ | 4545/5000 [00:31<00:03, 141.76it/s]Running 5000 simulations.:  91%|█████████ | 4560/5000 [00:31<00:03, 142.34it/s]Running 5000 simulations.:  92%|█████████▏| 4575/5000 [00:31<00:02, 142.60it/s]Running 5000 simulations.:  92%|█████████▏| 4590/5000 [00:31<00:02, 141.97it/s]Running 5000 simulations.:  92%|█████████▏| 4605/5000 [00:31<00:02, 142.11it/s]Running 5000 simulations.:  92%|█████████▏| 4620/5000 [00:31<00:02, 141.87it/s]Running 5000 simulations.:  93%|█████████▎| 4635/5000 [00:31<00:02, 141.85it/s]Running 5000 simulations.:  93%|█████████▎| 4650/5000 [00:32<00:02, 141.77it/s]Running 5000 simulations.:  93%|█████████▎| 4665/5000 [00:32<00:02, 141.41it/s]Running 5000 simulations.:  94%|█████████▎| 4680/5000 [00:32<00:02, 141.04it/s]Running 5000 simulations.:  94%|█████████▍| 4695/5000 [00:32<00:02, 140.85it/s]Running 5000 simulations.:  94%|█████████▍| 4710/5000 [00:32<00:02, 140.90it/s]Running 5000 simulations.:  94%|█████████▍| 4725/5000 [00:32<00:01, 140.85it/s]Running 5000 simulations.:  95%|█████████▍| 4740/5000 [00:32<00:01, 140.91it/s]Running 5000 simulations.:  95%|█████████▌| 4755/5000 [00:32<00:01, 142.74it/s]Running 5000 simulations.:  95%|█████████▌| 4770/5000 [00:32<00:01, 144.44it/s]Running 5000 simulations.:  96%|█████████▌| 4785/5000 [00:32<00:01, 145.67it/s]Running 5000 simulations.:  96%|█████████▌| 4800/5000 [00:33<00:01, 145.97it/s]Running 5000 simulations.:  96%|█████████▋| 4815/5000 [00:33<00:01, 146.26it/s]Running 5000 simulations.:  97%|█████████▋| 4830/5000 [00:33<00:01, 142.33it/s]Running 5000 simulations.:  97%|█████████▋| 4845/5000 [00:33<00:01, 143.10it/s]Running 5000 simulations.:  97%|█████████▋| 4860/5000 [00:33<00:00, 143.92it/s]Running 5000 simulations.:  98%|█████████▊| 4875/5000 [00:33<00:00, 144.21it/s]Running 5000 simulations.:  98%|█████████▊| 4890/5000 [00:33<00:00, 144.87it/s]Running 5000 simulations.:  98%|█████████▊| 4905/5000 [00:33<00:00, 145.08it/s]Running 5000 simulations.:  98%|█████████▊| 4920/5000 [00:33<00:00, 145.27it/s]Running 5000 simulations.:  99%|█████████▊| 4935/5000 [00:34<00:00, 145.44it/s]Running 5000 simulations.:  99%|█████████▉| 4950/5000 [00:34<00:00, 145.34it/s]Running 5000 simulations.:  99%|█████████▉| 4965/5000 [00:34<00:00, 145.07it/s]Running 5000 simulations.: 100%|█████████▉| 4980/5000 [00:34<00:00, 145.37it/s]Running 5000 simulations.: 100%|█████████▉| 4995/5000 [00:34<00:00, 145.77it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:34<00:00, 145.05it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:32, 153.53it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:32, 153.42it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:32, 153.42it/s]Running 5000 simulations.:   1%|▏         | 64/5000 [00:00<00:32, 153.24it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:32, 152.76it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:32, 152.62it/s]Running 5000 simulations.:   2%|▏         | 112/5000 [00:00<00:32, 152.30it/s]Running 5000 simulations.:   3%|▎         | 128/5000 [00:00<00:32, 152.25it/s]Running 5000 simulations.:   3%|▎         | 144/5000 [00:00<00:31, 152.00it/s]Running 5000 simulations.:   3%|▎         | 160/5000 [00:01<00:31, 151.84it/s]Running 5000 simulations.:   4%|▎         | 176/5000 [00:01<00:31, 152.12it/s]Running 5000 simulations.:   4%|▍         | 192/5000 [00:01<00:31, 151.91it/s]Running 5000 simulations.:   4%|▍         | 208/5000 [00:01<00:31, 152.23it/s]Running 5000 simulations.:   4%|▍         | 224/5000 [00:01<00:31, 152.26it/s]Running 5000 simulations.:   5%|▍         | 240/5000 [00:01<00:31, 152.00it/s]Running 5000 simulations.:   5%|▌         | 256/5000 [00:01<00:31, 152.12it/s]Running 5000 simulations.:   5%|▌         | 272/5000 [00:01<00:31, 151.60it/s]Running 5000 simulations.:   6%|▌         | 288/5000 [00:01<00:31, 150.97it/s]Running 5000 simulations.:   6%|▌         | 304/5000 [00:02<00:31, 151.14it/s]Running 5000 simulations.:   6%|▋         | 320/5000 [00:02<00:30, 151.40it/s]Running 5000 simulations.:   7%|▋         | 336/5000 [00:02<00:30, 151.33it/s]Running 5000 simulations.:   7%|▋         | 352/5000 [00:02<00:30, 150.83it/s]Running 5000 simulations.:   7%|▋         | 368/5000 [00:02<00:30, 150.77it/s]Running 5000 simulations.:   8%|▊         | 384/5000 [00:02<00:30, 150.82it/s]Running 5000 simulations.:   8%|▊         | 400/5000 [00:02<00:30, 149.86it/s]Running 5000 simulations.:   8%|▊         | 415/5000 [00:02<00:30, 149.10it/s]Running 5000 simulations.:   9%|▊         | 430/5000 [00:02<00:30, 148.81it/s]Running 5000 simulations.:   9%|▉         | 445/5000 [00:02<00:30, 148.37it/s]Running 5000 simulations.:   9%|▉         | 460/5000 [00:03<00:30, 148.05it/s]Running 5000 simulations.:  10%|▉         | 475/5000 [00:03<00:30, 147.72it/s]Running 5000 simulations.:  10%|▉         | 490/5000 [00:03<00:30, 147.56it/s]Running 5000 simulations.:  10%|█         | 505/5000 [00:03<00:30, 147.39it/s]Running 5000 simulations.:  10%|█         | 520/5000 [00:03<00:30, 147.27it/s]Running 5000 simulations.:  11%|█         | 535/5000 [00:03<00:30, 147.21it/s]Running 5000 simulations.:  11%|█         | 550/5000 [00:03<00:30, 147.31it/s]Running 5000 simulations.:  11%|█▏        | 565/5000 [00:03<00:30, 147.55it/s]Running 5000 simulations.:  12%|█▏        | 580/5000 [00:03<00:29, 147.62it/s]Running 5000 simulations.:  12%|█▏        | 595/5000 [00:03<00:29, 147.44it/s]Running 5000 simulations.:  12%|█▏        | 610/5000 [00:04<00:29, 147.27it/s]Running 5000 simulations.:  12%|█▎        | 625/5000 [00:04<00:29, 147.07it/s]Running 5000 simulations.:  13%|█▎        | 640/5000 [00:04<00:29, 146.89it/s]Running 5000 simulations.:  13%|█▎        | 655/5000 [00:04<00:29, 146.87it/s]Running 5000 simulations.:  13%|█▎        | 670/5000 [00:04<00:29, 146.71it/s]Running 5000 simulations.:  14%|█▎        | 685/5000 [00:04<00:29, 147.21it/s]Running 5000 simulations.:  14%|█▍        | 700/5000 [00:04<00:29, 147.38it/s]Running 5000 simulations.:  14%|█▍        | 715/5000 [00:04<00:29, 146.97it/s]Running 5000 simulations.:  15%|█▍        | 730/5000 [00:04<00:29, 146.63it/s]Running 5000 simulations.:  15%|█▍        | 745/5000 [00:04<00:29, 146.51it/s]Running 5000 simulations.:  15%|█▌        | 760/5000 [00:05<00:28, 146.93it/s]Running 5000 simulations.:  16%|█▌        | 775/5000 [00:05<00:28, 147.04it/s]Running 5000 simulations.:  16%|█▌        | 790/5000 [00:05<00:28, 146.91it/s]Running 5000 simulations.:  16%|█▌        | 805/5000 [00:05<00:28, 147.48it/s]Running 5000 simulations.:  16%|█▋        | 820/5000 [00:05<00:28, 148.03it/s]Running 5000 simulations.:  17%|█▋        | 835/5000 [00:05<00:28, 148.26it/s]Running 5000 simulations.:  17%|█▋        | 850/5000 [00:05<00:27, 148.29it/s]Running 5000 simulations.:  17%|█▋        | 865/5000 [00:05<00:28, 144.16it/s]Running 5000 simulations.:  18%|█▊        | 880/5000 [00:05<00:29, 140.42it/s]Running 5000 simulations.:  18%|█▊        | 895/5000 [00:06<00:29, 137.99it/s]Running 5000 simulations.:  18%|█▊        | 909/5000 [00:06<00:30, 136.31it/s]Running 5000 simulations.:  18%|█▊        | 923/5000 [00:06<00:30, 135.81it/s]Running 5000 simulations.:  19%|█▊        | 937/5000 [00:06<00:30, 135.13it/s]Running 5000 simulations.:  19%|█▉        | 951/5000 [00:06<00:29, 135.19it/s]Running 5000 simulations.:  19%|█▉        | 966/5000 [00:06<00:29, 137.13it/s]Running 5000 simulations.:  20%|█▉        | 981/5000 [00:06<00:28, 138.86it/s]Running 5000 simulations.:  20%|█▉        | 996/5000 [00:06<00:28, 140.45it/s]Running 5000 simulations.:  20%|██        | 1011/5000 [00:06<00:28, 141.70it/s]Running 5000 simulations.:  21%|██        | 1026/5000 [00:06<00:27, 142.36it/s]Running 5000 simulations.:  21%|██        | 1041/5000 [00:07<00:27, 142.69it/s]Running 5000 simulations.:  21%|██        | 1056/5000 [00:07<00:27, 142.95it/s]Running 5000 simulations.:  21%|██▏       | 1071/5000 [00:07<00:27, 142.78it/s]Running 5000 simulations.:  22%|██▏       | 1086/5000 [00:07<00:27, 142.60it/s]Running 5000 simulations.:  22%|██▏       | 1101/5000 [00:07<00:27, 142.63it/s]Running 5000 simulations.:  22%|██▏       | 1116/5000 [00:07<00:27, 142.68it/s]Running 5000 simulations.:  23%|██▎       | 1131/5000 [00:07<00:27, 142.47it/s]Running 5000 simulations.:  23%|██▎       | 1146/5000 [00:07<00:27, 142.48it/s]Running 5000 simulations.:  23%|██▎       | 1161/5000 [00:07<00:27, 142.18it/s]Running 5000 simulations.:  24%|██▎       | 1176/5000 [00:08<00:26, 141.96it/s]Running 5000 simulations.:  24%|██▍       | 1191/5000 [00:08<00:26, 142.56it/s]Running 5000 simulations.:  24%|██▍       | 1206/5000 [00:08<00:26, 142.20it/s]Running 5000 simulations.:  24%|██▍       | 1221/5000 [00:08<00:26, 142.40it/s]Running 5000 simulations.:  25%|██▍       | 1236/5000 [00:08<00:26, 142.57it/s]Running 5000 simulations.:  25%|██▌       | 1251/5000 [00:08<00:26, 143.52it/s]Running 5000 simulations.:  25%|██▌       | 1267/5000 [00:08<00:25, 145.55it/s]Running 5000 simulations.:  26%|██▌       | 1283/5000 [00:08<00:25, 146.93it/s]Running 5000 simulations.:  26%|██▌       | 1298/5000 [00:08<00:25, 146.97it/s]Running 5000 simulations.:  26%|██▋       | 1313/5000 [00:08<00:25, 146.43it/s]Running 5000 simulations.:  27%|██▋       | 1328/5000 [00:09<00:25, 145.83it/s]Running 5000 simulations.:  27%|██▋       | 1343/5000 [00:09<00:25, 145.27it/s]Running 5000 simulations.:  27%|██▋       | 1358/5000 [00:09<00:25, 145.11it/s]Running 5000 simulations.:  27%|██▋       | 1373/5000 [00:09<00:24, 145.17it/s]Running 5000 simulations.:  28%|██▊       | 1388/5000 [00:09<00:24, 145.08it/s]Running 5000 simulations.:  28%|██▊       | 1403/5000 [00:09<00:24, 145.29it/s]Running 5000 simulations.:  28%|██▊       | 1418/5000 [00:09<00:24, 145.56it/s]Running 5000 simulations.:  29%|██▊       | 1433/5000 [00:09<00:24, 145.74it/s]Running 5000 simulations.:  29%|██▉       | 1448/5000 [00:09<00:24, 145.34it/s]Running 5000 simulations.:  29%|██▉       | 1463/5000 [00:09<00:24, 145.52it/s]Running 5000 simulations.:  30%|██▉       | 1478/5000 [00:10<00:24, 145.86it/s]Running 5000 simulations.:  30%|██▉       | 1493/5000 [00:10<00:24, 145.48it/s]Running 5000 simulations.:  30%|███       | 1508/5000 [00:10<00:24, 145.38it/s]Running 5000 simulations.:  30%|███       | 1523/5000 [00:10<00:23, 145.02it/s]Running 5000 simulations.:  31%|███       | 1538/5000 [00:10<00:23, 144.88it/s]Running 5000 simulations.:  31%|███       | 1553/5000 [00:10<00:23, 145.16it/s]Running 5000 simulations.:  31%|███▏      | 1568/5000 [00:10<00:23, 145.09it/s]Running 5000 simulations.:  32%|███▏      | 1583/5000 [00:10<00:23, 145.13it/s]Running 5000 simulations.:  32%|███▏      | 1598/5000 [00:10<00:23, 144.86it/s]Running 5000 simulations.:  32%|███▏      | 1613/5000 [00:11<00:23, 144.83it/s]Running 5000 simulations.:  33%|███▎      | 1628/5000 [00:11<00:23, 145.11it/s]Running 5000 simulations.:  33%|███▎      | 1643/5000 [00:11<00:23, 145.01it/s]Running 5000 simulations.:  33%|███▎      | 1658/5000 [00:11<00:23, 145.04it/s]Running 5000 simulations.:  33%|███▎      | 1673/5000 [00:11<00:22, 144.95it/s]Running 5000 simulations.:  34%|███▍      | 1688/5000 [00:11<00:22, 144.70it/s]Running 5000 simulations.:  34%|███▍      | 1703/5000 [00:11<00:22, 144.60it/s]Running 5000 simulations.:  34%|███▍      | 1718/5000 [00:11<00:22, 144.82it/s]Running 5000 simulations.:  35%|███▍      | 1733/5000 [00:11<00:22, 144.85it/s]Running 5000 simulations.:  35%|███▍      | 1748/5000 [00:11<00:22, 145.12it/s]Running 5000 simulations.:  35%|███▌      | 1763/5000 [00:12<00:22, 144.91it/s]Running 5000 simulations.:  36%|███▌      | 1778/5000 [00:12<00:22, 144.78it/s]Running 5000 simulations.:  36%|███▌      | 1793/5000 [00:12<00:22, 144.77it/s]Running 5000 simulations.:  36%|███▌      | 1808/5000 [00:12<00:21, 145.12it/s]Running 5000 simulations.:  36%|███▋      | 1823/5000 [00:12<00:21, 145.51it/s]Running 5000 simulations.:  37%|███▋      | 1838/5000 [00:12<00:21, 145.69it/s]Running 5000 simulations.:  37%|███▋      | 1853/5000 [00:12<00:21, 145.50it/s]Running 5000 simulations.:  37%|███▋      | 1868/5000 [00:12<00:21, 145.06it/s]Running 5000 simulations.:  38%|███▊      | 1883/5000 [00:12<00:22, 138.76it/s]Running 5000 simulations.:  38%|███▊      | 1897/5000 [00:13<00:22, 138.85it/s]Running 5000 simulations.:  38%|███▊      | 1912/5000 [00:13<00:21, 141.13it/s]Running 5000 simulations.:  39%|███▊      | 1927/5000 [00:13<00:21, 142.64it/s]Running 5000 simulations.:  39%|███▉      | 1942/5000 [00:13<00:21, 143.20it/s]Running 5000 simulations.:  39%|███▉      | 1957/5000 [00:13<00:21, 144.08it/s]Running 5000 simulations.:  39%|███▉      | 1972/5000 [00:13<00:20, 144.83it/s]Running 5000 simulations.:  40%|███▉      | 1987/5000 [00:13<00:20, 145.30it/s]Running 5000 simulations.:  40%|████      | 2002/5000 [00:13<00:20, 145.73it/s]Running 5000 simulations.:  40%|████      | 2017/5000 [00:13<00:20, 145.36it/s]Running 5000 simulations.:  41%|████      | 2032/5000 [00:13<00:20, 145.57it/s]Running 5000 simulations.:  41%|████      | 2047/5000 [00:14<00:20, 145.71it/s]Running 5000 simulations.:  41%|████      | 2062/5000 [00:14<00:20, 145.78it/s]Running 5000 simulations.:  42%|████▏     | 2077/5000 [00:14<00:20, 145.59it/s]Running 5000 simulations.:  42%|████▏     | 2092/5000 [00:14<00:20, 144.24it/s]Running 5000 simulations.:  42%|████▏     | 2107/5000 [00:14<00:19, 144.82it/s]Running 5000 simulations.:  42%|████▏     | 2122/5000 [00:14<00:19, 145.25it/s]Running 5000 simulations.:  43%|████▎     | 2137/5000 [00:14<00:19, 145.14it/s]Running 5000 simulations.:  43%|████▎     | 2152/5000 [00:14<00:19, 145.60it/s]Running 5000 simulations.:  43%|████▎     | 2167/5000 [00:14<00:19, 145.43it/s]Running 5000 simulations.:  44%|████▎     | 2182/5000 [00:14<00:19, 146.01it/s]Running 5000 simulations.:  44%|████▍     | 2197/5000 [00:15<00:19, 146.69it/s]Running 5000 simulations.:  44%|████▍     | 2212/5000 [00:15<00:18, 146.94it/s]Running 5000 simulations.:  45%|████▍     | 2227/5000 [00:15<00:18, 146.78it/s]Running 5000 simulations.:  45%|████▍     | 2242/5000 [00:15<00:18, 146.03it/s]Running 5000 simulations.:  45%|████▌     | 2257/5000 [00:15<00:19, 143.44it/s]Running 5000 simulations.:  45%|████▌     | 2272/5000 [00:15<00:19, 141.68it/s]Running 5000 simulations.:  46%|████▌     | 2287/5000 [00:15<00:19, 140.00it/s]Running 5000 simulations.:  46%|████▌     | 2302/5000 [00:15<00:19, 139.02it/s]Running 5000 simulations.:  46%|████▋     | 2316/5000 [00:15<00:19, 138.01it/s]Running 5000 simulations.:  47%|████▋     | 2330/5000 [00:16<00:19, 137.27it/s]Running 5000 simulations.:  47%|████▋     | 2344/5000 [00:16<00:19, 136.95it/s]Running 5000 simulations.:  47%|████▋     | 2358/5000 [00:16<00:19, 137.03it/s]Running 5000 simulations.:  47%|████▋     | 2372/5000 [00:16<00:19, 136.55it/s]Running 5000 simulations.:  48%|████▊     | 2386/5000 [00:16<00:19, 136.63it/s]Running 5000 simulations.:  48%|████▊     | 2400/5000 [00:16<00:19, 136.44it/s]Running 5000 simulations.:  48%|████▊     | 2414/5000 [00:16<00:18, 136.30it/s]Running 5000 simulations.:  49%|████▊     | 2428/5000 [00:16<00:18, 136.17it/s]Running 5000 simulations.:  49%|████▉     | 2442/5000 [00:16<00:18, 136.21it/s]Running 5000 simulations.:  49%|████▉     | 2456/5000 [00:16<00:18, 136.26it/s]Running 5000 simulations.:  49%|████▉     | 2470/5000 [00:17<00:18, 136.59it/s]Running 5000 simulations.:  50%|████▉     | 2484/5000 [00:17<00:18, 136.48it/s]Running 5000 simulations.:  50%|████▉     | 2498/5000 [00:17<00:18, 136.07it/s]Running 5000 simulations.:  50%|█████     | 2512/5000 [00:17<00:18, 136.02it/s]Running 5000 simulations.:  51%|█████     | 2526/5000 [00:17<00:18, 135.78it/s]Running 5000 simulations.:  51%|█████     | 2540/5000 [00:17<00:18, 136.06it/s]Running 5000 simulations.:  51%|█████     | 2554/5000 [00:17<00:17, 136.22it/s]Running 5000 simulations.:  51%|█████▏    | 2568/5000 [00:17<00:17, 135.88it/s]Running 5000 simulations.:  52%|█████▏    | 2582/5000 [00:17<00:17, 136.23it/s]Running 5000 simulations.:  52%|█████▏    | 2596/5000 [00:17<00:17, 136.55it/s]Running 5000 simulations.:  52%|█████▏    | 2610/5000 [00:18<00:17, 136.44it/s]Running 5000 simulations.:  52%|█████▏    | 2624/5000 [00:18<00:17, 136.63it/s]Running 5000 simulations.:  53%|█████▎    | 2638/5000 [00:18<00:17, 136.96it/s]Running 5000 simulations.:  53%|█████▎    | 2652/5000 [00:18<00:17, 137.25it/s]Running 5000 simulations.:  53%|█████▎    | 2666/5000 [00:18<00:17, 137.22it/s]Running 5000 simulations.:  54%|█████▎    | 2680/5000 [00:18<00:16, 137.26it/s]Running 5000 simulations.:  54%|█████▍    | 2694/5000 [00:18<00:16, 137.04it/s]Running 5000 simulations.:  54%|█████▍    | 2708/5000 [00:18<00:16, 137.15it/s]Running 5000 simulations.:  54%|█████▍    | 2722/5000 [00:18<00:16, 137.34it/s]Running 5000 simulations.:  55%|█████▍    | 2736/5000 [00:18<00:16, 137.12it/s]Running 5000 simulations.:  55%|█████▌    | 2750/5000 [00:19<00:16, 137.08it/s]Running 5000 simulations.:  55%|█████▌    | 2764/5000 [00:19<00:16, 136.65it/s]Running 5000 simulations.:  56%|█████▌    | 2778/5000 [00:19<00:16, 136.18it/s]Running 5000 simulations.:  56%|█████▌    | 2792/5000 [00:19<00:16, 136.23it/s]Running 5000 simulations.:  56%|█████▌    | 2806/5000 [00:19<00:16, 136.53it/s]Running 5000 simulations.:  56%|█████▋    | 2820/5000 [00:19<00:15, 136.54it/s]Running 5000 simulations.:  57%|█████▋    | 2834/5000 [00:19<00:16, 134.75it/s]Running 5000 simulations.:  57%|█████▋    | 2848/5000 [00:19<00:16, 133.29it/s]Running 5000 simulations.:  57%|█████▋    | 2862/5000 [00:19<00:16, 130.94it/s]Running 5000 simulations.:  58%|█████▊    | 2877/5000 [00:20<00:15, 134.63it/s]Running 5000 simulations.:  58%|█████▊    | 2892/5000 [00:20<00:15, 138.01it/s]Running 5000 simulations.:  58%|█████▊    | 2907/5000 [00:20<00:14, 139.92it/s]Running 5000 simulations.:  58%|█████▊    | 2922/5000 [00:20<00:14, 142.01it/s]Running 5000 simulations.:  59%|█████▊    | 2937/5000 [00:20<00:14, 143.50it/s]Running 5000 simulations.:  59%|█████▉    | 2952/5000 [00:20<00:14, 144.18it/s]Running 5000 simulations.:  59%|█████▉    | 2967/5000 [00:20<00:14, 144.19it/s]Running 5000 simulations.:  60%|█████▉    | 2982/5000 [00:20<00:13, 144.35it/s]Running 5000 simulations.:  60%|█████▉    | 2997/5000 [00:20<00:13, 144.46it/s]Running 5000 simulations.:  60%|██████    | 3012/5000 [00:20<00:13, 144.56it/s]Running 5000 simulations.:  61%|██████    | 3027/5000 [00:21<00:13, 145.42it/s]Running 5000 simulations.:  61%|██████    | 3042/5000 [00:21<00:13, 146.08it/s]Running 5000 simulations.:  61%|██████    | 3057/5000 [00:21<00:13, 146.02it/s]Running 5000 simulations.:  61%|██████▏   | 3072/5000 [00:21<00:13, 146.03it/s]Running 5000 simulations.:  62%|██████▏   | 3087/5000 [00:21<00:13, 145.81it/s]Running 5000 simulations.:  62%|██████▏   | 3102/5000 [00:21<00:13, 145.99it/s]Running 5000 simulations.:  62%|██████▏   | 3117/5000 [00:21<00:12, 145.72it/s]Running 5000 simulations.:  63%|██████▎   | 3132/5000 [00:21<00:12, 145.48it/s]Running 5000 simulations.:  63%|██████▎   | 3147/5000 [00:21<00:12, 145.54it/s]Running 5000 simulations.:  63%|██████▎   | 3162/5000 [00:21<00:12, 145.56it/s]Running 5000 simulations.:  64%|██████▎   | 3177/5000 [00:22<00:12, 145.77it/s]Running 5000 simulations.:  64%|██████▍   | 3192/5000 [00:22<00:12, 146.12it/s]Running 5000 simulations.:  64%|██████▍   | 3207/5000 [00:22<00:12, 145.96it/s]Running 5000 simulations.:  64%|██████▍   | 3222/5000 [00:22<00:12, 145.62it/s]Running 5000 simulations.:  65%|██████▍   | 3237/5000 [00:22<00:12, 145.90it/s]Running 5000 simulations.:  65%|██████▌   | 3252/5000 [00:22<00:11, 145.97it/s]Running 5000 simulations.:  65%|██████▌   | 3267/5000 [00:22<00:11, 145.68it/s]Running 5000 simulations.:  66%|██████▌   | 3282/5000 [00:22<00:11, 145.44it/s]Running 5000 simulations.:  66%|██████▌   | 3297/5000 [00:22<00:11, 145.50it/s]Running 5000 simulations.:  66%|██████▌   | 3312/5000 [00:23<00:11, 146.01it/s]Running 5000 simulations.:  67%|██████▋   | 3327/5000 [00:23<00:11, 146.06it/s]Running 5000 simulations.:  67%|██████▋   | 3342/5000 [00:23<00:11, 146.17it/s]Running 5000 simulations.:  67%|██████▋   | 3357/5000 [00:23<00:11, 146.24it/s]Running 5000 simulations.:  67%|██████▋   | 3372/5000 [00:23<00:11, 146.07it/s]Running 5000 simulations.:  68%|██████▊   | 3387/5000 [00:23<00:11, 145.64it/s]Running 5000 simulations.:  68%|██████▊   | 3402/5000 [00:23<00:10, 146.04it/s]Running 5000 simulations.:  68%|██████▊   | 3417/5000 [00:23<00:10, 146.10it/s]Running 5000 simulations.:  69%|██████▊   | 3432/5000 [00:23<00:10, 145.80it/s]Running 5000 simulations.:  69%|██████▉   | 3447/5000 [00:23<00:10, 145.48it/s]Running 5000 simulations.:  69%|██████▉   | 3462/5000 [00:24<00:10, 145.25it/s]Running 5000 simulations.:  70%|██████▉   | 3477/5000 [00:24<00:10, 145.15it/s]Running 5000 simulations.:  70%|██████▉   | 3492/5000 [00:24<00:10, 145.23it/s]Running 5000 simulations.:  70%|███████   | 3507/5000 [00:24<00:10, 144.99it/s]Running 5000 simulations.:  70%|███████   | 3522/5000 [00:24<00:10, 144.90it/s]Running 5000 simulations.:  71%|███████   | 3537/5000 [00:24<00:10, 144.67it/s]Running 5000 simulations.:  71%|███████   | 3552/5000 [00:24<00:10, 144.48it/s]Running 5000 simulations.:  71%|███████▏  | 3567/5000 [00:24<00:09, 144.38it/s]Running 5000 simulations.:  72%|███████▏  | 3582/5000 [00:24<00:09, 144.87it/s]Running 5000 simulations.:  72%|███████▏  | 3597/5000 [00:24<00:09, 145.21it/s]Running 5000 simulations.:  72%|███████▏  | 3612/5000 [00:25<00:09, 145.19it/s]Running 5000 simulations.:  73%|███████▎  | 3627/5000 [00:25<00:09, 144.83it/s]Running 5000 simulations.:  73%|███████▎  | 3642/5000 [00:25<00:09, 144.73it/s]Running 5000 simulations.:  73%|███████▎  | 3657/5000 [00:25<00:09, 144.72it/s]Running 5000 simulations.:  73%|███████▎  | 3672/5000 [00:25<00:09, 145.07it/s]Running 5000 simulations.:  74%|███████▎  | 3687/5000 [00:25<00:09, 144.82it/s]Running 5000 simulations.:  74%|███████▍  | 3702/5000 [00:25<00:08, 144.25it/s]Running 5000 simulations.:  74%|███████▍  | 3717/5000 [00:25<00:08, 143.98it/s]Running 5000 simulations.:  75%|███████▍  | 3732/5000 [00:25<00:08, 144.14it/s]Running 5000 simulations.:  75%|███████▍  | 3747/5000 [00:26<00:08, 143.78it/s]Running 5000 simulations.:  75%|███████▌  | 3762/5000 [00:26<00:08, 143.55it/s]Running 5000 simulations.:  76%|███████▌  | 3777/5000 [00:26<00:08, 143.85it/s]Running 5000 simulations.:  76%|███████▌  | 3792/5000 [00:26<00:08, 144.06it/s]Running 5000 simulations.:  76%|███████▌  | 3807/5000 [00:26<00:08, 144.23it/s]Running 5000 simulations.:  76%|███████▋  | 3822/5000 [00:26<00:08, 144.27it/s]Running 5000 simulations.:  77%|███████▋  | 3837/5000 [00:26<00:08, 144.34it/s]Running 5000 simulations.:  77%|███████▋  | 3852/5000 [00:26<00:07, 144.44it/s]Running 5000 simulations.:  77%|███████▋  | 3867/5000 [00:26<00:07, 144.01it/s]Running 5000 simulations.:  78%|███████▊  | 3883/5000 [00:26<00:07, 147.04it/s]Running 5000 simulations.:  78%|███████▊  | 3899/5000 [00:27<00:07, 150.25it/s]Running 5000 simulations.:  78%|███████▊  | 3915/5000 [00:27<00:07, 149.84it/s]Running 5000 simulations.:  79%|███████▊  | 3931/5000 [00:27<00:07, 149.01it/s]Running 5000 simulations.:  79%|███████▉  | 3946/5000 [00:27<00:07, 148.13it/s]Running 5000 simulations.:  79%|███████▉  | 3961/5000 [00:27<00:07, 148.39it/s]Running 5000 simulations.:  80%|███████▉  | 3976/5000 [00:27<00:06, 148.33it/s]Running 5000 simulations.:  80%|███████▉  | 3991/5000 [00:27<00:06, 148.10it/s]Running 5000 simulations.:  80%|████████  | 4006/5000 [00:27<00:06, 148.01it/s]Running 5000 simulations.:  80%|████████  | 4021/5000 [00:27<00:06, 147.71it/s]Running 5000 simulations.:  81%|████████  | 4036/5000 [00:27<00:06, 147.32it/s]Running 5000 simulations.:  81%|████████  | 4051/5000 [00:28<00:06, 147.33it/s]Running 5000 simulations.:  81%|████████▏ | 4066/5000 [00:28<00:06, 147.08it/s]Running 5000 simulations.:  82%|████████▏ | 4081/5000 [00:28<00:06, 147.41it/s]Running 5000 simulations.:  82%|████████▏ | 4096/5000 [00:28<00:06, 147.47it/s]Running 5000 simulations.:  82%|████████▏ | 4111/5000 [00:28<00:06, 147.38it/s]Running 5000 simulations.:  83%|████████▎ | 4126/5000 [00:28<00:05, 147.08it/s]Running 5000 simulations.:  83%|████████▎ | 4141/5000 [00:28<00:05, 146.85it/s]Running 5000 simulations.:  83%|████████▎ | 4156/5000 [00:28<00:05, 146.99it/s]Running 5000 simulations.:  83%|████████▎ | 4171/5000 [00:28<00:05, 147.55it/s]Running 5000 simulations.:  84%|████████▎ | 4186/5000 [00:29<00:05, 147.89it/s]Running 5000 simulations.:  84%|████████▍ | 4201/5000 [00:29<00:05, 147.51it/s]Running 5000 simulations.:  84%|████████▍ | 4216/5000 [00:29<00:05, 147.26it/s]Running 5000 simulations.:  85%|████████▍ | 4231/5000 [00:29<00:05, 146.79it/s]Running 5000 simulations.:  85%|████████▍ | 4246/5000 [00:29<00:05, 146.32it/s]Running 5000 simulations.:  85%|████████▌ | 4261/5000 [00:29<00:05, 146.48it/s]Running 5000 simulations.:  86%|████████▌ | 4276/5000 [00:29<00:04, 147.11it/s]Running 5000 simulations.:  86%|████████▌ | 4291/5000 [00:29<00:04, 146.98it/s]Running 5000 simulations.:  86%|████████▌ | 4306/5000 [00:29<00:04, 147.48it/s]Running 5000 simulations.:  86%|████████▋ | 4321/5000 [00:29<00:04, 147.09it/s]Running 5000 simulations.:  87%|████████▋ | 4336/5000 [00:30<00:04, 147.17it/s]Running 5000 simulations.:  87%|████████▋ | 4351/5000 [00:30<00:04, 147.52it/s]Running 5000 simulations.:  87%|████████▋ | 4366/5000 [00:30<00:04, 147.45it/s]Running 5000 simulations.:  88%|████████▊ | 4381/5000 [00:30<00:04, 147.29it/s]Running 5000 simulations.:  88%|████████▊ | 4396/5000 [00:30<00:04, 147.34it/s]Running 5000 simulations.:  88%|████████▊ | 4411/5000 [00:30<00:04, 143.44it/s]Running 5000 simulations.:  89%|████████▊ | 4426/5000 [00:30<00:04, 141.37it/s]Running 5000 simulations.:  89%|████████▉ | 4441/5000 [00:30<00:04, 139.73it/s]Running 5000 simulations.:  89%|████████▉ | 4455/5000 [00:30<00:03, 138.57it/s]Running 5000 simulations.:  89%|████████▉ | 4469/5000 [00:30<00:03, 138.38it/s]Running 5000 simulations.:  90%|████████▉ | 4483/5000 [00:31<00:03, 137.58it/s]Running 5000 simulations.:  90%|████████▉ | 4497/5000 [00:31<00:03, 137.31it/s]Running 5000 simulations.:  90%|█████████ | 4511/5000 [00:31<00:03, 136.87it/s]Running 5000 simulations.:  90%|█████████ | 4525/5000 [00:31<00:03, 136.63it/s]Running 5000 simulations.:  91%|█████████ | 4539/5000 [00:31<00:03, 136.86it/s]Running 5000 simulations.:  91%|█████████ | 4553/5000 [00:31<00:03, 136.88it/s]Running 5000 simulations.:  91%|█████████▏| 4567/5000 [00:31<00:03, 136.76it/s]Running 5000 simulations.:  92%|█████████▏| 4581/5000 [00:31<00:03, 136.52it/s]Running 5000 simulations.:  92%|█████████▏| 4595/5000 [00:31<00:02, 136.77it/s]Running 5000 simulations.:  92%|█████████▏| 4609/5000 [00:31<00:02, 136.84it/s]Running 5000 simulations.:  92%|█████████▏| 4624/5000 [00:32<00:02, 138.78it/s]Running 5000 simulations.:  93%|█████████▎| 4640/5000 [00:32<00:02, 142.67it/s]Running 5000 simulations.:  93%|█████████▎| 4656/5000 [00:32<00:02, 145.66it/s]Running 5000 simulations.:  93%|█████████▎| 4671/5000 [00:32<00:02, 146.91it/s]Running 5000 simulations.:  94%|█████████▎| 4687/5000 [00:32<00:02, 148.48it/s]Running 5000 simulations.:  94%|█████████▍| 4703/5000 [00:32<00:01, 149.72it/s]Running 5000 simulations.:  94%|█████████▍| 4719/5000 [00:32<00:01, 151.23it/s]Running 5000 simulations.:  95%|█████████▍| 4735/5000 [00:32<00:01, 151.60it/s]Running 5000 simulations.:  95%|█████████▌| 4751/5000 [00:32<00:01, 152.39it/s]Running 5000 simulations.:  95%|█████████▌| 4767/5000 [00:33<00:01, 152.98it/s]Running 5000 simulations.:  96%|█████████▌| 4783/5000 [00:33<00:01, 152.94it/s]Running 5000 simulations.:  96%|█████████▌| 4799/5000 [00:33<00:01, 151.32it/s]Running 5000 simulations.:  96%|█████████▋| 4815/5000 [00:33<00:01, 151.55it/s]Running 5000 simulations.:  97%|█████████▋| 4831/5000 [00:33<00:01, 151.85it/s]Running 5000 simulations.:  97%|█████████▋| 4847/5000 [00:33<00:01, 152.49it/s]Running 5000 simulations.:  97%|█████████▋| 4863/5000 [00:33<00:00, 151.62it/s]Running 5000 simulations.:  98%|█████████▊| 4879/5000 [00:33<00:00, 152.26it/s]Running 5000 simulations.:  98%|█████████▊| 4895/5000 [00:33<00:00, 151.65it/s]Running 5000 simulations.:  98%|█████████▊| 4911/5000 [00:33<00:00, 150.97it/s]Running 5000 simulations.:  99%|█████████▊| 4927/5000 [00:34<00:00, 150.28it/s]Running 5000 simulations.:  99%|█████████▉| 4943/5000 [00:34<00:00, 149.83it/s]Running 5000 simulations.:  99%|█████████▉| 4958/5000 [00:34<00:00, 149.73it/s]Running 5000 simulations.:  99%|█████████▉| 4973/5000 [00:34<00:00, 149.40it/s]Running 5000 simulations.: 100%|█████████▉| 4988/5000 [00:34<00:00, 148.82it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:34<00:00, 144.57it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 14/5000 [00:00<00:37, 131.49it/s]Running 5000 simulations.:   1%|          | 29/5000 [00:00<00:36, 136.35it/s]Running 5000 simulations.:   1%|          | 45/5000 [00:00<00:35, 141.50it/s]Running 5000 simulations.:   1%|          | 61/5000 [00:00<00:33, 146.27it/s]Running 5000 simulations.:   2%|▏         | 76/5000 [00:00<00:33, 145.28it/s]Running 5000 simulations.:   2%|▏         | 91/5000 [00:00<00:34, 143.78it/s]Running 5000 simulations.:   2%|▏         | 106/5000 [00:00<00:34, 142.57it/s]Running 5000 simulations.:   2%|▏         | 121/5000 [00:00<00:34, 142.01it/s]Running 5000 simulations.:   3%|▎         | 135/5000 [00:00<00:34, 139.87it/s]Running 5000 simulations.:   3%|▎         | 149/5000 [00:01<00:35, 136.49it/s]Running 5000 simulations.:   3%|▎         | 163/5000 [00:01<00:36, 134.23it/s]Running 5000 simulations.:   4%|▎         | 177/5000 [00:01<00:36, 132.00it/s]Running 5000 simulations.:   4%|▍         | 192/5000 [00:01<00:35, 136.22it/s]Running 5000 simulations.:   4%|▍         | 207/5000 [00:01<00:34, 139.34it/s]Running 5000 simulations.:   4%|▍         | 222/5000 [00:01<00:33, 142.02it/s]Running 5000 simulations.:   5%|▍         | 237/5000 [00:01<00:33, 144.19it/s]Running 5000 simulations.:   5%|▌         | 253/5000 [00:01<00:32, 146.27it/s]Running 5000 simulations.:   5%|▌         | 269/5000 [00:01<00:31, 148.50it/s]Running 5000 simulations.:   6%|▌         | 285/5000 [00:01<00:31, 149.73it/s]Running 5000 simulations.:   6%|▌         | 301/5000 [00:02<00:31, 151.20it/s]Running 5000 simulations.:   6%|▋         | 317/5000 [00:02<00:30, 151.72it/s]Running 5000 simulations.:   7%|▋         | 333/5000 [00:02<00:31, 149.93it/s]Running 5000 simulations.:   7%|▋         | 349/5000 [00:02<00:31, 149.37it/s]Running 5000 simulations.:   7%|▋         | 365/5000 [00:02<00:30, 149.86it/s]Running 5000 simulations.:   8%|▊         | 381/5000 [00:02<00:30, 150.46it/s]Running 5000 simulations.:   8%|▊         | 397/5000 [00:02<00:30, 151.41it/s]Running 5000 simulations.:   8%|▊         | 413/5000 [00:02<00:30, 151.97it/s]Running 5000 simulations.:   9%|▊         | 429/5000 [00:02<00:30, 152.35it/s]Running 5000 simulations.:   9%|▉         | 445/5000 [00:03<00:30, 150.29it/s]Running 5000 simulations.:   9%|▉         | 461/5000 [00:03<00:30, 149.35it/s]Running 5000 simulations.:  10%|▉         | 476/5000 [00:03<00:30, 149.35it/s]Running 5000 simulations.:  10%|▉         | 491/5000 [00:03<00:30, 149.07it/s]Running 5000 simulations.:  10%|█         | 510/5000 [00:03<00:28, 156.92it/s]Running 5000 simulations.:  11%|█         | 526/5000 [00:03<00:28, 155.60it/s]Running 5000 simulations.:  11%|█         | 542/5000 [00:03<00:28, 154.77it/s]Running 5000 simulations.:  11%|█         | 558/5000 [00:03<00:28, 153.96it/s]Running 5000 simulations.:  11%|█▏        | 574/5000 [00:03<00:29, 151.00it/s]Running 5000 simulations.:  12%|█▏        | 590/5000 [00:03<00:29, 149.73it/s]Running 5000 simulations.:  12%|█▏        | 606/5000 [00:04<00:29, 149.74it/s]Running 5000 simulations.:  12%|█▏        | 622/5000 [00:04<00:29, 150.18it/s]Running 5000 simulations.:  13%|█▎        | 638/5000 [00:04<00:28, 150.78it/s]Running 5000 simulations.:  13%|█▎        | 654/5000 [00:04<00:28, 151.08it/s]Running 5000 simulations.:  13%|█▎        | 670/5000 [00:04<00:28, 151.63it/s]Running 5000 simulations.:  14%|█▎        | 686/5000 [00:04<00:28, 149.87it/s]Running 5000 simulations.:  14%|█▍        | 701/5000 [00:04<00:28, 149.17it/s]Running 5000 simulations.:  14%|█▍        | 716/5000 [00:04<00:28, 148.61it/s]Running 5000 simulations.:  15%|█▍        | 731/5000 [00:04<00:28, 148.77it/s]Running 5000 simulations.:  15%|█▍        | 747/5000 [00:05<00:28, 149.86it/s]Running 5000 simulations.:  15%|█▌        | 763/5000 [00:05<00:28, 150.42it/s]Running 5000 simulations.:  16%|█▌        | 779/5000 [00:05<00:27, 151.07it/s]Running 5000 simulations.:  16%|█▌        | 795/5000 [00:05<00:27, 150.82it/s]Running 5000 simulations.:  16%|█▌        | 811/5000 [00:05<00:28, 148.93it/s]Running 5000 simulations.:  17%|█▋        | 826/5000 [00:05<00:28, 148.27it/s]Running 5000 simulations.:  17%|█▋        | 842/5000 [00:05<00:27, 149.08it/s]Running 5000 simulations.:  17%|█▋        | 857/5000 [00:05<00:27, 149.27it/s]Running 5000 simulations.:  17%|█▋        | 873/5000 [00:05<00:27, 149.90it/s]Running 5000 simulations.:  18%|█▊        | 889/5000 [00:05<00:27, 150.51it/s]Running 5000 simulations.:  18%|█▊        | 905/5000 [00:06<00:27, 151.47it/s]Running 5000 simulations.:  18%|█▊        | 921/5000 [00:06<00:27, 149.47it/s]Running 5000 simulations.:  19%|█▊        | 937/5000 [00:06<00:26, 150.78it/s]Running 5000 simulations.:  19%|█▉        | 954/5000 [00:06<00:26, 155.41it/s]Running 5000 simulations.:  19%|█▉        | 970/5000 [00:06<00:25, 156.46it/s]Running 5000 simulations.:  20%|█▉        | 986/5000 [00:06<00:26, 154.21it/s]Running 5000 simulations.:  20%|██        | 1002/5000 [00:06<00:26, 152.71it/s]Running 5000 simulations.:  20%|██        | 1018/5000 [00:06<00:25, 153.38it/s]Running 5000 simulations.:  21%|██        | 1034/5000 [00:06<00:25, 152.66it/s]Running 5000 simulations.:  21%|██        | 1050/5000 [00:07<00:25, 152.62it/s]Running 5000 simulations.:  21%|██▏       | 1066/5000 [00:07<00:25, 152.02it/s]Running 5000 simulations.:  22%|██▏       | 1082/5000 [00:07<00:26, 149.76it/s]Running 5000 simulations.:  22%|██▏       | 1097/5000 [00:07<00:26, 148.97it/s]Running 5000 simulations.:  22%|██▏       | 1113/5000 [00:07<00:25, 149.91it/s]Running 5000 simulations.:  23%|██▎       | 1130/5000 [00:07<00:25, 154.20it/s]Running 5000 simulations.:  23%|██▎       | 1146/5000 [00:07<00:25, 153.26it/s]Running 5000 simulations.:  23%|██▎       | 1162/5000 [00:07<00:25, 152.99it/s]Running 5000 simulations.:  24%|██▎       | 1178/5000 [00:07<00:24, 152.93it/s]Running 5000 simulations.:  24%|██▍       | 1194/5000 [00:07<00:25, 150.73it/s]Running 5000 simulations.:  24%|██▍       | 1210/5000 [00:08<00:25, 148.99it/s]Running 5000 simulations.:  24%|██▍       | 1225/5000 [00:08<00:25, 148.25it/s]Running 5000 simulations.:  25%|██▍       | 1240/5000 [00:08<00:25, 148.26it/s]Running 5000 simulations.:  25%|██▌       | 1256/5000 [00:08<00:25, 149.41it/s]Running 5000 simulations.:  25%|██▌       | 1272/5000 [00:08<00:24, 150.22it/s]Running 5000 simulations.:  26%|██▌       | 1288/5000 [00:08<00:24, 150.33it/s]Running 5000 simulations.:  26%|██▌       | 1304/5000 [00:08<00:24, 149.79it/s]Running 5000 simulations.:  26%|██▋       | 1319/5000 [00:08<00:24, 148.12it/s]Running 5000 simulations.:  27%|██▋       | 1334/5000 [00:08<00:24, 147.36it/s]Running 5000 simulations.:  27%|██▋       | 1349/5000 [00:09<00:24, 147.87it/s]Running 5000 simulations.:  27%|██▋       | 1364/5000 [00:09<00:24, 148.43it/s]Running 5000 simulations.:  28%|██▊       | 1380/5000 [00:09<00:24, 149.11it/s]Running 5000 simulations.:  28%|██▊       | 1396/5000 [00:09<00:24, 149.53it/s]Running 5000 simulations.:  28%|██▊       | 1412/5000 [00:09<00:23, 150.48it/s]Running 5000 simulations.:  29%|██▊       | 1428/5000 [00:09<00:24, 148.33it/s]Running 5000 simulations.:  29%|██▉       | 1443/5000 [00:09<00:24, 147.88it/s]Running 5000 simulations.:  29%|██▉       | 1458/5000 [00:09<00:23, 147.74it/s]Running 5000 simulations.:  29%|██▉       | 1473/5000 [00:09<00:23, 147.63it/s]Running 5000 simulations.:  30%|██▉       | 1489/5000 [00:09<00:23, 148.83it/s]Running 5000 simulations.:  30%|███       | 1505/5000 [00:10<00:23, 149.58it/s]Running 5000 simulations.:  30%|███       | 1521/5000 [00:10<00:23, 150.45it/s]Running 5000 simulations.:  31%|███       | 1537/5000 [00:10<00:23, 150.37it/s]Running 5000 simulations.:  31%|███       | 1553/5000 [00:10<00:23, 148.53it/s]Running 5000 simulations.:  31%|███▏      | 1568/5000 [00:10<00:23, 148.50it/s]Running 5000 simulations.:  32%|███▏      | 1584/5000 [00:10<00:22, 149.25it/s]Running 5000 simulations.:  32%|███▏      | 1600/5000 [00:10<00:22, 149.67it/s]Running 5000 simulations.:  32%|███▏      | 1616/5000 [00:10<00:22, 150.13it/s]Running 5000 simulations.:  33%|███▎      | 1632/5000 [00:10<00:22, 150.90it/s]Running 5000 simulations.:  33%|███▎      | 1648/5000 [00:11<00:22, 151.07it/s]Running 5000 simulations.:  33%|███▎      | 1664/5000 [00:11<00:22, 148.30it/s]Running 5000 simulations.:  34%|███▎      | 1679/5000 [00:11<00:22, 147.78it/s]Running 5000 simulations.:  34%|███▍      | 1694/5000 [00:11<00:22, 147.91it/s]Running 5000 simulations.:  34%|███▍      | 1710/5000 [00:11<00:22, 148.76it/s]Running 5000 simulations.:  35%|███▍      | 1726/5000 [00:11<00:21, 149.82it/s]Running 5000 simulations.:  35%|███▍      | 1742/5000 [00:11<00:21, 150.26it/s]Running 5000 simulations.:  35%|███▌      | 1758/5000 [00:11<00:21, 150.73it/s]Running 5000 simulations.:  35%|███▌      | 1774/5000 [00:11<00:21, 149.30it/s]Running 5000 simulations.:  36%|███▌      | 1789/5000 [00:11<00:21, 147.84it/s]Running 5000 simulations.:  36%|███▌      | 1804/5000 [00:12<00:21, 147.76it/s]Running 5000 simulations.:  36%|███▋      | 1820/5000 [00:12<00:21, 148.49it/s]Running 5000 simulations.:  37%|███▋      | 1836/5000 [00:12<00:21, 149.35it/s]Running 5000 simulations.:  37%|███▋      | 1852/5000 [00:12<00:20, 150.51it/s]Running 5000 simulations.:  37%|███▋      | 1868/5000 [00:12<00:20, 150.81it/s]Running 5000 simulations.:  38%|███▊      | 1884/5000 [00:12<00:20, 150.96it/s]Running 5000 simulations.:  38%|███▊      | 1900/5000 [00:12<00:20, 148.23it/s]Running 5000 simulations.:  38%|███▊      | 1916/5000 [00:12<00:20, 150.29it/s]Running 5000 simulations.:  39%|███▊      | 1933/5000 [00:12<00:19, 154.66it/s]Running 5000 simulations.:  39%|███▉      | 1949/5000 [00:13<00:19, 155.69it/s]Running 5000 simulations.:  39%|███▉      | 1965/5000 [00:13<00:19, 153.29it/s]Running 5000 simulations.:  40%|███▉      | 1981/5000 [00:13<00:19, 152.12it/s]Running 5000 simulations.:  40%|███▉      | 1997/5000 [00:13<00:19, 152.61it/s]Running 5000 simulations.:  40%|████      | 2013/5000 [00:13<00:19, 152.49it/s]Running 5000 simulations.:  41%|████      | 2029/5000 [00:13<00:19, 152.37it/s]Running 5000 simulations.:  41%|████      | 2045/5000 [00:13<00:19, 150.86it/s]Running 5000 simulations.:  41%|████      | 2061/5000 [00:13<00:19, 148.61it/s]Running 5000 simulations.:  42%|████▏     | 2076/5000 [00:13<00:19, 148.53it/s]Running 5000 simulations.:  42%|████▏     | 2091/5000 [00:13<00:19, 148.52it/s]Running 5000 simulations.:  42%|████▏     | 2108/5000 [00:14<00:18, 152.39it/s]Running 5000 simulations.:  42%|████▏     | 2124/5000 [00:14<00:18, 152.32it/s]Running 5000 simulations.:  43%|████▎     | 2140/5000 [00:14<00:18, 153.91it/s]Running 5000 simulations.:  43%|████▎     | 2156/5000 [00:14<00:18, 155.36it/s]Running 5000 simulations.:  43%|████▎     | 2172/5000 [00:14<00:18, 153.99it/s]Running 5000 simulations.:  44%|████▍     | 2188/5000 [00:14<00:18, 152.77it/s]Running 5000 simulations.:  44%|████▍     | 2204/5000 [00:14<00:18, 152.55it/s]Running 5000 simulations.:  44%|████▍     | 2220/5000 [00:14<00:18, 154.33it/s]Running 5000 simulations.:  45%|████▍     | 2236/5000 [00:14<00:17, 155.63it/s]Running 5000 simulations.:  45%|████▌     | 2252/5000 [00:15<00:17, 154.63it/s]Running 5000 simulations.:  45%|████▌     | 2268/5000 [00:15<00:17, 153.23it/s]Running 5000 simulations.:  46%|████▌     | 2284/5000 [00:15<00:18, 150.53it/s]Running 5000 simulations.:  46%|████▌     | 2300/5000 [00:15<00:18, 148.50it/s]Running 5000 simulations.:  46%|████▋     | 2315/5000 [00:15<00:18, 147.78it/s]Running 5000 simulations.:  47%|████▋     | 2330/5000 [00:15<00:18, 147.71it/s]Running 5000 simulations.:  47%|████▋     | 2345/5000 [00:15<00:17, 148.05it/s]Running 5000 simulations.:  47%|████▋     | 2361/5000 [00:15<00:17, 148.65it/s]Running 5000 simulations.:  48%|████▊     | 2377/5000 [00:15<00:17, 149.17it/s]Running 5000 simulations.:  48%|████▊     | 2392/5000 [00:15<00:17, 148.69it/s]Running 5000 simulations.:  48%|████▊     | 2407/5000 [00:16<00:17, 146.88it/s]Running 5000 simulations.:  48%|████▊     | 2422/5000 [00:16<00:17, 146.19it/s]Running 5000 simulations.:  49%|████▊     | 2437/5000 [00:16<00:17, 146.62it/s]Running 5000 simulations.:  49%|████▉     | 2453/5000 [00:16<00:17, 147.76it/s]Running 5000 simulations.:  49%|████▉     | 2469/5000 [00:16<00:17, 148.65it/s]Running 5000 simulations.:  50%|████▉     | 2485/5000 [00:16<00:16, 149.08it/s]Running 5000 simulations.:  50%|█████     | 2501/5000 [00:16<00:16, 149.36it/s]Running 5000 simulations.:  50%|█████     | 2516/5000 [00:16<00:16, 148.02it/s]Running 5000 simulations.:  51%|█████     | 2531/5000 [00:16<00:16, 146.64it/s]Running 5000 simulations.:  51%|█████     | 2546/5000 [00:17<00:17, 139.19it/s]Running 5000 simulations.:  51%|█████     | 2562/5000 [00:17<00:17, 142.63it/s]Running 5000 simulations.:  52%|█████▏    | 2578/5000 [00:17<00:16, 145.07it/s]Running 5000 simulations.:  52%|█████▏    | 2594/5000 [00:17<00:16, 146.70it/s]Running 5000 simulations.:  52%|█████▏    | 2610/5000 [00:17<00:16, 147.64it/s]Running 5000 simulations.:  52%|█████▎    | 2625/5000 [00:17<00:16, 147.22it/s]Running 5000 simulations.:  53%|█████▎    | 2640/5000 [00:17<00:16, 146.23it/s]Running 5000 simulations.:  53%|█████▎    | 2655/5000 [00:17<00:15, 146.81it/s]Running 5000 simulations.:  53%|█████▎    | 2670/5000 [00:17<00:15, 147.62it/s]Running 5000 simulations.:  54%|█████▎    | 2685/5000 [00:17<00:15, 147.63it/s]Running 5000 simulations.:  54%|█████▍    | 2700/5000 [00:18<00:15, 147.96it/s]Running 5000 simulations.:  54%|█████▍    | 2715/5000 [00:18<00:15, 148.38it/s]Running 5000 simulations.:  55%|█████▍    | 2731/5000 [00:18<00:15, 149.06it/s]Running 5000 simulations.:  55%|█████▍    | 2746/5000 [00:18<00:15, 147.54it/s]Running 5000 simulations.:  55%|█████▌    | 2761/5000 [00:18<00:15, 147.08it/s]Running 5000 simulations.:  56%|█████▌    | 2776/5000 [00:18<00:15, 146.55it/s]Running 5000 simulations.:  56%|█████▌    | 2791/5000 [00:18<00:15, 146.86it/s]Running 5000 simulations.:  56%|█████▌    | 2808/5000 [00:18<00:14, 151.06it/s]Running 5000 simulations.:  56%|█████▋    | 2824/5000 [00:18<00:14, 151.01it/s]Running 5000 simulations.:  57%|█████▋    | 2840/5000 [00:19<00:14, 150.44it/s]Running 5000 simulations.:  57%|█████▋    | 2856/5000 [00:19<00:14, 149.03it/s]Running 5000 simulations.:  57%|█████▋    | 2871/5000 [00:19<00:14, 146.94it/s]Running 5000 simulations.:  58%|█████▊    | 2888/5000 [00:19<00:13, 151.30it/s]Running 5000 simulations.:  58%|█████▊    | 2905/5000 [00:19<00:13, 154.78it/s]Running 5000 simulations.:  58%|█████▊    | 2921/5000 [00:19<00:13, 153.32it/s]Running 5000 simulations.:  59%|█████▊    | 2937/5000 [00:19<00:13, 152.03it/s]Running 5000 simulations.:  59%|█████▉    | 2953/5000 [00:19<00:13, 150.86it/s]Running 5000 simulations.:  59%|█████▉    | 2969/5000 [00:19<00:13, 150.48it/s]Running 5000 simulations.:  60%|█████▉    | 2985/5000 [00:19<00:13, 150.18it/s]Running 5000 simulations.:  60%|██████    | 3001/5000 [00:20<00:13, 149.41it/s]Running 5000 simulations.:  60%|██████    | 3016/5000 [00:20<00:13, 148.16it/s]Running 5000 simulations.:  61%|██████    | 3031/5000 [00:20<00:13, 147.14it/s]Running 5000 simulations.:  61%|██████    | 3046/5000 [00:20<00:13, 146.79it/s]Running 5000 simulations.:  61%|██████    | 3061/5000 [00:20<00:13, 147.50it/s]Running 5000 simulations.:  62%|██████▏   | 3077/5000 [00:20<00:12, 148.27it/s]Running 5000 simulations.:  62%|██████▏   | 3092/5000 [00:20<00:12, 148.61it/s]Running 5000 simulations.:  62%|██████▏   | 3107/5000 [00:20<00:12, 148.51it/s]Running 5000 simulations.:  62%|██████▏   | 3122/5000 [00:20<00:12, 147.89it/s]Running 5000 simulations.:  63%|██████▎   | 3137/5000 [00:21<00:12, 145.98it/s]Running 5000 simulations.:  63%|██████▎   | 3152/5000 [00:21<00:12, 145.54it/s]Running 5000 simulations.:  63%|██████▎   | 3167/5000 [00:21<00:12, 146.47it/s]Running 5000 simulations.:  64%|██████▎   | 3182/5000 [00:21<00:12, 146.89it/s]Running 5000 simulations.:  64%|██████▍   | 3198/5000 [00:21<00:12, 148.05it/s]Running 5000 simulations.:  64%|██████▍   | 3213/5000 [00:21<00:12, 148.16it/s]Running 5000 simulations.:  65%|██████▍   | 3228/5000 [00:21<00:11, 148.55it/s]Running 5000 simulations.:  65%|██████▍   | 3243/5000 [00:21<00:11, 147.93it/s]Running 5000 simulations.:  65%|██████▌   | 3258/5000 [00:21<00:11, 146.97it/s]Running 5000 simulations.:  65%|██████▌   | 3273/5000 [00:21<00:11, 146.86it/s]Running 5000 simulations.:  66%|██████▌   | 3288/5000 [00:22<00:11, 147.55it/s]Running 5000 simulations.:  66%|██████▌   | 3304/5000 [00:22<00:11, 148.71it/s]Running 5000 simulations.:  66%|██████▋   | 3320/5000 [00:22<00:11, 150.82it/s]Running 5000 simulations.:  67%|██████▋   | 3336/5000 [00:22<00:11, 150.85it/s]Running 5000 simulations.:  67%|██████▋   | 3352/5000 [00:22<00:10, 152.39it/s]Running 5000 simulations.:  67%|██████▋   | 3368/5000 [00:22<00:10, 151.14it/s]Running 5000 simulations.:  68%|██████▊   | 3384/5000 [00:22<00:10, 151.68it/s]Running 5000 simulations.:  68%|██████▊   | 3400/5000 [00:22<00:10, 152.66it/s]Running 5000 simulations.:  68%|██████▊   | 3416/5000 [00:22<00:10, 151.85it/s]Running 5000 simulations.:  69%|██████▊   | 3432/5000 [00:22<00:10, 151.42it/s]Running 5000 simulations.:  69%|██████▉   | 3448/5000 [00:23<00:10, 151.06it/s]Running 5000 simulations.:  69%|██████▉   | 3464/5000 [00:23<00:10, 150.44it/s]Running 5000 simulations.:  70%|██████▉   | 3480/5000 [00:23<00:10, 147.76it/s]Running 5000 simulations.:  70%|██████▉   | 3495/5000 [00:23<00:10, 147.11it/s]Running 5000 simulations.:  70%|███████   | 3510/5000 [00:23<00:10, 146.91it/s]Running 5000 simulations.:  70%|███████   | 3525/5000 [00:23<00:10, 147.39it/s]Running 5000 simulations.:  71%|███████   | 3541/5000 [00:23<00:09, 148.19it/s]Running 5000 simulations.:  71%|███████   | 3557/5000 [00:23<00:09, 148.94it/s]Running 5000 simulations.:  71%|███████▏  | 3573/5000 [00:23<00:09, 149.62it/s]Running 5000 simulations.:  72%|███████▏  | 3588/5000 [00:24<00:09, 148.86it/s]Running 5000 simulations.:  72%|███████▏  | 3603/5000 [00:24<00:09, 147.02it/s]Running 5000 simulations.:  72%|███████▏  | 3618/5000 [00:24<00:09, 147.51it/s]Running 5000 simulations.:  73%|███████▎  | 3633/5000 [00:24<00:09, 148.16it/s]Running 5000 simulations.:  73%|███████▎  | 3648/5000 [00:24<00:09, 148.57it/s]Running 5000 simulations.:  73%|███████▎  | 3664/5000 [00:24<00:08, 149.34it/s]Running 5000 simulations.:  74%|███████▎  | 3680/5000 [00:24<00:08, 149.93it/s]Running 5000 simulations.:  74%|███████▍  | 3696/5000 [00:24<00:08, 150.20it/s]Running 5000 simulations.:  74%|███████▍  | 3712/5000 [00:24<00:08, 148.19it/s]Running 5000 simulations.:  75%|███████▍  | 3727/5000 [00:24<00:08, 147.89it/s]Running 5000 simulations.:  75%|███████▍  | 3742/5000 [00:25<00:08, 148.45it/s]Running 5000 simulations.:  75%|███████▌  | 3757/5000 [00:25<00:08, 148.56it/s]Running 5000 simulations.:  75%|███████▌  | 3773/5000 [00:25<00:08, 149.68it/s]Running 5000 simulations.:  76%|███████▌  | 3789/5000 [00:25<00:08, 150.61it/s]Running 5000 simulations.:  76%|███████▌  | 3805/5000 [00:25<00:07, 151.20it/s]Running 5000 simulations.:  76%|███████▋  | 3821/5000 [00:25<00:07, 149.21it/s]Running 5000 simulations.:  77%|███████▋  | 3836/5000 [00:25<00:07, 149.20it/s]Running 5000 simulations.:  77%|███████▋  | 3853/5000 [00:25<00:07, 153.80it/s]Running 5000 simulations.:  77%|███████▋  | 3870/5000 [00:25<00:07, 156.74it/s]Running 5000 simulations.:  78%|███████▊  | 3886/5000 [00:26<00:07, 154.78it/s]Running 5000 simulations.:  78%|███████▊  | 3902/5000 [00:26<00:07, 152.67it/s]Running 5000 simulations.:  78%|███████▊  | 3918/5000 [00:26<00:07, 152.37it/s]Running 5000 simulations.:  79%|███████▊  | 3934/5000 [00:26<00:06, 152.44it/s]Running 5000 simulations.:  79%|███████▉  | 3950/5000 [00:26<00:06, 152.45it/s]Running 5000 simulations.:  79%|███████▉  | 3966/5000 [00:26<00:06, 151.19it/s]Running 5000 simulations.:  80%|███████▉  | 3982/5000 [00:26<00:06, 149.11it/s]Running 5000 simulations.:  80%|███████▉  | 3997/5000 [00:26<00:06, 148.70it/s]Running 5000 simulations.:  80%|████████  | 4013/5000 [00:26<00:06, 149.35it/s]Running 5000 simulations.:  81%|████████  | 4028/5000 [00:26<00:06, 149.37it/s]Running 5000 simulations.:  81%|████████  | 4044/5000 [00:27<00:06, 150.16it/s]Running 5000 simulations.:  81%|████████  | 4060/5000 [00:27<00:06, 150.97it/s]Running 5000 simulations.:  82%|████████▏ | 4076/5000 [00:27<00:06, 151.41it/s]Running 5000 simulations.:  82%|████████▏ | 4092/5000 [00:27<00:06, 149.28it/s]Running 5000 simulations.:  82%|████████▏ | 4107/5000 [00:27<00:06, 148.73it/s]Running 5000 simulations.:  82%|████████▏ | 4122/5000 [00:27<00:05, 148.16it/s]Running 5000 simulations.:  83%|████████▎ | 4138/5000 [00:27<00:05, 149.01it/s]Running 5000 simulations.:  83%|████████▎ | 4154/5000 [00:27<00:05, 150.28it/s]Running 5000 simulations.:  83%|████████▎ | 4170/5000 [00:27<00:05, 150.66it/s]Running 5000 simulations.:  84%|████████▎ | 4186/5000 [00:28<00:05, 150.94it/s]Running 5000 simulations.:  84%|████████▍ | 4202/5000 [00:28<00:05, 149.27it/s]Running 5000 simulations.:  84%|████████▍ | 4217/5000 [00:28<00:05, 147.74it/s]Running 5000 simulations.:  85%|████████▍ | 4232/5000 [00:28<00:05, 147.83it/s]Running 5000 simulations.:  85%|████████▍ | 4247/5000 [00:28<00:05, 147.92it/s]Running 5000 simulations.:  85%|████████▌ | 4263/5000 [00:28<00:04, 149.09it/s]Running 5000 simulations.:  86%|████████▌ | 4279/5000 [00:28<00:04, 149.69it/s]Running 5000 simulations.:  86%|████████▌ | 4295/5000 [00:28<00:04, 150.21it/s]Running 5000 simulations.:  86%|████████▌ | 4311/5000 [00:28<00:04, 148.16it/s]Running 5000 simulations.:  87%|████████▋ | 4326/5000 [00:28<00:04, 145.21it/s]Running 5000 simulations.:  87%|████████▋ | 4341/5000 [00:29<00:04, 142.88it/s]Running 5000 simulations.:  87%|████████▋ | 4356/5000 [00:29<00:04, 141.77it/s]Running 5000 simulations.:  87%|████████▋ | 4371/5000 [00:29<00:04, 140.80it/s]Running 5000 simulations.:  88%|████████▊ | 4386/5000 [00:29<00:04, 139.38it/s]Running 5000 simulations.:  88%|████████▊ | 4400/5000 [00:29<00:04, 135.07it/s]Running 5000 simulations.:  88%|████████▊ | 4414/5000 [00:29<00:04, 132.76it/s]Running 5000 simulations.:  89%|████████▊ | 4428/5000 [00:29<00:04, 131.92it/s]Running 5000 simulations.:  89%|████████▉ | 4442/5000 [00:29<00:04, 132.00it/s]Running 5000 simulations.:  89%|████████▉ | 4456/5000 [00:29<00:04, 133.60it/s]Running 5000 simulations.:  89%|████████▉ | 4470/5000 [00:30<00:03, 135.32it/s]Running 5000 simulations.:  90%|████████▉ | 4484/5000 [00:30<00:03, 136.34it/s]Running 5000 simulations.:  90%|████████▉ | 4498/5000 [00:30<00:03, 137.19it/s]Running 5000 simulations.:  90%|█████████ | 4513/5000 [00:30<00:03, 138.23it/s]Running 5000 simulations.:  91%|█████████ | 4529/5000 [00:30<00:03, 141.80it/s]Running 5000 simulations.:  91%|█████████ | 4544/5000 [00:30<00:03, 143.98it/s]Running 5000 simulations.:  91%|█████████ | 4559/5000 [00:30<00:03, 145.59it/s]Running 5000 simulations.:  91%|█████████▏| 4574/5000 [00:30<00:02, 146.40it/s]Running 5000 simulations.:  92%|█████████▏| 4589/5000 [00:30<00:02, 145.59it/s]Running 5000 simulations.:  92%|█████████▏| 4604/5000 [00:30<00:02, 145.91it/s]Running 5000 simulations.:  92%|█████████▏| 4619/5000 [00:31<00:02, 146.57it/s]Running 5000 simulations.:  93%|█████████▎| 4634/5000 [00:31<00:02, 147.58it/s]Running 5000 simulations.:  93%|█████████▎| 4650/5000 [00:31<00:02, 148.85it/s]Running 5000 simulations.:  93%|█████████▎| 4666/5000 [00:31<00:02, 149.34it/s]Running 5000 simulations.:  94%|█████████▎| 4682/5000 [00:31<00:02, 149.85it/s]Running 5000 simulations.:  94%|█████████▍| 4697/5000 [00:31<00:02, 148.87it/s]Running 5000 simulations.:  94%|█████████▍| 4712/5000 [00:31<00:01, 147.05it/s]Running 5000 simulations.:  95%|█████████▍| 4727/5000 [00:31<00:01, 147.15it/s]Running 5000 simulations.:  95%|█████████▍| 4742/5000 [00:31<00:01, 147.67it/s]Running 5000 simulations.:  95%|█████████▌| 4757/5000 [00:31<00:01, 148.33it/s]Running 5000 simulations.:  95%|█████████▌| 4772/5000 [00:32<00:01, 148.76it/s]Running 5000 simulations.:  96%|█████████▌| 4788/5000 [00:32<00:01, 149.52it/s]Running 5000 simulations.:  96%|█████████▌| 4804/5000 [00:32<00:01, 149.81it/s]Running 5000 simulations.:  96%|█████████▋| 4819/5000 [00:32<00:01, 147.04it/s]Running 5000 simulations.:  97%|█████████▋| 4835/5000 [00:32<00:01, 149.45it/s]Running 5000 simulations.:  97%|█████████▋| 4852/5000 [00:32<00:00, 153.90it/s]Running 5000 simulations.:  97%|█████████▋| 4868/5000 [00:32<00:00, 155.17it/s]Running 5000 simulations.:  98%|█████████▊| 4884/5000 [00:32<00:00, 152.02it/s]Running 5000 simulations.:  98%|█████████▊| 4900/5000 [00:32<00:00, 150.92it/s]Running 5000 simulations.:  98%|█████████▊| 4916/5000 [00:33<00:00, 150.65it/s]Running 5000 simulations.:  99%|█████████▊| 4932/5000 [00:33<00:00, 150.58it/s]Running 5000 simulations.:  99%|█████████▉| 4948/5000 [00:33<00:00, 150.33it/s]Running 5000 simulations.:  99%|█████████▉| 4964/5000 [00:33<00:00, 149.34it/s]Running 5000 simulations.: 100%|█████████▉| 4979/5000 [00:33<00:00, 147.32it/s]Running 5000 simulations.: 100%|█████████▉| 4994/5000 [00:33<00:00, 146.78it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:33<00:00, 148.82it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 25/5000 [00:00<00:20, 245.48it/s]Running 5000 simulations.:   1%|          | 50/5000 [00:00<00:20, 244.64it/s]Running 5000 simulations.:   2%|▏         | 75/5000 [00:00<00:20, 243.94it/s]Running 5000 simulations.:   2%|▏         | 100/5000 [00:00<00:20, 243.32it/s]Running 5000 simulations.:   2%|▎         | 125/5000 [00:00<00:20, 243.22it/s]Running 5000 simulations.:   3%|▎         | 150/5000 [00:00<00:19, 242.65it/s]Running 5000 simulations.:   4%|▎         | 175/5000 [00:00<00:19, 242.52it/s]Running 5000 simulations.:   4%|▍         | 200/5000 [00:00<00:19, 241.87it/s]Running 5000 simulations.:   4%|▍         | 224/5000 [00:00<00:19, 240.77it/s]Running 5000 simulations.:   5%|▍         | 249/5000 [00:01<00:19, 240.60it/s]Running 5000 simulations.:   5%|▌         | 274/5000 [00:01<00:19, 240.66it/s]Running 5000 simulations.:   6%|▌         | 298/5000 [00:01<00:20, 231.67it/s]Running 5000 simulations.:   6%|▋         | 321/5000 [00:01<00:21, 222.26it/s]Running 5000 simulations.:   7%|▋         | 344/5000 [00:01<00:21, 216.54it/s]Running 5000 simulations.:   7%|▋         | 366/5000 [00:01<00:21, 212.59it/s]Running 5000 simulations.:   8%|▊         | 388/5000 [00:01<00:22, 209.50it/s]Running 5000 simulations.:   8%|▊         | 409/5000 [00:01<00:22, 206.89it/s]Running 5000 simulations.:   9%|▊         | 430/5000 [00:01<00:22, 205.14it/s]Running 5000 simulations.:   9%|▉         | 451/5000 [00:02<00:22, 203.68it/s]Running 5000 simulations.:   9%|▉         | 472/5000 [00:02<00:22, 205.21it/s]Running 5000 simulations.:  10%|▉         | 498/5000 [00:02<00:20, 217.59it/s]Running 5000 simulations.:  10%|█         | 524/5000 [00:02<00:19, 226.78it/s]Running 5000 simulations.:  11%|█         | 549/5000 [00:02<00:19, 233.24it/s]Running 5000 simulations.:  12%|█▏        | 575/5000 [00:02<00:18, 238.77it/s]Running 5000 simulations.:  12%|█▏        | 601/5000 [00:02<00:18, 242.76it/s]Running 5000 simulations.:  13%|█▎        | 627/5000 [00:02<00:17, 245.43it/s]Running 5000 simulations.:  13%|█▎        | 652/5000 [00:02<00:17, 246.66it/s]Running 5000 simulations.:  14%|█▎        | 678/5000 [00:02<00:17, 247.82it/s]Running 5000 simulations.:  14%|█▍        | 704/5000 [00:03<00:17, 248.87it/s]Running 5000 simulations.:  15%|█▍        | 730/5000 [00:03<00:17, 249.39it/s]Running 5000 simulations.:  15%|█▌        | 755/5000 [00:03<00:17, 248.99it/s]Running 5000 simulations.:  16%|█▌        | 780/5000 [00:03<00:16, 248.67it/s]Running 5000 simulations.:  16%|█▌        | 805/5000 [00:03<00:16, 248.97it/s]Running 5000 simulations.:  17%|█▋        | 830/5000 [00:03<00:16, 248.64it/s]Running 5000 simulations.:  17%|█▋        | 855/5000 [00:03<00:16, 248.98it/s]Running 5000 simulations.:  18%|█▊        | 881/5000 [00:03<00:16, 249.83it/s]Running 5000 simulations.:  18%|█▊        | 906/5000 [00:03<00:16, 249.78it/s]Running 5000 simulations.:  19%|█▊        | 931/5000 [00:03<00:16, 249.84it/s]Running 5000 simulations.:  19%|█▉        | 957/5000 [00:04<00:16, 249.94it/s]Running 5000 simulations.:  20%|█▉        | 983/5000 [00:04<00:16, 250.06it/s]Running 5000 simulations.:  20%|██        | 1009/5000 [00:04<00:15, 249.95it/s]Running 5000 simulations.:  21%|██        | 1034/5000 [00:04<00:15, 249.53it/s]Running 5000 simulations.:  21%|██        | 1059/5000 [00:04<00:15, 249.67it/s]Running 5000 simulations.:  22%|██▏       | 1085/5000 [00:04<00:15, 249.95it/s]Running 5000 simulations.:  22%|██▏       | 1110/5000 [00:04<00:15, 249.94it/s]Running 5000 simulations.:  23%|██▎       | 1135/5000 [00:04<00:15, 249.65it/s]Running 5000 simulations.:  23%|██▎       | 1161/5000 [00:04<00:15, 249.86it/s]Running 5000 simulations.:  24%|██▎       | 1187/5000 [00:04<00:15, 250.45it/s]Running 5000 simulations.:  24%|██▍       | 1213/5000 [00:05<00:15, 249.84it/s]Running 5000 simulations.:  25%|██▍       | 1238/5000 [00:05<00:15, 248.91it/s]Running 5000 simulations.:  25%|██▌       | 1264/5000 [00:05<00:14, 249.22it/s]Running 5000 simulations.:  26%|██▌       | 1289/5000 [00:05<00:14, 248.96it/s]Running 5000 simulations.:  26%|██▋       | 1314/5000 [00:05<00:14, 248.41it/s]Running 5000 simulations.:  27%|██▋       | 1339/5000 [00:05<00:14, 248.08it/s]Running 5000 simulations.:  27%|██▋       | 1364/5000 [00:05<00:14, 247.54it/s]Running 5000 simulations.:  28%|██▊       | 1389/5000 [00:05<00:14, 247.13it/s]Running 5000 simulations.:  28%|██▊       | 1414/5000 [00:05<00:14, 246.92it/s]Running 5000 simulations.:  29%|██▉       | 1439/5000 [00:05<00:14, 247.42it/s]Running 5000 simulations.:  29%|██▉       | 1465/5000 [00:06<00:14, 248.27it/s]Running 5000 simulations.:  30%|██▉       | 1490/5000 [00:06<00:14, 248.06it/s]Running 5000 simulations.:  30%|███       | 1515/5000 [00:06<00:14, 248.00it/s]Running 5000 simulations.:  31%|███       | 1540/5000 [00:06<00:13, 247.82it/s]Running 5000 simulations.:  31%|███▏      | 1566/5000 [00:06<00:13, 248.57it/s]Running 5000 simulations.:  32%|███▏      | 1591/5000 [00:06<00:13, 246.79it/s]Running 5000 simulations.:  32%|███▏      | 1616/5000 [00:06<00:13, 242.77it/s]Running 5000 simulations.:  33%|███▎      | 1641/5000 [00:06<00:13, 240.03it/s]Running 5000 simulations.:  33%|███▎      | 1666/5000 [00:06<00:14, 237.98it/s]Running 5000 simulations.:  34%|███▍      | 1690/5000 [00:07<00:14, 236.35it/s]Running 5000 simulations.:  34%|███▍      | 1714/5000 [00:07<00:13, 234.81it/s]Running 5000 simulations.:  35%|███▍      | 1738/5000 [00:07<00:13, 233.51it/s]Running 5000 simulations.:  35%|███▌      | 1762/5000 [00:07<00:13, 233.23it/s]Running 5000 simulations.:  36%|███▌      | 1786/5000 [00:07<00:13, 232.63it/s]Running 5000 simulations.:  36%|███▌      | 1810/5000 [00:07<00:13, 231.46it/s]Running 5000 simulations.:  37%|███▋      | 1834/5000 [00:07<00:13, 230.66it/s]Running 5000 simulations.:  37%|███▋      | 1858/5000 [00:07<00:13, 230.10it/s]Running 5000 simulations.:  38%|███▊      | 1882/5000 [00:07<00:13, 229.31it/s]Running 5000 simulations.:  38%|███▊      | 1905/5000 [00:07<00:13, 227.61it/s]Running 5000 simulations.:  39%|███▊      | 1929/5000 [00:08<00:13, 229.67it/s]Running 5000 simulations.:  39%|███▉      | 1952/5000 [00:08<00:13, 229.68it/s]Running 5000 simulations.:  40%|███▉      | 1976/5000 [00:08<00:13, 230.44it/s]Running 5000 simulations.:  40%|████      | 2000/5000 [00:08<00:13, 230.25it/s]Running 5000 simulations.:  40%|████      | 2024/5000 [00:08<00:12, 229.69it/s]Running 5000 simulations.:  41%|████      | 2048/5000 [00:08<00:12, 230.00it/s]Running 5000 simulations.:  41%|████▏     | 2073/5000 [00:08<00:12, 233.52it/s]Running 5000 simulations.:  42%|████▏     | 2099/5000 [00:08<00:12, 239.43it/s]Running 5000 simulations.:  42%|████▎     | 2125/5000 [00:08<00:11, 243.60it/s]Running 5000 simulations.:  43%|████▎     | 2150/5000 [00:08<00:12, 234.55it/s]Running 5000 simulations.:  43%|████▎     | 2174/5000 [00:09<00:12, 222.05it/s]Running 5000 simulations.:  44%|████▍     | 2197/5000 [00:09<00:13, 214.10it/s]Running 5000 simulations.:  44%|████▍     | 2219/5000 [00:09<00:13, 209.33it/s]Running 5000 simulations.:  45%|████▍     | 2241/5000 [00:09<00:13, 206.15it/s]Running 5000 simulations.:  45%|████▌     | 2262/5000 [00:09<00:13, 205.45it/s]Running 5000 simulations.:  46%|████▌     | 2284/5000 [00:09<00:13, 207.78it/s]Running 5000 simulations.:  46%|████▌     | 2306/5000 [00:09<00:12, 209.67it/s]Running 5000 simulations.:  47%|████▋     | 2328/5000 [00:09<00:12, 210.51it/s]Running 5000 simulations.:  47%|████▋     | 2350/5000 [00:09<00:12, 209.93it/s]Running 5000 simulations.:  47%|████▋     | 2372/5000 [00:10<00:12, 208.66it/s]Running 5000 simulations.:  48%|████▊     | 2393/5000 [00:10<00:12, 208.82it/s]Running 5000 simulations.:  48%|████▊     | 2414/5000 [00:10<00:12, 208.73it/s]Running 5000 simulations.:  49%|████▊     | 2435/5000 [00:10<00:12, 209.06it/s]Running 5000 simulations.:  49%|████▉     | 2456/5000 [00:10<00:12, 208.61it/s]Running 5000 simulations.:  50%|████▉     | 2477/5000 [00:10<00:12, 208.00it/s]Running 5000 simulations.:  50%|████▉     | 2498/5000 [00:10<00:12, 208.48it/s]Running 5000 simulations.:  50%|█████     | 2523/5000 [00:10<00:11, 219.41it/s]Running 5000 simulations.:  51%|█████     | 2549/5000 [00:10<00:10, 227.93it/s]Running 5000 simulations.:  51%|█████▏    | 2573/5000 [00:10<00:10, 229.62it/s]Running 5000 simulations.:  52%|█████▏    | 2597/5000 [00:11<00:10, 230.06it/s]Running 5000 simulations.:  52%|█████▏    | 2621/5000 [00:11<00:10, 230.50it/s]Running 5000 simulations.:  53%|█████▎    | 2645/5000 [00:11<00:10, 231.43it/s]Running 5000 simulations.:  53%|█████▎    | 2669/5000 [00:11<00:10, 231.25it/s]Running 5000 simulations.:  54%|█████▍    | 2693/5000 [00:11<00:09, 231.59it/s]Running 5000 simulations.:  54%|█████▍    | 2717/5000 [00:11<00:09, 232.03it/s]Running 5000 simulations.:  55%|█████▍    | 2741/5000 [00:11<00:09, 232.25it/s]Running 5000 simulations.:  55%|█████▌    | 2765/5000 [00:11<00:09, 232.29it/s]Running 5000 simulations.:  56%|█████▌    | 2789/5000 [00:11<00:09, 231.59it/s]Running 5000 simulations.:  56%|█████▋    | 2813/5000 [00:12<00:09, 231.33it/s]Running 5000 simulations.:  57%|█████▋    | 2837/5000 [00:12<00:09, 231.84it/s]Running 5000 simulations.:  57%|█████▋    | 2861/5000 [00:12<00:09, 232.09it/s]Running 5000 simulations.:  58%|█████▊    | 2885/5000 [00:12<00:09, 232.82it/s]Running 5000 simulations.:  58%|█████▊    | 2909/5000 [00:12<00:08, 233.15it/s]Running 5000 simulations.:  59%|█████▊    | 2933/5000 [00:12<00:08, 233.13it/s]Running 5000 simulations.:  59%|█████▉    | 2957/5000 [00:12<00:08, 233.17it/s]Running 5000 simulations.:  60%|█████▉    | 2981/5000 [00:12<00:08, 233.07it/s]Running 5000 simulations.:  60%|██████    | 3005/5000 [00:12<00:08, 232.96it/s]Running 5000 simulations.:  61%|██████    | 3029/5000 [00:12<00:08, 232.89it/s]Running 5000 simulations.:  61%|██████    | 3053/5000 [00:13<00:08, 233.19it/s]Running 5000 simulations.:  62%|██████▏   | 3077/5000 [00:13<00:08, 233.41it/s]Running 5000 simulations.:  62%|██████▏   | 3101/5000 [00:13<00:08, 233.71it/s]Running 5000 simulations.:  62%|██████▎   | 3125/5000 [00:13<00:08, 234.10it/s]Running 5000 simulations.:  63%|██████▎   | 3149/5000 [00:13<00:07, 234.27it/s]Running 5000 simulations.:  63%|██████▎   | 3173/5000 [00:13<00:07, 234.53it/s]Running 5000 simulations.:  64%|██████▍   | 3197/5000 [00:13<00:07, 234.43it/s]Running 5000 simulations.:  64%|██████▍   | 3221/5000 [00:13<00:07, 234.48it/s]Running 5000 simulations.:  65%|██████▍   | 3245/5000 [00:13<00:07, 234.64it/s]Running 5000 simulations.:  65%|██████▌   | 3269/5000 [00:13<00:07, 234.58it/s]Running 5000 simulations.:  66%|██████▌   | 3293/5000 [00:14<00:07, 234.61it/s]Running 5000 simulations.:  66%|██████▋   | 3317/5000 [00:14<00:07, 234.65it/s]Running 5000 simulations.:  67%|██████▋   | 3341/5000 [00:14<00:07, 224.55it/s]Running 5000 simulations.:  67%|██████▋   | 3365/5000 [00:14<00:07, 226.26it/s]Running 5000 simulations.:  68%|██████▊   | 3389/5000 [00:14<00:07, 227.47it/s]Running 5000 simulations.:  68%|██████▊   | 3412/5000 [00:14<00:06, 228.08it/s]Running 5000 simulations.:  69%|██████▊   | 3436/5000 [00:14<00:06, 228.66it/s]Running 5000 simulations.:  69%|██████▉   | 3460/5000 [00:14<00:06, 229.11it/s]Running 5000 simulations.:  70%|██████▉   | 3484/5000 [00:14<00:06, 229.54it/s]Running 5000 simulations.:  70%|███████   | 3508/5000 [00:15<00:06, 229.91it/s]Running 5000 simulations.:  71%|███████   | 3532/5000 [00:15<00:06, 230.16it/s]Running 5000 simulations.:  71%|███████   | 3556/5000 [00:15<00:06, 230.45it/s]Running 5000 simulations.:  72%|███████▏  | 3580/5000 [00:15<00:06, 231.01it/s]Running 5000 simulations.:  72%|███████▏  | 3604/5000 [00:15<00:06, 231.60it/s]Running 5000 simulations.:  73%|███████▎  | 3628/5000 [00:15<00:05, 231.86it/s]Running 5000 simulations.:  73%|███████▎  | 3652/5000 [00:15<00:05, 231.36it/s]Running 5000 simulations.:  74%|███████▎  | 3676/5000 [00:15<00:05, 231.19it/s]Running 5000 simulations.:  74%|███████▍  | 3700/5000 [00:15<00:05, 230.46it/s]Running 5000 simulations.:  74%|███████▍  | 3724/5000 [00:15<00:05, 230.10it/s]Running 5000 simulations.:  75%|███████▍  | 3748/5000 [00:16<00:05, 230.58it/s]Running 5000 simulations.:  75%|███████▌  | 3772/5000 [00:16<00:05, 230.59it/s]Running 5000 simulations.:  76%|███████▌  | 3796/5000 [00:16<00:05, 229.77it/s]Running 5000 simulations.:  76%|███████▋  | 3820/5000 [00:16<00:05, 230.42it/s]Running 5000 simulations.:  77%|███████▋  | 3844/5000 [00:16<00:05, 230.19it/s]Running 5000 simulations.:  77%|███████▋  | 3868/5000 [00:16<00:04, 229.56it/s]Running 5000 simulations.:  78%|███████▊  | 3891/5000 [00:16<00:04, 226.69it/s]Running 5000 simulations.:  78%|███████▊  | 3914/5000 [00:16<00:04, 224.84it/s]Running 5000 simulations.:  79%|███████▊  | 3937/5000 [00:16<00:04, 224.29it/s]Running 5000 simulations.:  79%|███████▉  | 3960/5000 [00:16<00:04, 223.40it/s]Running 5000 simulations.:  80%|███████▉  | 3983/5000 [00:17<00:04, 223.22it/s]Running 5000 simulations.:  80%|████████  | 4006/5000 [00:17<00:04, 222.76it/s]Running 5000 simulations.:  81%|████████  | 4029/5000 [00:17<00:04, 222.61it/s]Running 5000 simulations.:  81%|████████  | 4052/5000 [00:17<00:04, 221.87it/s]Running 5000 simulations.:  82%|████████▏ | 4075/5000 [00:17<00:04, 220.43it/s]Running 5000 simulations.:  82%|████████▏ | 4098/5000 [00:17<00:04, 220.67it/s]Running 5000 simulations.:  82%|████████▏ | 4121/5000 [00:17<00:03, 221.12it/s]Running 5000 simulations.:  83%|████████▎ | 4144/5000 [00:17<00:03, 221.34it/s]Running 5000 simulations.:  83%|████████▎ | 4167/5000 [00:17<00:03, 220.95it/s]Running 5000 simulations.:  84%|████████▍ | 4190/5000 [00:18<00:03, 221.46it/s]Running 5000 simulations.:  84%|████████▍ | 4213/5000 [00:18<00:03, 221.87it/s]Running 5000 simulations.:  85%|████████▍ | 4236/5000 [00:18<00:03, 221.93it/s]Running 5000 simulations.:  85%|████████▌ | 4259/5000 [00:18<00:03, 220.45it/s]Running 5000 simulations.:  86%|████████▌ | 4282/5000 [00:18<00:03, 220.60it/s]Running 5000 simulations.:  86%|████████▌ | 4305/5000 [00:18<00:03, 221.66it/s]Running 5000 simulations.:  87%|████████▋ | 4328/5000 [00:18<00:03, 221.30it/s]Running 5000 simulations.:  87%|████████▋ | 4351/5000 [00:18<00:02, 221.95it/s]Running 5000 simulations.:  87%|████████▋ | 4374/5000 [00:18<00:02, 221.88it/s]Running 5000 simulations.:  88%|████████▊ | 4397/5000 [00:18<00:02, 221.83it/s]Running 5000 simulations.:  88%|████████▊ | 4420/5000 [00:19<00:02, 221.76it/s]Running 5000 simulations.:  89%|████████▉ | 4443/5000 [00:19<00:02, 221.41it/s]Running 5000 simulations.:  89%|████████▉ | 4466/5000 [00:19<00:02, 222.05it/s]Running 5000 simulations.:  90%|████████▉ | 4489/5000 [00:19<00:02, 222.21it/s]Running 5000 simulations.:  90%|█████████ | 4512/5000 [00:19<00:02, 221.93it/s]Running 5000 simulations.:  91%|█████████ | 4535/5000 [00:19<00:02, 222.35it/s]Running 5000 simulations.:  91%|█████████ | 4558/5000 [00:19<00:01, 222.65it/s]Running 5000 simulations.:  92%|█████████▏| 4581/5000 [00:19<00:01, 222.25it/s]Running 5000 simulations.:  92%|█████████▏| 4604/5000 [00:19<00:01, 222.33it/s]Running 5000 simulations.:  93%|█████████▎| 4627/5000 [00:20<00:01, 222.20it/s]Running 5000 simulations.:  93%|█████████▎| 4650/5000 [00:20<00:01, 222.29it/s]Running 5000 simulations.:  93%|█████████▎| 4673/5000 [00:20<00:01, 222.68it/s]Running 5000 simulations.:  94%|█████████▍| 4696/5000 [00:20<00:01, 222.88it/s]Running 5000 simulations.:  94%|█████████▍| 4719/5000 [00:20<00:01, 222.99it/s]Running 5000 simulations.:  95%|█████████▍| 4742/5000 [00:20<00:01, 224.43it/s]Running 5000 simulations.:  95%|█████████▌| 4765/5000 [00:20<00:01, 225.15it/s]Running 5000 simulations.:  96%|█████████▌| 4788/5000 [00:20<00:00, 225.56it/s]Running 5000 simulations.:  96%|█████████▌| 4811/5000 [00:20<00:00, 226.54it/s]Running 5000 simulations.:  97%|█████████▋| 4834/5000 [00:20<00:00, 226.97it/s]Running 5000 simulations.:  97%|█████████▋| 4857/5000 [00:21<00:00, 226.80it/s]Running 5000 simulations.:  98%|█████████▊| 4880/5000 [00:21<00:00, 225.93it/s]Running 5000 simulations.:  98%|█████████▊| 4903/5000 [00:21<00:00, 225.29it/s]Running 5000 simulations.:  99%|█████████▊| 4926/5000 [00:21<00:00, 224.63it/s]Running 5000 simulations.:  99%|█████████▉| 4949/5000 [00:21<00:00, 224.42it/s]Running 5000 simulations.:  99%|█████████▉| 4972/5000 [00:21<00:00, 224.37it/s]Running 5000 simulations.: 100%|█████████▉| 4995/5000 [00:21<00:00, 224.18it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:21<00:00, 230.76it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   1%|          | 26/5000 [00:00<00:19, 253.96it/s]Running 5000 simulations.:   1%|          | 52/5000 [00:00<00:19, 253.45it/s]Running 5000 simulations.:   2%|▏         | 78/5000 [00:00<00:19, 252.96it/s]Running 5000 simulations.:   2%|▏         | 103/5000 [00:00<00:19, 251.41it/s]Running 5000 simulations.:   3%|▎         | 129/5000 [00:00<00:19, 251.28it/s]Running 5000 simulations.:   3%|▎         | 154/5000 [00:00<00:19, 250.58it/s]Running 5000 simulations.:   4%|▎         | 179/5000 [00:00<00:19, 250.03it/s]Running 5000 simulations.:   4%|▍         | 205/5000 [00:00<00:19, 250.31it/s]Running 5000 simulations.:   5%|▍         | 230/5000 [00:00<00:19, 249.53it/s]Running 5000 simulations.:   5%|▌         | 255/5000 [00:01<00:19, 248.56it/s]Running 5000 simulations.:   6%|▌         | 280/5000 [00:01<00:19, 246.33it/s]Running 5000 simulations.:   6%|▌         | 305/5000 [00:01<00:19, 244.77it/s]Running 5000 simulations.:   7%|▋         | 330/5000 [00:01<00:19, 243.53it/s]Running 5000 simulations.:   7%|▋         | 355/5000 [00:01<00:19, 242.47it/s]Running 5000 simulations.:   8%|▊         | 380/5000 [00:01<00:19, 241.46it/s]Running 5000 simulations.:   8%|▊         | 405/5000 [00:01<00:19, 240.74it/s]Running 5000 simulations.:   9%|▊         | 429/5000 [00:01<00:19, 240.25it/s]Running 5000 simulations.:   9%|▉         | 454/5000 [00:01<00:18, 241.08it/s]Running 5000 simulations.:  10%|▉         | 479/5000 [00:01<00:18, 240.84it/s]Running 5000 simulations.:  10%|█         | 504/5000 [00:02<00:18, 240.86it/s]Running 5000 simulations.:  11%|█         | 529/5000 [00:02<00:18, 240.58it/s]Running 5000 simulations.:  11%|█         | 554/5000 [00:02<00:18, 241.12it/s]Running 5000 simulations.:  12%|█▏        | 579/5000 [00:02<00:18, 241.27it/s]Running 5000 simulations.:  12%|█▏        | 604/5000 [00:02<00:18, 240.88it/s]Running 5000 simulations.:  13%|█▎        | 629/5000 [00:02<00:18, 240.12it/s]Running 5000 simulations.:  13%|█▎        | 654/5000 [00:02<00:18, 239.52it/s]Running 5000 simulations.:  14%|█▎        | 679/5000 [00:02<00:17, 240.22it/s]Running 5000 simulations.:  14%|█▍        | 704/5000 [00:02<00:17, 239.75it/s]Running 5000 simulations.:  15%|█▍        | 729/5000 [00:02<00:17, 239.88it/s]Running 5000 simulations.:  15%|█▌        | 754/5000 [00:03<00:17, 240.01it/s]Running 5000 simulations.:  16%|█▌        | 779/5000 [00:03<00:17, 239.69it/s]Running 5000 simulations.:  16%|█▌        | 804/5000 [00:03<00:17, 239.96it/s]Running 5000 simulations.:  17%|█▋        | 829/5000 [00:03<00:17, 240.40it/s]Running 5000 simulations.:  17%|█▋        | 854/5000 [00:03<00:17, 239.31it/s]Running 5000 simulations.:  18%|█▊        | 878/5000 [00:03<00:17, 239.05it/s]Running 5000 simulations.:  18%|█▊        | 903/5000 [00:03<00:17, 239.71it/s]Running 5000 simulations.:  19%|█▊        | 927/5000 [00:03<00:16, 239.66it/s]Running 5000 simulations.:  19%|█▉        | 952/5000 [00:03<00:16, 240.09it/s]Running 5000 simulations.:  20%|█▉        | 977/5000 [00:04<00:16, 240.42it/s]Running 5000 simulations.:  20%|██        | 1002/5000 [00:04<00:16, 240.46it/s]Running 5000 simulations.:  21%|██        | 1027/5000 [00:04<00:16, 240.49it/s]Running 5000 simulations.:  21%|██        | 1052/5000 [00:04<00:16, 240.68it/s]Running 5000 simulations.:  22%|██▏       | 1077/5000 [00:04<00:16, 237.88it/s]Running 5000 simulations.:  22%|██▏       | 1101/5000 [00:04<00:16, 236.77it/s]Running 5000 simulations.:  22%|██▎       | 1125/5000 [00:04<00:16, 237.61it/s]Running 5000 simulations.:  23%|██▎       | 1149/5000 [00:04<00:16, 238.06it/s]Running 5000 simulations.:  23%|██▎       | 1173/5000 [00:04<00:16, 238.29it/s]Running 5000 simulations.:  24%|██▍       | 1197/5000 [00:04<00:15, 238.37it/s]Running 5000 simulations.:  24%|██▍       | 1222/5000 [00:05<00:15, 239.08it/s]Running 5000 simulations.:  25%|██▍       | 1246/5000 [00:05<00:15, 238.48it/s]Running 5000 simulations.:  25%|██▌       | 1270/5000 [00:05<00:15, 238.87it/s]Running 5000 simulations.:  26%|██▌       | 1294/5000 [00:05<00:15, 238.57it/s]Running 5000 simulations.:  26%|██▋       | 1318/5000 [00:05<00:15, 238.28it/s]Running 5000 simulations.:  27%|██▋       | 1342/5000 [00:05<00:15, 238.10it/s]Running 5000 simulations.:  27%|██▋       | 1367/5000 [00:05<00:15, 238.65it/s]Running 5000 simulations.:  28%|██▊       | 1391/5000 [00:05<00:15, 238.87it/s]Running 5000 simulations.:  28%|██▊       | 1415/5000 [00:05<00:15, 238.44it/s]Running 5000 simulations.:  29%|██▉       | 1439/5000 [00:05<00:14, 238.23it/s]Running 5000 simulations.:  29%|██▉       | 1464/5000 [00:06<00:14, 239.64it/s]Running 5000 simulations.:  30%|██▉       | 1488/5000 [00:06<00:14, 239.42it/s]Running 5000 simulations.:  30%|███       | 1512/5000 [00:06<00:14, 238.86it/s]Running 5000 simulations.:  31%|███       | 1536/5000 [00:06<00:14, 238.56it/s]Running 5000 simulations.:  31%|███       | 1560/5000 [00:06<00:14, 238.40it/s]Running 5000 simulations.:  32%|███▏      | 1584/5000 [00:06<00:14, 238.05it/s]Running 5000 simulations.:  32%|███▏      | 1608/5000 [00:06<00:14, 238.03it/s]Running 5000 simulations.:  33%|███▎      | 1633/5000 [00:06<00:14, 238.62it/s]Running 5000 simulations.:  33%|███▎      | 1657/5000 [00:06<00:14, 238.55it/s]Running 5000 simulations.:  34%|███▎      | 1681/5000 [00:06<00:13, 238.41it/s]Running 5000 simulations.:  34%|███▍      | 1705/5000 [00:07<00:13, 238.62it/s]Running 5000 simulations.:  35%|███▍      | 1729/5000 [00:07<00:13, 238.97it/s]Running 5000 simulations.:  35%|███▌      | 1753/5000 [00:07<00:13, 238.52it/s]Running 5000 simulations.:  36%|███▌      | 1777/5000 [00:07<00:13, 237.69it/s]Running 5000 simulations.:  36%|███▌      | 1801/5000 [00:07<00:13, 236.88it/s]Running 5000 simulations.:  36%|███▋      | 1825/5000 [00:07<00:13, 236.16it/s]Running 5000 simulations.:  37%|███▋      | 1849/5000 [00:07<00:13, 236.61it/s]Running 5000 simulations.:  37%|███▋      | 1873/5000 [00:07<00:13, 236.67it/s]Running 5000 simulations.:  38%|███▊      | 1897/5000 [00:07<00:13, 236.47it/s]Running 5000 simulations.:  38%|███▊      | 1921/5000 [00:07<00:13, 235.99it/s]Running 5000 simulations.:  39%|███▉      | 1945/5000 [00:08<00:12, 235.88it/s]Running 5000 simulations.:  39%|███▉      | 1970/5000 [00:08<00:12, 237.49it/s]Running 5000 simulations.:  40%|███▉      | 1994/5000 [00:08<00:12, 238.21it/s]Running 5000 simulations.:  40%|████      | 2019/5000 [00:08<00:12, 238.85it/s]Running 5000 simulations.:  41%|████      | 2044/5000 [00:08<00:12, 239.87it/s]Running 5000 simulations.:  41%|████▏     | 2069/5000 [00:08<00:12, 240.27it/s]Running 5000 simulations.:  42%|████▏     | 2094/5000 [00:08<00:12, 240.04it/s]Running 5000 simulations.:  42%|████▏     | 2119/5000 [00:08<00:12, 239.68it/s]Running 5000 simulations.:  43%|████▎     | 2143/5000 [00:08<00:11, 239.39it/s]Running 5000 simulations.:  43%|████▎     | 2167/5000 [00:09<00:11, 239.39it/s]Running 5000 simulations.:  44%|████▍     | 2192/5000 [00:09<00:11, 240.20it/s]Running 5000 simulations.:  44%|████▍     | 2217/5000 [00:09<00:11, 240.62it/s]Running 5000 simulations.:  45%|████▍     | 2242/5000 [00:09<00:11, 240.06it/s]Running 5000 simulations.:  45%|████▌     | 2267/5000 [00:09<00:11, 239.40it/s]Running 5000 simulations.:  46%|████▌     | 2291/5000 [00:09<00:11, 239.30it/s]Running 5000 simulations.:  46%|████▋     | 2316/5000 [00:09<00:11, 239.79it/s]Running 5000 simulations.:  47%|████▋     | 2340/5000 [00:09<00:11, 238.69it/s]Running 5000 simulations.:  47%|████▋     | 2364/5000 [00:09<00:11, 237.97it/s]Running 5000 simulations.:  48%|████▊     | 2388/5000 [00:09<00:10, 237.96it/s]Running 5000 simulations.:  48%|████▊     | 2412/5000 [00:10<00:10, 238.40it/s]Running 5000 simulations.:  49%|████▊     | 2436/5000 [00:10<00:10, 237.84it/s]Running 5000 simulations.:  49%|████▉     | 2461/5000 [00:10<00:10, 238.56it/s]Running 5000 simulations.:  50%|████▉     | 2486/5000 [00:10<00:10, 239.24it/s]Running 5000 simulations.:  50%|█████     | 2511/5000 [00:10<00:10, 239.51it/s]Running 5000 simulations.:  51%|█████     | 2535/5000 [00:10<00:10, 238.91it/s]Running 5000 simulations.:  51%|█████     | 2559/5000 [00:10<00:10, 238.43it/s]Running 5000 simulations.:  52%|█████▏    | 2585/5000 [00:10<00:09, 242.18it/s]Running 5000 simulations.:  52%|█████▏    | 2611/5000 [00:10<00:09, 246.70it/s]Running 5000 simulations.:  53%|█████▎    | 2637/5000 [00:10<00:09, 248.60it/s]Running 5000 simulations.:  53%|█████▎    | 2663/5000 [00:11<00:09, 249.99it/s]Running 5000 simulations.:  54%|█████▍    | 2689/5000 [00:11<00:09, 250.92it/s]Running 5000 simulations.:  54%|█████▍    | 2715/5000 [00:11<00:09, 251.91it/s]Running 5000 simulations.:  55%|█████▍    | 2741/5000 [00:11<00:08, 253.21it/s]Running 5000 simulations.:  55%|█████▌    | 2767/5000 [00:11<00:08, 253.34it/s]Running 5000 simulations.:  56%|█████▌    | 2793/5000 [00:11<00:08, 254.10it/s]Running 5000 simulations.:  56%|█████▋    | 2819/5000 [00:11<00:09, 238.50it/s]Running 5000 simulations.:  57%|█████▋    | 2844/5000 [00:11<00:09, 228.44it/s]Running 5000 simulations.:  57%|█████▋    | 2868/5000 [00:11<00:09, 225.79it/s]Running 5000 simulations.:  58%|█████▊    | 2892/5000 [00:12<00:09, 229.59it/s]Running 5000 simulations.:  58%|█████▊    | 2917/5000 [00:12<00:08, 232.68it/s]Running 5000 simulations.:  59%|█████▉    | 2942/5000 [00:12<00:08, 235.09it/s]Running 5000 simulations.:  59%|█████▉    | 2967/5000 [00:12<00:08, 236.59it/s]Running 5000 simulations.:  60%|█████▉    | 2991/5000 [00:12<00:08, 235.33it/s]Running 5000 simulations.:  60%|██████    | 3015/5000 [00:12<00:08, 234.33it/s]Running 5000 simulations.:  61%|██████    | 3039/5000 [00:12<00:08, 235.68it/s]Running 5000 simulations.:  61%|██████▏   | 3064/5000 [00:12<00:08, 237.23it/s]Running 5000 simulations.:  62%|██████▏   | 3089/5000 [00:12<00:08, 238.09it/s]Running 5000 simulations.:  62%|██████▏   | 3113/5000 [00:12<00:07, 237.78it/s]Running 5000 simulations.:  63%|██████▎   | 3138/5000 [00:13<00:07, 238.81it/s]Running 5000 simulations.:  63%|██████▎   | 3163/5000 [00:13<00:07, 239.72it/s]Running 5000 simulations.:  64%|██████▎   | 3187/5000 [00:13<00:07, 239.50it/s]Running 5000 simulations.:  64%|██████▍   | 3211/5000 [00:13<00:07, 238.29it/s]Running 5000 simulations.:  65%|██████▍   | 3236/5000 [00:13<00:07, 238.92it/s]Running 5000 simulations.:  65%|██████▌   | 3260/5000 [00:13<00:07, 238.76it/s]Running 5000 simulations.:  66%|██████▌   | 3284/5000 [00:13<00:07, 237.90it/s]Running 5000 simulations.:  66%|██████▌   | 3308/5000 [00:13<00:07, 237.10it/s]Running 5000 simulations.:  67%|██████▋   | 3332/5000 [00:13<00:07, 237.02it/s]Running 5000 simulations.:  67%|██████▋   | 3356/5000 [00:13<00:06, 236.12it/s]Running 5000 simulations.:  68%|██████▊   | 3380/5000 [00:14<00:06, 235.97it/s]Running 5000 simulations.:  68%|██████▊   | 3404/5000 [00:14<00:06, 236.31it/s]Running 5000 simulations.:  69%|██████▊   | 3428/5000 [00:14<00:06, 236.32it/s]Running 5000 simulations.:  69%|██████▉   | 3452/5000 [00:14<00:06, 236.71it/s]Running 5000 simulations.:  70%|██████▉   | 3476/5000 [00:14<00:06, 237.20it/s]Running 5000 simulations.:  70%|███████   | 3500/5000 [00:14<00:06, 237.65it/s]Running 5000 simulations.:  70%|███████   | 3524/5000 [00:14<00:06, 237.71it/s]Running 5000 simulations.:  71%|███████   | 3548/5000 [00:14<00:06, 237.60it/s]Running 5000 simulations.:  71%|███████▏  | 3572/5000 [00:14<00:06, 237.11it/s]Running 5000 simulations.:  72%|███████▏  | 3596/5000 [00:14<00:05, 236.37it/s]Running 5000 simulations.:  72%|███████▏  | 3620/5000 [00:15<00:05, 236.05it/s]Running 5000 simulations.:  73%|███████▎  | 3644/5000 [00:15<00:05, 236.04it/s]Running 5000 simulations.:  73%|███████▎  | 3668/5000 [00:15<00:05, 236.19it/s]Running 5000 simulations.:  74%|███████▍  | 3692/5000 [00:15<00:05, 237.03it/s]Running 5000 simulations.:  74%|███████▍  | 3716/5000 [00:15<00:05, 236.93it/s]Running 5000 simulations.:  75%|███████▍  | 3740/5000 [00:15<00:05, 236.47it/s]Running 5000 simulations.:  75%|███████▌  | 3764/5000 [00:15<00:05, 237.00it/s]Running 5000 simulations.:  76%|███████▌  | 3788/5000 [00:15<00:05, 237.09it/s]Running 5000 simulations.:  76%|███████▌  | 3812/5000 [00:15<00:05, 237.17it/s]Running 5000 simulations.:  77%|███████▋  | 3836/5000 [00:16<00:04, 236.78it/s]Running 5000 simulations.:  77%|███████▋  | 3860/5000 [00:16<00:04, 237.02it/s]Running 5000 simulations.:  78%|███████▊  | 3884/5000 [00:16<00:04, 237.52it/s]Running 5000 simulations.:  78%|███████▊  | 3908/5000 [00:16<00:04, 237.01it/s]Running 5000 simulations.:  79%|███████▊  | 3933/5000 [00:16<00:04, 239.24it/s]Running 5000 simulations.:  79%|███████▉  | 3959/5000 [00:16<00:04, 244.21it/s]Running 5000 simulations.:  80%|███████▉  | 3985/5000 [00:16<00:04, 247.48it/s]Running 5000 simulations.:  80%|████████  | 4011/5000 [00:16<00:03, 249.74it/s]Running 5000 simulations.:  81%|████████  | 4037/5000 [00:16<00:03, 249.58it/s]Running 5000 simulations.:  81%|████████  | 4062/5000 [00:16<00:03, 247.14it/s]Running 5000 simulations.:  82%|████████▏ | 4088/5000 [00:17<00:03, 249.30it/s]Running 5000 simulations.:  82%|████████▏ | 4114/5000 [00:17<00:03, 251.00it/s]Running 5000 simulations.:  83%|████████▎ | 4140/5000 [00:17<00:03, 252.13it/s]Running 5000 simulations.:  83%|████████▎ | 4166/5000 [00:17<00:03, 249.98it/s]Running 5000 simulations.:  84%|████████▍ | 4192/5000 [00:17<00:03, 250.98it/s]Running 5000 simulations.:  84%|████████▍ | 4218/5000 [00:17<00:03, 251.80it/s]Running 5000 simulations.:  85%|████████▍ | 4244/5000 [00:17<00:02, 252.84it/s]Running 5000 simulations.:  85%|████████▌ | 4270/5000 [00:17<00:02, 253.52it/s]Running 5000 simulations.:  86%|████████▌ | 4296/5000 [00:17<00:02, 253.34it/s]Running 5000 simulations.:  86%|████████▋ | 4322/5000 [00:17<00:02, 252.81it/s]Running 5000 simulations.:  87%|████████▋ | 4348/5000 [00:18<00:02, 251.36it/s]Running 5000 simulations.:  88%|████████▊ | 4375/5000 [00:18<00:02, 254.41it/s]Running 5000 simulations.:  88%|████████▊ | 4402/5000 [00:18<00:02, 256.18it/s]Running 5000 simulations.:  89%|████████▊ | 4428/5000 [00:18<00:02, 248.60it/s]Running 5000 simulations.:  89%|████████▉ | 4453/5000 [00:18<00:02, 243.32it/s]Running 5000 simulations.:  90%|████████▉ | 4478/5000 [00:18<00:02, 241.05it/s]Running 5000 simulations.:  90%|█████████ | 4503/5000 [00:18<00:02, 239.28it/s]Running 5000 simulations.:  91%|█████████ | 4527/5000 [00:18<00:01, 237.93it/s]Running 5000 simulations.:  91%|█████████ | 4551/5000 [00:18<00:01, 237.26it/s]Running 5000 simulations.:  92%|█████████▏| 4575/5000 [00:19<00:01, 236.46it/s]Running 5000 simulations.:  92%|█████████▏| 4599/5000 [00:19<00:01, 236.09it/s]Running 5000 simulations.:  92%|█████████▏| 4623/5000 [00:19<00:01, 235.70it/s]Running 5000 simulations.:  93%|█████████▎| 4647/5000 [00:19<00:01, 235.45it/s]Running 5000 simulations.:  93%|█████████▎| 4671/5000 [00:19<00:01, 235.67it/s]Running 5000 simulations.:  94%|█████████▍| 4695/5000 [00:19<00:01, 236.42it/s]Running 5000 simulations.:  94%|█████████▍| 4719/5000 [00:19<00:01, 236.70it/s]Running 5000 simulations.:  95%|█████████▍| 4743/5000 [00:19<00:01, 231.88it/s]Running 5000 simulations.:  95%|█████████▌| 4767/5000 [00:19<00:01, 223.30it/s]Running 5000 simulations.:  96%|█████████▌| 4790/5000 [00:19<00:01, 209.17it/s]Running 5000 simulations.:  96%|█████████▌| 4812/5000 [00:20<00:00, 208.34it/s]Running 5000 simulations.:  97%|█████████▋| 4834/5000 [00:20<00:00, 207.75it/s]Running 5000 simulations.:  97%|█████████▋| 4855/5000 [00:20<00:00, 207.84it/s]Running 5000 simulations.:  98%|█████████▊| 4876/5000 [00:20<00:00, 207.50it/s]Running 5000 simulations.:  98%|█████████▊| 4897/5000 [00:20<00:00, 207.50it/s]Running 5000 simulations.:  98%|█████████▊| 4918/5000 [00:20<00:00, 207.71it/s]Running 5000 simulations.:  99%|█████████▉| 4940/5000 [00:20<00:00, 210.94it/s]Running 5000 simulations.:  99%|█████████▉| 4964/5000 [00:20<00:00, 218.33it/s]Running 5000 simulations.: 100%|█████████▉| 4988/5000 [00:20<00:00, 223.41it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:20<00:00, 238.79it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 25/5000 [00:00<00:20, 243.53it/s]Running 5000 simulations.:   1%|          | 50/5000 [00:00<00:20, 243.21it/s]Running 5000 simulations.:   1%|▏         | 71/5000 [00:00<00:21, 231.56it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:20, 235.93it/s]Running 5000 simulations.:   2%|▏         | 121/5000 [00:00<00:20, 239.39it/s]Running 5000 simulations.:   3%|▎         | 146/5000 [00:00<00:20, 241.89it/s]Running 5000 simulations.:   3%|▎         | 171/5000 [00:00<00:19, 242.87it/s]Running 5000 simulations.:   4%|▍         | 196/5000 [00:00<00:19, 242.22it/s]Running 5000 simulations.:   4%|▍         | 220/5000 [00:00<00:19, 241.54it/s]Running 5000 simulations.:   5%|▍         | 244/5000 [00:01<00:19, 240.91it/s]Running 5000 simulations.:   5%|▌         | 269/5000 [00:01<00:19, 240.72it/s]Running 5000 simulations.:   6%|▌         | 293/5000 [00:01<00:19, 240.37it/s]Running 5000 simulations.:   6%|▋         | 317/5000 [00:01<00:19, 240.06it/s]Running 5000 simulations.:   7%|▋         | 341/5000 [00:01<00:19, 239.98it/s]Running 5000 simulations.:   7%|▋         | 365/5000 [00:01<00:19, 239.69it/s]Running 5000 simulations.:   8%|▊         | 389/5000 [00:01<00:19, 239.48it/s]Running 5000 simulations.:   8%|▊         | 414/5000 [00:01<00:19, 239.86it/s]Running 5000 simulations.:   9%|▉         | 438/5000 [00:01<00:19, 239.60it/s]Running 5000 simulations.:   9%|▉         | 462/5000 [00:01<00:19, 236.82it/s]Running 5000 simulations.:  10%|▉         | 486/5000 [00:02<00:19, 236.57it/s]Running 5000 simulations.:  10%|█         | 510/5000 [00:02<00:18, 237.21it/s]Running 5000 simulations.:  11%|█         | 534/5000 [00:02<00:18, 237.18it/s]Running 5000 simulations.:  11%|█         | 559/5000 [00:02<00:18, 238.21it/s]Running 5000 simulations.:  12%|█▏        | 583/5000 [00:02<00:18, 237.91it/s]Running 5000 simulations.:  12%|█▏        | 607/5000 [00:02<00:18, 237.94it/s]Running 5000 simulations.:  13%|█▎        | 631/5000 [00:02<00:18, 237.59it/s]Running 5000 simulations.:  13%|█▎        | 655/5000 [00:02<00:18, 238.25it/s]Running 5000 simulations.:  14%|█▎        | 679/5000 [00:02<00:18, 237.75it/s]Running 5000 simulations.:  14%|█▍        | 704/5000 [00:02<00:18, 238.39it/s]Running 5000 simulations.:  15%|█▍        | 728/5000 [00:03<00:17, 237.51it/s]Running 5000 simulations.:  15%|█▌        | 752/5000 [00:03<00:17, 237.98it/s]Running 5000 simulations.:  16%|█▌        | 776/5000 [00:03<00:17, 237.91it/s]Running 5000 simulations.:  16%|█▌        | 800/5000 [00:03<00:17, 237.47it/s]Running 5000 simulations.:  16%|█▋        | 824/5000 [00:03<00:17, 237.75it/s]Running 5000 simulations.:  17%|█▋        | 848/5000 [00:03<00:17, 238.18it/s]Running 5000 simulations.:  17%|█▋        | 873/5000 [00:03<00:17, 239.04it/s]Running 5000 simulations.:  18%|█▊        | 898/5000 [00:03<00:17, 239.45it/s]Running 5000 simulations.:  18%|█▊        | 923/5000 [00:03<00:17, 239.72it/s]Running 5000 simulations.:  19%|█▉        | 947/5000 [00:03<00:16, 239.54it/s]Running 5000 simulations.:  19%|█▉        | 971/5000 [00:04<00:16, 239.30it/s]Running 5000 simulations.:  20%|█▉        | 995/5000 [00:04<00:16, 239.20it/s]Running 5000 simulations.:  20%|██        | 1019/5000 [00:04<00:16, 239.42it/s]Running 5000 simulations.:  21%|██        | 1044/5000 [00:04<00:16, 239.69it/s]Running 5000 simulations.:  21%|██▏       | 1068/5000 [00:04<00:16, 239.65it/s]Running 5000 simulations.:  22%|██▏       | 1092/5000 [00:04<00:16, 239.10it/s]Running 5000 simulations.:  22%|██▏       | 1116/5000 [00:04<00:16, 238.41it/s]Running 5000 simulations.:  23%|██▎       | 1140/5000 [00:04<00:16, 237.98it/s]Running 5000 simulations.:  23%|██▎       | 1164/5000 [00:04<00:16, 237.25it/s]Running 5000 simulations.:  24%|██▍       | 1188/5000 [00:04<00:16, 236.83it/s]Running 5000 simulations.:  24%|██▍       | 1212/5000 [00:05<00:16, 236.74it/s]Running 5000 simulations.:  25%|██▍       | 1236/5000 [00:05<00:15, 236.58it/s]Running 5000 simulations.:  25%|██▌       | 1260/5000 [00:05<00:15, 236.34it/s]Running 5000 simulations.:  26%|██▌       | 1284/5000 [00:05<00:15, 237.39it/s]Running 5000 simulations.:  26%|██▌       | 1308/5000 [00:05<00:15, 237.82it/s]Running 5000 simulations.:  27%|██▋       | 1332/5000 [00:05<00:15, 237.52it/s]Running 5000 simulations.:  27%|██▋       | 1356/5000 [00:05<00:15, 237.09it/s]Running 5000 simulations.:  28%|██▊       | 1380/5000 [00:05<00:15, 237.73it/s]Running 5000 simulations.:  28%|██▊       | 1404/5000 [00:05<00:15, 238.16it/s]Running 5000 simulations.:  29%|██▊       | 1428/5000 [00:05<00:14, 238.19it/s]Running 5000 simulations.:  29%|██▉       | 1452/5000 [00:06<00:14, 237.01it/s]Running 5000 simulations.:  30%|██▉       | 1476/5000 [00:06<00:14, 236.55it/s]Running 5000 simulations.:  30%|███       | 1500/5000 [00:06<00:14, 233.33it/s]Running 5000 simulations.:  30%|███       | 1524/5000 [00:06<00:14, 233.59it/s]Running 5000 simulations.:  31%|███       | 1548/5000 [00:06<00:14, 234.36it/s]Running 5000 simulations.:  31%|███▏      | 1572/5000 [00:06<00:14, 235.24it/s]Running 5000 simulations.:  32%|███▏      | 1596/5000 [00:06<00:14, 235.96it/s]Running 5000 simulations.:  32%|███▏      | 1620/5000 [00:06<00:14, 236.63it/s]Running 5000 simulations.:  33%|███▎      | 1644/5000 [00:06<00:14, 236.89it/s]Running 5000 simulations.:  33%|███▎      | 1668/5000 [00:07<00:14, 237.73it/s]Running 5000 simulations.:  34%|███▍      | 1694/5000 [00:07<00:13, 243.71it/s]Running 5000 simulations.:  34%|███▍      | 1719/5000 [00:07<00:13, 241.69it/s]Running 5000 simulations.:  35%|███▍      | 1746/5000 [00:07<00:13, 247.89it/s]Running 5000 simulations.:  35%|███▌      | 1773/5000 [00:07<00:12, 253.03it/s]Running 5000 simulations.:  36%|███▌      | 1800/5000 [00:07<00:12, 256.09it/s]Running 5000 simulations.:  37%|███▋      | 1826/5000 [00:07<00:12, 252.69it/s]Running 5000 simulations.:  37%|███▋      | 1852/5000 [00:07<00:12, 251.02it/s]Running 5000 simulations.:  38%|███▊      | 1878/5000 [00:07<00:12, 243.97it/s]Running 5000 simulations.:  38%|███▊      | 1903/5000 [00:07<00:12, 239.95it/s]Running 5000 simulations.:  39%|███▊      | 1928/5000 [00:08<00:12, 237.75it/s]Running 5000 simulations.:  39%|███▉      | 1952/5000 [00:08<00:12, 236.30it/s]Running 5000 simulations.:  40%|███▉      | 1976/5000 [00:08<00:12, 236.06it/s]Running 5000 simulations.:  40%|████      | 2000/5000 [00:08<00:12, 235.38it/s]Running 5000 simulations.:  40%|████      | 2024/5000 [00:08<00:12, 234.66it/s]Running 5000 simulations.:  41%|████      | 2048/5000 [00:08<00:12, 234.29it/s]Running 5000 simulations.:  41%|████▏     | 2072/5000 [00:08<00:12, 233.70it/s]Running 5000 simulations.:  42%|████▏     | 2096/5000 [00:08<00:12, 234.20it/s]Running 5000 simulations.:  42%|████▏     | 2120/5000 [00:08<00:12, 235.52it/s]Running 5000 simulations.:  43%|████▎     | 2144/5000 [00:08<00:12, 236.29it/s]Running 5000 simulations.:  43%|████▎     | 2168/5000 [00:09<00:11, 236.48it/s]Running 5000 simulations.:  44%|████▍     | 2192/5000 [00:09<00:11, 236.06it/s]Running 5000 simulations.:  44%|████▍     | 2216/5000 [00:09<00:11, 236.92it/s]Running 5000 simulations.:  45%|████▍     | 2240/5000 [00:09<00:11, 237.46it/s]Running 5000 simulations.:  45%|████▌     | 2264/5000 [00:09<00:11, 237.86it/s]Running 5000 simulations.:  46%|████▌     | 2288/5000 [00:09<00:11, 237.52it/s]Running 5000 simulations.:  46%|████▌     | 2312/5000 [00:09<00:11, 238.20it/s]Running 5000 simulations.:  47%|████▋     | 2336/5000 [00:09<00:11, 238.28it/s]Running 5000 simulations.:  47%|████▋     | 2360/5000 [00:09<00:11, 238.17it/s]Running 5000 simulations.:  48%|████▊     | 2384/5000 [00:09<00:11, 236.96it/s]Running 5000 simulations.:  48%|████▊     | 2408/5000 [00:10<00:10, 236.79it/s]Running 5000 simulations.:  49%|████▊     | 2432/5000 [00:10<00:10, 237.51it/s]Running 5000 simulations.:  49%|████▉     | 2456/5000 [00:10<00:10, 238.02it/s]Running 5000 simulations.:  50%|████▉     | 2480/5000 [00:10<00:10, 238.34it/s]Running 5000 simulations.:  50%|█████     | 2504/5000 [00:10<00:10, 238.64it/s]Running 5000 simulations.:  51%|█████     | 2528/5000 [00:10<00:10, 238.98it/s]Running 5000 simulations.:  51%|█████     | 2552/5000 [00:10<00:10, 237.79it/s]Running 5000 simulations.:  52%|█████▏    | 2576/5000 [00:10<00:10, 236.56it/s]Running 5000 simulations.:  52%|█████▏    | 2600/5000 [00:10<00:10, 236.78it/s]Running 5000 simulations.:  52%|█████▏    | 2624/5000 [00:10<00:10, 237.24it/s]Running 5000 simulations.:  53%|█████▎    | 2648/5000 [00:11<00:09, 237.45it/s]Running 5000 simulations.:  53%|█████▎    | 2672/5000 [00:11<00:09, 237.00it/s]Running 5000 simulations.:  54%|█████▍    | 2696/5000 [00:11<00:09, 237.04it/s]Running 5000 simulations.:  54%|█████▍    | 2720/5000 [00:11<00:09, 236.38it/s]Running 5000 simulations.:  55%|█████▍    | 2744/5000 [00:11<00:09, 235.82it/s]Running 5000 simulations.:  55%|█████▌    | 2768/5000 [00:11<00:09, 236.22it/s]Running 5000 simulations.:  56%|█████▌    | 2792/5000 [00:11<00:09, 235.38it/s]Running 5000 simulations.:  56%|█████▋    | 2816/5000 [00:11<00:09, 234.10it/s]Running 5000 simulations.:  57%|█████▋    | 2840/5000 [00:11<00:09, 235.17it/s]Running 5000 simulations.:  57%|█████▋    | 2864/5000 [00:12<00:09, 236.18it/s]Running 5000 simulations.:  58%|█████▊    | 2888/5000 [00:12<00:08, 236.76it/s]Running 5000 simulations.:  58%|█████▊    | 2912/5000 [00:12<00:08, 237.15it/s]Running 5000 simulations.:  59%|█████▊    | 2936/5000 [00:12<00:08, 236.95it/s]Running 5000 simulations.:  59%|█████▉    | 2960/5000 [00:12<00:08, 235.83it/s]Running 5000 simulations.:  60%|█████▉    | 2984/5000 [00:12<00:08, 235.10it/s]Running 5000 simulations.:  60%|██████    | 3008/5000 [00:12<00:08, 234.55it/s]Running 5000 simulations.:  61%|██████    | 3032/5000 [00:12<00:08, 234.27it/s]Running 5000 simulations.:  61%|██████    | 3056/5000 [00:12<00:08, 234.04it/s]Running 5000 simulations.:  62%|██████▏   | 3080/5000 [00:12<00:08, 234.08it/s]Running 5000 simulations.:  62%|██████▏   | 3104/5000 [00:13<00:08, 234.88it/s]Running 5000 simulations.:  63%|██████▎   | 3128/5000 [00:13<00:07, 235.74it/s]Running 5000 simulations.:  63%|██████▎   | 3152/5000 [00:13<00:07, 236.49it/s]Running 5000 simulations.:  64%|██████▎   | 3176/5000 [00:13<00:07, 236.94it/s]Running 5000 simulations.:  64%|██████▍   | 3200/5000 [00:13<00:07, 235.97it/s]Running 5000 simulations.:  64%|██████▍   | 3224/5000 [00:13<00:07, 235.07it/s]Running 5000 simulations.:  65%|██████▍   | 3248/5000 [00:13<00:07, 230.27it/s]Running 5000 simulations.:  65%|██████▌   | 3272/5000 [00:13<00:07, 223.67it/s]Running 5000 simulations.:  66%|██████▌   | 3295/5000 [00:13<00:07, 218.91it/s]Running 5000 simulations.:  66%|██████▋   | 3317/5000 [00:13<00:07, 216.58it/s]Running 5000 simulations.:  67%|██████▋   | 3339/5000 [00:14<00:07, 214.85it/s]Running 5000 simulations.:  67%|██████▋   | 3361/5000 [00:14<00:07, 213.83it/s]Running 5000 simulations.:  68%|██████▊   | 3383/5000 [00:14<00:07, 212.67it/s]Running 5000 simulations.:  68%|██████▊   | 3405/5000 [00:14<00:07, 212.28it/s]Running 5000 simulations.:  69%|██████▊   | 3427/5000 [00:14<00:07, 211.31it/s]Running 5000 simulations.:  69%|██████▉   | 3449/5000 [00:14<00:07, 211.62it/s]Running 5000 simulations.:  69%|██████▉   | 3471/5000 [00:14<00:07, 211.17it/s]Running 5000 simulations.:  70%|██████▉   | 3493/5000 [00:14<00:07, 210.79it/s]Running 5000 simulations.:  70%|███████   | 3515/5000 [00:14<00:07, 210.04it/s]Running 5000 simulations.:  71%|███████   | 3537/5000 [00:15<00:06, 209.54it/s]Running 5000 simulations.:  71%|███████   | 3560/5000 [00:15<00:06, 214.67it/s]Running 5000 simulations.:  72%|███████▏  | 3584/5000 [00:15<00:06, 221.47it/s]Running 5000 simulations.:  72%|███████▏  | 3609/5000 [00:15<00:06, 226.77it/s]Running 5000 simulations.:  73%|███████▎  | 3633/5000 [00:15<00:05, 229.45it/s]Running 5000 simulations.:  73%|███████▎  | 3657/5000 [00:15<00:05, 231.72it/s]Running 5000 simulations.:  74%|███████▎  | 3681/5000 [00:15<00:05, 233.73it/s]Running 5000 simulations.:  74%|███████▍  | 3705/5000 [00:15<00:05, 234.78it/s]Running 5000 simulations.:  75%|███████▍  | 3729/5000 [00:15<00:05, 235.08it/s]Running 5000 simulations.:  75%|███████▌  | 3753/5000 [00:15<00:05, 236.09it/s]Running 5000 simulations.:  76%|███████▌  | 3778/5000 [00:16<00:05, 237.28it/s]Running 5000 simulations.:  76%|███████▌  | 3802/5000 [00:16<00:05, 237.32it/s]Running 5000 simulations.:  77%|███████▋  | 3826/5000 [00:16<00:04, 237.25it/s]Running 5000 simulations.:  77%|███████▋  | 3850/5000 [00:16<00:04, 237.25it/s]Running 5000 simulations.:  77%|███████▋  | 3874/5000 [00:16<00:04, 237.70it/s]Running 5000 simulations.:  78%|███████▊  | 3898/5000 [00:16<00:04, 238.10it/s]Running 5000 simulations.:  78%|███████▊  | 3922/5000 [00:16<00:04, 237.15it/s]Running 5000 simulations.:  79%|███████▉  | 3946/5000 [00:16<00:04, 237.52it/s]Running 5000 simulations.:  79%|███████▉  | 3970/5000 [00:16<00:04, 238.11it/s]Running 5000 simulations.:  80%|███████▉  | 3994/5000 [00:16<00:04, 238.42it/s]Running 5000 simulations.:  80%|████████  | 4018/5000 [00:17<00:04, 237.59it/s]Running 5000 simulations.:  81%|████████  | 4042/5000 [00:17<00:04, 236.50it/s]Running 5000 simulations.:  81%|████████▏ | 4066/5000 [00:17<00:03, 235.74it/s]Running 5000 simulations.:  82%|████████▏ | 4090/5000 [00:17<00:03, 235.94it/s]Running 5000 simulations.:  82%|████████▏ | 4114/5000 [00:17<00:03, 236.52it/s]Running 5000 simulations.:  83%|████████▎ | 4139/5000 [00:17<00:03, 237.66it/s]Running 5000 simulations.:  83%|████████▎ | 4163/5000 [00:17<00:03, 236.72it/s]Running 5000 simulations.:  84%|████████▎ | 4187/5000 [00:17<00:03, 237.15it/s]Running 5000 simulations.:  84%|████████▍ | 4211/5000 [00:17<00:03, 236.15it/s]Running 5000 simulations.:  85%|████████▍ | 4235/5000 [00:17<00:03, 236.89it/s]Running 5000 simulations.:  85%|████████▌ | 4259/5000 [00:18<00:03, 236.73it/s]Running 5000 simulations.:  86%|████████▌ | 4283/5000 [00:18<00:03, 237.31it/s]Running 5000 simulations.:  86%|████████▌ | 4307/5000 [00:18<00:02, 236.86it/s]Running 5000 simulations.:  87%|████████▋ | 4331/5000 [00:18<00:02, 236.74it/s]Running 5000 simulations.:  87%|████████▋ | 4355/5000 [00:18<00:02, 237.31it/s]Running 5000 simulations.:  88%|████████▊ | 4379/5000 [00:18<00:02, 237.89it/s]Running 5000 simulations.:  88%|████████▊ | 4404/5000 [00:18<00:02, 239.17it/s]Running 5000 simulations.:  89%|████████▊ | 4429/5000 [00:18<00:02, 241.46it/s]Running 5000 simulations.:  89%|████████▉ | 4454/5000 [00:18<00:02, 241.31it/s]Running 5000 simulations.:  90%|████████▉ | 4479/5000 [00:18<00:02, 241.22it/s]Running 5000 simulations.:  90%|█████████ | 4504/5000 [00:19<00:02, 241.09it/s]Running 5000 simulations.:  91%|█████████ | 4529/5000 [00:19<00:01, 240.70it/s]Running 5000 simulations.:  91%|█████████ | 4554/5000 [00:19<00:01, 241.94it/s]Running 5000 simulations.:  92%|█████████▏| 4579/5000 [00:19<00:01, 241.33it/s]Running 5000 simulations.:  92%|█████████▏| 4604/5000 [00:19<00:01, 242.64it/s]Running 5000 simulations.:  93%|█████████▎| 4629/5000 [00:19<00:01, 242.36it/s]Running 5000 simulations.:  93%|█████████▎| 4654/5000 [00:19<00:01, 241.19it/s]Running 5000 simulations.:  94%|█████████▎| 4679/5000 [00:19<00:01, 240.43it/s]Running 5000 simulations.:  94%|█████████▍| 4704/5000 [00:19<00:01, 240.84it/s]Running 5000 simulations.:  95%|█████████▍| 4729/5000 [00:20<00:01, 241.38it/s]Running 5000 simulations.:  95%|█████████▌| 4754/5000 [00:20<00:01, 241.60it/s]Running 5000 simulations.:  96%|█████████▌| 4779/5000 [00:20<00:00, 243.82it/s]Running 5000 simulations.:  96%|█████████▌| 4804/5000 [00:20<00:00, 243.56it/s]Running 5000 simulations.:  97%|█████████▋| 4829/5000 [00:20<00:00, 244.65it/s]Running 5000 simulations.:  97%|█████████▋| 4854/5000 [00:20<00:00, 243.68it/s]Running 5000 simulations.:  98%|█████████▊| 4879/5000 [00:20<00:00, 243.35it/s]Running 5000 simulations.:  98%|█████████▊| 4904/5000 [00:20<00:00, 243.28it/s]Running 5000 simulations.:  99%|█████████▊| 4929/5000 [00:20<00:00, 244.03it/s]Running 5000 simulations.:  99%|█████████▉| 4954/5000 [00:20<00:00, 241.15it/s]Running 5000 simulations.: 100%|█████████▉| 4979/5000 [00:21<00:00, 242.20it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:21<00:00, 236.51it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 25/5000 [00:00<00:20, 242.50it/s]Running 5000 simulations.:   1%|          | 50/5000 [00:00<00:20, 242.58it/s]Running 5000 simulations.:   2%|▏         | 75/5000 [00:00<00:20, 242.84it/s]Running 5000 simulations.:   2%|▏         | 100/5000 [00:00<00:20, 242.64it/s]Running 5000 simulations.:   2%|▎         | 125/5000 [00:00<00:20, 242.60it/s]Running 5000 simulations.:   3%|▎         | 150/5000 [00:00<00:19, 242.50it/s]Running 5000 simulations.:   4%|▎         | 175/5000 [00:00<00:19, 242.05it/s]Running 5000 simulations.:   4%|▍         | 199/5000 [00:00<00:20, 239.68it/s]Running 5000 simulations.:   4%|▍         | 223/5000 [00:00<00:19, 239.63it/s]Running 5000 simulations.:   5%|▍         | 248/5000 [00:01<00:19, 240.13it/s]Running 5000 simulations.:   5%|▌         | 273/5000 [00:01<00:19, 240.29it/s]Running 5000 simulations.:   6%|▌         | 298/5000 [00:01<00:19, 240.33it/s]Running 5000 simulations.:   6%|▋         | 323/5000 [00:01<00:19, 240.42it/s]Running 5000 simulations.:   7%|▋         | 348/5000 [00:01<00:19, 240.86it/s]Running 5000 simulations.:   7%|▋         | 373/5000 [00:01<00:19, 241.10it/s]Running 5000 simulations.:   8%|▊         | 398/5000 [00:01<00:19, 241.26it/s]Running 5000 simulations.:   8%|▊         | 423/5000 [00:01<00:18, 241.22it/s]Running 5000 simulations.:   9%|▉         | 448/5000 [00:01<00:18, 241.39it/s]Running 5000 simulations.:   9%|▉         | 473/5000 [00:01<00:18, 240.56it/s]Running 5000 simulations.:  10%|▉         | 498/5000 [00:02<00:19, 226.84it/s]Running 5000 simulations.:  10%|█         | 521/5000 [00:02<00:20, 218.08it/s]Running 5000 simulations.:  11%|█         | 543/5000 [00:02<00:20, 216.19it/s]Running 5000 simulations.:  11%|█▏        | 565/5000 [00:02<00:20, 213.97it/s]Running 5000 simulations.:  12%|█▏        | 587/5000 [00:02<00:20, 210.95it/s]Running 5000 simulations.:  12%|█▏        | 609/5000 [00:02<00:20, 209.44it/s]Running 5000 simulations.:  13%|█▎        | 630/5000 [00:02<00:21, 207.69it/s]Running 5000 simulations.:  13%|█▎        | 651/5000 [00:02<00:21, 206.74it/s]Running 5000 simulations.:  13%|█▎        | 672/5000 [00:02<00:21, 205.92it/s]Running 5000 simulations.:  14%|█▍        | 693/5000 [00:03<00:20, 205.72it/s]Running 5000 simulations.:  14%|█▍        | 714/5000 [00:03<00:20, 205.24it/s]Running 5000 simulations.:  15%|█▍        | 735/5000 [00:03<00:20, 204.78it/s]Running 5000 simulations.:  15%|█▌        | 756/5000 [00:03<00:20, 204.06it/s]Running 5000 simulations.:  16%|█▌        | 777/5000 [00:03<00:20, 203.52it/s]Running 5000 simulations.:  16%|█▌        | 798/5000 [00:03<00:20, 202.70it/s]Running 5000 simulations.:  16%|█▋        | 819/5000 [00:03<00:20, 202.70it/s]Running 5000 simulations.:  17%|█▋        | 840/5000 [00:03<00:20, 202.77it/s]Running 5000 simulations.:  17%|█▋        | 861/5000 [00:03<00:20, 202.84it/s]Running 5000 simulations.:  18%|█▊        | 882/5000 [00:03<00:20, 203.13it/s]Running 5000 simulations.:  18%|█▊        | 903/5000 [00:04<00:20, 202.59it/s]Running 5000 simulations.:  18%|█▊        | 924/5000 [00:04<00:20, 202.35it/s]Running 5000 simulations.:  19%|█▉        | 945/5000 [00:04<00:20, 202.18it/s]Running 5000 simulations.:  19%|█▉        | 966/5000 [00:04<00:19, 202.00it/s]Running 5000 simulations.:  20%|█▉        | 987/5000 [00:04<00:19, 201.69it/s]Running 5000 simulations.:  20%|██        | 1008/5000 [00:04<00:19, 201.64it/s]Running 5000 simulations.:  21%|██        | 1029/5000 [00:04<00:19, 201.11it/s]Running 5000 simulations.:  21%|██        | 1050/5000 [00:04<00:19, 200.84it/s]Running 5000 simulations.:  21%|██▏       | 1071/5000 [00:04<00:19, 200.94it/s]Running 5000 simulations.:  22%|██▏       | 1092/5000 [00:05<00:19, 201.34it/s]Running 5000 simulations.:  22%|██▏       | 1113/5000 [00:05<00:19, 201.07it/s]Running 5000 simulations.:  23%|██▎       | 1134/5000 [00:05<00:19, 200.27it/s]Running 5000 simulations.:  23%|██▎       | 1155/5000 [00:05<00:19, 200.42it/s]Running 5000 simulations.:  24%|██▎       | 1176/5000 [00:05<00:19, 200.31it/s]Running 5000 simulations.:  24%|██▍       | 1198/5000 [00:05<00:18, 203.73it/s]Running 5000 simulations.:  24%|██▍       | 1223/5000 [00:05<00:17, 214.94it/s]Running 5000 simulations.:  25%|██▍       | 1248/5000 [00:05<00:16, 223.70it/s]Running 5000 simulations.:  25%|██▌       | 1273/5000 [00:05<00:16, 230.87it/s]Running 5000 simulations.:  26%|██▌       | 1298/5000 [00:05<00:15, 235.73it/s]Running 5000 simulations.:  26%|██▋       | 1323/5000 [00:06<00:15, 239.02it/s]Running 5000 simulations.:  27%|██▋       | 1348/5000 [00:06<00:15, 241.36it/s]Running 5000 simulations.:  27%|██▋       | 1374/5000 [00:06<00:14, 244.22it/s]Running 5000 simulations.:  28%|██▊       | 1400/5000 [00:06<00:14, 246.01it/s]Running 5000 simulations.:  29%|██▊       | 1426/5000 [00:06<00:14, 247.18it/s]Running 5000 simulations.:  29%|██▉       | 1452/5000 [00:06<00:14, 248.13it/s]Running 5000 simulations.:  30%|██▉       | 1477/5000 [00:06<00:14, 248.15it/s]Running 5000 simulations.:  30%|███       | 1502/5000 [00:06<00:14, 247.52it/s]Running 5000 simulations.:  31%|███       | 1527/5000 [00:06<00:14, 247.46it/s]Running 5000 simulations.:  31%|███       | 1552/5000 [00:06<00:13, 247.83it/s]Running 5000 simulations.:  32%|███▏      | 1577/5000 [00:07<00:13, 248.25it/s]Running 5000 simulations.:  32%|███▏      | 1602/5000 [00:07<00:13, 247.84it/s]Running 5000 simulations.:  33%|███▎      | 1628/5000 [00:07<00:13, 248.70it/s]Running 5000 simulations.:  33%|███▎      | 1654/5000 [00:07<00:13, 249.92it/s]Running 5000 simulations.:  34%|███▎      | 1680/5000 [00:07<00:13, 250.72it/s]Running 5000 simulations.:  34%|███▍      | 1706/5000 [00:07<00:13, 251.83it/s]Running 5000 simulations.:  35%|███▍      | 1732/5000 [00:07<00:12, 251.65it/s]Running 5000 simulations.:  35%|███▌      | 1758/5000 [00:07<00:12, 251.79it/s]Running 5000 simulations.:  36%|███▌      | 1784/5000 [00:07<00:12, 251.43it/s]Running 5000 simulations.:  36%|███▌      | 1810/5000 [00:07<00:12, 251.86it/s]Running 5000 simulations.:  37%|███▋      | 1836/5000 [00:08<00:12, 251.10it/s]Running 5000 simulations.:  37%|███▋      | 1862/5000 [00:08<00:12, 250.27it/s]Running 5000 simulations.:  38%|███▊      | 1888/5000 [00:08<00:12, 249.99it/s]Running 5000 simulations.:  38%|███▊      | 1914/5000 [00:08<00:12, 250.07it/s]Running 5000 simulations.:  39%|███▉      | 1940/5000 [00:08<00:12, 250.18it/s]Running 5000 simulations.:  39%|███▉      | 1966/5000 [00:08<00:12, 251.20it/s]Running 5000 simulations.:  40%|███▉      | 1992/5000 [00:08<00:11, 251.36it/s]Running 5000 simulations.:  40%|████      | 2018/5000 [00:08<00:11, 251.66it/s]Running 5000 simulations.:  41%|████      | 2044/5000 [00:08<00:11, 252.02it/s]Running 5000 simulations.:  41%|████▏     | 2070/5000 [00:09<00:11, 251.84it/s]Running 5000 simulations.:  42%|████▏     | 2096/5000 [00:09<00:11, 251.98it/s]Running 5000 simulations.:  42%|████▏     | 2122/5000 [00:09<00:11, 251.93it/s]Running 5000 simulations.:  43%|████▎     | 2148/5000 [00:09<00:11, 251.96it/s]Running 5000 simulations.:  43%|████▎     | 2174/5000 [00:09<00:11, 252.23it/s]Running 5000 simulations.:  44%|████▍     | 2200/5000 [00:09<00:11, 252.17it/s]Running 5000 simulations.:  45%|████▍     | 2226/5000 [00:09<00:11, 251.22it/s]Running 5000 simulations.:  45%|████▌     | 2252/5000 [00:09<00:10, 250.27it/s]Running 5000 simulations.:  46%|████▌     | 2278/5000 [00:09<00:10, 249.92it/s]Running 5000 simulations.:  46%|████▌     | 2304/5000 [00:09<00:10, 250.69it/s]Running 5000 simulations.:  47%|████▋     | 2330/5000 [00:10<00:10, 251.04it/s]Running 5000 simulations.:  47%|████▋     | 2356/5000 [00:10<00:10, 251.40it/s]Running 5000 simulations.:  48%|████▊     | 2382/5000 [00:10<00:10, 250.77it/s]Running 5000 simulations.:  48%|████▊     | 2408/5000 [00:10<00:10, 248.65it/s]Running 5000 simulations.:  49%|████▊     | 2434/5000 [00:10<00:10, 250.35it/s]Running 5000 simulations.:  49%|████▉     | 2460/5000 [00:10<00:10, 251.24it/s]Running 5000 simulations.:  50%|████▉     | 2486/5000 [00:10<00:09, 251.81it/s]Running 5000 simulations.:  50%|█████     | 2512/5000 [00:10<00:09, 252.50it/s]Running 5000 simulations.:  51%|█████     | 2538/5000 [00:10<00:09, 252.71it/s]Running 5000 simulations.:  51%|█████▏    | 2564/5000 [00:10<00:09, 253.10it/s]Running 5000 simulations.:  52%|█████▏    | 2590/5000 [00:11<00:09, 253.27it/s]Running 5000 simulations.:  52%|█████▏    | 2616/5000 [00:11<00:09, 253.24it/s]Running 5000 simulations.:  53%|█████▎    | 2642/5000 [00:11<00:09, 253.01it/s]Running 5000 simulations.:  53%|█████▎    | 2668/5000 [00:11<00:09, 252.05it/s]Running 5000 simulations.:  54%|█████▍    | 2694/5000 [00:11<00:09, 252.04it/s]Running 5000 simulations.:  54%|█████▍    | 2720/5000 [00:11<00:09, 251.98it/s]Running 5000 simulations.:  55%|█████▍    | 2746/5000 [00:11<00:08, 251.15it/s]Running 5000 simulations.:  55%|█████▌    | 2772/5000 [00:11<00:08, 251.01it/s]Running 5000 simulations.:  56%|█████▌    | 2798/5000 [00:11<00:08, 250.81it/s]Running 5000 simulations.:  56%|█████▋    | 2824/5000 [00:12<00:08, 250.51it/s]Running 5000 simulations.:  57%|█████▋    | 2850/5000 [00:12<00:08, 250.15it/s]Running 5000 simulations.:  58%|█████▊    | 2876/5000 [00:12<00:08, 250.02it/s]Running 5000 simulations.:  58%|█████▊    | 2902/5000 [00:12<00:08, 249.51it/s]Running 5000 simulations.:  59%|█████▊    | 2927/5000 [00:12<00:08, 249.00it/s]Running 5000 simulations.:  59%|█████▉    | 2952/5000 [00:12<00:08, 248.96it/s]Running 5000 simulations.:  60%|█████▉    | 2977/5000 [00:12<00:08, 248.91it/s]Running 5000 simulations.:  60%|██████    | 3002/5000 [00:12<00:08, 247.95it/s]Running 5000 simulations.:  61%|██████    | 3027/5000 [00:12<00:07, 247.82it/s]Running 5000 simulations.:  61%|██████    | 3052/5000 [00:12<00:07, 248.42it/s]Running 5000 simulations.:  62%|██████▏   | 3078/5000 [00:13<00:07, 249.22it/s]Running 5000 simulations.:  62%|██████▏   | 3103/5000 [00:13<00:07, 249.38it/s]Running 5000 simulations.:  63%|██████▎   | 3129/5000 [00:13<00:07, 249.66it/s]Running 5000 simulations.:  63%|██████▎   | 3155/5000 [00:13<00:07, 249.70it/s]Running 5000 simulations.:  64%|██████▎   | 3180/5000 [00:13<00:07, 249.22it/s]Running 5000 simulations.:  64%|██████▍   | 3205/5000 [00:13<00:07, 248.71it/s]Running 5000 simulations.:  65%|██████▍   | 3230/5000 [00:13<00:07, 248.08it/s]Running 5000 simulations.:  65%|██████▌   | 3255/5000 [00:13<00:07, 248.00it/s]Running 5000 simulations.:  66%|██████▌   | 3280/5000 [00:13<00:06, 247.98it/s]Running 5000 simulations.:  66%|██████▌   | 3305/5000 [00:13<00:06, 248.23it/s]Running 5000 simulations.:  67%|██████▋   | 3330/5000 [00:14<00:06, 247.73it/s]Running 5000 simulations.:  67%|██████▋   | 3355/5000 [00:14<00:06, 247.76it/s]Running 5000 simulations.:  68%|██████▊   | 3381/5000 [00:14<00:06, 248.77it/s]Running 5000 simulations.:  68%|██████▊   | 3407/5000 [00:14<00:06, 250.18it/s]Running 5000 simulations.:  69%|██████▊   | 3433/5000 [00:14<00:06, 250.01it/s]Running 5000 simulations.:  69%|██████▉   | 3459/5000 [00:14<00:06, 249.78it/s]Running 5000 simulations.:  70%|██████▉   | 3485/5000 [00:14<00:06, 250.29it/s]Running 5000 simulations.:  70%|███████   | 3511/5000 [00:14<00:05, 250.94it/s]Running 5000 simulations.:  71%|███████   | 3537/5000 [00:14<00:05, 251.72it/s]Running 5000 simulations.:  71%|███████▏  | 3563/5000 [00:14<00:05, 252.01it/s]Running 5000 simulations.:  72%|███████▏  | 3589/5000 [00:15<00:05, 250.74it/s]Running 5000 simulations.:  72%|███████▏  | 3615/5000 [00:15<00:05, 249.59it/s]Running 5000 simulations.:  73%|███████▎  | 3640/5000 [00:15<00:05, 249.05it/s]Running 5000 simulations.:  73%|███████▎  | 3665/5000 [00:15<00:05, 248.93it/s]Running 5000 simulations.:  74%|███████▍  | 3690/5000 [00:15<00:05, 247.97it/s]Running 5000 simulations.:  74%|███████▍  | 3715/5000 [00:15<00:05, 247.52it/s]Running 5000 simulations.:  75%|███████▍  | 3740/5000 [00:15<00:05, 248.01it/s]Running 5000 simulations.:  75%|███████▌  | 3765/5000 [00:15<00:04, 248.60it/s]Running 5000 simulations.:  76%|███████▌  | 3790/5000 [00:15<00:04, 248.14it/s]Running 5000 simulations.:  76%|███████▋  | 3815/5000 [00:16<00:04, 247.61it/s]Running 5000 simulations.:  77%|███████▋  | 3840/5000 [00:16<00:04, 247.60it/s]Running 5000 simulations.:  77%|███████▋  | 3865/5000 [00:16<00:04, 247.76it/s]Running 5000 simulations.:  78%|███████▊  | 3890/5000 [00:16<00:04, 247.84it/s]Running 5000 simulations.:  78%|███████▊  | 3915/5000 [00:16<00:04, 247.72it/s]Running 5000 simulations.:  79%|███████▉  | 3940/5000 [00:16<00:04, 247.41it/s]Running 5000 simulations.:  79%|███████▉  | 3965/5000 [00:16<00:04, 247.95it/s]Running 5000 simulations.:  80%|███████▉  | 3990/5000 [00:16<00:04, 248.08it/s]Running 5000 simulations.:  80%|████████  | 4015/5000 [00:16<00:04, 239.54it/s]Running 5000 simulations.:  81%|████████  | 4041/5000 [00:16<00:03, 242.72it/s]Running 5000 simulations.:  81%|████████▏ | 4067/5000 [00:17<00:03, 245.11it/s]Running 5000 simulations.:  82%|████████▏ | 4093/5000 [00:17<00:03, 247.57it/s]Running 5000 simulations.:  82%|████████▏ | 4119/5000 [00:17<00:03, 249.05it/s]Running 5000 simulations.:  83%|████████▎ | 4145/5000 [00:17<00:03, 249.55it/s]Running 5000 simulations.:  83%|████████▎ | 4170/5000 [00:17<00:03, 247.88it/s]Running 5000 simulations.:  84%|████████▍ | 4195/5000 [00:17<00:03, 247.81it/s]Running 5000 simulations.:  84%|████████▍ | 4220/5000 [00:17<00:03, 247.94it/s]Running 5000 simulations.:  85%|████████▍ | 4245/5000 [00:17<00:03, 248.07it/s]Running 5000 simulations.:  85%|████████▌ | 4270/5000 [00:17<00:02, 247.73it/s]Running 5000 simulations.:  86%|████████▌ | 4295/5000 [00:17<00:02, 247.88it/s]Running 5000 simulations.:  86%|████████▋ | 4320/5000 [00:18<00:02, 248.27it/s]Running 5000 simulations.:  87%|████████▋ | 4345/5000 [00:18<00:02, 248.47it/s]Running 5000 simulations.:  87%|████████▋ | 4370/5000 [00:18<00:02, 248.82it/s]Running 5000 simulations.:  88%|████████▊ | 4396/5000 [00:18<00:02, 249.53it/s]Running 5000 simulations.:  88%|████████▊ | 4421/5000 [00:18<00:02, 248.41it/s]Running 5000 simulations.:  89%|████████▉ | 4446/5000 [00:18<00:02, 246.94it/s]Running 5000 simulations.:  89%|████████▉ | 4471/5000 [00:18<00:02, 244.97it/s]Running 5000 simulations.:  90%|████████▉ | 4496/5000 [00:18<00:02, 243.83it/s]Running 5000 simulations.:  90%|█████████ | 4521/5000 [00:18<00:01, 242.80it/s]Running 5000 simulations.:  91%|█████████ | 4546/5000 [00:18<00:01, 241.94it/s]Running 5000 simulations.:  91%|█████████▏| 4571/5000 [00:19<00:01, 242.83it/s]Running 5000 simulations.:  92%|█████████▏| 4597/5000 [00:19<00:01, 245.68it/s]Running 5000 simulations.:  92%|█████████▏| 4623/5000 [00:19<00:01, 247.76it/s]Running 5000 simulations.:  93%|█████████▎| 4649/5000 [00:19<00:01, 249.53it/s]Running 5000 simulations.:  93%|█████████▎| 4674/5000 [00:19<00:01, 246.03it/s]Running 5000 simulations.:  94%|█████████▍| 4700/5000 [00:19<00:01, 248.12it/s]Running 5000 simulations.:  95%|█████████▍| 4726/5000 [00:19<00:01, 250.44it/s]Running 5000 simulations.:  95%|█████████▌| 4752/5000 [00:19<00:00, 251.52it/s]Running 5000 simulations.:  96%|█████████▌| 4778/5000 [00:19<00:00, 252.23it/s]Running 5000 simulations.:  96%|█████████▌| 4804/5000 [00:19<00:00, 252.52it/s]Running 5000 simulations.:  97%|█████████▋| 4830/5000 [00:20<00:00, 253.00it/s]Running 5000 simulations.:  97%|█████████▋| 4856/5000 [00:20<00:00, 254.29it/s]Running 5000 simulations.:  98%|█████████▊| 4882/5000 [00:20<00:00, 254.36it/s]Running 5000 simulations.:  98%|█████████▊| 4908/5000 [00:20<00:00, 254.04it/s]Running 5000 simulations.:  99%|█████████▊| 4934/5000 [00:20<00:00, 254.19it/s]Running 5000 simulations.:  99%|█████████▉| 4960/5000 [00:20<00:00, 253.54it/s]Running 5000 simulations.: 100%|█████████▉| 4986/5000 [00:20<00:00, 252.99it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:20<00:00, 240.74it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 25/5000 [00:00<00:20, 241.98it/s]Running 5000 simulations.:   1%|          | 49/5000 [00:00<00:20, 241.35it/s]Running 5000 simulations.:   1%|▏         | 74/5000 [00:00<00:20, 241.23it/s]Running 5000 simulations.:   2%|▏         | 98/5000 [00:00<00:20, 240.40it/s]Running 5000 simulations.:   2%|▏         | 122/5000 [00:00<00:20, 239.73it/s]Running 5000 simulations.:   3%|▎         | 146/5000 [00:00<00:20, 239.52it/s]Running 5000 simulations.:   3%|▎         | 171/5000 [00:00<00:20, 239.68it/s]Running 5000 simulations.:   4%|▍         | 195/5000 [00:00<00:20, 239.29it/s]Running 5000 simulations.:   4%|▍         | 219/5000 [00:00<00:19, 239.18it/s]Running 5000 simulations.:   5%|▍         | 244/5000 [00:01<00:19, 239.81it/s]Running 5000 simulations.:   5%|▌         | 268/5000 [00:01<00:19, 239.41it/s]Running 5000 simulations.:   6%|▌         | 292/5000 [00:01<00:19, 238.93it/s]Running 5000 simulations.:   6%|▋         | 316/5000 [00:01<00:19, 238.11it/s]Running 5000 simulations.:   7%|▋         | 340/5000 [00:01<00:19, 237.87it/s]Running 5000 simulations.:   7%|▋         | 364/5000 [00:01<00:19, 238.02it/s]Running 5000 simulations.:   8%|▊         | 388/5000 [00:01<00:19, 237.48it/s]Running 5000 simulations.:   8%|▊         | 412/5000 [00:01<00:19, 237.35it/s]Running 5000 simulations.:   9%|▊         | 436/5000 [00:01<00:19, 237.08it/s]Running 5000 simulations.:   9%|▉         | 460/5000 [00:01<00:19, 237.47it/s]Running 5000 simulations.:  10%|▉         | 484/5000 [00:02<00:18, 237.89it/s]Running 5000 simulations.:  10%|█         | 508/5000 [00:02<00:18, 238.51it/s]Running 5000 simulations.:  11%|█         | 532/5000 [00:02<00:19, 227.11it/s]Running 5000 simulations.:  11%|█         | 555/5000 [00:02<00:20, 216.65it/s]Running 5000 simulations.:  12%|█▏        | 577/5000 [00:02<00:21, 210.07it/s]Running 5000 simulations.:  12%|█▏        | 599/5000 [00:02<00:21, 208.26it/s]Running 5000 simulations.:  12%|█▏        | 620/5000 [00:02<00:21, 206.28it/s]Running 5000 simulations.:  13%|█▎        | 641/5000 [00:02<00:21, 205.16it/s]Running 5000 simulations.:  13%|█▎        | 662/5000 [00:02<00:21, 204.92it/s]Running 5000 simulations.:  14%|█▎        | 683/5000 [00:02<00:21, 204.79it/s]Running 5000 simulations.:  14%|█▍        | 704/5000 [00:03<00:21, 204.36it/s]Running 5000 simulations.:  14%|█▍        | 725/5000 [00:03<00:20, 204.06it/s]Running 5000 simulations.:  15%|█▍        | 746/5000 [00:03<00:20, 204.48it/s]Running 5000 simulations.:  15%|█▌        | 767/5000 [00:03<00:20, 204.04it/s]Running 5000 simulations.:  16%|█▌        | 788/5000 [00:03<00:20, 204.41it/s]Running 5000 simulations.:  16%|█▌        | 809/5000 [00:03<00:20, 204.65it/s]Running 5000 simulations.:  17%|█▋        | 830/5000 [00:03<00:20, 204.82it/s]Running 5000 simulations.:  17%|█▋        | 851/5000 [00:03<00:20, 204.27it/s]Running 5000 simulations.:  17%|█▋        | 872/5000 [00:03<00:20, 203.35it/s]Running 5000 simulations.:  18%|█▊        | 893/5000 [00:04<00:20, 202.61it/s]Running 5000 simulations.:  18%|█▊        | 914/5000 [00:04<00:20, 202.25it/s]Running 5000 simulations.:  19%|█▊        | 935/5000 [00:04<00:20, 201.95it/s]Running 5000 simulations.:  19%|█▉        | 956/5000 [00:04<00:20, 201.83it/s]Running 5000 simulations.:  20%|█▉        | 977/5000 [00:04<00:19, 202.68it/s]Running 5000 simulations.:  20%|█▉        | 998/5000 [00:04<00:19, 202.94it/s]Running 5000 simulations.:  20%|██        | 1019/5000 [00:04<00:19, 201.74it/s]Running 5000 simulations.:  21%|██        | 1040/5000 [00:04<00:19, 201.27it/s]Running 5000 simulations.:  21%|██        | 1061/5000 [00:04<00:19, 200.92it/s]Running 5000 simulations.:  22%|██▏       | 1082/5000 [00:04<00:19, 200.98it/s]Running 5000 simulations.:  22%|██▏       | 1103/5000 [00:05<00:19, 201.01it/s]Running 5000 simulations.:  22%|██▏       | 1124/5000 [00:05<00:19, 200.94it/s]Running 5000 simulations.:  23%|██▎       | 1145/5000 [00:05<00:19, 201.71it/s]Running 5000 simulations.:  23%|██▎       | 1166/5000 [00:05<00:18, 201.94it/s]Running 5000 simulations.:  24%|██▎       | 1187/5000 [00:05<00:18, 201.80it/s]Running 5000 simulations.:  24%|██▍       | 1208/5000 [00:05<00:18, 202.40it/s]Running 5000 simulations.:  25%|██▍       | 1229/5000 [00:05<00:18, 203.08it/s]Running 5000 simulations.:  25%|██▌       | 1254/5000 [00:05<00:17, 215.04it/s]Running 5000 simulations.:  26%|██▌       | 1280/5000 [00:05<00:16, 224.68it/s]Running 5000 simulations.:  26%|██▌       | 1306/5000 [00:06<00:15, 231.72it/s]Running 5000 simulations.:  27%|██▋       | 1331/5000 [00:06<00:15, 236.44it/s]Running 5000 simulations.:  27%|██▋       | 1356/5000 [00:06<00:15, 240.11it/s]Running 5000 simulations.:  28%|██▊       | 1382/5000 [00:06<00:14, 243.20it/s]Running 5000 simulations.:  28%|██▊       | 1408/5000 [00:06<00:14, 245.43it/s]Running 5000 simulations.:  29%|██▊       | 1434/5000 [00:06<00:14, 246.98it/s]Running 5000 simulations.:  29%|██▉       | 1460/5000 [00:06<00:14, 248.02it/s]Running 5000 simulations.:  30%|██▉       | 1485/5000 [00:06<00:14, 245.93it/s]Running 5000 simulations.:  30%|███       | 1510/5000 [00:06<00:14, 246.69it/s]Running 5000 simulations.:  31%|███       | 1535/5000 [00:06<00:14, 247.19it/s]Running 5000 simulations.:  31%|███       | 1560/5000 [00:07<00:13, 246.83it/s]Running 5000 simulations.:  32%|███▏      | 1585/5000 [00:07<00:13, 246.35it/s]Running 5000 simulations.:  32%|███▏      | 1610/5000 [00:07<00:13, 246.72it/s]Running 5000 simulations.:  33%|███▎      | 1635/5000 [00:07<00:13, 246.36it/s]Running 5000 simulations.:  33%|███▎      | 1661/5000 [00:07<00:13, 247.75it/s]Running 5000 simulations.:  34%|███▎      | 1687/5000 [00:07<00:13, 248.69it/s]Running 5000 simulations.:  34%|███▍      | 1712/5000 [00:07<00:13, 248.40it/s]Running 5000 simulations.:  35%|███▍      | 1737/5000 [00:07<00:13, 247.89it/s]Running 5000 simulations.:  35%|███▌      | 1762/5000 [00:07<00:13, 248.38it/s]Running 5000 simulations.:  36%|███▌      | 1787/5000 [00:07<00:12, 247.89it/s]Running 5000 simulations.:  36%|███▌      | 1812/5000 [00:08<00:12, 246.88it/s]Running 5000 simulations.:  37%|███▋      | 1837/5000 [00:08<00:12, 246.90it/s]Running 5000 simulations.:  37%|███▋      | 1862/5000 [00:08<00:12, 246.44it/s]Running 5000 simulations.:  38%|███▊      | 1887/5000 [00:08<00:12, 244.23it/s]Running 5000 simulations.:  38%|███▊      | 1912/5000 [00:08<00:12, 245.31it/s]Running 5000 simulations.:  39%|███▊      | 1937/5000 [00:08<00:12, 246.23it/s]Running 5000 simulations.:  39%|███▉      | 1962/5000 [00:08<00:12, 247.05it/s]Running 5000 simulations.:  40%|███▉      | 1987/5000 [00:08<00:12, 247.44it/s]Running 5000 simulations.:  40%|████      | 2012/5000 [00:08<00:12, 247.85it/s]Running 5000 simulations.:  41%|████      | 2037/5000 [00:08<00:11, 247.88it/s]Running 5000 simulations.:  41%|████      | 2062/5000 [00:09<00:11, 248.15it/s]Running 5000 simulations.:  42%|████▏     | 2087/5000 [00:09<00:11, 248.32it/s]Running 5000 simulations.:  42%|████▏     | 2112/5000 [00:09<00:11, 248.53it/s]Running 5000 simulations.:  43%|████▎     | 2138/5000 [00:09<00:11, 249.04it/s]Running 5000 simulations.:  43%|████▎     | 2163/5000 [00:09<00:11, 248.97it/s]Running 5000 simulations.:  44%|████▍     | 2189/5000 [00:09<00:11, 249.42it/s]Running 5000 simulations.:  44%|████▍     | 2214/5000 [00:09<00:11, 249.59it/s]Running 5000 simulations.:  45%|████▍     | 2239/5000 [00:09<00:11, 248.70it/s]Running 5000 simulations.:  45%|████▌     | 2264/5000 [00:09<00:11, 247.85it/s]Running 5000 simulations.:  46%|████▌     | 2289/5000 [00:09<00:10, 247.29it/s]Running 5000 simulations.:  46%|████▋     | 2314/5000 [00:10<00:10, 247.10it/s]Running 5000 simulations.:  47%|████▋     | 2339/5000 [00:10<00:10, 247.62it/s]Running 5000 simulations.:  47%|████▋     | 2365/5000 [00:10<00:10, 248.42it/s]Running 5000 simulations.:  48%|████▊     | 2390/5000 [00:10<00:10, 248.30it/s]Running 5000 simulations.:  48%|████▊     | 2416/5000 [00:10<00:10, 248.90it/s]Running 5000 simulations.:  49%|████▉     | 2441/5000 [00:10<00:10, 248.93it/s]Running 5000 simulations.:  49%|████▉     | 2467/5000 [00:10<00:10, 249.24it/s]Running 5000 simulations.:  50%|████▉     | 2492/5000 [00:10<00:10, 249.39it/s]Running 5000 simulations.:  50%|█████     | 2518/5000 [00:10<00:09, 249.73it/s]Running 5000 simulations.:  51%|█████     | 2543/5000 [00:10<00:09, 248.20it/s]Running 5000 simulations.:  51%|█████▏    | 2568/5000 [00:11<00:09, 247.93it/s]Running 5000 simulations.:  52%|█████▏    | 2593/5000 [00:11<00:09, 247.20it/s]Running 5000 simulations.:  52%|█████▏    | 2618/5000 [00:11<00:09, 246.49it/s]Running 5000 simulations.:  53%|█████▎    | 2643/5000 [00:11<00:09, 245.89it/s]Running 5000 simulations.:  53%|█████▎    | 2668/5000 [00:11<00:09, 245.35it/s]Running 5000 simulations.:  54%|█████▍    | 2693/5000 [00:11<00:09, 245.07it/s]Running 5000 simulations.:  54%|█████▍    | 2718/5000 [00:11<00:09, 244.84it/s]Running 5000 simulations.:  55%|█████▍    | 2743/5000 [00:11<00:09, 245.24it/s]Running 5000 simulations.:  55%|█████▌    | 2768/5000 [00:11<00:09, 245.42it/s]Running 5000 simulations.:  56%|█████▌    | 2793/5000 [00:12<00:08, 245.37it/s]Running 5000 simulations.:  56%|█████▋    | 2818/5000 [00:12<00:08, 245.63it/s]Running 5000 simulations.:  57%|█████▋    | 2843/5000 [00:12<00:08, 246.19it/s]Running 5000 simulations.:  57%|█████▋    | 2868/5000 [00:12<00:08, 246.16it/s]Running 5000 simulations.:  58%|█████▊    | 2893/5000 [00:12<00:08, 246.78it/s]Running 5000 simulations.:  58%|█████▊    | 2918/5000 [00:12<00:08, 247.14it/s]Running 5000 simulations.:  59%|█████▉    | 2943/5000 [00:12<00:08, 247.47it/s]Running 5000 simulations.:  59%|█████▉    | 2968/5000 [00:12<00:08, 247.59it/s]Running 5000 simulations.:  60%|█████▉    | 2993/5000 [00:12<00:08, 247.71it/s]Running 5000 simulations.:  60%|██████    | 3018/5000 [00:12<00:08, 247.61it/s]Running 5000 simulations.:  61%|██████    | 3043/5000 [00:13<00:07, 246.68it/s]Running 5000 simulations.:  61%|██████▏   | 3068/5000 [00:13<00:07, 246.36it/s]Running 5000 simulations.:  62%|██████▏   | 3093/5000 [00:13<00:07, 246.26it/s]Running 5000 simulations.:  62%|██████▏   | 3118/5000 [00:13<00:07, 246.17it/s]Running 5000 simulations.:  63%|██████▎   | 3144/5000 [00:13<00:07, 247.68it/s]Running 5000 simulations.:  63%|██████▎   | 3169/5000 [00:13<00:07, 247.33it/s]Running 5000 simulations.:  64%|██████▍   | 3194/5000 [00:13<00:07, 246.55it/s]Running 5000 simulations.:  64%|██████▍   | 3219/5000 [00:13<00:07, 246.38it/s]Running 5000 simulations.:  65%|██████▍   | 3244/5000 [00:13<00:07, 246.01it/s]Running 5000 simulations.:  65%|██████▌   | 3269/5000 [00:13<00:07, 246.69it/s]Running 5000 simulations.:  66%|██████▌   | 3294/5000 [00:14<00:06, 246.32it/s]Running 5000 simulations.:  66%|██████▋   | 3319/5000 [00:14<00:06, 245.51it/s]Running 5000 simulations.:  67%|██████▋   | 3344/5000 [00:14<00:06, 245.19it/s]Running 5000 simulations.:  67%|██████▋   | 3369/5000 [00:14<00:06, 244.42it/s]Running 5000 simulations.:  68%|██████▊   | 3394/5000 [00:14<00:06, 244.27it/s]Running 5000 simulations.:  68%|██████▊   | 3419/5000 [00:14<00:06, 243.80it/s]Running 5000 simulations.:  69%|██████▉   | 3444/5000 [00:14<00:06, 244.05it/s]Running 5000 simulations.:  69%|██████▉   | 3469/5000 [00:14<00:06, 244.06it/s]Running 5000 simulations.:  70%|██████▉   | 3494/5000 [00:14<00:06, 243.69it/s]Running 5000 simulations.:  70%|███████   | 3519/5000 [00:14<00:06, 243.80it/s]Running 5000 simulations.:  71%|███████   | 3544/5000 [00:15<00:05, 243.49it/s]Running 5000 simulations.:  71%|███████▏  | 3569/5000 [00:15<00:05, 243.33it/s]Running 5000 simulations.:  72%|███████▏  | 3594/5000 [00:15<00:05, 243.89it/s]Running 5000 simulations.:  72%|███████▏  | 3619/5000 [00:15<00:05, 243.88it/s]Running 5000 simulations.:  73%|███████▎  | 3644/5000 [00:15<00:05, 243.98it/s]Running 5000 simulations.:  73%|███████▎  | 3669/5000 [00:15<00:05, 244.24it/s]Running 5000 simulations.:  74%|███████▍  | 3694/5000 [00:15<00:05, 244.45it/s]Running 5000 simulations.:  74%|███████▍  | 3719/5000 [00:15<00:05, 245.42it/s]Running 5000 simulations.:  75%|███████▍  | 3744/5000 [00:15<00:05, 245.42it/s]Running 5000 simulations.:  75%|███████▌  | 3769/5000 [00:15<00:05, 245.87it/s]Running 5000 simulations.:  76%|███████▌  | 3794/5000 [00:16<00:04, 246.31it/s]Running 5000 simulations.:  76%|███████▋  | 3819/5000 [00:16<00:04, 246.85it/s]Running 5000 simulations.:  77%|███████▋  | 3844/5000 [00:16<00:04, 247.38it/s]Running 5000 simulations.:  77%|███████▋  | 3869/5000 [00:16<00:04, 247.29it/s]Running 5000 simulations.:  78%|███████▊  | 3894/5000 [00:16<00:04, 247.21it/s]Running 5000 simulations.:  78%|███████▊  | 3919/5000 [00:16<00:04, 246.49it/s]Running 5000 simulations.:  79%|███████▉  | 3944/5000 [00:16<00:04, 244.41it/s]Running 5000 simulations.:  79%|███████▉  | 3969/5000 [00:16<00:04, 241.62it/s]Running 5000 simulations.:  80%|███████▉  | 3994/5000 [00:16<00:04, 242.72it/s]Running 5000 simulations.:  80%|████████  | 4019/5000 [00:17<00:04, 243.98it/s]Running 5000 simulations.:  81%|████████  | 4044/5000 [00:17<00:03, 244.61it/s]Running 5000 simulations.:  81%|████████▏ | 4069/5000 [00:17<00:03, 245.28it/s]Running 5000 simulations.:  82%|████████▏ | 4094/5000 [00:17<00:03, 245.56it/s]Running 5000 simulations.:  82%|████████▏ | 4119/5000 [00:17<00:03, 245.89it/s]Running 5000 simulations.:  83%|████████▎ | 4144/5000 [00:17<00:03, 245.78it/s]Running 5000 simulations.:  83%|████████▎ | 4169/5000 [00:17<00:03, 245.91it/s]Running 5000 simulations.:  84%|████████▍ | 4194/5000 [00:17<00:03, 246.49it/s]Running 5000 simulations.:  84%|████████▍ | 4220/5000 [00:17<00:03, 247.51it/s]Running 5000 simulations.:  85%|████████▍ | 4245/5000 [00:17<00:03, 246.41it/s]Running 5000 simulations.:  85%|████████▌ | 4270/5000 [00:18<00:02, 246.54it/s]Running 5000 simulations.:  86%|████████▌ | 4295/5000 [00:18<00:02, 245.77it/s]Running 5000 simulations.:  86%|████████▋ | 4320/5000 [00:18<00:02, 245.91it/s]Running 5000 simulations.:  87%|████████▋ | 4345/5000 [00:18<00:02, 246.22it/s]Running 5000 simulations.:  87%|████████▋ | 4370/5000 [00:18<00:02, 245.91it/s]Running 5000 simulations.:  88%|████████▊ | 4395/5000 [00:18<00:02, 245.75it/s]Running 5000 simulations.:  88%|████████▊ | 4420/5000 [00:18<00:02, 245.74it/s]Running 5000 simulations.:  89%|████████▉ | 4445/5000 [00:18<00:02, 246.24it/s]Running 5000 simulations.:  89%|████████▉ | 4470/5000 [00:18<00:02, 245.98it/s]Running 5000 simulations.:  90%|████████▉ | 4495/5000 [00:18<00:02, 246.25it/s]Running 5000 simulations.:  90%|█████████ | 4520/5000 [00:19<00:01, 246.61it/s]Running 5000 simulations.:  91%|█████████ | 4545/5000 [00:19<00:01, 246.57it/s]Running 5000 simulations.:  91%|█████████▏| 4570/5000 [00:19<00:01, 247.12it/s]Running 5000 simulations.:  92%|█████████▏| 4595/5000 [00:19<00:01, 247.02it/s]Running 5000 simulations.:  92%|█████████▏| 4620/5000 [00:19<00:01, 247.61it/s]Running 5000 simulations.:  93%|█████████▎| 4645/5000 [00:19<00:01, 247.77it/s]Running 5000 simulations.:  93%|█████████▎| 4670/5000 [00:19<00:01, 248.10it/s]Running 5000 simulations.:  94%|█████████▍| 4695/5000 [00:19<00:01, 248.34it/s]Running 5000 simulations.:  94%|█████████▍| 4720/5000 [00:19<00:01, 248.23it/s]Running 5000 simulations.:  95%|█████████▍| 4746/5000 [00:19<00:01, 249.18it/s]Running 5000 simulations.:  95%|█████████▌| 4772/5000 [00:20<00:00, 249.89it/s]Running 5000 simulations.:  96%|█████████▌| 4798/5000 [00:20<00:00, 250.69it/s]Running 5000 simulations.:  96%|█████████▋| 4824/5000 [00:20<00:00, 250.46it/s]Running 5000 simulations.:  97%|█████████▋| 4850/5000 [00:20<00:00, 250.34it/s]Running 5000 simulations.:  98%|█████████▊| 4876/5000 [00:20<00:00, 250.89it/s]Running 5000 simulations.:  98%|█████████▊| 4902/5000 [00:20<00:00, 250.91it/s]Running 5000 simulations.:  99%|█████████▊| 4928/5000 [00:20<00:00, 250.07it/s]Running 5000 simulations.:  99%|█████████▉| 4954/5000 [00:20<00:00, 249.11it/s]Running 5000 simulations.: 100%|█████████▉| 4980/5000 [00:20<00:00, 249.53it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:20<00:00, 238.53it/s]
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159997.56it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19177it [00:00, 158834.62it/s]           Drawing 10000 posterior samples: 19177it [00:00, 158216.94it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163629.71it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163651.42it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18504it [00:00, 153284.37it/s]           Drawing 10000 posterior samples: 18504it [00:00, 152713.12it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16522it [00:00, 128379.14it/s]           Drawing 10000 posterior samples: 16522it [00:00, 127841.06it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 140603.61it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 134601.50it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166512.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 133878.00it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19828it [00:00, 159205.81it/s]           Drawing 10000 posterior samples: 19828it [00:00, 158585.59it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159365.32it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159318.71it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158237.41it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159833.55it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19471it [00:00, 156237.65it/s]           Drawing 10000 posterior samples: 19471it [00:00, 155644.21it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19988it [00:00, 160365.56it/s]           Drawing 10000 posterior samples: 19988it [00:00, 159735.52it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160635.45it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19626it [00:00, 158351.13it/s]           Drawing 10000 posterior samples: 19626it [00:00, 157759.96it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158729.05it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158048.39it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159550.83it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159002.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 140021.57it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 150063.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 133350.63it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19999it [00:00, 133547.13it/s]           Drawing 10000 posterior samples: 19999it [00:00, 133068.12it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 131522.05it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 131920.00it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18600it [00:00, 123139.51it/s]           Drawing 10000 posterior samples: 18600it [00:00, 122670.17it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18287it [00:00, 154699.79it/s]           Drawing 10000 posterior samples: 18287it [00:00, 154112.32it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18079it [00:00, 148931.31it/s]           Drawing 10000 posterior samples: 18079it [00:00, 148386.03it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18306it [00:00, 152856.58it/s]           Drawing 10000 posterior samples: 18306it [00:00, 152269.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18382it [00:00, 148515.12it/s]           Drawing 10000 posterior samples: 18382it [00:00, 147955.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 14502it [00:00, 117461.96it/s]           Drawing 10000 posterior samples: 14502it [00:00, 117031.88it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18282it [00:00, 146350.91it/s]           Drawing 10000 posterior samples: 18282it [00:00, 145798.82it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18298it [00:00, 113073.80it/s]           Drawing 10000 posterior samples: 18298it [00:00, 112660.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18300it [00:00, 121811.54it/s]           Drawing 10000 posterior samples: 18300it [00:00, 121466.87it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18387it [00:00, 153232.15it/s]           Drawing 10000 posterior samples: 18387it [00:00, 152642.56it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18299it [00:00, 153426.73it/s]           Drawing 10000 posterior samples: 18299it [00:00, 152848.03it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18296it [00:00, 154489.57it/s]           Drawing 10000 posterior samples: 18296it [00:00, 153889.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18266it [00:00, 146897.39it/s]           Drawing 10000 posterior samples: 18266it [00:00, 146336.50it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18322it [00:00, 146617.11it/s]           Drawing 10000 posterior samples: 18322it [00:00, 146029.25it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18266it [00:00, 122271.75it/s]           Drawing 10000 posterior samples: 18266it [00:00, 121820.89it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18326it [00:00, 147861.88it/s]           Drawing 10000 posterior samples: 18326it [00:00, 147303.37it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18350it [00:00, 146954.72it/s]           Drawing 10000 posterior samples: 18350it [00:00, 146363.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18238it [00:00, 146960.75it/s]           Drawing 10000 posterior samples: 18238it [00:00, 146403.57it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18244it [00:00, 120591.73it/s]           Drawing 10000 posterior samples: 18244it [00:00, 120227.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18322it [00:00, 152813.12it/s]           Drawing 10000 posterior samples: 18322it [00:00, 152208.39it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18329it [00:00, 152015.13it/s]           Drawing 10000 posterior samples: 18329it [00:00, 151437.80it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18315it [00:00, 151946.19it/s]           Drawing 10000 posterior samples: 18315it [00:00, 151349.85it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18338it [00:00, 153105.19it/s]           Drawing 10000 posterior samples: 18338it [00:00, 152522.26it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18301it [00:00, 151849.87it/s]           Drawing 10000 posterior samples: 18301it [00:00, 151283.64it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18349it [00:00, 152885.70it/s]           Drawing 10000 posterior samples: 18349it [00:00, 152289.42it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18253it [00:00, 152071.01it/s]           Drawing 10000 posterior samples: 18253it [00:00, 151509.44it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18340it [00:00, 146079.17it/s]           Drawing 10000 posterior samples: 18340it [00:00, 145545.45it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18355it [00:00, 145538.92it/s]           Drawing 10000 posterior samples: 18355it [00:00, 145001.92it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18311it [00:00, 146079.54it/s]           Drawing 10000 posterior samples: 18311it [00:00, 145544.15it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18316it [00:00, 156102.17it/s]           Drawing 10000 posterior samples: 18316it [00:00, 155492.37it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18330it [00:00, 118134.52it/s]           Drawing 10000 posterior samples: 18330it [00:00, 117705.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 127289.13it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19977it [00:00, 130929.45it/s]           Drawing 10000 posterior samples: 19977it [00:00, 130464.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161877.24it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159946.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19387it [00:00, 157745.51it/s]           Drawing 10000 posterior samples: 19387it [00:00, 157120.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17457it [00:00, 141079.75it/s]           Drawing 10000 posterior samples: 17457it [00:00, 140486.94it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161023.97it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160170.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159837.20it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160783.85it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19979it [00:00, 162854.33it/s]           Drawing 10000 posterior samples: 19979it [00:00, 162170.74it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 134872.45it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 155586.04it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 136126.11it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 138667.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19762it [00:00, 134760.97it/s]           Drawing 10000 posterior samples: 19762it [00:00, 134250.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 133274.78it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163793.29it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19660it [00:00, 162380.40it/s]           Drawing 10000 posterior samples: 19660it [00:00, 161744.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160224.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162031.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159322.34it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159281.80it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159018.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159710.61it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161906.61it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159646.17it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 153531.22it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159165.75it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17393it [00:00, 100068.76it/s]           Drawing 10000 posterior samples: 17393it [00:00, 99725.95it/s] 
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 115740.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19881it [00:00, 116449.06it/s]           Drawing 10000 posterior samples: 19881it [00:00, 116040.21it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 118296.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 116512.00it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19127it [00:00, 159291.49it/s]           Drawing 10000 posterior samples: 19127it [00:00, 158667.07it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18249it [00:00, 151744.11it/s]           Drawing 10000 posterior samples: 18249it [00:00, 151177.96it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162869.78it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165769.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165022.88it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164507.67it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19995it [00:00, 165763.93it/s]           Drawing 10000 posterior samples: 19995it [00:00, 165253.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164825.09it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163541.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166480.93it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163929.65it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19892it [00:00, 164440.04it/s]           Drawing 10000 posterior samples: 19892it [00:00, 163892.85it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19999it [00:00, 166761.20it/s]           Drawing 10000 posterior samples: 19999it [00:00, 166120.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162780.65it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19668it [00:00, 161220.39it/s]           Drawing 10000 posterior samples: 19668it [00:00, 160563.30it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166315.90it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165672.75it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164537.36it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163951.44it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164452.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163768.35it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164558.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164437.37it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163285.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165980.24it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17868it [00:00, 148932.40it/s]           Drawing 10000 posterior samples: 17868it [00:00, 148278.82it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18333it [00:00, 152452.26it/s]           Drawing 10000 posterior samples: 18333it [00:00, 151870.23it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18268it [00:00, 151347.61it/s]           Drawing 10000 posterior samples: 18268it [00:00, 150671.13it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18356it [00:00, 151006.46it/s]           Drawing 10000 posterior samples: 18356it [00:00, 150393.79it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18402it [00:00, 129024.44it/s]           Drawing 10000 posterior samples: 18402it [00:00, 128676.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17584it [00:00, 145389.29it/s]           Drawing 10000 posterior samples: 17584it [00:00, 144796.15it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18402it [00:00, 151763.81it/s]           Drawing 10000 posterior samples: 18402it [00:00, 151167.85it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18402it [00:00, 150647.09it/s]           Drawing 10000 posterior samples: 18402it [00:00, 150132.23it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18298it [00:00, 144447.70it/s]           Drawing 10000 posterior samples: 18298it [00:00, 143875.52it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18395it [00:00, 143228.30it/s]           Drawing 10000 posterior samples: 18395it [00:00, 142662.08it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18385it [00:00, 144229.46it/s]           Drawing 10000 posterior samples: 18385it [00:00, 143663.83it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18417it [00:00, 142146.96it/s]           Drawing 10000 posterior samples: 18417it [00:00, 141572.26it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18414it [00:00, 143823.99it/s]           Drawing 10000 posterior samples: 18414it [00:00, 143283.94it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18363it [00:00, 147076.80it/s]           Drawing 10000 posterior samples: 18363it [00:00, 146521.69it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18393it [00:00, 145636.22it/s]           Drawing 10000 posterior samples: 18393it [00:00, 145049.41it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18389it [00:00, 146094.36it/s]           Drawing 10000 posterior samples: 18389it [00:00, 145495.77it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18336it [00:00, 146068.11it/s]           Drawing 10000 posterior samples: 18336it [00:00, 145482.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18157it [00:00, 144521.66it/s]           Drawing 10000 posterior samples: 18157it [00:00, 143967.05it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18459it [00:00, 146137.01it/s]           Drawing 10000 posterior samples: 18459it [00:00, 145552.65it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18357it [00:00, 145593.86it/s]           Drawing 10000 posterior samples: 18357it [00:00, 145025.64it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18438it [00:00, 146394.36it/s]           Drawing 10000 posterior samples: 18438it [00:00, 145808.65it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18357it [00:00, 153927.49it/s]           Drawing 10000 posterior samples: 18357it [00:00, 153350.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18347it [00:00, 149783.74it/s]           Drawing 10000 posterior samples: 18347it [00:00, 149134.10it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18411it [00:00, 144284.73it/s]           Drawing 10000 posterior samples: 18411it [00:00, 143653.16it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18417it [00:00, 142774.89it/s]           Drawing 10000 posterior samples: 18417it [00:00, 142228.10it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18361it [00:00, 144677.36it/s]           Drawing 10000 posterior samples: 18361it [00:00, 144015.32it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18409it [00:00, 113966.60it/s]           Drawing 10000 posterior samples: 18409it [00:00, 113493.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18425it [00:00, 116732.48it/s]           Drawing 10000 posterior samples: 18425it [00:00, 116302.77it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18405it [00:00, 143581.50it/s]           Drawing 10000 posterior samples: 18405it [00:00, 142957.45it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18421it [00:00, 145073.56it/s]           Drawing 10000 posterior samples: 18421it [00:00, 144497.55it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18389it [00:00, 145525.15it/s]           Drawing 10000 posterior samples: 18389it [00:00, 144933.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 156263.07it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19634it [00:00, 153184.88it/s]           Drawing 10000 posterior samples: 19634it [00:00, 152584.30it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 156701.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160164.35it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18587it [00:00, 149505.85it/s]           Drawing 10000 posterior samples: 18587it [00:00, 148947.71it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17746it [00:00, 143359.79it/s]           Drawing 10000 posterior samples: 17746it [00:00, 142741.76it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159654.07it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162588.83it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160266.56it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161492.06it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160891.17it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159511.99it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163843.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162005.42it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164053.40it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19920it [00:00, 163870.21it/s]           Drawing 10000 posterior samples: 19920it [00:00, 163214.29it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19996it [00:00, 164503.16it/s]           Drawing 10000 posterior samples: 19996it [00:00, 163844.35it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 167024.56it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19456it [00:00, 166553.82it/s]           Drawing 10000 posterior samples: 19456it [00:00, 165902.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162398.71it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162052.36it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162480.50it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161937.24it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161348.55it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 114538.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161663.23it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160430.23it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163754.92it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161539.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17131it [00:00, 139000.63it/s]           Drawing 10000 posterior samples: 17131it [00:00, 138472.91it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161231.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19616it [00:00, 159144.79it/s]           Drawing 10000 posterior samples: 19616it [00:00, 158404.83it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160591.17it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 171668.34it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19012it [00:00, 156258.72it/s]           Drawing 10000 posterior samples: 19012it [00:00, 155600.12it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17907it [00:00, 146652.90it/s]           Drawing 10000 posterior samples: 17907it [00:00, 146063.39it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 137196.09it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 136989.52it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 134793.57it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 136189.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19993it [00:00, 133524.49it/s]           Drawing 10000 posterior samples: 19993it [00:00, 132954.59it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158195.63it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159413.78it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159005.25it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158595.81it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19857it [00:00, 157423.75it/s]           Drawing 10000 posterior samples: 19857it [00:00, 156717.56it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19986it [00:00, 161485.33it/s]           Drawing 10000 posterior samples: 19986it [00:00, 160678.08it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158763.90it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17835it [00:00, 142907.60it/s]           Drawing 10000 posterior samples: 17835it [00:00, 142320.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 157145.95it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159343.52it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158166.99it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158390.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 154218.80it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 131720.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 131980.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 133035.10it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 124831.22it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 131132.65it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16305it [00:00, 138244.69it/s]           Drawing 10000 posterior samples: 16305it [00:00, 137711.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 168450.00it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18868it [00:00, 153645.98it/s]           Drawing 10000 posterior samples: 18868it [00:00, 153028.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158760.89it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159445.90it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16089it [00:00, 129188.06it/s]           Drawing 10000 posterior samples: 16089it [00:00, 128680.83it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17012it [00:00, 138255.18it/s]           Drawing 10000 posterior samples: 17012it [00:00, 137714.04it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162485.53it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164208.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162193.36it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 168977.99it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19947it [00:00, 166947.59it/s]           Drawing 10000 posterior samples: 19947it [00:00, 166273.06it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166877.03it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166176.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158771.71it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160125.83it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19792it [00:00, 159793.47it/s]           Drawing 10000 posterior samples: 19792it [00:00, 159158.68it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19995it [00:00, 131686.97it/s]           Drawing 10000 posterior samples: 19995it [00:00, 131189.69it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160676.68it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19628it [00:00, 157416.73it/s]           Drawing 10000 posterior samples: 19628it [00:00, 156782.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160730.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 118314.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166732.42it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165432.29it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 167593.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164842.58it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166201.88it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19996it [00:00, 167638.69it/s]           Drawing 10000 posterior samples: 19996it [00:00, 166952.59it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166666.83it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 167288.36it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17513it [00:00, 145782.79it/s]           Drawing 10000 posterior samples: 17513it [00:00, 145227.95it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18350it [00:00, 144483.74it/s]           Drawing 10000 posterior samples: 18350it [00:00, 143926.08it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17903it [00:00, 142339.48it/s]           Drawing 10000 posterior samples: 17903it [00:00, 141751.30it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18321it [00:00, 145042.84it/s]           Drawing 10000 posterior samples: 18321it [00:00, 144440.06it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18318it [00:00, 144762.25it/s]           Drawing 10000 posterior samples: 18318it [00:00, 144226.64it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16190it [00:00, 103558.48it/s]           Drawing 10000 posterior samples: 16190it [00:00, 103139.62it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18357it [00:00, 125029.17it/s]           Drawing 10000 posterior samples: 18357it [00:00, 124554.87it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18247it [00:00, 128512.98it/s]           Drawing 10000 posterior samples: 18247it [00:00, 128075.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18293it [00:00, 140040.82it/s]           Drawing 10000 posterior samples: 18293it [00:00, 139509.15it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18317it [00:00, 148215.51it/s]           Drawing 10000 posterior samples: 18317it [00:00, 147620.87it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18306it [00:00, 149344.95it/s]           Drawing 10000 posterior samples: 18306it [00:00, 148619.95it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18303it [00:00, 147715.23it/s]           Drawing 10000 posterior samples: 18303it [00:00, 147040.45it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18324it [00:00, 148427.93it/s]           Drawing 10000 posterior samples: 18324it [00:00, 147824.13it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18297it [00:00, 149621.05it/s]           Drawing 10000 posterior samples: 18297it [00:00, 148860.95it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18321it [00:00, 148916.69it/s]           Drawing 10000 posterior samples: 18321it [00:00, 148210.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18404it [00:00, 149964.20it/s]           Drawing 10000 posterior samples: 18404it [00:00, 149380.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18267it [00:00, 142788.58it/s]           Drawing 10000 posterior samples: 18267it [00:00, 142175.18it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18341it [00:00, 144578.71it/s]           Drawing 10000 posterior samples: 18341it [00:00, 143962.10it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18354it [00:00, 145166.00it/s]           Drawing 10000 posterior samples: 18354it [00:00, 144586.35it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18233it [00:00, 143583.81it/s]           Drawing 10000 posterior samples: 18233it [00:00, 142890.30it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18268it [00:00, 143469.92it/s]           Drawing 10000 posterior samples: 18268it [00:00, 142885.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18276it [00:00, 144349.83it/s]           Drawing 10000 posterior samples: 18276it [00:00, 143757.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18245it [00:00, 143116.17it/s]           Drawing 10000 posterior samples: 18245it [00:00, 142495.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18319it [00:00, 143746.64it/s]           Drawing 10000 posterior samples: 18319it [00:00, 143183.57it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18286it [00:00, 144901.96it/s]           Drawing 10000 posterior samples: 18286it [00:00, 144317.39it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18315it [00:00, 143264.44it/s]           Drawing 10000 posterior samples: 18315it [00:00, 142666.05it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18271it [00:00, 149232.42it/s]           Drawing 10000 posterior samples: 18271it [00:00, 148632.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18306it [00:00, 147434.76it/s]           Drawing 10000 posterior samples: 18306it [00:00, 146822.13it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18334it [00:00, 147228.86it/s]           Drawing 10000 posterior samples: 18334it [00:00, 146638.44it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18340it [00:00, 149405.54it/s]           Drawing 10000 posterior samples: 18340it [00:00, 148805.25it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18360it [00:00, 147707.16it/s]           Drawing 10000 posterior samples: 18360it [00:00, 147109.52it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161488.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19826it [00:00, 161561.03it/s]           Drawing 10000 posterior samples: 19826it [00:00, 160903.61it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162467.28it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161577.29it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16431it [00:00, 103734.14it/s]           Drawing 10000 posterior samples: 16431it [00:00, 103355.02it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18155it [00:00, 113858.23it/s]           Drawing 10000 posterior samples: 18155it [00:00, 113454.49it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 124642.77it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 123371.32it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 124032.75it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166554.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19979it [00:00, 167924.80it/s]           Drawing 10000 posterior samples: 19979it [00:00, 167159.05it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165856.85it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164677.54it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166465.08it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165050.15it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19795it [00:00, 166078.07it/s]           Drawing 10000 posterior samples: 19795it [00:00, 165416.63it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 167776.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164236.48it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19329it [00:00, 161640.70it/s]           Drawing 10000 posterior samples: 19329it [00:00, 160994.55it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 168300.62it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165632.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164385.81it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 166398.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165532.18it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165553.09it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165498.87it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165724.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 167628.00it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164255.77it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16270it [00:00, 135817.69it/s]           Drawing 10000 posterior samples: 16270it [00:00, 135293.15it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
30
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Neural network successfully converged after 278 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Neural network successfully converged after 181 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Training neural network. Epochs trained:  340Training neural network. Epochs trained:  341Training neural network. Epochs trained:  342Training neural network. Epochs trained:  343Training neural network. Epochs trained:  344Training neural network. Epochs trained:  345Training neural network. Epochs trained:  346Training neural network. Epochs trained:  347Training neural network. Epochs trained:  348Training neural network. Epochs trained:  349Training neural network. Epochs trained:  350Training neural network. Epochs trained:  351Training neural network. Epochs trained:  352Training neural network. Epochs trained:  353Training neural network. Epochs trained:  354Training neural network. Epochs trained:  355Training neural network. Epochs trained:  356Training neural network. Epochs trained:  357Training neural network. Epochs trained:  358Training neural network. Epochs trained:  359Training neural network. Epochs trained:  360Neural network successfully converged after 360 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Training neural network. Epochs trained:  340Training neural network. Epochs trained:  341Training neural network. Epochs trained:  342Training neural network. Epochs trained:  343Training neural network. Epochs trained:  344Training neural network. Epochs trained:  345Training neural network. Epochs trained:  346Training neural network. Epochs trained:  347Training neural network. Epochs trained:  348Training neural network. Epochs trained:  349Training neural network. Epochs trained:  350Training neural network. Epochs trained:  351Training neural network. Epochs trained:  352Training neural network. Epochs trained:  353Training neural network. Epochs trained:  354Training neural network. Epochs trained:  355Training neural network. Epochs trained:  356Training neural network. Epochs trained:  357Training neural network. Epochs trained:  358Training neural network. Epochs trained:  359Training neural network. Epochs trained:  360Training neural network. Epochs trained:  361Training neural network. Epochs trained:  362Training neural network. Epochs trained:  363Training neural network. Epochs trained:  364Training neural network. Epochs trained:  365Training neural network. Epochs trained:  366Training neural network. Epochs trained:  367Training neural network. Epochs trained:  368Training neural network. Epochs trained:  369Training neural network. Epochs trained:  370Training neural network. Epochs trained:  371Training neural network. Epochs trained:  372Training neural network. Epochs trained:  373Training neural network. Epochs trained:  374Training neural network. Epochs trained:  375Training neural network. Epochs trained:  376Training neural network. Epochs trained:  377Training neural network. Epochs trained:  378Training neural network. Epochs trained:  379Training neural network. Epochs trained:  380Neural network successfully converged after 380 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Neural network successfully converged after 251 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Training neural network. Epochs trained:  340Training neural network. Epochs trained:  341Training neural network. Epochs trained:  342Training neural network. Epochs trained:  343Training neural network. Epochs trained:  344Training neural network. Epochs trained:  345Training neural network. Epochs trained:  346Training neural network. Epochs trained:  347Training neural network. Epochs trained:  348Training neural network. Epochs trained:  349Training neural network. Epochs trained:  350Training neural network. Epochs trained:  351Training neural network. Epochs trained:  352Training neural network. Epochs trained:  353Training neural network. Epochs trained:  354Training neural network. Epochs trained:  355Training neural network. Epochs trained:  356Training neural network. Epochs trained:  357Training neural network. Epochs trained:  358Training neural network. Epochs trained:  359Training neural network. Epochs trained:  360Training neural network. Epochs trained:  361Training neural network. Epochs trained:  362Training neural network. Epochs trained:  363Training neural network. Epochs trained:  364Training neural network. Epochs trained:  365Training neural network. Epochs trained:  366Training neural network. Epochs trained:  367Training neural network. Epochs trained:  368Training neural network. Epochs trained:  369Training neural network. Epochs trained:  370Training neural network. Epochs trained:  371Training neural network. Epochs trained:  372Training neural network. Epochs trained:  373Training neural network. Epochs trained:  374Training neural network. Epochs trained:  375Training neural network. Epochs trained:  376Training neural network. Epochs trained:  377Training neural network. Epochs trained:  378Training neural network. Epochs trained:  379Training neural network. Epochs trained:  380Training neural network. Epochs trained:  381Training neural network. Epochs trained:  382Training neural network. Epochs trained:  383Training neural network. Epochs trained:  384Training neural network. Epochs trained:  385Training neural network. Epochs trained:  386Training neural network. Epochs trained:  387Training neural network. Epochs trained:  388Training neural network. Epochs trained:  389Training neural network. Epochs trained:  390Training neural network. Epochs trained:  391Training neural network. Epochs trained:  392Training neural network. Epochs trained:  393Training neural network. Epochs trained:  394Training neural network. Epochs trained:  395Training neural network. Epochs trained:  396Training neural network. Epochs trained:  397Training neural network. Epochs trained:  398Training neural network. Epochs trained:  399Training neural network. Epochs trained:  400Training neural network. Epochs trained:  401Training neural network. Epochs trained:  402Training neural network. Epochs trained:  403Training neural network. Epochs trained:  404Training neural network. Epochs trained:  405Training neural network. Epochs trained:  406Neural network successfully converged after 406 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Training neural network. Epochs trained:  340Training neural network. Epochs trained:  341Training neural network. Epochs trained:  342Training neural network. Epochs trained:  343Training neural network. Epochs trained:  344Training neural network. Epochs trained:  345Training neural network. Epochs trained:  346Training neural network. Epochs trained:  347Training neural network. Epochs trained:  348Training neural network. Epochs trained:  349Training neural network. Epochs trained:  350Training neural network. Epochs trained:  351Training neural network. Epochs trained:  352Training neural network. Epochs trained:  353Training neural network. Epochs trained:  354Training neural network. Epochs trained:  355Training neural network. Epochs trained:  356Training neural network. Epochs trained:  357Training neural network. Epochs trained:  358Training neural network. Epochs trained:  359Training neural network. Epochs trained:  360Training neural network. Epochs trained:  361Training neural network. Epochs trained:  362Training neural network. Epochs trained:  363Training neural network. Epochs trained:  364Training neural network. Epochs trained:  365Training neural network. Epochs trained:  366Training neural network. Epochs trained:  367Training neural network. Epochs trained:  368Training neural network. Epochs trained:  369Training neural network. Epochs trained:  370Training neural network. Epochs trained:  371Training neural network. Epochs trained:  372Training neural network. Epochs trained:  373Training neural network. Epochs trained:  374Training neural network. Epochs trained:  375Training neural network. Epochs trained:  376Training neural network. Epochs trained:  377Training neural network. Epochs trained:  378Training neural network. Epochs trained:  379Training neural network. Epochs trained:  380Training neural network. Epochs trained:  381Training neural network. Epochs trained:  382Training neural network. Epochs trained:  383Training neural network. Epochs trained:  384Training neural network. Epochs trained:  385Training neural network. Epochs trained:  386Training neural network. Epochs trained:  387Training neural network. Epochs trained:  388Training neural network. Epochs trained:  389Training neural network. Epochs trained:  390Training neural network. Epochs trained:  391Training neural network. Epochs trained:  392Training neural network. Epochs trained:  393Training neural network. Epochs trained:  394Training neural network. Epochs trained:  395Training neural network. Epochs trained:  396Training neural network. Epochs trained:  397Neural network successfully converged after 397 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Neural network successfully converged after 314 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Neural network successfully converged after 191 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Neural network successfully converged after 326 epochs.
log prob true 7.10543
log prob true 6.5522556
log prob true 6.747671
log prob true 6.4362817
log prob true 5.795037
log prob true 6.394366
log prob true 6.1884165
log prob true 6.8030114
log prob true 6.3854914
log prob true 6.6947355
log prob true 6.424665
log prob true 6.7023516
log prob true 7.116479
log prob true 6.7261624
log prob true 6.0865927
log prob true 6.029409
log prob true 6.302719
log prob true 6.4133997
log prob true 6.3134875
log prob true 6.877104
log prob true 6.8183837
log prob true 6.5525584
log prob true 6.914265
log prob true 6.3945823
log prob true 6.815587
log prob true 6.5988903
log prob true 5.795656
log prob true 6.243592
log prob true 6.85445
log prob true 6.004795
log prob true 3.9636035
log prob true 3.6503348
log prob true 4.0141325
log prob true 2.861717
log prob true 3.2439377
log prob true 2.823038
log prob true 3.320069
log prob true 3.935479
log prob true 3.4937203
log prob true 3.4029505
log prob true 2.993334
log prob true 3.7795403
log prob true 4.1991014
log prob true 3.6837142
log prob true 2.5011096
log prob true 2.747006
log prob true 3.4104269
log prob true 3.8160172
log prob true 2.7783864
log prob true 3.9954019
log prob true 3.8504443
log prob true 3.584406
log prob true 4.149053
log prob true 3.5354278
log prob true 4.005705
log prob true 3.7313366
log prob true 2.6821966
log prob true 3.4372952
log prob true 4.020746
log prob true 1.9413303
log prob true 7.3690863
log prob true 6.793203
log prob true 7.1138353
log prob true 6.4041924
log prob true 5.9371963
log prob true 6.7211385
log prob true 6.532109
log prob true 7.0625
log prob true 6.6906304
log prob true 6.7599487
log prob true 6.550901
log prob true 6.9809966
log prob true 7.218025
log prob true 6.9292827
log prob true 6.1132865
log prob true 6.3220143
log prob true 6.518916
log prob true 6.88952
log prob true 6.4794464
log prob true 7.1616826
log prob true 6.999748
log prob true 6.7674456
log prob true 7.247234
log prob true 6.6967754
log prob true 7.1784234
log prob true 6.904041
log prob true 6.303134
log prob true 6.6277776
log prob true 7.0453525
log prob true 6.256813
log prob true 7.391569
log prob true 6.916036
log prob true 7.1182775
log prob true 6.8645053
log prob true 6.211625
log prob true 6.6070404
log prob true 6.7934275
log prob true 7.0860605
log prob true 6.817711
log prob true 7.023757
log prob true 6.817736
log prob true 7.185549
log prob true 7.4381404
log prob true 7.1010814
log prob true 6.6431317
log prob true 6.351125
log prob true 6.534906
log prob true 6.7638006
log prob true 6.6020103
log prob true 7.254674
log prob true 7.1340847
log prob true 6.9055877
log prob true 7.27845
log prob true 6.811341
log prob true 7.2319427
log prob true 7.093381
log prob true 6.4352303
log prob true 6.717448
log prob true 7.204369
log prob true 6.401096
log prob true 4.3295593
log prob true 3.7923262
log prob true 4.1679406
log prob true 2.7361376
log prob true 2.8767645
log prob true 2.8074157
log prob true 3.4449172
log prob true 4.1235747
log prob true 3.6586273
log prob true 3.7247367
log prob true 3.159303
log prob true 3.8599238
log prob true 4.1129293
log prob true 4.0405774
log prob true 3.324854
log prob true 2.8388126
log prob true 3.5595546
log prob true 3.9531958
log prob true 2.2938406
log prob true 4.161937
log prob true 3.8030903
log prob true 3.4528444
log prob true 4.288035
log prob true 3.631005
log prob true 4.1567836
log prob true 3.9374328
log prob true 2.903203
log prob true 3.5245352
log prob true 3.968429
log prob true 2.2505488
log prob true 7.229445
log prob true 6.684509
log prob true 6.9360523
log prob true 6.6340995
log prob true 5.7883196
log prob true 6.6588187
log prob true 6.1458035
log prob true 6.949324
log prob true 6.599935
log prob true 6.8859773
log prob true 6.6235743
log prob true 6.7201886
log prob true 7.166191
log prob true 6.880284
log prob true 6.459461
log prob true 6.294458
log prob true 6.3866897
log prob true 6.7795544
log prob true 6.448339
log prob true 6.980267
log prob true 6.920726
log prob true 6.7626686
log prob true 7.1406994
log prob true 6.496392
log prob true 6.9895315
log prob true 6.755954
log prob true 5.959972
log prob true 6.5712037
log prob true 7.0631905
log prob true 6.3558536
log prob true 7.325215
log prob true 6.678161
log prob true 7.0622907
log prob true 6.6662555
log prob true 5.9527097
log prob true 6.517839
log prob true 6.48043
log prob true 6.817731
log prob true 6.7747397
log prob true 6.750018
log prob true 6.6318984
log prob true 6.857451
log prob true 7.327407
log prob true 7.0495358
log prob true 6.587022
log prob true 6.351905
log prob true 6.557421
log prob true 6.819068
log prob true 6.3109617
log prob true 7.019134
log prob true 7.142023
log prob true 6.764266
log prob true 7.2454333
log prob true 6.4686227
log prob true 7.096917
log prob true 6.596862
log prob true 6.0962577
log prob true 6.544403
log prob true 6.984615
log prob true 6.2542405
log prob true 7.0800977
log prob true 6.4958973
log prob true 6.83593
log prob true 6.621318
log prob true 6.3759594
log prob true 6.586836
log prob true 6.424242
log prob true 6.681585
log prob true 6.515727
log prob true 6.7064843
log prob true 6.507123
log prob true 6.6792636
log prob true 7.3351765
log prob true 6.864676
log prob true 6.245447
log prob true 6.0808477
log prob true 6.3655405
log prob true 6.650549
log prob true 6.383951
log prob true 6.9868526
log prob true 6.9916425
log prob true 6.809583
log prob true 6.9931474
log prob true 6.433124
log prob true 6.636003
log prob true 6.728219
log prob true 5.981468
log prob true 6.412275
log prob true 6.975588
log prob true 6.2806435
log prob true 3.8989365
log prob true 3.7274601
log prob true 4.013627
log prob true 2.5368807
log prob true 2.959473
log prob true 2.9603138
log prob true 3.3560522
log prob true 4.0139437
log prob true 3.5086467
log prob true 3.4825368
log prob true 2.8776019
log prob true 3.8052273
log prob true 4.232818
log prob true 3.4267569
log prob true 3.0299397
log prob true 2.7379503
log prob true 3.5615878
log prob true 3.8238485
log prob true 2.775022
log prob true 4.042826
log prob true 3.8319707
log prob true 3.535141
log prob true 4.083588
log prob true 3.5440302
log prob true 4.021918
log prob true 3.7925837
log prob true 2.8726006
log prob true 3.4459534
log prob true 4.032092
log prob true 1.4949762
log prob true 7.1259365
log prob true 6.5505953
log prob true 6.883641
log prob true 6.6309457
log prob true 6.372292
log prob true 6.5128546
log prob true 6.391793
log prob true 6.941408
log prob true 6.6453147
log prob true 6.6945744
log prob true 6.616939
log prob true 6.740791
log prob true 7.2654448
log prob true 6.855369
log prob true 6.0182247
log prob true 5.7731028
log prob true 6.51005
log prob true 6.74728
log prob true 6.4251857
log prob true 6.896338
log prob true 7.024142
log prob true 6.7378497
log prob true 6.759439
log prob true 6.493503
log prob true 7.0353875
log prob true 6.7135463
log prob true 6.1360245
log prob true 6.4974403
log prob true 7.0172815
log prob true 6.4529123
script complete
