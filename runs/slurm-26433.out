Loading parflow-ml/latest
  Loading requirement: openmpi/gcc/4.1.0 parflow/3.9.0 gdal/3.2.1
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  warnings.warn(
/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/05_utils/sbiutils.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y_out)
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   1%|          | 27/5000 [00:00<00:19, 261.50it/s]Running 5000 simulations.:   1%|          | 53/5000 [00:00<00:19, 260.23it/s]Running 5000 simulations.:   2%|▏         | 79/5000 [00:00<00:19, 258.69it/s]Running 5000 simulations.:   2%|▏         | 105/5000 [00:00<00:18, 257.78it/s]Running 5000 simulations.:   3%|▎         | 131/5000 [00:00<00:18, 257.59it/s]Running 5000 simulations.:   3%|▎         | 157/5000 [00:00<00:18, 257.75it/s]Running 5000 simulations.:   4%|▎         | 183/5000 [00:00<00:18, 257.81it/s]Running 5000 simulations.:   4%|▍         | 209/5000 [00:00<00:18, 258.12it/s]Running 5000 simulations.:   5%|▍         | 235/5000 [00:00<00:18, 258.60it/s]Running 5000 simulations.:   5%|▌         | 261/5000 [00:01<00:18, 257.60it/s]Running 5000 simulations.:   6%|▌         | 287/5000 [00:01<00:18, 257.46it/s]Running 5000 simulations.:   6%|▋         | 313/5000 [00:01<00:18, 256.61it/s]Running 5000 simulations.:   7%|▋         | 339/5000 [00:01<00:18, 256.91it/s]Running 5000 simulations.:   7%|▋         | 365/5000 [00:01<00:18, 256.82it/s]Running 5000 simulations.:   8%|▊         | 391/5000 [00:01<00:18, 255.75it/s]Running 5000 simulations.:   8%|▊         | 417/5000 [00:01<00:17, 254.68it/s]Running 5000 simulations.:   9%|▉         | 443/5000 [00:01<00:17, 255.18it/s]Running 5000 simulations.:   9%|▉         | 469/5000 [00:01<00:17, 256.29it/s]Running 5000 simulations.:  10%|▉         | 495/5000 [00:01<00:17, 257.05it/s]Running 5000 simulations.:  10%|█         | 521/5000 [00:02<00:17, 255.97it/s]Running 5000 simulations.:  11%|█         | 547/5000 [00:02<00:17, 255.38it/s]Running 5000 simulations.:  11%|█▏        | 573/5000 [00:02<00:17, 254.89it/s]Running 5000 simulations.:  12%|█▏        | 599/5000 [00:02<00:17, 254.11it/s]Running 5000 simulations.:  12%|█▎        | 625/5000 [00:02<00:17, 253.39it/s]Running 5000 simulations.:  13%|█▎        | 651/5000 [00:02<00:17, 252.75it/s]Running 5000 simulations.:  14%|█▎        | 677/5000 [00:02<00:17, 252.40it/s]Running 5000 simulations.:  14%|█▍        | 703/5000 [00:02<00:17, 251.97it/s]Running 5000 simulations.:  15%|█▍        | 729/5000 [00:02<00:16, 251.75it/s]Running 5000 simulations.:  15%|█▌        | 755/5000 [00:02<00:16, 252.45it/s]Running 5000 simulations.:  16%|█▌        | 781/5000 [00:03<00:16, 252.42it/s]Running 5000 simulations.:  16%|█▌        | 807/5000 [00:03<00:16, 251.56it/s]Running 5000 simulations.:  17%|█▋        | 833/5000 [00:03<00:16, 252.26it/s]Running 5000 simulations.:  17%|█▋        | 859/5000 [00:03<00:16, 252.35it/s]Running 5000 simulations.:  18%|█▊        | 885/5000 [00:03<00:16, 251.59it/s]Running 5000 simulations.:  18%|█▊        | 911/5000 [00:03<00:16, 251.56it/s]Running 5000 simulations.:  19%|█▊        | 937/5000 [00:03<00:16, 250.55it/s]Running 5000 simulations.:  19%|█▉        | 963/5000 [00:03<00:16, 250.81it/s]Running 5000 simulations.:  20%|█▉        | 989/5000 [00:03<00:16, 250.38it/s]Running 5000 simulations.:  20%|██        | 1015/5000 [00:03<00:15, 251.28it/s]Running 5000 simulations.:  21%|██        | 1041/5000 [00:04<00:15, 250.49it/s]Running 5000 simulations.:  21%|██▏       | 1067/5000 [00:04<00:15, 250.70it/s]Running 5000 simulations.:  22%|██▏       | 1093/5000 [00:04<00:15, 250.65it/s]Running 5000 simulations.:  22%|██▏       | 1119/5000 [00:04<00:15, 251.72it/s]Running 5000 simulations.:  23%|██▎       | 1145/5000 [00:04<00:15, 251.78it/s]Running 5000 simulations.:  23%|██▎       | 1171/5000 [00:04<00:15, 252.71it/s]Running 5000 simulations.:  24%|██▍       | 1197/5000 [00:04<00:15, 253.30it/s]Running 5000 simulations.:  24%|██▍       | 1223/5000 [00:04<00:14, 253.34it/s]Running 5000 simulations.:  25%|██▍       | 1249/5000 [00:04<00:14, 253.57it/s]Running 5000 simulations.:  26%|██▌       | 1275/5000 [00:05<00:14, 253.41it/s]Running 5000 simulations.:  26%|██▌       | 1301/5000 [00:05<00:14, 252.62it/s]Running 5000 simulations.:  27%|██▋       | 1327/5000 [00:05<00:14, 252.83it/s]Running 5000 simulations.:  27%|██▋       | 1353/5000 [00:05<00:14, 252.59it/s]Running 5000 simulations.:  28%|██▊       | 1379/5000 [00:05<00:14, 251.67it/s]Running 5000 simulations.:  28%|██▊       | 1405/5000 [00:05<00:14, 251.60it/s]Running 5000 simulations.:  29%|██▊       | 1431/5000 [00:05<00:14, 251.23it/s]Running 5000 simulations.:  29%|██▉       | 1457/5000 [00:05<00:14, 251.13it/s]Running 5000 simulations.:  30%|██▉       | 1483/5000 [00:05<00:14, 250.48it/s]Running 5000 simulations.:  30%|███       | 1509/5000 [00:05<00:13, 251.06it/s]Running 5000 simulations.:  31%|███       | 1536/5000 [00:06<00:13, 254.49it/s]Running 5000 simulations.:  31%|███       | 1562/5000 [00:06<00:14, 243.27it/s]Running 5000 simulations.:  32%|███▏      | 1587/5000 [00:06<00:16, 209.80it/s]Running 5000 simulations.:  32%|███▏      | 1609/5000 [00:06<00:17, 190.00it/s]Running 5000 simulations.:  33%|███▎      | 1629/5000 [00:06<00:18, 179.04it/s]Running 5000 simulations.:  33%|███▎      | 1648/5000 [00:06<00:19, 172.37it/s]Running 5000 simulations.:  33%|███▎      | 1666/5000 [00:06<00:19, 168.99it/s]Running 5000 simulations.:  34%|███▎      | 1684/5000 [00:06<00:19, 165.83it/s]Running 5000 simulations.:  34%|███▍      | 1701/5000 [00:07<00:20, 163.34it/s]Running 5000 simulations.:  34%|███▍      | 1718/5000 [00:07<00:20, 161.22it/s]Running 5000 simulations.:  35%|███▍      | 1735/5000 [00:07<00:20, 158.65it/s]Running 5000 simulations.:  35%|███▌      | 1751/5000 [00:07<00:20, 156.76it/s]Running 5000 simulations.:  35%|███▌      | 1768/5000 [00:07<00:20, 160.11it/s]Running 5000 simulations.:  36%|███▌      | 1785/5000 [00:07<00:20, 159.62it/s]Running 5000 simulations.:  36%|███▌      | 1802/5000 [00:07<00:20, 159.86it/s]Running 5000 simulations.:  36%|███▋      | 1819/5000 [00:07<00:19, 160.27it/s]Running 5000 simulations.:  37%|███▋      | 1836/5000 [00:07<00:19, 159.04it/s]Running 5000 simulations.:  37%|███▋      | 1852/5000 [00:08<00:19, 158.30it/s]Running 5000 simulations.:  37%|███▋      | 1869/5000 [00:08<00:19, 159.67it/s]Running 5000 simulations.:  38%|███▊      | 1885/5000 [00:08<00:19, 159.04it/s]Running 5000 simulations.:  38%|███▊      | 1901/5000 [00:08<00:19, 157.83it/s]Running 5000 simulations.:  38%|███▊      | 1918/5000 [00:08<00:19, 160.82it/s]Running 5000 simulations.:  39%|███▊      | 1936/5000 [00:08<00:18, 164.85it/s]Running 5000 simulations.:  39%|███▉      | 1953/5000 [00:08<00:18, 163.41it/s]Running 5000 simulations.:  39%|███▉      | 1970/5000 [00:08<00:18, 162.34it/s]Running 5000 simulations.:  40%|███▉      | 1987/5000 [00:08<00:18, 162.19it/s]Running 5000 simulations.:  40%|████      | 2005/5000 [00:08<00:18, 165.40it/s]Running 5000 simulations.:  40%|████      | 2023/5000 [00:09<00:17, 166.97it/s]Running 5000 simulations.:  41%|████      | 2040/5000 [00:09<00:18, 163.67it/s]Running 5000 simulations.:  41%|████      | 2057/5000 [00:09<00:18, 161.90it/s]Running 5000 simulations.:  41%|████▏     | 2074/5000 [00:09<00:18, 160.76it/s]Running 5000 simulations.:  42%|████▏     | 2091/5000 [00:09<00:18, 158.42it/s]Running 5000 simulations.:  42%|████▏     | 2107/5000 [00:09<00:20, 138.35it/s]Running 5000 simulations.:  42%|████▏     | 2122/5000 [00:09<00:20, 140.51it/s]Running 5000 simulations.:  43%|████▎     | 2137/5000 [00:09<00:20, 142.08it/s]Running 5000 simulations.:  43%|████▎     | 2155/5000 [00:09<00:19, 149.24it/s]Running 5000 simulations.:  43%|████▎     | 2171/5000 [00:10<00:18, 149.94it/s]Running 5000 simulations.:  44%|████▎     | 2187/5000 [00:10<00:18, 151.74it/s]Running 5000 simulations.:  44%|████▍     | 2203/5000 [00:10<00:18, 153.37it/s]Running 5000 simulations.:  44%|████▍     | 2219/5000 [00:10<00:17, 154.53it/s]Running 5000 simulations.:  45%|████▍     | 2235/5000 [00:10<00:17, 154.31it/s]Running 5000 simulations.:  45%|████▌     | 2251/5000 [00:10<00:17, 154.63it/s]Running 5000 simulations.:  45%|████▌     | 2267/5000 [00:10<00:17, 155.58it/s]Running 5000 simulations.:  46%|████▌     | 2283/5000 [00:10<00:17, 155.94it/s]Running 5000 simulations.:  46%|████▌     | 2299/5000 [00:10<00:17, 155.08it/s]Running 5000 simulations.:  46%|████▋     | 2315/5000 [00:10<00:17, 155.35it/s]Running 5000 simulations.:  47%|████▋     | 2331/5000 [00:11<00:17, 155.31it/s]Running 5000 simulations.:  47%|████▋     | 2347/5000 [00:11<00:17, 155.50it/s]Running 5000 simulations.:  47%|████▋     | 2363/5000 [00:11<00:16, 155.42it/s]Running 5000 simulations.:  48%|████▊     | 2379/5000 [00:11<00:16, 155.89it/s]Running 5000 simulations.:  48%|████▊     | 2395/5000 [00:11<00:16, 155.91it/s]Running 5000 simulations.:  48%|████▊     | 2411/5000 [00:11<00:16, 155.84it/s]Running 5000 simulations.:  49%|████▊     | 2427/5000 [00:11<00:16, 155.40it/s]Running 5000 simulations.:  49%|████▉     | 2443/5000 [00:11<00:16, 154.82it/s]Running 5000 simulations.:  49%|████▉     | 2459/5000 [00:11<00:16, 154.81it/s]Running 5000 simulations.:  50%|████▉     | 2475/5000 [00:12<00:16, 154.83it/s]Running 5000 simulations.:  50%|████▉     | 2491/5000 [00:12<00:16, 154.96it/s]Running 5000 simulations.:  50%|█████     | 2507/5000 [00:12<00:16, 153.90it/s]Running 5000 simulations.:  50%|█████     | 2523/5000 [00:12<00:16, 152.54it/s]Running 5000 simulations.:  51%|█████     | 2539/5000 [00:12<00:16, 150.65it/s]Running 5000 simulations.:  51%|█████     | 2555/5000 [00:12<00:16, 149.66it/s]Running 5000 simulations.:  51%|█████▏    | 2570/5000 [00:12<00:16, 148.76it/s]Running 5000 simulations.:  52%|█████▏    | 2585/5000 [00:12<00:16, 148.00it/s]Running 5000 simulations.:  52%|█████▏    | 2600/5000 [00:12<00:16, 148.48it/s]Running 5000 simulations.:  52%|█████▏    | 2615/5000 [00:12<00:16, 148.44it/s]Running 5000 simulations.:  53%|█████▎    | 2630/5000 [00:13<00:15, 148.62it/s]Running 5000 simulations.:  53%|█████▎    | 2645/5000 [00:13<00:16, 146.77it/s]Running 5000 simulations.:  53%|█████▎    | 2660/5000 [00:13<00:16, 146.14it/s]Running 5000 simulations.:  54%|█████▎    | 2675/5000 [00:13<00:15, 146.78it/s]Running 5000 simulations.:  54%|█████▍    | 2691/5000 [00:13<00:15, 147.99it/s]Running 5000 simulations.:  54%|█████▍    | 2706/5000 [00:13<00:15, 147.86it/s]Running 5000 simulations.:  54%|█████▍    | 2721/5000 [00:13<00:15, 147.45it/s]Running 5000 simulations.:  55%|█████▍    | 2736/5000 [00:13<00:15, 147.44it/s]Running 5000 simulations.:  55%|█████▌    | 2751/5000 [00:13<00:15, 146.69it/s]Running 5000 simulations.:  55%|█████▌    | 2767/5000 [00:13<00:15, 147.68it/s]Running 5000 simulations.:  56%|█████▌    | 2782/5000 [00:14<00:15, 146.48it/s]Running 5000 simulations.:  56%|█████▌    | 2797/5000 [00:14<00:15, 145.72it/s]Running 5000 simulations.:  56%|█████▌    | 2812/5000 [00:14<00:15, 145.34it/s]Running 5000 simulations.:  57%|█████▋    | 2827/5000 [00:14<00:15, 144.82it/s]Running 5000 simulations.:  57%|█████▋    | 2842/5000 [00:14<00:14, 144.80it/s]Running 5000 simulations.:  57%|█████▋    | 2857/5000 [00:14<00:14, 145.15it/s]Running 5000 simulations.:  57%|█████▋    | 2872/5000 [00:14<00:14, 142.94it/s]Running 5000 simulations.:  58%|█████▊    | 2887/5000 [00:14<00:14, 140.94it/s]Running 5000 simulations.:  58%|█████▊    | 2902/5000 [00:14<00:15, 139.65it/s]Running 5000 simulations.:  58%|█████▊    | 2917/5000 [00:15<00:14, 141.38it/s]Running 5000 simulations.:  59%|█████▊    | 2932/5000 [00:15<00:14, 142.95it/s]Running 5000 simulations.:  59%|█████▉    | 2947/5000 [00:15<00:14, 143.37it/s]Running 5000 simulations.:  59%|█████▉    | 2962/5000 [00:15<00:14, 143.96it/s]Running 5000 simulations.:  60%|█████▉    | 2977/5000 [00:15<00:14, 144.37it/s]Running 5000 simulations.:  60%|█████▉    | 2992/5000 [00:15<00:13, 144.50it/s]Running 5000 simulations.:  60%|██████    | 3007/5000 [00:15<00:13, 145.71it/s]Running 5000 simulations.:  60%|██████    | 3022/5000 [00:15<00:13, 146.95it/s]Running 5000 simulations.:  61%|██████    | 3037/5000 [00:15<00:13, 146.55it/s]Running 5000 simulations.:  61%|██████    | 3052/5000 [00:15<00:13, 146.53it/s]Running 5000 simulations.:  61%|██████▏   | 3067/5000 [00:16<00:13, 146.37it/s]Running 5000 simulations.:  62%|██████▏   | 3082/5000 [00:16<00:13, 146.25it/s]Running 5000 simulations.:  62%|██████▏   | 3097/5000 [00:16<00:12, 146.59it/s]Running 5000 simulations.:  62%|██████▏   | 3112/5000 [00:16<00:12, 146.35it/s]Running 5000 simulations.:  63%|██████▎   | 3127/5000 [00:16<00:12, 145.59it/s]Running 5000 simulations.:  63%|██████▎   | 3142/5000 [00:16<00:12, 146.56it/s]Running 5000 simulations.:  63%|██████▎   | 3157/5000 [00:16<00:12, 146.18it/s]Running 5000 simulations.:  63%|██████▎   | 3172/5000 [00:16<00:12, 145.51it/s]Running 5000 simulations.:  64%|██████▎   | 3187/5000 [00:16<00:12, 144.50it/s]Running 5000 simulations.:  64%|██████▍   | 3202/5000 [00:16<00:12, 142.97it/s]Running 5000 simulations.:  64%|██████▍   | 3217/5000 [00:17<00:12, 142.22it/s]Running 5000 simulations.:  65%|██████▍   | 3232/5000 [00:17<00:12, 140.28it/s]Running 5000 simulations.:  65%|██████▍   | 3247/5000 [00:17<00:12, 139.34it/s]Running 5000 simulations.:  65%|██████▌   | 3262/5000 [00:17<00:12, 140.24it/s]Running 5000 simulations.:  66%|██████▌   | 3277/5000 [00:17<00:12, 139.48it/s]Running 5000 simulations.:  66%|██████▌   | 3292/5000 [00:17<00:12, 139.89it/s]Running 5000 simulations.:  66%|██████▌   | 3307/5000 [00:17<00:12, 140.09it/s]Running 5000 simulations.:  66%|██████▋   | 3322/5000 [00:17<00:11, 140.29it/s]Running 5000 simulations.:  67%|██████▋   | 3337/5000 [00:17<00:11, 140.20it/s]Running 5000 simulations.:  67%|██████▋   | 3352/5000 [00:18<00:11, 139.89it/s]Running 5000 simulations.:  67%|██████▋   | 3366/5000 [00:18<00:11, 139.67it/s]Running 5000 simulations.:  68%|██████▊   | 3381/5000 [00:18<00:11, 140.19it/s]Running 5000 simulations.:  68%|██████▊   | 3396/5000 [00:18<00:11, 141.06it/s]Running 5000 simulations.:  68%|██████▊   | 3411/5000 [00:18<00:11, 140.64it/s]Running 5000 simulations.:  69%|██████▊   | 3426/5000 [00:18<00:11, 140.28it/s]Running 5000 simulations.:  69%|██████▉   | 3441/5000 [00:18<00:11, 140.14it/s]Running 5000 simulations.:  69%|██████▉   | 3456/5000 [00:18<00:11, 139.75it/s]Running 5000 simulations.:  69%|██████▉   | 3470/5000 [00:18<00:10, 139.49it/s]Running 5000 simulations.:  70%|██████▉   | 3485/5000 [00:19<00:10, 140.71it/s]Running 5000 simulations.:  70%|███████   | 3500/5000 [00:19<00:10, 140.71it/s]Running 5000 simulations.:  70%|███████   | 3515/5000 [00:19<00:10, 140.46it/s]Running 5000 simulations.:  71%|███████   | 3530/5000 [00:19<00:10, 140.44it/s]Running 5000 simulations.:  71%|███████   | 3545/5000 [00:19<00:10, 139.99it/s]Running 5000 simulations.:  71%|███████   | 3560/5000 [00:19<00:10, 140.11it/s]Running 5000 simulations.:  72%|███████▏  | 3575/5000 [00:19<00:10, 140.24it/s]Running 5000 simulations.:  72%|███████▏  | 3590/5000 [00:19<00:10, 140.18it/s]Running 5000 simulations.:  72%|███████▏  | 3605/5000 [00:19<00:10, 138.37it/s]Running 5000 simulations.:  72%|███████▏  | 3620/5000 [00:19<00:09, 139.00it/s]Running 5000 simulations.:  73%|███████▎  | 3634/5000 [00:20<00:09, 139.29it/s]Running 5000 simulations.:  73%|███████▎  | 3648/5000 [00:20<00:09, 139.32it/s]Running 5000 simulations.:  73%|███████▎  | 3663/5000 [00:20<00:09, 139.56it/s]Running 5000 simulations.:  74%|███████▎  | 3678/5000 [00:20<00:09, 139.81it/s]Running 5000 simulations.:  74%|███████▍  | 3693/5000 [00:20<00:09, 140.15it/s]Running 5000 simulations.:  74%|███████▍  | 3708/5000 [00:20<00:09, 139.72it/s]Running 5000 simulations.:  74%|███████▍  | 3723/5000 [00:20<00:09, 140.71it/s]Running 5000 simulations.:  75%|███████▍  | 3738/5000 [00:20<00:08, 140.70it/s]Running 5000 simulations.:  75%|███████▌  | 3753/5000 [00:20<00:08, 140.43it/s]Running 5000 simulations.:  75%|███████▌  | 3768/5000 [00:21<00:08, 139.85it/s]Running 5000 simulations.:  76%|███████▌  | 3783/5000 [00:21<00:08, 140.14it/s]Running 5000 simulations.:  76%|███████▌  | 3798/5000 [00:21<00:08, 140.09it/s]Running 5000 simulations.:  76%|███████▋  | 3813/5000 [00:21<00:08, 140.15it/s]Running 5000 simulations.:  77%|███████▋  | 3828/5000 [00:21<00:08, 139.64it/s]Running 5000 simulations.:  77%|███████▋  | 3842/5000 [00:21<00:08, 139.48it/s]Running 5000 simulations.:  77%|███████▋  | 3856/5000 [00:21<00:08, 139.15it/s]Running 5000 simulations.:  77%|███████▋  | 3870/5000 [00:21<00:08, 139.14it/s]Running 5000 simulations.:  78%|███████▊  | 3884/5000 [00:21<00:08, 138.83it/s]Running 5000 simulations.:  78%|███████▊  | 3898/5000 [00:21<00:07, 138.97it/s]Running 5000 simulations.:  78%|███████▊  | 3912/5000 [00:22<00:07, 139.02it/s]Running 5000 simulations.:  79%|███████▊  | 3926/5000 [00:22<00:07, 139.06it/s]Running 5000 simulations.:  79%|███████▉  | 3940/5000 [00:22<00:07, 138.73it/s]Running 5000 simulations.:  79%|███████▉  | 3955/5000 [00:22<00:07, 140.79it/s]Running 5000 simulations.:  79%|███████▉  | 3970/5000 [00:22<00:07, 141.15it/s]Running 5000 simulations.:  80%|███████▉  | 3985/5000 [00:22<00:07, 140.76it/s]Running 5000 simulations.:  80%|████████  | 4000/5000 [00:22<00:07, 141.04it/s]Running 5000 simulations.:  80%|████████  | 4015/5000 [00:22<00:07, 140.08it/s]Running 5000 simulations.:  81%|████████  | 4030/5000 [00:22<00:06, 141.14it/s]Running 5000 simulations.:  81%|████████  | 4045/5000 [00:23<00:06, 140.76it/s]Running 5000 simulations.:  81%|████████  | 4060/5000 [00:23<00:06, 140.71it/s]Running 5000 simulations.:  82%|████████▏ | 4075/5000 [00:23<00:06, 140.25it/s]Running 5000 simulations.:  82%|████████▏ | 4090/5000 [00:23<00:06, 139.28it/s]Running 5000 simulations.:  82%|████████▏ | 4104/5000 [00:23<00:06, 138.23it/s]Running 5000 simulations.:  82%|████████▏ | 4118/5000 [00:23<00:06, 137.53it/s]Running 5000 simulations.:  83%|████████▎ | 4132/5000 [00:23<00:06, 137.08it/s]Running 5000 simulations.:  83%|████████▎ | 4146/5000 [00:23<00:06, 136.91it/s]Running 5000 simulations.:  83%|████████▎ | 4160/5000 [00:23<00:06, 136.02it/s]Running 5000 simulations.:  83%|████████▎ | 4174/5000 [00:23<00:06, 135.24it/s]Running 5000 simulations.:  84%|████████▍ | 4188/5000 [00:24<00:06, 135.20it/s]Running 5000 simulations.:  84%|████████▍ | 4202/5000 [00:24<00:05, 135.28it/s]Running 5000 simulations.:  84%|████████▍ | 4216/5000 [00:24<00:05, 132.79it/s]Running 5000 simulations.:  85%|████████▍ | 4230/5000 [00:24<00:05, 133.16it/s]Running 5000 simulations.:  85%|████████▍ | 4244/5000 [00:24<00:05, 133.48it/s]Running 5000 simulations.:  85%|████████▌ | 4258/5000 [00:24<00:05, 133.57it/s]Running 5000 simulations.:  85%|████████▌ | 4272/5000 [00:24<00:05, 133.88it/s]Running 5000 simulations.:  86%|████████▌ | 4286/5000 [00:24<00:05, 133.91it/s]Running 5000 simulations.:  86%|████████▌ | 4300/5000 [00:24<00:05, 134.04it/s]Running 5000 simulations.:  86%|████████▋ | 4314/5000 [00:25<00:05, 133.77it/s]Running 5000 simulations.:  87%|████████▋ | 4328/5000 [00:25<00:05, 133.55it/s]Running 5000 simulations.:  87%|████████▋ | 4342/5000 [00:25<00:04, 133.68it/s]Running 5000 simulations.:  87%|████████▋ | 4356/5000 [00:25<00:04, 133.91it/s]Running 5000 simulations.:  87%|████████▋ | 4370/5000 [00:25<00:04, 133.57it/s]Running 5000 simulations.:  88%|████████▊ | 4384/5000 [00:25<00:04, 133.84it/s]Running 5000 simulations.:  88%|████████▊ | 4398/5000 [00:25<00:04, 134.36it/s]Running 5000 simulations.:  88%|████████▊ | 4412/5000 [00:25<00:04, 133.85it/s]Running 5000 simulations.:  89%|████████▊ | 4426/5000 [00:25<00:04, 134.73it/s]Running 5000 simulations.:  89%|████████▉ | 4440/5000 [00:25<00:04, 134.79it/s]Running 5000 simulations.:  89%|████████▉ | 4454/5000 [00:26<00:04, 133.28it/s]Running 5000 simulations.:  89%|████████▉ | 4468/5000 [00:26<00:03, 133.15it/s]Running 5000 simulations.:  90%|████████▉ | 4482/5000 [00:26<00:03, 133.53it/s]Running 5000 simulations.:  90%|████████▉ | 4496/5000 [00:26<00:03, 135.39it/s]Running 5000 simulations.:  90%|█████████ | 4512/5000 [00:26<00:03, 140.96it/s]Running 5000 simulations.:  91%|█████████ | 4528/5000 [00:26<00:03, 144.61it/s]Running 5000 simulations.:  91%|█████████ | 4543/5000 [00:26<00:03, 143.67it/s]Running 5000 simulations.:  91%|█████████ | 4558/5000 [00:26<00:03, 140.27it/s]Running 5000 simulations.:  91%|█████████▏| 4573/5000 [00:26<00:03, 138.10it/s]Running 5000 simulations.:  92%|█████████▏| 4587/5000 [00:27<00:03, 136.50it/s]Running 5000 simulations.:  92%|█████████▏| 4601/5000 [00:27<00:03, 127.92it/s]Running 5000 simulations.:  92%|█████████▏| 4615/5000 [00:27<00:02, 129.18it/s]Running 5000 simulations.:  93%|█████████▎| 4629/5000 [00:27<00:02, 130.40it/s]Running 5000 simulations.:  93%|█████████▎| 4643/5000 [00:27<00:02, 131.07it/s]Running 5000 simulations.:  93%|█████████▎| 4657/5000 [00:27<00:02, 131.71it/s]Running 5000 simulations.:  93%|█████████▎| 4671/5000 [00:27<00:02, 131.77it/s]Running 5000 simulations.:  94%|█████████▎| 4685/5000 [00:27<00:02, 132.33it/s]Running 5000 simulations.:  94%|█████████▍| 4699/5000 [00:27<00:02, 133.29it/s]Running 5000 simulations.:  94%|█████████▍| 4713/5000 [00:27<00:02, 134.73it/s]Running 5000 simulations.:  95%|█████████▍| 4727/5000 [00:28<00:02, 134.38it/s]Running 5000 simulations.:  95%|█████████▍| 4741/5000 [00:28<00:01, 134.48it/s]Running 5000 simulations.:  95%|█████████▌| 4755/5000 [00:28<00:01, 133.77it/s]Running 5000 simulations.:  95%|█████████▌| 4769/5000 [00:28<00:01, 133.72it/s]Running 5000 simulations.:  96%|█████████▌| 4783/5000 [00:28<00:01, 133.34it/s]Running 5000 simulations.:  96%|█████████▌| 4797/5000 [00:28<00:01, 133.61it/s]Running 5000 simulations.:  96%|█████████▌| 4811/5000 [00:28<00:01, 133.89it/s]Running 5000 simulations.:  96%|█████████▋| 4825/5000 [00:28<00:01, 134.12it/s]Running 5000 simulations.:  97%|█████████▋| 4839/5000 [00:28<00:01, 133.99it/s]Running 5000 simulations.:  97%|█████████▋| 4853/5000 [00:29<00:01, 133.74it/s]Running 5000 simulations.:  97%|█████████▋| 4867/5000 [00:29<00:00, 135.27it/s]Running 5000 simulations.:  98%|█████████▊| 4881/5000 [00:29<00:00, 134.64it/s]Running 5000 simulations.:  98%|█████████▊| 4895/5000 [00:29<00:00, 134.21it/s]Running 5000 simulations.:  98%|█████████▊| 4909/5000 [00:29<00:00, 134.17it/s]Running 5000 simulations.:  98%|█████████▊| 4923/5000 [00:29<00:00, 134.05it/s]Running 5000 simulations.:  99%|█████████▊| 4937/5000 [00:29<00:00, 134.87it/s]Running 5000 simulations.:  99%|█████████▉| 4951/5000 [00:29<00:00, 136.02it/s]Running 5000 simulations.:  99%|█████████▉| 4965/5000 [00:29<00:00, 135.28it/s]Running 5000 simulations.: 100%|█████████▉| 4979/5000 [00:29<00:00, 134.98it/s]Running 5000 simulations.: 100%|█████████▉| 4993/5000 [00:30<00:00, 134.48it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:30<00:00, 166.09it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 15/5000 [00:00<00:34, 142.59it/s]Running 5000 simulations.:   1%|          | 30/5000 [00:00<00:34, 142.53it/s]Running 5000 simulations.:   1%|          | 45/5000 [00:00<00:34, 142.97it/s]Running 5000 simulations.:   1%|          | 60/5000 [00:00<00:34, 143.44it/s]Running 5000 simulations.:   2%|▏         | 75/5000 [00:00<00:34, 142.62it/s]Running 5000 simulations.:   2%|▏         | 90/5000 [00:00<00:34, 142.08it/s]Running 5000 simulations.:   2%|▏         | 104/5000 [00:00<00:34, 141.44it/s]Running 5000 simulations.:   2%|▏         | 119/5000 [00:00<00:34, 141.42it/s]Running 5000 simulations.:   3%|▎         | 133/5000 [00:00<00:34, 140.79it/s]Running 5000 simulations.:   3%|▎         | 148/5000 [00:01<00:34, 140.79it/s]Running 5000 simulations.:   3%|▎         | 163/5000 [00:01<00:34, 140.87it/s]Running 5000 simulations.:   4%|▎         | 177/5000 [00:01<00:34, 140.28it/s]Running 5000 simulations.:   4%|▍         | 191/5000 [00:01<00:34, 139.67it/s]Running 5000 simulations.:   4%|▍         | 207/5000 [00:01<00:33, 143.04it/s]Running 5000 simulations.:   4%|▍         | 223/5000 [00:01<00:32, 146.90it/s]Running 5000 simulations.:   5%|▍         | 239/5000 [00:01<00:31, 150.34it/s]Running 5000 simulations.:   5%|▌         | 255/5000 [00:01<00:31, 152.72it/s]Running 5000 simulations.:   5%|▌         | 271/5000 [00:01<00:30, 153.96it/s]Running 5000 simulations.:   6%|▌         | 287/5000 [00:01<00:30, 153.62it/s]Running 5000 simulations.:   6%|▌         | 303/5000 [00:02<00:30, 151.64it/s]Running 5000 simulations.:   6%|▋         | 319/5000 [00:02<00:31, 149.95it/s]Running 5000 simulations.:   7%|▋         | 335/5000 [00:02<00:31, 147.96it/s]Running 5000 simulations.:   7%|▋         | 350/5000 [00:02<00:31, 146.45it/s]Running 5000 simulations.:   7%|▋         | 365/5000 [00:02<00:31, 145.74it/s]Running 5000 simulations.:   8%|▊         | 380/5000 [00:02<00:31, 145.86it/s]Running 5000 simulations.:   8%|▊         | 395/5000 [00:02<00:31, 145.70it/s]Running 5000 simulations.:   8%|▊         | 410/5000 [00:02<00:31, 145.57it/s]Running 5000 simulations.:   8%|▊         | 425/5000 [00:02<00:31, 145.45it/s]Running 5000 simulations.:   9%|▉         | 440/5000 [00:03<00:31, 144.68it/s]Running 5000 simulations.:   9%|▉         | 455/5000 [00:03<00:31, 144.63it/s]Running 5000 simulations.:   9%|▉         | 470/5000 [00:03<00:31, 144.75it/s]Running 5000 simulations.:  10%|▉         | 485/5000 [00:03<00:31, 144.67it/s]Running 5000 simulations.:  10%|█         | 500/5000 [00:03<00:31, 144.83it/s]Running 5000 simulations.:  10%|█         | 515/5000 [00:03<00:30, 145.01it/s]Running 5000 simulations.:  11%|█         | 530/5000 [00:03<00:30, 145.10it/s]Running 5000 simulations.:  11%|█         | 545/5000 [00:03<00:30, 145.27it/s]Running 5000 simulations.:  11%|█         | 560/5000 [00:03<00:30, 146.08it/s]Running 5000 simulations.:  12%|█▏        | 575/5000 [00:03<00:30, 145.88it/s]Running 5000 simulations.:  12%|█▏        | 590/5000 [00:04<00:30, 144.72it/s]Running 5000 simulations.:  12%|█▏        | 605/5000 [00:04<00:30, 144.10it/s]Running 5000 simulations.:  12%|█▏        | 620/5000 [00:04<00:30, 143.67it/s]Running 5000 simulations.:  13%|█▎        | 635/5000 [00:04<00:30, 143.32it/s]Running 5000 simulations.:  13%|█▎        | 650/5000 [00:04<00:30, 143.46it/s]Running 5000 simulations.:  13%|█▎        | 665/5000 [00:04<00:30, 143.78it/s]Running 5000 simulations.:  14%|█▎        | 680/5000 [00:04<00:29, 144.06it/s]Running 5000 simulations.:  14%|█▍        | 695/5000 [00:04<00:29, 144.47it/s]Running 5000 simulations.:  14%|█▍        | 710/5000 [00:04<00:29, 144.42it/s]Running 5000 simulations.:  14%|█▍        | 725/5000 [00:04<00:29, 144.48it/s]Running 5000 simulations.:  15%|█▍        | 740/5000 [00:05<00:29, 143.13it/s]Running 5000 simulations.:  15%|█▌        | 755/5000 [00:05<00:29, 143.25it/s]Running 5000 simulations.:  15%|█▌        | 770/5000 [00:05<00:29, 143.29it/s]Running 5000 simulations.:  16%|█▌        | 785/5000 [00:05<00:29, 143.38it/s]Running 5000 simulations.:  16%|█▌        | 800/5000 [00:05<00:29, 143.87it/s]Running 5000 simulations.:  16%|█▋        | 815/5000 [00:05<00:28, 144.35it/s]Running 5000 simulations.:  17%|█▋        | 830/5000 [00:05<00:29, 143.64it/s]Running 5000 simulations.:  17%|█▋        | 845/5000 [00:05<00:28, 143.50it/s]Running 5000 simulations.:  17%|█▋        | 860/5000 [00:05<00:28, 144.94it/s]Running 5000 simulations.:  18%|█▊        | 875/5000 [00:06<00:28, 143.95it/s]Running 5000 simulations.:  18%|█▊        | 890/5000 [00:06<00:28, 143.79it/s]Running 5000 simulations.:  18%|█▊        | 905/5000 [00:06<00:28, 143.56it/s]Running 5000 simulations.:  18%|█▊        | 920/5000 [00:06<00:29, 139.68it/s]Running 5000 simulations.:  19%|█▊        | 934/5000 [00:06<00:29, 139.76it/s]Running 5000 simulations.:  19%|█▉        | 949/5000 [00:06<00:28, 140.02it/s]Running 5000 simulations.:  19%|█▉        | 964/5000 [00:06<00:28, 139.92it/s]Running 5000 simulations.:  20%|█▉        | 979/5000 [00:06<00:28, 141.58it/s]Running 5000 simulations.:  20%|█▉        | 994/5000 [00:06<00:28, 142.95it/s]Running 5000 simulations.:  20%|██        | 1009/5000 [00:06<00:27, 143.86it/s]Running 5000 simulations.:  20%|██        | 1024/5000 [00:07<00:27, 144.52it/s]Running 5000 simulations.:  21%|██        | 1039/5000 [00:07<00:27, 144.90it/s]Running 5000 simulations.:  21%|██        | 1054/5000 [00:07<00:27, 144.43it/s]Running 5000 simulations.:  21%|██▏       | 1069/5000 [00:07<00:27, 144.56it/s]Running 5000 simulations.:  22%|██▏       | 1084/5000 [00:07<00:27, 144.47it/s]Running 5000 simulations.:  22%|██▏       | 1099/5000 [00:07<00:26, 145.30it/s]Running 5000 simulations.:  22%|██▏       | 1114/5000 [00:07<00:26, 146.63it/s]Running 5000 simulations.:  23%|██▎       | 1129/5000 [00:07<00:26, 144.75it/s]Running 5000 simulations.:  23%|██▎       | 1144/5000 [00:07<00:26, 144.51it/s]Running 5000 simulations.:  23%|██▎       | 1159/5000 [00:08<00:26, 144.71it/s]Running 5000 simulations.:  23%|██▎       | 1174/5000 [00:08<00:26, 144.97it/s]Running 5000 simulations.:  24%|██▍       | 1189/5000 [00:08<00:26, 145.07it/s]Running 5000 simulations.:  24%|██▍       | 1204/5000 [00:08<00:26, 145.48it/s]Running 5000 simulations.:  24%|██▍       | 1219/5000 [00:08<00:25, 145.65it/s]Running 5000 simulations.:  25%|██▍       | 1234/5000 [00:08<00:26, 144.81it/s]Running 5000 simulations.:  25%|██▍       | 1249/5000 [00:08<00:25, 144.90it/s]Running 5000 simulations.:  25%|██▌       | 1264/5000 [00:08<00:25, 145.14it/s]Running 5000 simulations.:  26%|██▌       | 1279/5000 [00:08<00:25, 145.08it/s]Running 5000 simulations.:  26%|██▌       | 1294/5000 [00:08<00:25, 143.44it/s]Running 5000 simulations.:  26%|██▌       | 1309/5000 [00:09<00:25, 142.45it/s]Running 5000 simulations.:  26%|██▋       | 1324/5000 [00:09<00:25, 141.75it/s]Running 5000 simulations.:  27%|██▋       | 1339/5000 [00:09<00:25, 141.15it/s]Running 5000 simulations.:  27%|██▋       | 1354/5000 [00:09<00:25, 142.09it/s]Running 5000 simulations.:  27%|██▋       | 1369/5000 [00:09<00:25, 140.81it/s]Running 5000 simulations.:  28%|██▊       | 1384/5000 [00:09<00:25, 140.24it/s]Running 5000 simulations.:  28%|██▊       | 1399/5000 [00:09<00:25, 139.48it/s]Running 5000 simulations.:  28%|██▊       | 1413/5000 [00:09<00:25, 138.89it/s]Running 5000 simulations.:  29%|██▊       | 1427/5000 [00:09<00:25, 138.77it/s]Running 5000 simulations.:  29%|██▉       | 1441/5000 [00:10<00:25, 138.28it/s]Running 5000 simulations.:  29%|██▉       | 1455/5000 [00:10<00:25, 138.52it/s]Running 5000 simulations.:  29%|██▉       | 1469/5000 [00:10<00:25, 138.25it/s]Running 5000 simulations.:  30%|██▉       | 1483/5000 [00:10<00:25, 138.06it/s]Running 5000 simulations.:  30%|██▉       | 1497/5000 [00:10<00:25, 137.96it/s]Running 5000 simulations.:  30%|███       | 1511/5000 [00:10<00:25, 137.80it/s]Running 5000 simulations.:  30%|███       | 1525/5000 [00:10<00:25, 137.87it/s]Running 5000 simulations.:  31%|███       | 1539/5000 [00:10<00:25, 138.08it/s]Running 5000 simulations.:  31%|███       | 1553/5000 [00:10<00:24, 138.36it/s]Running 5000 simulations.:  31%|███▏      | 1567/5000 [00:10<00:24, 138.45it/s]Running 5000 simulations.:  32%|███▏      | 1582/5000 [00:11<00:24, 139.99it/s]Running 5000 simulations.:  32%|███▏      | 1597/5000 [00:11<00:24, 140.27it/s]Running 5000 simulations.:  32%|███▏      | 1612/5000 [00:11<00:24, 139.20it/s]Running 5000 simulations.:  33%|███▎      | 1626/5000 [00:11<00:24, 139.17it/s]Running 5000 simulations.:  33%|███▎      | 1640/5000 [00:11<00:24, 139.20it/s]Running 5000 simulations.:  33%|███▎      | 1654/5000 [00:11<00:24, 139.02it/s]Running 5000 simulations.:  33%|███▎      | 1668/5000 [00:11<00:23, 139.28it/s]Running 5000 simulations.:  34%|███▎      | 1682/5000 [00:11<00:23, 139.16it/s]Running 5000 simulations.:  34%|███▍      | 1696/5000 [00:11<00:23, 138.99it/s]Running 5000 simulations.:  34%|███▍      | 1710/5000 [00:11<00:23, 138.79it/s]Running 5000 simulations.:  34%|███▍      | 1724/5000 [00:12<00:23, 138.52it/s]Running 5000 simulations.:  35%|███▍      | 1738/5000 [00:12<00:23, 138.43it/s]Running 5000 simulations.:  35%|███▌      | 1752/5000 [00:12<00:23, 138.60it/s]Running 5000 simulations.:  35%|███▌      | 1766/5000 [00:12<00:23, 138.59it/s]Running 5000 simulations.:  36%|███▌      | 1780/5000 [00:12<00:23, 138.58it/s]Running 5000 simulations.:  36%|███▌      | 1794/5000 [00:12<00:23, 138.81it/s]Running 5000 simulations.:  36%|███▌      | 1808/5000 [00:12<00:22, 138.81it/s]Running 5000 simulations.:  36%|███▋      | 1823/5000 [00:12<00:22, 140.94it/s]Running 5000 simulations.:  37%|███▋      | 1838/5000 [00:12<00:22, 142.83it/s]Running 5000 simulations.:  37%|███▋      | 1853/5000 [00:12<00:21, 143.72it/s]Running 5000 simulations.:  37%|███▋      | 1868/5000 [00:13<00:21, 144.22it/s]Running 5000 simulations.:  38%|███▊      | 1883/5000 [00:13<00:21, 145.35it/s]Running 5000 simulations.:  38%|███▊      | 1898/5000 [00:13<00:21, 145.98it/s]Running 5000 simulations.:  38%|███▊      | 1913/5000 [00:13<00:21, 146.52it/s]Running 5000 simulations.:  39%|███▊      | 1928/5000 [00:13<00:21, 146.27it/s]Running 5000 simulations.:  39%|███▉      | 1943/5000 [00:13<00:20, 145.63it/s]Running 5000 simulations.:  39%|███▉      | 1958/5000 [00:13<00:20, 144.97it/s]Running 5000 simulations.:  39%|███▉      | 1973/5000 [00:13<00:20, 145.00it/s]Running 5000 simulations.:  40%|███▉      | 1988/5000 [00:13<00:20, 145.18it/s]Running 5000 simulations.:  40%|████      | 2003/5000 [00:13<00:20, 146.18it/s]Running 5000 simulations.:  40%|████      | 2018/5000 [00:14<00:20, 145.90it/s]Running 5000 simulations.:  41%|████      | 2033/5000 [00:14<00:20, 145.52it/s]Running 5000 simulations.:  41%|████      | 2048/5000 [00:14<00:20, 145.88it/s]Running 5000 simulations.:  41%|████▏     | 2063/5000 [00:14<00:20, 146.26it/s]Running 5000 simulations.:  42%|████▏     | 2078/5000 [00:14<00:20, 144.69it/s]Running 5000 simulations.:  42%|████▏     | 2093/5000 [00:14<00:20, 143.77it/s]Running 5000 simulations.:  42%|████▏     | 2108/5000 [00:14<00:20, 142.69it/s]Running 5000 simulations.:  42%|████▏     | 2123/5000 [00:14<00:20, 140.96it/s]Running 5000 simulations.:  43%|████▎     | 2138/5000 [00:14<00:20, 140.35it/s]Running 5000 simulations.:  43%|████▎     | 2153/5000 [00:15<00:20, 140.20it/s]Running 5000 simulations.:  43%|████▎     | 2168/5000 [00:15<00:20, 140.03it/s]Running 5000 simulations.:  44%|████▎     | 2183/5000 [00:15<00:20, 139.91it/s]Running 5000 simulations.:  44%|████▍     | 2197/5000 [00:15<00:20, 139.75it/s]Running 5000 simulations.:  44%|████▍     | 2212/5000 [00:15<00:19, 140.44it/s]Running 5000 simulations.:  45%|████▍     | 2227/5000 [00:15<00:19, 139.79it/s]Running 5000 simulations.:  45%|████▍     | 2241/5000 [00:15<00:19, 138.68it/s]Running 5000 simulations.:  45%|████▌     | 2255/5000 [00:15<00:19, 139.07it/s]Running 5000 simulations.:  45%|████▌     | 2269/5000 [00:15<00:19, 138.60it/s]Running 5000 simulations.:  46%|████▌     | 2284/5000 [00:15<00:19, 139.49it/s]Running 5000 simulations.:  46%|████▌     | 2299/5000 [00:16<00:19, 139.98it/s]Running 5000 simulations.:  46%|████▋     | 2314/5000 [00:16<00:19, 140.74it/s]Running 5000 simulations.:  47%|████▋     | 2329/5000 [00:16<00:19, 140.10it/s]Running 5000 simulations.:  47%|████▋     | 2344/5000 [00:16<00:19, 139.63it/s]Running 5000 simulations.:  47%|████▋     | 2358/5000 [00:16<00:19, 139.05it/s]Running 5000 simulations.:  47%|████▋     | 2372/5000 [00:16<00:18, 138.65it/s]Running 5000 simulations.:  48%|████▊     | 2386/5000 [00:16<00:18, 138.64it/s]Running 5000 simulations.:  48%|████▊     | 2400/5000 [00:16<00:18, 138.74it/s]Running 5000 simulations.:  48%|████▊     | 2414/5000 [00:16<00:18, 138.37it/s]Running 5000 simulations.:  49%|████▊     | 2428/5000 [00:17<00:18, 138.21it/s]Running 5000 simulations.:  49%|████▉     | 2442/5000 [00:17<00:18, 137.85it/s]Running 5000 simulations.:  49%|████▉     | 2456/5000 [00:17<00:18, 137.79it/s]Running 5000 simulations.:  49%|████▉     | 2470/5000 [00:17<00:18, 138.31it/s]Running 5000 simulations.:  50%|████▉     | 2484/5000 [00:17<00:18, 138.43it/s]Running 5000 simulations.:  50%|████▉     | 2499/5000 [00:17<00:17, 139.23it/s]Running 5000 simulations.:  50%|█████     | 2514/5000 [00:17<00:17, 139.92it/s]Running 5000 simulations.:  51%|█████     | 2528/5000 [00:17<00:17, 139.89it/s]Running 5000 simulations.:  51%|█████     | 2543/5000 [00:17<00:17, 141.34it/s]Running 5000 simulations.:  51%|█████     | 2558/5000 [00:17<00:17, 141.50it/s]Running 5000 simulations.:  51%|█████▏    | 2573/5000 [00:18<00:17, 141.02it/s]Running 5000 simulations.:  52%|█████▏    | 2588/5000 [00:18<00:17, 140.93it/s]Running 5000 simulations.:  52%|█████▏    | 2603/5000 [00:18<00:17, 140.79it/s]Running 5000 simulations.:  52%|█████▏    | 2618/5000 [00:18<00:16, 140.39it/s]Running 5000 simulations.:  53%|█████▎    | 2633/5000 [00:18<00:16, 140.88it/s]Running 5000 simulations.:  53%|█████▎    | 2648/5000 [00:18<00:16, 141.24it/s]Running 5000 simulations.:  53%|█████▎    | 2663/5000 [00:18<00:16, 140.82it/s]Running 5000 simulations.:  54%|█████▎    | 2678/5000 [00:18<00:16, 141.39it/s]Running 5000 simulations.:  54%|█████▍    | 2693/5000 [00:18<00:16, 141.50it/s]Running 5000 simulations.:  54%|█████▍    | 2708/5000 [00:19<00:16, 140.91it/s]Running 5000 simulations.:  54%|█████▍    | 2723/5000 [00:19<00:16, 140.03it/s]Running 5000 simulations.:  55%|█████▍    | 2738/5000 [00:19<00:16, 139.80it/s]Running 5000 simulations.:  55%|█████▌    | 2753/5000 [00:19<00:16, 140.24it/s]Running 5000 simulations.:  55%|█████▌    | 2768/5000 [00:19<00:15, 139.93it/s]Running 5000 simulations.:  56%|█████▌    | 2783/5000 [00:19<00:15, 141.26it/s]Running 5000 simulations.:  56%|█████▌    | 2798/5000 [00:19<00:15, 140.27it/s]Running 5000 simulations.:  56%|█████▋    | 2813/5000 [00:19<00:15, 139.69it/s]Running 5000 simulations.:  57%|█████▋    | 2827/5000 [00:19<00:15, 139.51it/s]Running 5000 simulations.:  57%|█████▋    | 2841/5000 [00:19<00:15, 139.33it/s]Running 5000 simulations.:  57%|█████▋    | 2855/5000 [00:20<00:15, 139.31it/s]Running 5000 simulations.:  57%|█████▋    | 2869/5000 [00:20<00:15, 139.34it/s]Running 5000 simulations.:  58%|█████▊    | 2883/5000 [00:20<00:15, 138.77it/s]Running 5000 simulations.:  58%|█████▊    | 2897/5000 [00:20<00:15, 138.96it/s]Running 5000 simulations.:  58%|█████▊    | 2912/5000 [00:20<00:14, 139.39it/s]Running 5000 simulations.:  59%|█████▊    | 2926/5000 [00:20<00:14, 139.16it/s]Running 5000 simulations.:  59%|█████▉    | 2940/5000 [00:20<00:14, 139.40it/s]Running 5000 simulations.:  59%|█████▉    | 2955/5000 [00:20<00:14, 140.79it/s]Running 5000 simulations.:  59%|█████▉    | 2970/5000 [00:20<00:14, 140.84it/s]Running 5000 simulations.:  60%|█████▉    | 2985/5000 [00:20<00:14, 140.44it/s]Running 5000 simulations.:  60%|██████    | 3000/5000 [00:21<00:14, 139.67it/s]Running 5000 simulations.:  60%|██████    | 3015/5000 [00:21<00:14, 141.00it/s]Running 5000 simulations.:  61%|██████    | 3030/5000 [00:21<00:14, 139.96it/s]Running 5000 simulations.:  61%|██████    | 3045/5000 [00:21<00:14, 139.49it/s]Running 5000 simulations.:  61%|██████    | 3059/5000 [00:21<00:13, 139.56it/s]Running 5000 simulations.:  61%|██████▏   | 3073/5000 [00:21<00:13, 138.69it/s]Running 5000 simulations.:  62%|██████▏   | 3087/5000 [00:21<00:13, 138.46it/s]Running 5000 simulations.:  62%|██████▏   | 3101/5000 [00:21<00:13, 138.74it/s]Running 5000 simulations.:  62%|██████▏   | 3115/5000 [00:21<00:13, 138.99it/s]Running 5000 simulations.:  63%|██████▎   | 3129/5000 [00:22<00:13, 139.10it/s]Running 5000 simulations.:  63%|██████▎   | 3143/5000 [00:22<00:13, 139.31it/s]Running 5000 simulations.:  63%|██████▎   | 3157/5000 [00:22<00:13, 139.34it/s]Running 5000 simulations.:  63%|██████▎   | 3171/5000 [00:22<00:13, 138.88it/s]Running 5000 simulations.:  64%|██████▎   | 3185/5000 [00:22<00:13, 138.68it/s]Running 5000 simulations.:  64%|██████▍   | 3199/5000 [00:22<00:13, 138.37it/s]Running 5000 simulations.:  64%|██████▍   | 3213/5000 [00:22<00:12, 138.63it/s]Running 5000 simulations.:  65%|██████▍   | 3227/5000 [00:22<00:12, 138.47it/s]Running 5000 simulations.:  65%|██████▍   | 3242/5000 [00:22<00:12, 139.71it/s]Running 5000 simulations.:  65%|██████▌   | 3256/5000 [00:22<00:12, 138.40it/s]Running 5000 simulations.:  65%|██████▌   | 3270/5000 [00:23<00:12, 138.07it/s]Running 5000 simulations.:  66%|██████▌   | 3284/5000 [00:23<00:12, 138.05it/s]Running 5000 simulations.:  66%|██████▌   | 3298/5000 [00:23<00:12, 138.12it/s]Running 5000 simulations.:  66%|██████▌   | 3312/5000 [00:23<00:12, 138.16it/s]Running 5000 simulations.:  67%|██████▋   | 3326/5000 [00:23<00:12, 136.97it/s]Running 5000 simulations.:  67%|██████▋   | 3340/5000 [00:23<00:12, 129.06it/s]Running 5000 simulations.:  67%|██████▋   | 3354/5000 [00:23<00:12, 131.75it/s]Running 5000 simulations.:  67%|██████▋   | 3368/5000 [00:23<00:12, 133.64it/s]Running 5000 simulations.:  68%|██████▊   | 3382/5000 [00:23<00:12, 134.76it/s]Running 5000 simulations.:  68%|██████▊   | 3396/5000 [00:23<00:11, 135.17it/s]Running 5000 simulations.:  68%|██████▊   | 3410/5000 [00:24<00:11, 135.89it/s]Running 5000 simulations.:  68%|██████▊   | 3424/5000 [00:24<00:11, 136.25it/s]Running 5000 simulations.:  69%|██████▉   | 3438/5000 [00:24<00:11, 136.74it/s]Running 5000 simulations.:  69%|██████▉   | 3452/5000 [00:24<00:11, 137.25it/s]Running 5000 simulations.:  69%|██████▉   | 3466/5000 [00:24<00:11, 137.11it/s]Running 5000 simulations.:  70%|██████▉   | 3480/5000 [00:24<00:11, 136.81it/s]Running 5000 simulations.:  70%|██████▉   | 3494/5000 [00:24<00:11, 136.54it/s]Running 5000 simulations.:  70%|███████   | 3509/5000 [00:24<00:10, 138.22it/s]Running 5000 simulations.:  70%|███████   | 3523/5000 [00:24<00:10, 138.20it/s]Running 5000 simulations.:  71%|███████   | 3537/5000 [00:25<00:10, 137.80it/s]Running 5000 simulations.:  71%|███████   | 3551/5000 [00:25<00:10, 137.34it/s]Running 5000 simulations.:  71%|███████▏  | 3565/5000 [00:25<00:10, 137.05it/s]Running 5000 simulations.:  72%|███████▏  | 3579/5000 [00:25<00:10, 136.54it/s]Running 5000 simulations.:  72%|███████▏  | 3593/5000 [00:25<00:10, 136.34it/s]Running 5000 simulations.:  72%|███████▏  | 3607/5000 [00:25<00:10, 136.91it/s]Running 5000 simulations.:  72%|███████▏  | 3621/5000 [00:25<00:10, 136.73it/s]Running 5000 simulations.:  73%|███████▎  | 3635/5000 [00:25<00:09, 136.66it/s]Running 5000 simulations.:  73%|███████▎  | 3649/5000 [00:25<00:09, 136.81it/s]Running 5000 simulations.:  73%|███████▎  | 3664/5000 [00:25<00:09, 137.81it/s]Running 5000 simulations.:  74%|███████▎  | 3678/5000 [00:26<00:09, 137.71it/s]Running 5000 simulations.:  74%|███████▍  | 3692/5000 [00:26<00:09, 137.86it/s]Running 5000 simulations.:  74%|███████▍  | 3706/5000 [00:26<00:09, 137.70it/s]Running 5000 simulations.:  74%|███████▍  | 3720/5000 [00:26<00:09, 137.30it/s]Running 5000 simulations.:  75%|███████▍  | 3734/5000 [00:26<00:09, 136.81it/s]Running 5000 simulations.:  75%|███████▍  | 3749/5000 [00:26<00:09, 138.29it/s]Running 5000 simulations.:  75%|███████▌  | 3763/5000 [00:26<00:08, 138.41it/s]Running 5000 simulations.:  76%|███████▌  | 3777/5000 [00:26<00:08, 138.86it/s]Running 5000 simulations.:  76%|███████▌  | 3791/5000 [00:26<00:08, 137.96it/s]Running 5000 simulations.:  76%|███████▌  | 3805/5000 [00:26<00:08, 137.51it/s]Running 5000 simulations.:  76%|███████▋  | 3819/5000 [00:27<00:08, 137.60it/s]Running 5000 simulations.:  77%|███████▋  | 3833/5000 [00:27<00:08, 137.53it/s]Running 5000 simulations.:  77%|███████▋  | 3847/5000 [00:27<00:08, 137.57it/s]Running 5000 simulations.:  77%|███████▋  | 3861/5000 [00:27<00:08, 137.59it/s]Running 5000 simulations.:  78%|███████▊  | 3875/5000 [00:27<00:08, 137.12it/s]Running 5000 simulations.:  78%|███████▊  | 3889/5000 [00:27<00:08, 137.17it/s]Running 5000 simulations.:  78%|███████▊  | 3903/5000 [00:27<00:07, 137.20it/s]Running 5000 simulations.:  78%|███████▊  | 3918/5000 [00:27<00:07, 138.27it/s]Running 5000 simulations.:  79%|███████▊  | 3933/5000 [00:27<00:07, 139.26it/s]Running 5000 simulations.:  79%|███████▉  | 3948/5000 [00:27<00:07, 140.19it/s]Running 5000 simulations.:  79%|███████▉  | 3963/5000 [00:28<00:07, 141.15it/s]Running 5000 simulations.:  80%|███████▉  | 3978/5000 [00:28<00:07, 141.69it/s]Running 5000 simulations.:  80%|███████▉  | 3993/5000 [00:28<00:07, 142.25it/s]Running 5000 simulations.:  80%|████████  | 4008/5000 [00:28<00:06, 142.85it/s]Running 5000 simulations.:  80%|████████  | 4023/5000 [00:28<00:06, 142.30it/s]Running 5000 simulations.:  81%|████████  | 4038/5000 [00:28<00:06, 141.73it/s]Running 5000 simulations.:  81%|████████  | 4053/5000 [00:28<00:06, 141.46it/s]Running 5000 simulations.:  81%|████████▏ | 4068/5000 [00:28<00:06, 141.59it/s]Running 5000 simulations.:  82%|████████▏ | 4083/5000 [00:28<00:06, 142.18it/s]Running 5000 simulations.:  82%|████████▏ | 4098/5000 [00:29<00:06, 142.30it/s]Running 5000 simulations.:  82%|████████▏ | 4113/5000 [00:29<00:06, 142.08it/s]Running 5000 simulations.:  83%|████████▎ | 4128/5000 [00:29<00:06, 141.88it/s]Running 5000 simulations.:  83%|████████▎ | 4143/5000 [00:29<00:06, 141.54it/s]Running 5000 simulations.:  83%|████████▎ | 4158/5000 [00:29<00:05, 140.96it/s]Running 5000 simulations.:  83%|████████▎ | 4173/5000 [00:29<00:05, 140.82it/s]Running 5000 simulations.:  84%|████████▍ | 4188/5000 [00:29<00:05, 140.60it/s]Running 5000 simulations.:  84%|████████▍ | 4203/5000 [00:29<00:05, 140.39it/s]Running 5000 simulations.:  84%|████████▍ | 4218/5000 [00:29<00:05, 140.88it/s]Running 5000 simulations.:  85%|████████▍ | 4233/5000 [00:30<00:05, 140.64it/s]Running 5000 simulations.:  85%|████████▍ | 4248/5000 [00:30<00:05, 139.38it/s]Running 5000 simulations.:  85%|████████▌ | 4262/5000 [00:30<00:05, 138.64it/s]Running 5000 simulations.:  86%|████████▌ | 4276/5000 [00:30<00:05, 138.10it/s]Running 5000 simulations.:  86%|████████▌ | 4290/5000 [00:30<00:05, 137.50it/s]Running 5000 simulations.:  86%|████████▌ | 4304/5000 [00:30<00:05, 137.45it/s]Running 5000 simulations.:  86%|████████▋ | 4318/5000 [00:30<00:04, 137.01it/s]Running 5000 simulations.:  87%|████████▋ | 4332/5000 [00:30<00:04, 136.90it/s]Running 5000 simulations.:  87%|████████▋ | 4346/5000 [00:30<00:04, 136.74it/s]Running 5000 simulations.:  87%|████████▋ | 4360/5000 [00:30<00:04, 136.68it/s]Running 5000 simulations.:  87%|████████▋ | 4374/5000 [00:31<00:04, 136.90it/s]Running 5000 simulations.:  88%|████████▊ | 4388/5000 [00:31<00:04, 137.25it/s]Running 5000 simulations.:  88%|████████▊ | 4402/5000 [00:31<00:04, 137.04it/s]Running 5000 simulations.:  88%|████████▊ | 4416/5000 [00:31<00:04, 136.96it/s]Running 5000 simulations.:  89%|████████▊ | 4430/5000 [00:31<00:04, 136.83it/s]Running 5000 simulations.:  89%|████████▉ | 4444/5000 [00:31<00:04, 136.67it/s]Running 5000 simulations.:  89%|████████▉ | 4459/5000 [00:31<00:03, 138.76it/s]Running 5000 simulations.:  89%|████████▉ | 4473/5000 [00:31<00:03, 138.11it/s]Running 5000 simulations.:  90%|████████▉ | 4487/5000 [00:31<00:03, 137.54it/s]Running 5000 simulations.:  90%|█████████ | 4501/5000 [00:31<00:03, 137.21it/s]Running 5000 simulations.:  90%|█████████ | 4515/5000 [00:32<00:03, 137.54it/s]Running 5000 simulations.:  91%|█████████ | 4529/5000 [00:32<00:03, 137.50it/s]Running 5000 simulations.:  91%|█████████ | 4543/5000 [00:32<00:03, 137.42it/s]Running 5000 simulations.:  91%|█████████ | 4557/5000 [00:32<00:03, 137.26it/s]Running 5000 simulations.:  91%|█████████▏| 4571/5000 [00:32<00:03, 137.18it/s]Running 5000 simulations.:  92%|█████████▏| 4586/5000 [00:32<00:02, 138.67it/s]Running 5000 simulations.:  92%|█████████▏| 4600/5000 [00:32<00:02, 138.04it/s]Running 5000 simulations.:  92%|█████████▏| 4614/5000 [00:32<00:02, 138.41it/s]Running 5000 simulations.:  93%|█████████▎| 4628/5000 [00:32<00:02, 138.54it/s]Running 5000 simulations.:  93%|█████████▎| 4642/5000 [00:32<00:02, 137.66it/s]Running 5000 simulations.:  93%|█████████▎| 4656/5000 [00:33<00:02, 137.46it/s]Running 5000 simulations.:  93%|█████████▎| 4670/5000 [00:33<00:02, 137.69it/s]Running 5000 simulations.:  94%|█████████▎| 4684/5000 [00:33<00:02, 138.18it/s]Running 5000 simulations.:  94%|█████████▍| 4698/5000 [00:33<00:02, 137.51it/s]Running 5000 simulations.:  94%|█████████▍| 4712/5000 [00:33<00:02, 136.66it/s]Running 5000 simulations.:  95%|█████████▍| 4726/5000 [00:33<00:02, 136.08it/s]Running 5000 simulations.:  95%|█████████▍| 4740/5000 [00:33<00:01, 135.60it/s]Running 5000 simulations.:  95%|█████████▌| 4754/5000 [00:33<00:01, 135.34it/s]Running 5000 simulations.:  95%|█████████▌| 4768/5000 [00:33<00:01, 135.02it/s]Running 5000 simulations.:  96%|█████████▌| 4782/5000 [00:34<00:01, 135.05it/s]Running 5000 simulations.:  96%|█████████▌| 4796/5000 [00:34<00:01, 135.07it/s]Running 5000 simulations.:  96%|█████████▌| 4810/5000 [00:34<00:01, 135.50it/s]Running 5000 simulations.:  96%|█████████▋| 4824/5000 [00:34<00:01, 135.17it/s]Running 5000 simulations.:  97%|█████████▋| 4838/5000 [00:34<00:01, 134.93it/s]Running 5000 simulations.:  97%|█████████▋| 4852/5000 [00:34<00:01, 135.04it/s]Running 5000 simulations.:  97%|█████████▋| 4866/5000 [00:34<00:00, 135.44it/s]Running 5000 simulations.:  98%|█████████▊| 4880/5000 [00:34<00:00, 135.06it/s]Running 5000 simulations.:  98%|█████████▊| 4894/5000 [00:34<00:00, 135.23it/s]Running 5000 simulations.:  98%|█████████▊| 4908/5000 [00:34<00:00, 136.08it/s]Running 5000 simulations.:  98%|█████████▊| 4922/5000 [00:35<00:00, 135.94it/s]Running 5000 simulations.:  99%|█████████▊| 4936/5000 [00:35<00:00, 136.06it/s]Running 5000 simulations.:  99%|█████████▉| 4950/5000 [00:35<00:00, 135.90it/s]Running 5000 simulations.:  99%|█████████▉| 4964/5000 [00:35<00:00, 135.44it/s]Running 5000 simulations.: 100%|█████████▉| 4978/5000 [00:35<00:00, 135.28it/s]Running 5000 simulations.: 100%|█████████▉| 4992/5000 [00:35<00:00, 135.01it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:35<00:00, 140.36it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:32, 151.08it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:32, 151.75it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:32, 151.71it/s]Running 5000 simulations.:   1%|▏         | 63/5000 [00:00<00:32, 150.82it/s]Running 5000 simulations.:   2%|▏         | 78/5000 [00:00<00:32, 149.74it/s]Running 5000 simulations.:   2%|▏         | 93/5000 [00:00<00:32, 149.80it/s]Running 5000 simulations.:   2%|▏         | 108/5000 [00:00<00:32, 149.24it/s]Running 5000 simulations.:   2%|▏         | 123/5000 [00:00<00:32, 148.79it/s]Running 5000 simulations.:   3%|▎         | 138/5000 [00:00<00:32, 148.74it/s]Running 5000 simulations.:   3%|▎         | 153/5000 [00:01<00:32, 148.42it/s]Running 5000 simulations.:   3%|▎         | 168/5000 [00:01<00:32, 148.52it/s]Running 5000 simulations.:   4%|▎         | 183/5000 [00:01<00:32, 148.76it/s]Running 5000 simulations.:   4%|▍         | 198/5000 [00:01<00:32, 147.89it/s]Running 5000 simulations.:   4%|▍         | 213/5000 [00:01<00:32, 148.35it/s]Running 5000 simulations.:   5%|▍         | 228/5000 [00:01<00:32, 148.42it/s]Running 5000 simulations.:   5%|▍         | 243/5000 [00:01<00:31, 148.81it/s]Running 5000 simulations.:   5%|▌         | 258/5000 [00:01<00:31, 148.31it/s]Running 5000 simulations.:   5%|▌         | 273/5000 [00:01<00:31, 148.37it/s]Running 5000 simulations.:   6%|▌         | 288/5000 [00:01<00:31, 148.41it/s]Running 5000 simulations.:   6%|▌         | 303/5000 [00:02<00:31, 147.64it/s]Running 5000 simulations.:   6%|▋         | 318/5000 [00:02<00:32, 144.75it/s]Running 5000 simulations.:   7%|▋         | 333/5000 [00:02<00:32, 142.70it/s]Running 5000 simulations.:   7%|▋         | 348/5000 [00:02<00:32, 143.58it/s]Running 5000 simulations.:   7%|▋         | 363/5000 [00:02<00:32, 143.03it/s]Running 5000 simulations.:   8%|▊         | 378/5000 [00:02<00:32, 141.70it/s]Running 5000 simulations.:   8%|▊         | 393/5000 [00:02<00:32, 143.71it/s]Running 5000 simulations.:   8%|▊         | 408/5000 [00:02<00:31, 145.52it/s]Running 5000 simulations.:   8%|▊         | 424/5000 [00:02<00:31, 147.30it/s]Running 5000 simulations.:   9%|▉         | 440/5000 [00:02<00:30, 148.36it/s]Running 5000 simulations.:   9%|▉         | 455/5000 [00:03<00:30, 148.78it/s]Running 5000 simulations.:   9%|▉         | 470/5000 [00:03<00:30, 148.89it/s]Running 5000 simulations.:  10%|▉         | 485/5000 [00:03<00:30, 148.35it/s]Running 5000 simulations.:  10%|█         | 500/5000 [00:03<00:30, 148.25it/s]Running 5000 simulations.:  10%|█         | 515/5000 [00:03<00:30, 148.15it/s]Running 5000 simulations.:  11%|█         | 530/5000 [00:03<00:30, 147.96it/s]Running 5000 simulations.:  11%|█         | 545/5000 [00:03<00:30, 147.80it/s]Running 5000 simulations.:  11%|█         | 560/5000 [00:03<00:29, 148.16it/s]Running 5000 simulations.:  12%|█▏        | 575/5000 [00:03<00:29, 147.91it/s]Running 5000 simulations.:  12%|█▏        | 590/5000 [00:03<00:30, 146.21it/s]Running 5000 simulations.:  12%|█▏        | 605/5000 [00:04<00:30, 144.03it/s]Running 5000 simulations.:  12%|█▏        | 620/5000 [00:04<00:30, 142.86it/s]Running 5000 simulations.:  13%|█▎        | 635/5000 [00:04<00:30, 142.81it/s]Running 5000 simulations.:  13%|█▎        | 650/5000 [00:04<00:30, 142.70it/s]Running 5000 simulations.:  13%|█▎        | 665/5000 [00:04<00:30, 143.46it/s]Running 5000 simulations.:  14%|█▎        | 680/5000 [00:04<00:29, 144.48it/s]Running 5000 simulations.:  14%|█▍        | 695/5000 [00:04<00:29, 145.13it/s]Running 5000 simulations.:  14%|█▍        | 710/5000 [00:04<00:29, 144.53it/s]Running 5000 simulations.:  14%|█▍        | 725/5000 [00:04<00:29, 142.70it/s]Running 5000 simulations.:  15%|█▍        | 740/5000 [00:05<00:30, 141.29it/s]Running 5000 simulations.:  15%|█▌        | 755/5000 [00:05<00:30, 141.38it/s]Running 5000 simulations.:  15%|█▌        | 771/5000 [00:05<00:28, 146.31it/s]Running 5000 simulations.:  16%|█▌        | 788/5000 [00:05<00:27, 150.96it/s]Running 5000 simulations.:  16%|█▌        | 804/5000 [00:05<00:27, 152.26it/s]Running 5000 simulations.:  16%|█▋        | 820/5000 [00:05<00:27, 149.90it/s]Running 5000 simulations.:  17%|█▋        | 836/5000 [00:05<00:28, 148.38it/s]Running 5000 simulations.:  17%|█▋        | 851/5000 [00:05<00:28, 146.03it/s]Running 5000 simulations.:  17%|█▋        | 866/5000 [00:05<00:28, 143.92it/s]Running 5000 simulations.:  18%|█▊        | 881/5000 [00:06<00:29, 141.97it/s]Running 5000 simulations.:  18%|█▊        | 896/5000 [00:06<00:29, 141.04it/s]Running 5000 simulations.:  18%|█▊        | 911/5000 [00:06<00:28, 141.29it/s]Running 5000 simulations.:  19%|█▊        | 926/5000 [00:06<00:28, 142.16it/s]Running 5000 simulations.:  19%|█▉        | 941/5000 [00:06<00:28, 143.62it/s]Running 5000 simulations.:  19%|█▉        | 956/5000 [00:06<00:27, 144.90it/s]Running 5000 simulations.:  19%|█▉        | 971/5000 [00:06<00:27, 145.50it/s]Running 5000 simulations.:  20%|█▉        | 986/5000 [00:06<00:27, 144.27it/s]Running 5000 simulations.:  20%|██        | 1001/5000 [00:06<00:27, 143.76it/s]Running 5000 simulations.:  20%|██        | 1016/5000 [00:06<00:27, 142.97it/s]Running 5000 simulations.:  21%|██        | 1031/5000 [00:07<00:27, 142.36it/s]Running 5000 simulations.:  21%|██        | 1046/5000 [00:07<00:27, 143.97it/s]Running 5000 simulations.:  21%|██        | 1061/5000 [00:07<00:27, 144.82it/s]Running 5000 simulations.:  22%|██▏       | 1076/5000 [00:07<00:26, 146.06it/s]Running 5000 simulations.:  22%|██▏       | 1091/5000 [00:07<00:26, 145.62it/s]Running 5000 simulations.:  22%|██▏       | 1106/5000 [00:07<00:26, 144.46it/s]Running 5000 simulations.:  22%|██▏       | 1121/5000 [00:07<00:27, 142.98it/s]Running 5000 simulations.:  23%|██▎       | 1136/5000 [00:07<00:27, 142.94it/s]Running 5000 simulations.:  23%|██▎       | 1151/5000 [00:07<00:26, 143.18it/s]Running 5000 simulations.:  23%|██▎       | 1166/5000 [00:07<00:26, 142.75it/s]Running 5000 simulations.:  24%|██▎       | 1181/5000 [00:08<00:26, 144.74it/s]Running 5000 simulations.:  24%|██▍       | 1196/5000 [00:08<00:26, 144.73it/s]Running 5000 simulations.:  24%|██▍       | 1211/5000 [00:08<00:26, 143.95it/s]Running 5000 simulations.:  25%|██▍       | 1226/5000 [00:08<00:26, 142.89it/s]Running 5000 simulations.:  25%|██▍       | 1241/5000 [00:08<00:26, 142.97it/s]Running 5000 simulations.:  25%|██▌       | 1256/5000 [00:08<00:26, 142.82it/s]Running 5000 simulations.:  25%|██▌       | 1271/5000 [00:08<00:26, 142.79it/s]Running 5000 simulations.:  26%|██▌       | 1286/5000 [00:08<00:25, 143.17it/s]Running 5000 simulations.:  26%|██▌       | 1301/5000 [00:08<00:25, 142.70it/s]Running 5000 simulations.:  26%|██▋       | 1316/5000 [00:09<00:25, 142.61it/s]Running 5000 simulations.:  27%|██▋       | 1331/5000 [00:09<00:25, 142.48it/s]Running 5000 simulations.:  27%|██▋       | 1346/5000 [00:09<00:25, 142.07it/s]Running 5000 simulations.:  27%|██▋       | 1361/5000 [00:09<00:25, 142.40it/s]Running 5000 simulations.:  28%|██▊       | 1376/5000 [00:09<00:25, 143.66it/s]Running 5000 simulations.:  28%|██▊       | 1391/5000 [00:09<00:25, 143.82it/s]Running 5000 simulations.:  28%|██▊       | 1406/5000 [00:09<00:25, 143.35it/s]Running 5000 simulations.:  28%|██▊       | 1421/5000 [00:09<00:25, 142.65it/s]Running 5000 simulations.:  29%|██▊       | 1436/5000 [00:09<00:25, 141.36it/s]Running 5000 simulations.:  29%|██▉       | 1451/5000 [00:09<00:25, 141.41it/s]Running 5000 simulations.:  29%|██▉       | 1466/5000 [00:10<00:25, 141.07it/s]Running 5000 simulations.:  30%|██▉       | 1481/5000 [00:10<00:24, 141.32it/s]Running 5000 simulations.:  30%|██▉       | 1496/5000 [00:10<00:24, 142.01it/s]Running 5000 simulations.:  30%|███       | 1511/5000 [00:10<00:24, 143.01it/s]Running 5000 simulations.:  31%|███       | 1526/5000 [00:10<00:24, 143.46it/s]Running 5000 simulations.:  31%|███       | 1541/5000 [00:10<00:23, 144.43it/s]Running 5000 simulations.:  31%|███       | 1556/5000 [00:10<00:23, 143.81it/s]Running 5000 simulations.:  31%|███▏      | 1571/5000 [00:10<00:23, 144.62it/s]Running 5000 simulations.:  32%|███▏      | 1586/5000 [00:10<00:23, 144.99it/s]Running 5000 simulations.:  32%|███▏      | 1601/5000 [00:11<00:23, 144.24it/s]Running 5000 simulations.:  32%|███▏      | 1616/5000 [00:11<00:23, 143.63it/s]Running 5000 simulations.:  33%|███▎      | 1631/5000 [00:11<00:23, 145.17it/s]Running 5000 simulations.:  33%|███▎      | 1646/5000 [00:11<00:23, 145.57it/s]Running 5000 simulations.:  33%|███▎      | 1661/5000 [00:11<00:22, 146.17it/s]Running 5000 simulations.:  34%|███▎      | 1676/5000 [00:11<00:22, 146.53it/s]Running 5000 simulations.:  34%|███▍      | 1691/5000 [00:11<00:22, 146.03it/s]Running 5000 simulations.:  34%|███▍      | 1706/5000 [00:11<00:22, 145.27it/s]Running 5000 simulations.:  34%|███▍      | 1721/5000 [00:11<00:22, 145.42it/s]Running 5000 simulations.:  35%|███▍      | 1736/5000 [00:11<00:22, 144.01it/s]Running 5000 simulations.:  35%|███▌      | 1751/5000 [00:12<00:22, 143.53it/s]Running 5000 simulations.:  35%|███▌      | 1766/5000 [00:12<00:22, 145.04it/s]Running 5000 simulations.:  36%|███▌      | 1781/5000 [00:12<00:22, 145.00it/s]Running 5000 simulations.:  36%|███▌      | 1796/5000 [00:12<00:21, 146.06it/s]Running 5000 simulations.:  36%|███▌      | 1811/5000 [00:12<00:21, 145.57it/s]Running 5000 simulations.:  37%|███▋      | 1826/5000 [00:12<00:21, 144.44it/s]Running 5000 simulations.:  37%|███▋      | 1841/5000 [00:12<00:21, 143.95it/s]Running 5000 simulations.:  37%|███▋      | 1856/5000 [00:12<00:21, 144.73it/s]Running 5000 simulations.:  37%|███▋      | 1871/5000 [00:12<00:21, 143.73it/s]Running 5000 simulations.:  38%|███▊      | 1886/5000 [00:12<00:21, 144.71it/s]Running 5000 simulations.:  38%|███▊      | 1901/5000 [00:13<00:21, 145.90it/s]Running 5000 simulations.:  38%|███▊      | 1916/5000 [00:13<00:21, 146.27it/s]Running 5000 simulations.:  39%|███▊      | 1931/5000 [00:13<00:21, 145.33it/s]Running 5000 simulations.:  39%|███▉      | 1946/5000 [00:13<00:21, 145.16it/s]Running 5000 simulations.:  39%|███▉      | 1961/5000 [00:13<00:21, 142.07it/s]Running 5000 simulations.:  40%|███▉      | 1976/5000 [00:13<00:21, 140.75it/s]Running 5000 simulations.:  40%|███▉      | 1991/5000 [00:13<00:21, 140.86it/s]Running 5000 simulations.:  40%|████      | 2006/5000 [00:13<00:21, 141.47it/s]Running 5000 simulations.:  40%|████      | 2021/5000 [00:13<00:20, 142.60it/s]Running 5000 simulations.:  41%|████      | 2036/5000 [00:14<00:20, 144.15it/s]Running 5000 simulations.:  41%|████      | 2051/5000 [00:14<00:20, 143.69it/s]Running 5000 simulations.:  41%|████▏     | 2066/5000 [00:14<00:20, 144.49it/s]Running 5000 simulations.:  42%|████▏     | 2081/5000 [00:14<00:20, 143.49it/s]Running 5000 simulations.:  42%|████▏     | 2096/5000 [00:14<00:20, 141.64it/s]Running 5000 simulations.:  42%|████▏     | 2111/5000 [00:14<00:20, 141.69it/s]Running 5000 simulations.:  43%|████▎     | 2126/5000 [00:14<00:20, 141.17it/s]Running 5000 simulations.:  43%|████▎     | 2141/5000 [00:14<00:20, 141.21it/s]Running 5000 simulations.:  43%|████▎     | 2156/5000 [00:14<00:19, 142.41it/s]Running 5000 simulations.:  43%|████▎     | 2171/5000 [00:14<00:19, 143.23it/s]Running 5000 simulations.:  44%|████▎     | 2186/5000 [00:15<00:19, 142.68it/s]Running 5000 simulations.:  44%|████▍     | 2201/5000 [00:15<00:19, 143.37it/s]Running 5000 simulations.:  44%|████▍     | 2216/5000 [00:15<00:19, 142.46it/s]Running 5000 simulations.:  45%|████▍     | 2231/5000 [00:15<00:19, 142.32it/s]Running 5000 simulations.:  45%|████▍     | 2246/5000 [00:15<00:19, 141.36it/s]Running 5000 simulations.:  45%|████▌     | 2261/5000 [00:15<00:19, 140.67it/s]Running 5000 simulations.:  46%|████▌     | 2276/5000 [00:15<00:19, 142.45it/s]Running 5000 simulations.:  46%|████▌     | 2291/5000 [00:15<00:19, 141.79it/s]Running 5000 simulations.:  46%|████▌     | 2306/5000 [00:15<00:18, 141.89it/s]Running 5000 simulations.:  46%|████▋     | 2321/5000 [00:16<00:18, 142.19it/s]Running 5000 simulations.:  47%|████▋     | 2336/5000 [00:16<00:18, 141.93it/s]Running 5000 simulations.:  47%|████▋     | 2351/5000 [00:16<00:18, 143.08it/s]Running 5000 simulations.:  47%|████▋     | 2366/5000 [00:16<00:18, 143.05it/s]Running 5000 simulations.:  48%|████▊     | 2381/5000 [00:16<00:18, 142.50it/s]Running 5000 simulations.:  48%|████▊     | 2396/5000 [00:16<00:18, 142.53it/s]Running 5000 simulations.:  48%|████▊     | 2411/5000 [00:16<00:18, 142.41it/s]Running 5000 simulations.:  49%|████▊     | 2426/5000 [00:16<00:18, 142.09it/s]Running 5000 simulations.:  49%|████▉     | 2441/5000 [00:16<00:17, 142.67it/s]Running 5000 simulations.:  49%|████▉     | 2456/5000 [00:16<00:17, 144.26it/s]Running 5000 simulations.:  49%|████▉     | 2471/5000 [00:17<00:17, 144.73it/s]Running 5000 simulations.:  50%|████▉     | 2486/5000 [00:17<00:17, 145.75it/s]Running 5000 simulations.:  50%|█████     | 2501/5000 [00:17<00:17, 145.56it/s]Running 5000 simulations.:  50%|█████     | 2516/5000 [00:17<00:16, 146.28it/s]Running 5000 simulations.:  51%|█████     | 2531/5000 [00:17<00:17, 144.47it/s]Running 5000 simulations.:  51%|█████     | 2546/5000 [00:17<00:17, 143.14it/s]Running 5000 simulations.:  51%|█████     | 2561/5000 [00:17<00:17, 142.22it/s]Running 5000 simulations.:  52%|█████▏    | 2576/5000 [00:17<00:17, 141.95it/s]Running 5000 simulations.:  52%|█████▏    | 2591/5000 [00:17<00:16, 143.43it/s]Running 5000 simulations.:  52%|█████▏    | 2606/5000 [00:18<00:16, 144.15it/s]Running 5000 simulations.:  52%|█████▏    | 2621/5000 [00:18<00:16, 144.97it/s]Running 5000 simulations.:  53%|█████▎    | 2636/5000 [00:18<00:16, 144.52it/s]Running 5000 simulations.:  53%|█████▎    | 2651/5000 [00:18<00:16, 143.83it/s]Running 5000 simulations.:  53%|█████▎    | 2666/5000 [00:18<00:16, 143.07it/s]Running 5000 simulations.:  54%|█████▎    | 2681/5000 [00:18<00:16, 141.32it/s]Running 5000 simulations.:  54%|█████▍    | 2696/5000 [00:18<00:16, 141.48it/s]Running 5000 simulations.:  54%|█████▍    | 2711/5000 [00:18<00:16, 142.18it/s]Running 5000 simulations.:  55%|█████▍    | 2726/5000 [00:18<00:15, 142.91it/s]Running 5000 simulations.:  55%|█████▍    | 2741/5000 [00:18<00:15, 144.35it/s]Running 5000 simulations.:  55%|█████▌    | 2756/5000 [00:19<00:15, 145.07it/s]Running 5000 simulations.:  55%|█████▌    | 2771/5000 [00:19<00:15, 145.36it/s]Running 5000 simulations.:  56%|█████▌    | 2786/5000 [00:19<00:15, 143.85it/s]Running 5000 simulations.:  56%|█████▌    | 2801/5000 [00:19<00:15, 142.19it/s]Running 5000 simulations.:  56%|█████▋    | 2816/5000 [00:19<00:15, 141.03it/s]Running 5000 simulations.:  57%|█████▋    | 2831/5000 [00:19<00:15, 140.29it/s]Running 5000 simulations.:  57%|█████▋    | 2846/5000 [00:19<00:15, 138.81it/s]Running 5000 simulations.:  57%|█████▋    | 2861/5000 [00:19<00:15, 140.31it/s]Running 5000 simulations.:  58%|█████▊    | 2876/5000 [00:19<00:15, 140.49it/s]Running 5000 simulations.:  58%|█████▊    | 2891/5000 [00:20<00:14, 140.85it/s]Running 5000 simulations.:  58%|█████▊    | 2906/5000 [00:20<00:14, 141.62it/s]Running 5000 simulations.:  58%|█████▊    | 2921/5000 [00:20<00:14, 140.78it/s]Running 5000 simulations.:  59%|█████▊    | 2936/5000 [00:20<00:14, 140.05it/s]Running 5000 simulations.:  59%|█████▉    | 2951/5000 [00:20<00:14, 140.01it/s]Running 5000 simulations.:  59%|█████▉    | 2966/5000 [00:20<00:14, 140.21it/s]Running 5000 simulations.:  60%|█████▉    | 2981/5000 [00:20<00:14, 139.72it/s]Running 5000 simulations.:  60%|█████▉    | 2996/5000 [00:20<00:14, 141.35it/s]Running 5000 simulations.:  60%|██████    | 3011/5000 [00:20<00:14, 139.24it/s]Running 5000 simulations.:  60%|██████    | 3025/5000 [00:21<00:14, 132.22it/s]Running 5000 simulations.:  61%|██████    | 3040/5000 [00:21<00:14, 134.60it/s]Running 5000 simulations.:  61%|██████    | 3054/5000 [00:21<00:14, 135.87it/s]Running 5000 simulations.:  61%|██████▏   | 3068/5000 [00:21<00:14, 136.87it/s]Running 5000 simulations.:  62%|██████▏   | 3082/5000 [00:21<00:13, 137.15it/s]Running 5000 simulations.:  62%|██████▏   | 3096/5000 [00:21<00:13, 136.97it/s]Running 5000 simulations.:  62%|██████▏   | 3110/5000 [00:21<00:13, 137.61it/s]Running 5000 simulations.:  62%|██████▏   | 3124/5000 [00:21<00:13, 138.20it/s]Running 5000 simulations.:  63%|██████▎   | 3138/5000 [00:21<00:13, 138.48it/s]Running 5000 simulations.:  63%|██████▎   | 3153/5000 [00:21<00:13, 139.39it/s]Running 5000 simulations.:  63%|██████▎   | 3167/5000 [00:22<00:13, 139.26it/s]Running 5000 simulations.:  64%|██████▎   | 3181/5000 [00:22<00:13, 138.54it/s]Running 5000 simulations.:  64%|██████▍   | 3195/5000 [00:22<00:13, 137.81it/s]Running 5000 simulations.:  64%|██████▍   | 3209/5000 [00:22<00:13, 137.36it/s]Running 5000 simulations.:  64%|██████▍   | 3223/5000 [00:22<00:13, 136.63it/s]Running 5000 simulations.:  65%|██████▍   | 3238/5000 [00:22<00:12, 138.54it/s]Running 5000 simulations.:  65%|██████▌   | 3253/5000 [00:22<00:12, 139.42it/s]Running 5000 simulations.:  65%|██████▌   | 3268/5000 [00:22<00:12, 140.76it/s]Running 5000 simulations.:  66%|██████▌   | 3283/5000 [00:22<00:12, 141.64it/s]Running 5000 simulations.:  66%|██████▌   | 3298/5000 [00:22<00:11, 143.31it/s]Running 5000 simulations.:  66%|██████▋   | 3313/5000 [00:23<00:11, 144.21it/s]Running 5000 simulations.:  67%|██████▋   | 3328/5000 [00:23<00:11, 145.22it/s]Running 5000 simulations.:  67%|██████▋   | 3343/5000 [00:23<00:11, 145.96it/s]Running 5000 simulations.:  67%|██████▋   | 3358/5000 [00:23<00:11, 146.24it/s]Running 5000 simulations.:  67%|██████▋   | 3373/5000 [00:23<00:11, 145.11it/s]Running 5000 simulations.:  68%|██████▊   | 3388/5000 [00:23<00:11, 144.28it/s]Running 5000 simulations.:  68%|██████▊   | 3403/5000 [00:23<00:11, 144.46it/s]Running 5000 simulations.:  68%|██████▊   | 3418/5000 [00:23<00:10, 144.92it/s]Running 5000 simulations.:  69%|██████▊   | 3433/5000 [00:23<00:10, 145.05it/s]Running 5000 simulations.:  69%|██████▉   | 3448/5000 [00:24<00:10, 145.12it/s]Running 5000 simulations.:  69%|██████▉   | 3463/5000 [00:24<00:10, 146.25it/s]Running 5000 simulations.:  70%|██████▉   | 3479/5000 [00:24<00:10, 147.47it/s]Running 5000 simulations.:  70%|██████▉   | 3494/5000 [00:24<00:10, 146.13it/s]Running 5000 simulations.:  70%|███████   | 3509/5000 [00:24<00:10, 146.57it/s]Running 5000 simulations.:  70%|███████   | 3524/5000 [00:24<00:10, 144.86it/s]Running 5000 simulations.:  71%|███████   | 3539/5000 [00:24<00:10, 143.67it/s]Running 5000 simulations.:  71%|███████   | 3554/5000 [00:24<00:10, 142.99it/s]Running 5000 simulations.:  71%|███████▏  | 3569/5000 [00:24<00:09, 143.74it/s]Running 5000 simulations.:  72%|███████▏  | 3584/5000 [00:24<00:09, 143.09it/s]Running 5000 simulations.:  72%|███████▏  | 3599/5000 [00:25<00:09, 142.97it/s]Running 5000 simulations.:  72%|███████▏  | 3614/5000 [00:25<00:09, 142.17it/s]Running 5000 simulations.:  73%|███████▎  | 3629/5000 [00:25<00:09, 140.96it/s]Running 5000 simulations.:  73%|███████▎  | 3644/5000 [00:25<00:09, 140.12it/s]Running 5000 simulations.:  73%|███████▎  | 3659/5000 [00:25<00:09, 140.06it/s]Running 5000 simulations.:  73%|███████▎  | 3674/5000 [00:25<00:09, 140.29it/s]Running 5000 simulations.:  74%|███████▍  | 3689/5000 [00:25<00:09, 140.63it/s]Running 5000 simulations.:  74%|███████▍  | 3704/5000 [00:25<00:09, 141.14it/s]Running 5000 simulations.:  74%|███████▍  | 3719/5000 [00:25<00:09, 142.08it/s]Running 5000 simulations.:  75%|███████▍  | 3734/5000 [00:26<00:08, 142.13it/s]Running 5000 simulations.:  75%|███████▍  | 3749/5000 [00:26<00:08, 142.77it/s]Running 5000 simulations.:  75%|███████▌  | 3764/5000 [00:26<00:08, 142.22it/s]Running 5000 simulations.:  76%|███████▌  | 3779/5000 [00:26<00:08, 143.66it/s]Running 5000 simulations.:  76%|███████▌  | 3794/5000 [00:26<00:08, 142.98it/s]Running 5000 simulations.:  76%|███████▌  | 3809/5000 [00:26<00:08, 143.48it/s]Running 5000 simulations.:  76%|███████▋  | 3824/5000 [00:26<00:08, 144.52it/s]Running 5000 simulations.:  77%|███████▋  | 3839/5000 [00:26<00:07, 145.72it/s]Running 5000 simulations.:  77%|███████▋  | 3854/5000 [00:26<00:07, 145.82it/s]Running 5000 simulations.:  77%|███████▋  | 3869/5000 [00:26<00:07, 146.17it/s]Running 5000 simulations.:  78%|███████▊  | 3884/5000 [00:27<00:07, 145.26it/s]Running 5000 simulations.:  78%|███████▊  | 3899/5000 [00:27<00:07, 144.59it/s]Running 5000 simulations.:  78%|███████▊  | 3914/5000 [00:27<00:07, 144.98it/s]Running 5000 simulations.:  79%|███████▊  | 3929/5000 [00:27<00:07, 145.33it/s]Running 5000 simulations.:  79%|███████▉  | 3944/5000 [00:27<00:07, 144.78it/s]Running 5000 simulations.:  79%|███████▉  | 3959/5000 [00:27<00:07, 145.10it/s]Running 5000 simulations.:  79%|███████▉  | 3974/5000 [00:27<00:07, 145.49it/s]Running 5000 simulations.:  80%|███████▉  | 3989/5000 [00:27<00:06, 146.22it/s]Running 5000 simulations.:  80%|████████  | 4004/5000 [00:27<00:06, 146.12it/s]Running 5000 simulations.:  80%|████████  | 4019/5000 [00:27<00:06, 145.99it/s]Running 5000 simulations.:  81%|████████  | 4034/5000 [00:28<00:06, 145.20it/s]Running 5000 simulations.:  81%|████████  | 4049/5000 [00:28<00:06, 145.09it/s]Running 5000 simulations.:  81%|████████▏ | 4064/5000 [00:28<00:06, 145.38it/s]Running 5000 simulations.:  82%|████████▏ | 4079/5000 [00:28<00:06, 144.92it/s]Running 5000 simulations.:  82%|████████▏ | 4094/5000 [00:28<00:06, 145.02it/s]Running 5000 simulations.:  82%|████████▏ | 4109/5000 [00:28<00:06, 144.35it/s]Running 5000 simulations.:  82%|████████▏ | 4124/5000 [00:28<00:06, 145.47it/s]Running 5000 simulations.:  83%|████████▎ | 4139/5000 [00:28<00:05, 145.85it/s]Running 5000 simulations.:  83%|████████▎ | 4154/5000 [00:28<00:05, 144.99it/s]Running 5000 simulations.:  83%|████████▎ | 4169/5000 [00:29<00:05, 145.03it/s]Running 5000 simulations.:  84%|████████▎ | 4184/5000 [00:29<00:05, 145.09it/s]Running 5000 simulations.:  84%|████████▍ | 4199/5000 [00:29<00:05, 144.73it/s]Running 5000 simulations.:  84%|████████▍ | 4214/5000 [00:29<00:05, 145.23it/s]Running 5000 simulations.:  85%|████████▍ | 4229/5000 [00:29<00:05, 145.70it/s]Running 5000 simulations.:  85%|████████▍ | 4244/5000 [00:29<00:05, 146.85it/s]Running 5000 simulations.:  85%|████████▌ | 4259/5000 [00:29<00:05, 146.31it/s]Running 5000 simulations.:  85%|████████▌ | 4274/5000 [00:29<00:04, 146.52it/s]Running 5000 simulations.:  86%|████████▌ | 4289/5000 [00:29<00:04, 144.94it/s]Running 5000 simulations.:  86%|████████▌ | 4304/5000 [00:29<00:04, 144.99it/s]Running 5000 simulations.:  86%|████████▋ | 4319/5000 [00:30<00:04, 145.08it/s]Running 5000 simulations.:  87%|████████▋ | 4334/5000 [00:30<00:04, 145.67it/s]Running 5000 simulations.:  87%|████████▋ | 4349/5000 [00:30<00:04, 145.31it/s]Running 5000 simulations.:  87%|████████▋ | 4364/5000 [00:30<00:04, 145.02it/s]Running 5000 simulations.:  88%|████████▊ | 4379/5000 [00:30<00:04, 144.22it/s]Running 5000 simulations.:  88%|████████▊ | 4394/5000 [00:30<00:04, 144.73it/s]Running 5000 simulations.:  88%|████████▊ | 4409/5000 [00:30<00:04, 145.46it/s]Running 5000 simulations.:  88%|████████▊ | 4424/5000 [00:30<00:03, 145.94it/s]Running 5000 simulations.:  89%|████████▉ | 4439/5000 [00:30<00:03, 146.05it/s]Running 5000 simulations.:  89%|████████▉ | 4454/5000 [00:30<00:03, 145.52it/s]Running 5000 simulations.:  89%|████████▉ | 4469/5000 [00:31<00:03, 144.32it/s]Running 5000 simulations.:  90%|████████▉ | 4484/5000 [00:31<00:03, 142.75it/s]Running 5000 simulations.:  90%|████████▉ | 4499/5000 [00:31<00:03, 143.01it/s]Running 5000 simulations.:  90%|█████████ | 4514/5000 [00:31<00:03, 142.47it/s]Running 5000 simulations.:  91%|█████████ | 4529/5000 [00:31<00:03, 142.15it/s]Running 5000 simulations.:  91%|█████████ | 4544/5000 [00:31<00:03, 142.46it/s]Running 5000 simulations.:  91%|█████████ | 4559/5000 [00:31<00:03, 143.31it/s]Running 5000 simulations.:  92%|█████████▏| 4575/5000 [00:31<00:02, 146.47it/s]Running 5000 simulations.:  92%|█████████▏| 4592/5000 [00:31<00:02, 151.21it/s]Running 5000 simulations.:  92%|█████████▏| 4608/5000 [00:32<00:02, 153.72it/s]Running 5000 simulations.:  92%|█████████▏| 4624/5000 [00:32<00:02, 155.26it/s]Running 5000 simulations.:  93%|█████████▎| 4640/5000 [00:32<00:02, 153.57it/s]Running 5000 simulations.:  93%|█████████▎| 4656/5000 [00:32<00:02, 151.89it/s]Running 5000 simulations.:  93%|█████████▎| 4672/5000 [00:32<00:02, 151.51it/s]Running 5000 simulations.:  94%|█████████▍| 4688/5000 [00:32<00:02, 151.88it/s]Running 5000 simulations.:  94%|█████████▍| 4704/5000 [00:32<00:01, 151.61it/s]Running 5000 simulations.:  94%|█████████▍| 4720/5000 [00:32<00:01, 151.25it/s]Running 5000 simulations.:  95%|█████████▍| 4736/5000 [00:32<00:01, 151.74it/s]Running 5000 simulations.:  95%|█████████▌| 4752/5000 [00:32<00:01, 149.52it/s]Running 5000 simulations.:  95%|█████████▌| 4767/5000 [00:33<00:01, 148.54it/s]Running 5000 simulations.:  96%|█████████▌| 4782/5000 [00:33<00:01, 147.84it/s]Running 5000 simulations.:  96%|█████████▌| 4797/5000 [00:33<00:01, 147.18it/s]Running 5000 simulations.:  96%|█████████▌| 4812/5000 [00:33<00:01, 147.93it/s]Running 5000 simulations.:  97%|█████████▋| 4828/5000 [00:33<00:01, 149.07it/s]Running 5000 simulations.:  97%|█████████▋| 4843/5000 [00:33<00:01, 148.72it/s]Running 5000 simulations.:  97%|█████████▋| 4858/5000 [00:33<00:00, 148.65it/s]Running 5000 simulations.:  97%|█████████▋| 4873/5000 [00:33<00:00, 148.66it/s]Running 5000 simulations.:  98%|█████████▊| 4888/5000 [00:33<00:00, 148.02it/s]Running 5000 simulations.:  98%|█████████▊| 4903/5000 [00:33<00:00, 147.87it/s]Running 5000 simulations.:  98%|█████████▊| 4918/5000 [00:34<00:00, 147.31it/s]Running 5000 simulations.:  99%|█████████▊| 4933/5000 [00:34<00:00, 146.99it/s]Running 5000 simulations.:  99%|█████████▉| 4948/5000 [00:34<00:00, 147.17it/s]Running 5000 simulations.:  99%|█████████▉| 4963/5000 [00:34<00:00, 147.94it/s]Running 5000 simulations.: 100%|█████████▉| 4979/5000 [00:34<00:00, 148.82it/s]Running 5000 simulations.: 100%|█████████▉| 4995/5000 [00:34<00:00, 149.68it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:34<00:00, 144.36it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:32, 151.86it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:32, 152.12it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:32, 152.19it/s]Running 5000 simulations.:   1%|▏         | 64/5000 [00:00<00:32, 152.37it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:32, 151.65it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:32, 151.93it/s]Running 5000 simulations.:   2%|▏         | 112/5000 [00:00<00:32, 152.50it/s]Running 5000 simulations.:   3%|▎         | 128/5000 [00:00<00:31, 152.84it/s]Running 5000 simulations.:   3%|▎         | 144/5000 [00:00<00:31, 152.79it/s]Running 5000 simulations.:   3%|▎         | 160/5000 [00:01<00:31, 152.32it/s]Running 5000 simulations.:   4%|▎         | 176/5000 [00:01<00:31, 151.76it/s]Running 5000 simulations.:   4%|▍         | 192/5000 [00:01<00:31, 151.79it/s]Running 5000 simulations.:   4%|▍         | 208/5000 [00:01<00:31, 151.91it/s]Running 5000 simulations.:   4%|▍         | 224/5000 [00:01<00:31, 151.19it/s]Running 5000 simulations.:   5%|▍         | 240/5000 [00:01<00:33, 143.24it/s]Running 5000 simulations.:   5%|▌         | 255/5000 [00:01<00:32, 145.12it/s]Running 5000 simulations.:   5%|▌         | 270/5000 [00:01<00:32, 146.49it/s]Running 5000 simulations.:   6%|▌         | 286/5000 [00:01<00:31, 147.60it/s]Running 5000 simulations.:   6%|▌         | 302/5000 [00:02<00:31, 148.37it/s]Running 5000 simulations.:   6%|▋         | 318/5000 [00:02<00:31, 149.11it/s]Running 5000 simulations.:   7%|▋         | 334/5000 [00:02<00:31, 149.42it/s]Running 5000 simulations.:   7%|▋         | 349/5000 [00:02<00:31, 149.54it/s]Running 5000 simulations.:   7%|▋         | 365/5000 [00:02<00:30, 149.77it/s]Running 5000 simulations.:   8%|▊         | 381/5000 [00:02<00:30, 150.06it/s]Running 5000 simulations.:   8%|▊         | 397/5000 [00:02<00:30, 149.78it/s]Running 5000 simulations.:   8%|▊         | 412/5000 [00:02<00:30, 149.77it/s]Running 5000 simulations.:   9%|▊         | 427/5000 [00:02<00:30, 149.82it/s]Running 5000 simulations.:   9%|▉         | 443/5000 [00:02<00:30, 150.27it/s]Running 5000 simulations.:   9%|▉         | 459/5000 [00:03<00:30, 150.65it/s]Running 5000 simulations.:  10%|▉         | 475/5000 [00:03<00:30, 150.15it/s]Running 5000 simulations.:  10%|▉         | 491/5000 [00:03<00:30, 149.53it/s]Running 5000 simulations.:  10%|█         | 506/5000 [00:03<00:30, 147.81it/s]Running 5000 simulations.:  10%|█         | 521/5000 [00:03<00:30, 146.87it/s]Running 5000 simulations.:  11%|█         | 536/5000 [00:03<00:30, 146.05it/s]Running 5000 simulations.:  11%|█         | 551/5000 [00:03<00:30, 145.78it/s]Running 5000 simulations.:  11%|█▏        | 566/5000 [00:03<00:30, 145.66it/s]Running 5000 simulations.:  12%|█▏        | 581/5000 [00:03<00:30, 145.31it/s]Running 5000 simulations.:  12%|█▏        | 596/5000 [00:03<00:30, 145.31it/s]Running 5000 simulations.:  12%|█▏        | 611/5000 [00:04<00:30, 144.97it/s]Running 5000 simulations.:  13%|█▎        | 626/5000 [00:04<00:30, 144.57it/s]Running 5000 simulations.:  13%|█▎        | 641/5000 [00:04<00:30, 144.03it/s]Running 5000 simulations.:  13%|█▎        | 656/5000 [00:04<00:30, 143.87it/s]Running 5000 simulations.:  13%|█▎        | 671/5000 [00:04<00:30, 143.64it/s]Running 5000 simulations.:  14%|█▎        | 686/5000 [00:04<00:30, 143.51it/s]Running 5000 simulations.:  14%|█▍        | 701/5000 [00:04<00:29, 143.39it/s]Running 5000 simulations.:  14%|█▍        | 716/5000 [00:04<00:29, 143.09it/s]Running 5000 simulations.:  15%|█▍        | 731/5000 [00:04<00:29, 143.34it/s]Running 5000 simulations.:  15%|█▍        | 746/5000 [00:05<00:29, 143.79it/s]Running 5000 simulations.:  15%|█▌        | 761/5000 [00:05<00:29, 143.67it/s]Running 5000 simulations.:  16%|█▌        | 776/5000 [00:05<00:29, 143.11it/s]Running 5000 simulations.:  16%|█▌        | 791/5000 [00:05<00:29, 142.78it/s]Running 5000 simulations.:  16%|█▌        | 806/5000 [00:05<00:29, 143.21it/s]Running 5000 simulations.:  16%|█▋        | 821/5000 [00:05<00:29, 144.01it/s]Running 5000 simulations.:  17%|█▋        | 836/5000 [00:05<00:28, 144.69it/s]Running 5000 simulations.:  17%|█▋        | 851/5000 [00:05<00:28, 145.15it/s]Running 5000 simulations.:  17%|█▋        | 866/5000 [00:05<00:28, 145.12it/s]Running 5000 simulations.:  18%|█▊        | 881/5000 [00:05<00:28, 144.96it/s]Running 5000 simulations.:  18%|█▊        | 896/5000 [00:06<00:28, 144.79it/s]Running 5000 simulations.:  18%|█▊        | 911/5000 [00:06<00:28, 144.69it/s]Running 5000 simulations.:  19%|█▊        | 926/5000 [00:06<00:28, 144.73it/s]Running 5000 simulations.:  19%|█▉        | 941/5000 [00:06<00:28, 144.78it/s]Running 5000 simulations.:  19%|█▉        | 956/5000 [00:06<00:27, 145.09it/s]Running 5000 simulations.:  19%|█▉        | 971/5000 [00:06<00:27, 145.31it/s]Running 5000 simulations.:  20%|█▉        | 986/5000 [00:06<00:27, 145.41it/s]Running 5000 simulations.:  20%|██        | 1001/5000 [00:06<00:27, 145.49it/s]Running 5000 simulations.:  20%|██        | 1016/5000 [00:06<00:27, 145.28it/s]Running 5000 simulations.:  21%|██        | 1031/5000 [00:07<00:27, 145.41it/s]Running 5000 simulations.:  21%|██        | 1046/5000 [00:07<00:27, 145.69it/s]Running 5000 simulations.:  21%|██        | 1061/5000 [00:07<00:27, 145.43it/s]Running 5000 simulations.:  22%|██▏       | 1076/5000 [00:07<00:27, 145.12it/s]Running 5000 simulations.:  22%|██▏       | 1091/5000 [00:07<00:26, 144.97it/s]Running 5000 simulations.:  22%|██▏       | 1106/5000 [00:07<00:26, 144.92it/s]Running 5000 simulations.:  22%|██▏       | 1121/5000 [00:07<00:26, 144.72it/s]Running 5000 simulations.:  23%|██▎       | 1136/5000 [00:07<00:26, 144.88it/s]Running 5000 simulations.:  23%|██▎       | 1151/5000 [00:07<00:26, 145.05it/s]Running 5000 simulations.:  23%|██▎       | 1166/5000 [00:07<00:26, 144.91it/s]Running 5000 simulations.:  24%|██▎       | 1181/5000 [00:08<00:26, 144.77it/s]Running 5000 simulations.:  24%|██▍       | 1196/5000 [00:08<00:26, 144.94it/s]Running 5000 simulations.:  24%|██▍       | 1211/5000 [00:08<00:26, 144.77it/s]Running 5000 simulations.:  25%|██▍       | 1226/5000 [00:08<00:26, 144.79it/s]Running 5000 simulations.:  25%|██▍       | 1241/5000 [00:08<00:25, 145.09it/s]Running 5000 simulations.:  25%|██▌       | 1256/5000 [00:08<00:25, 145.18it/s]Running 5000 simulations.:  25%|██▌       | 1271/5000 [00:08<00:25, 145.31it/s]Running 5000 simulations.:  26%|██▌       | 1286/5000 [00:08<00:25, 145.05it/s]Running 5000 simulations.:  26%|██▌       | 1301/5000 [00:08<00:25, 144.78it/s]Running 5000 simulations.:  26%|██▋       | 1316/5000 [00:08<00:25, 144.90it/s]Running 5000 simulations.:  27%|██▋       | 1331/5000 [00:09<00:25, 145.27it/s]Running 5000 simulations.:  27%|██▋       | 1346/5000 [00:09<00:25, 145.45it/s]Running 5000 simulations.:  27%|██▋       | 1361/5000 [00:09<00:25, 145.40it/s]Running 5000 simulations.:  28%|██▊       | 1376/5000 [00:09<00:24, 145.47it/s]Running 5000 simulations.:  28%|██▊       | 1391/5000 [00:09<00:24, 145.35it/s]Running 5000 simulations.:  28%|██▊       | 1406/5000 [00:09<00:24, 144.30it/s]Running 5000 simulations.:  28%|██▊       | 1421/5000 [00:09<00:24, 144.39it/s]Running 5000 simulations.:  29%|██▊       | 1436/5000 [00:09<00:24, 144.46it/s]Running 5000 simulations.:  29%|██▉       | 1451/5000 [00:09<00:24, 144.54it/s]Running 5000 simulations.:  29%|██▉       | 1466/5000 [00:10<00:24, 144.71it/s]Running 5000 simulations.:  30%|██▉       | 1481/5000 [00:10<00:24, 144.66it/s]Running 5000 simulations.:  30%|██▉       | 1496/5000 [00:10<00:24, 144.59it/s]Running 5000 simulations.:  30%|███       | 1511/5000 [00:10<00:24, 144.53it/s]Running 5000 simulations.:  31%|███       | 1526/5000 [00:10<00:24, 144.47it/s]Running 5000 simulations.:  31%|███       | 1541/5000 [00:10<00:23, 144.45it/s]Running 5000 simulations.:  31%|███       | 1556/5000 [00:10<00:23, 144.92it/s]Running 5000 simulations.:  31%|███▏      | 1571/5000 [00:10<00:23, 144.59it/s]Running 5000 simulations.:  32%|███▏      | 1586/5000 [00:10<00:23, 144.42it/s]Running 5000 simulations.:  32%|███▏      | 1601/5000 [00:10<00:23, 144.41it/s]Running 5000 simulations.:  32%|███▏      | 1616/5000 [00:11<00:23, 143.97it/s]Running 5000 simulations.:  33%|███▎      | 1631/5000 [00:11<00:23, 143.15it/s]Running 5000 simulations.:  33%|███▎      | 1646/5000 [00:11<00:23, 142.31it/s]Running 5000 simulations.:  33%|███▎      | 1661/5000 [00:11<00:23, 141.76it/s]Running 5000 simulations.:  34%|███▎      | 1676/5000 [00:11<00:23, 142.71it/s]Running 5000 simulations.:  34%|███▍      | 1691/5000 [00:11<00:23, 143.47it/s]Running 5000 simulations.:  34%|███▍      | 1706/5000 [00:11<00:22, 143.98it/s]Running 5000 simulations.:  34%|███▍      | 1721/5000 [00:11<00:22, 144.46it/s]Running 5000 simulations.:  35%|███▍      | 1736/5000 [00:11<00:22, 144.68it/s]Running 5000 simulations.:  35%|███▌      | 1751/5000 [00:11<00:22, 144.90it/s]Running 5000 simulations.:  35%|███▌      | 1766/5000 [00:12<00:22, 145.10it/s]Running 5000 simulations.:  36%|███▌      | 1781/5000 [00:12<00:22, 145.31it/s]Running 5000 simulations.:  36%|███▌      | 1796/5000 [00:12<00:22, 145.33it/s]Running 5000 simulations.:  36%|███▌      | 1811/5000 [00:12<00:21, 145.43it/s]Running 5000 simulations.:  37%|███▋      | 1826/5000 [00:12<00:21, 145.22it/s]Running 5000 simulations.:  37%|███▋      | 1841/5000 [00:12<00:21, 145.17it/s]Running 5000 simulations.:  37%|███▋      | 1856/5000 [00:12<00:21, 145.16it/s]Running 5000 simulations.:  37%|███▋      | 1871/5000 [00:12<00:21, 145.17it/s]Running 5000 simulations.:  38%|███▊      | 1886/5000 [00:12<00:21, 145.17it/s]Running 5000 simulations.:  38%|███▊      | 1901/5000 [00:13<00:21, 145.69it/s]Running 5000 simulations.:  38%|███▊      | 1916/5000 [00:13<00:21, 145.65it/s]Running 5000 simulations.:  39%|███▊      | 1931/5000 [00:13<00:21, 145.89it/s]Running 5000 simulations.:  39%|███▉      | 1946/5000 [00:13<00:21, 145.06it/s]Running 5000 simulations.:  39%|███▉      | 1961/5000 [00:13<00:20, 145.11it/s]Running 5000 simulations.:  40%|███▉      | 1976/5000 [00:13<00:20, 144.38it/s]Running 5000 simulations.:  40%|███▉      | 1991/5000 [00:13<00:20, 144.51it/s]Running 5000 simulations.:  40%|████      | 2006/5000 [00:13<00:20, 144.66it/s]Running 5000 simulations.:  40%|████      | 2021/5000 [00:13<00:20, 144.75it/s]Running 5000 simulations.:  41%|████      | 2036/5000 [00:13<00:20, 144.75it/s]Running 5000 simulations.:  41%|████      | 2051/5000 [00:14<00:20, 144.67it/s]Running 5000 simulations.:  41%|████▏     | 2066/5000 [00:14<00:20, 144.62it/s]Running 5000 simulations.:  42%|████▏     | 2081/5000 [00:14<00:20, 144.74it/s]Running 5000 simulations.:  42%|████▏     | 2096/5000 [00:14<00:20, 144.52it/s]Running 5000 simulations.:  42%|████▏     | 2111/5000 [00:14<00:19, 144.59it/s]Running 5000 simulations.:  43%|████▎     | 2126/5000 [00:14<00:19, 144.96it/s]Running 5000 simulations.:  43%|████▎     | 2141/5000 [00:14<00:19, 145.55it/s]Running 5000 simulations.:  43%|████▎     | 2156/5000 [00:14<00:19, 145.23it/s]Running 5000 simulations.:  43%|████▎     | 2171/5000 [00:14<00:19, 145.26it/s]Running 5000 simulations.:  44%|████▎     | 2186/5000 [00:14<00:19, 145.18it/s]Running 5000 simulations.:  44%|████▍     | 2201/5000 [00:15<00:19, 144.53it/s]Running 5000 simulations.:  44%|████▍     | 2216/5000 [00:15<00:19, 144.52it/s]Running 5000 simulations.:  45%|████▍     | 2231/5000 [00:15<00:19, 144.59it/s]Running 5000 simulations.:  45%|████▍     | 2246/5000 [00:15<00:19, 144.78it/s]Running 5000 simulations.:  45%|████▌     | 2261/5000 [00:15<00:18, 145.05it/s]Running 5000 simulations.:  46%|████▌     | 2276/5000 [00:15<00:18, 145.32it/s]Running 5000 simulations.:  46%|████▌     | 2291/5000 [00:15<00:18, 145.00it/s]Running 5000 simulations.:  46%|████▌     | 2306/5000 [00:15<00:18, 145.33it/s]Running 5000 simulations.:  46%|████▋     | 2321/5000 [00:15<00:18, 145.30it/s]Running 5000 simulations.:  47%|████▋     | 2336/5000 [00:16<00:18, 145.05it/s]Running 5000 simulations.:  47%|████▋     | 2351/5000 [00:16<00:18, 144.96it/s]Running 5000 simulations.:  47%|████▋     | 2366/5000 [00:16<00:18, 144.83it/s]Running 5000 simulations.:  48%|████▊     | 2381/5000 [00:16<00:18, 145.42it/s]Running 5000 simulations.:  48%|████▊     | 2396/5000 [00:16<00:17, 145.40it/s]Running 5000 simulations.:  48%|████▊     | 2411/5000 [00:16<00:17, 145.27it/s]Running 5000 simulations.:  49%|████▊     | 2426/5000 [00:16<00:17, 145.31it/s]Running 5000 simulations.:  49%|████▉     | 2441/5000 [00:16<00:17, 145.08it/s]Running 5000 simulations.:  49%|████▉     | 2456/5000 [00:16<00:17, 145.08it/s]Running 5000 simulations.:  49%|████▉     | 2471/5000 [00:16<00:17, 145.50it/s]Running 5000 simulations.:  50%|████▉     | 2486/5000 [00:17<00:17, 146.02it/s]Running 5000 simulations.:  50%|█████     | 2501/5000 [00:17<00:17, 146.18it/s]Running 5000 simulations.:  50%|█████     | 2516/5000 [00:17<00:16, 146.86it/s]Running 5000 simulations.:  51%|█████     | 2531/5000 [00:17<00:16, 147.07it/s]Running 5000 simulations.:  51%|█████     | 2548/5000 [00:17<00:16, 150.96it/s]Running 5000 simulations.:  51%|█████▏    | 2565/5000 [00:17<00:15, 154.83it/s]Running 5000 simulations.:  52%|█████▏    | 2581/5000 [00:17<00:15, 151.89it/s]Running 5000 simulations.:  52%|█████▏    | 2597/5000 [00:17<00:15, 150.53it/s]Running 5000 simulations.:  52%|█████▏    | 2613/5000 [00:17<00:16, 149.18it/s]Running 5000 simulations.:  53%|█████▎    | 2628/5000 [00:18<00:16, 147.92it/s]Running 5000 simulations.:  53%|█████▎    | 2643/5000 [00:18<00:16, 147.25it/s]Running 5000 simulations.:  53%|█████▎    | 2658/5000 [00:18<00:15, 146.93it/s]Running 5000 simulations.:  53%|█████▎    | 2673/5000 [00:18<00:15, 146.33it/s]Running 5000 simulations.:  54%|█████▍    | 2688/5000 [00:18<00:15, 145.85it/s]Running 5000 simulations.:  54%|█████▍    | 2703/5000 [00:18<00:15, 145.89it/s]Running 5000 simulations.:  54%|█████▍    | 2718/5000 [00:18<00:15, 145.79it/s]Running 5000 simulations.:  55%|█████▍    | 2733/5000 [00:18<00:15, 145.89it/s]Running 5000 simulations.:  55%|█████▍    | 2748/5000 [00:18<00:15, 145.88it/s]Running 5000 simulations.:  55%|█████▌    | 2763/5000 [00:18<00:15, 145.97it/s]Running 5000 simulations.:  56%|█████▌    | 2778/5000 [00:19<00:15, 145.79it/s]Running 5000 simulations.:  56%|█████▌    | 2793/5000 [00:19<00:15, 146.03it/s]Running 5000 simulations.:  56%|█████▌    | 2808/5000 [00:19<00:15, 146.02it/s]Running 5000 simulations.:  56%|█████▋    | 2823/5000 [00:19<00:14, 146.40it/s]Running 5000 simulations.:  57%|█████▋    | 2838/5000 [00:19<00:14, 146.42it/s]Running 5000 simulations.:  57%|█████▋    | 2853/5000 [00:19<00:14, 146.71it/s]Running 5000 simulations.:  57%|█████▋    | 2868/5000 [00:19<00:14, 147.01it/s]Running 5000 simulations.:  58%|█████▊    | 2883/5000 [00:19<00:14, 146.93it/s]Running 5000 simulations.:  58%|█████▊    | 2898/5000 [00:19<00:14, 146.49it/s]Running 5000 simulations.:  58%|█████▊    | 2913/5000 [00:19<00:14, 146.28it/s]Running 5000 simulations.:  59%|█████▊    | 2928/5000 [00:20<00:14, 146.15it/s]Running 5000 simulations.:  59%|█████▉    | 2943/5000 [00:20<00:14, 145.85it/s]Running 5000 simulations.:  59%|█████▉    | 2958/5000 [00:20<00:14, 145.76it/s]Running 5000 simulations.:  59%|█████▉    | 2973/5000 [00:20<00:13, 145.96it/s]Running 5000 simulations.:  60%|█████▉    | 2988/5000 [00:20<00:13, 146.16it/s]Running 5000 simulations.:  60%|██████    | 3003/5000 [00:20<00:13, 145.98it/s]Running 5000 simulations.:  60%|██████    | 3018/5000 [00:20<00:13, 145.81it/s]Running 5000 simulations.:  61%|██████    | 3033/5000 [00:20<00:13, 146.04it/s]Running 5000 simulations.:  61%|██████    | 3048/5000 [00:20<00:13, 145.96it/s]Running 5000 simulations.:  61%|██████▏   | 3063/5000 [00:20<00:13, 145.93it/s]Running 5000 simulations.:  62%|██████▏   | 3078/5000 [00:21<00:13, 146.54it/s]Running 5000 simulations.:  62%|██████▏   | 3093/5000 [00:21<00:13, 146.69it/s]Running 5000 simulations.:  62%|██████▏   | 3108/5000 [00:21<00:12, 147.01it/s]Running 5000 simulations.:  62%|██████▏   | 3123/5000 [00:21<00:12, 147.28it/s]Running 5000 simulations.:  63%|██████▎   | 3138/5000 [00:21<00:12, 147.01it/s]Running 5000 simulations.:  63%|██████▎   | 3153/5000 [00:21<00:12, 147.00it/s]Running 5000 simulations.:  63%|██████▎   | 3168/5000 [00:21<00:12, 146.74it/s]Running 5000 simulations.:  64%|██████▎   | 3183/5000 [00:21<00:12, 146.57it/s]Running 5000 simulations.:  64%|██████▍   | 3198/5000 [00:21<00:12, 146.67it/s]Running 5000 simulations.:  64%|██████▍   | 3213/5000 [00:22<00:12, 146.61it/s]Running 5000 simulations.:  65%|██████▍   | 3228/5000 [00:22<00:12, 146.23it/s]Running 5000 simulations.:  65%|██████▍   | 3243/5000 [00:22<00:12, 146.14it/s]Running 5000 simulations.:  65%|██████▌   | 3258/5000 [00:22<00:11, 145.99it/s]Running 5000 simulations.:  65%|██████▌   | 3273/5000 [00:22<00:11, 145.89it/s]Running 5000 simulations.:  66%|██████▌   | 3288/5000 [00:22<00:11, 146.01it/s]Running 5000 simulations.:  66%|██████▌   | 3303/5000 [00:22<00:11, 145.77it/s]Running 5000 simulations.:  66%|██████▋   | 3318/5000 [00:22<00:11, 145.55it/s]Running 5000 simulations.:  67%|██████▋   | 3333/5000 [00:22<00:11, 145.53it/s]Running 5000 simulations.:  67%|██████▋   | 3348/5000 [00:22<00:11, 145.72it/s]Running 5000 simulations.:  67%|██████▋   | 3363/5000 [00:23<00:11, 145.95it/s]Running 5000 simulations.:  68%|██████▊   | 3378/5000 [00:23<00:11, 146.17it/s]Running 5000 simulations.:  68%|██████▊   | 3393/5000 [00:23<00:10, 146.28it/s]Running 5000 simulations.:  68%|██████▊   | 3408/5000 [00:23<00:10, 146.22it/s]Running 5000 simulations.:  68%|██████▊   | 3423/5000 [00:23<00:10, 146.07it/s]Running 5000 simulations.:  69%|██████▉   | 3438/5000 [00:23<00:10, 145.92it/s]Running 5000 simulations.:  69%|██████▉   | 3453/5000 [00:23<00:10, 145.62it/s]Running 5000 simulations.:  69%|██████▉   | 3468/5000 [00:23<00:10, 145.39it/s]Running 5000 simulations.:  70%|██████▉   | 3483/5000 [00:23<00:10, 144.99it/s]Running 5000 simulations.:  70%|██████▉   | 3498/5000 [00:23<00:10, 144.99it/s]Running 5000 simulations.:  70%|███████   | 3513/5000 [00:24<00:10, 145.30it/s]Running 5000 simulations.:  71%|███████   | 3528/5000 [00:24<00:10, 145.48it/s]Running 5000 simulations.:  71%|███████   | 3543/5000 [00:24<00:10, 145.51it/s]Running 5000 simulations.:  71%|███████   | 3558/5000 [00:24<00:09, 145.53it/s]Running 5000 simulations.:  71%|███████▏  | 3573/5000 [00:24<00:09, 145.99it/s]Running 5000 simulations.:  72%|███████▏  | 3588/5000 [00:24<00:09, 146.13it/s]Running 5000 simulations.:  72%|███████▏  | 3603/5000 [00:24<00:09, 146.50it/s]Running 5000 simulations.:  72%|███████▏  | 3618/5000 [00:24<00:09, 146.80it/s]Running 5000 simulations.:  73%|███████▎  | 3633/5000 [00:24<00:09, 146.67it/s]Running 5000 simulations.:  73%|███████▎  | 3648/5000 [00:24<00:09, 146.93it/s]Running 5000 simulations.:  73%|███████▎  | 3663/5000 [00:25<00:09, 147.52it/s]Running 5000 simulations.:  74%|███████▎  | 3678/5000 [00:25<00:08, 147.81it/s]Running 5000 simulations.:  74%|███████▍  | 3693/5000 [00:25<00:08, 147.83it/s]Running 5000 simulations.:  74%|███████▍  | 3708/5000 [00:25<00:08, 147.82it/s]Running 5000 simulations.:  74%|███████▍  | 3723/5000 [00:25<00:08, 147.74it/s]Running 5000 simulations.:  75%|███████▍  | 3738/5000 [00:25<00:08, 147.41it/s]Running 5000 simulations.:  75%|███████▌  | 3753/5000 [00:25<00:08, 147.43it/s]Running 5000 simulations.:  75%|███████▌  | 3768/5000 [00:25<00:08, 147.40it/s]Running 5000 simulations.:  76%|███████▌  | 3783/5000 [00:25<00:08, 147.34it/s]Running 5000 simulations.:  76%|███████▌  | 3798/5000 [00:25<00:08, 147.16it/s]Running 5000 simulations.:  76%|███████▋  | 3813/5000 [00:26<00:08, 147.12it/s]Running 5000 simulations.:  77%|███████▋  | 3828/5000 [00:26<00:07, 147.77it/s]Running 5000 simulations.:  77%|███████▋  | 3843/5000 [00:26<00:07, 148.33it/s]Running 5000 simulations.:  77%|███████▋  | 3858/5000 [00:26<00:07, 148.71it/s]Running 5000 simulations.:  77%|███████▋  | 3873/5000 [00:26<00:07, 148.12it/s]Running 5000 simulations.:  78%|███████▊  | 3888/5000 [00:26<00:07, 147.86it/s]Running 5000 simulations.:  78%|███████▊  | 3903/5000 [00:26<00:07, 147.48it/s]Running 5000 simulations.:  78%|███████▊  | 3918/5000 [00:26<00:07, 147.56it/s]Running 5000 simulations.:  79%|███████▊  | 3933/5000 [00:26<00:07, 147.62it/s]Running 5000 simulations.:  79%|███████▉  | 3948/5000 [00:27<00:07, 147.64it/s]Running 5000 simulations.:  79%|███████▉  | 3963/5000 [00:27<00:07, 147.32it/s]Running 5000 simulations.:  80%|███████▉  | 3978/5000 [00:27<00:06, 147.06it/s]Running 5000 simulations.:  80%|███████▉  | 3993/5000 [00:27<00:06, 147.02it/s]Running 5000 simulations.:  80%|████████  | 4008/5000 [00:27<00:06, 146.81it/s]Running 5000 simulations.:  80%|████████  | 4023/5000 [00:27<00:06, 146.83it/s]Running 5000 simulations.:  81%|████████  | 4038/5000 [00:27<00:06, 146.95it/s]Running 5000 simulations.:  81%|████████  | 4053/5000 [00:27<00:06, 147.01it/s]Running 5000 simulations.:  81%|████████▏ | 4068/5000 [00:27<00:06, 147.57it/s]Running 5000 simulations.:  82%|████████▏ | 4083/5000 [00:27<00:06, 147.76it/s]Running 5000 simulations.:  82%|████████▏ | 4098/5000 [00:28<00:06, 147.96it/s]Running 5000 simulations.:  82%|████████▏ | 4113/5000 [00:28<00:06, 147.66it/s]Running 5000 simulations.:  83%|████████▎ | 4128/5000 [00:28<00:05, 147.42it/s]Running 5000 simulations.:  83%|████████▎ | 4143/5000 [00:28<00:05, 147.45it/s]Running 5000 simulations.:  83%|████████▎ | 4158/5000 [00:28<00:05, 147.37it/s]Running 5000 simulations.:  83%|████████▎ | 4173/5000 [00:28<00:05, 147.57it/s]Running 5000 simulations.:  84%|████████▍ | 4188/5000 [00:28<00:05, 147.68it/s]Running 5000 simulations.:  84%|████████▍ | 4203/5000 [00:28<00:05, 147.49it/s]Running 5000 simulations.:  84%|████████▍ | 4218/5000 [00:28<00:05, 147.67it/s]Running 5000 simulations.:  85%|████████▍ | 4233/5000 [00:28<00:05, 147.78it/s]Running 5000 simulations.:  85%|████████▍ | 4248/5000 [00:29<00:05, 148.01it/s]Running 5000 simulations.:  85%|████████▌ | 4263/5000 [00:29<00:04, 148.07it/s]Running 5000 simulations.:  86%|████████▌ | 4279/5000 [00:29<00:04, 148.64it/s]Running 5000 simulations.:  86%|████████▌ | 4295/5000 [00:29<00:04, 150.43it/s]Running 5000 simulations.:  86%|████████▌ | 4312/5000 [00:29<00:04, 154.93it/s]Running 5000 simulations.:  87%|████████▋ | 4328/5000 [00:29<00:04, 153.61it/s]Running 5000 simulations.:  87%|████████▋ | 4344/5000 [00:29<00:04, 153.51it/s]Running 5000 simulations.:  87%|████████▋ | 4360/5000 [00:29<00:04, 152.88it/s]Running 5000 simulations.:  88%|████████▊ | 4376/5000 [00:29<00:04, 152.25it/s]Running 5000 simulations.:  88%|████████▊ | 4392/5000 [00:29<00:04, 151.03it/s]Running 5000 simulations.:  88%|████████▊ | 4408/5000 [00:30<00:03, 150.22it/s]Running 5000 simulations.:  88%|████████▊ | 4424/5000 [00:30<00:03, 148.94it/s]Running 5000 simulations.:  89%|████████▉ | 4439/5000 [00:30<00:03, 148.98it/s]Running 5000 simulations.:  89%|████████▉ | 4454/5000 [00:30<00:03, 148.89it/s]Running 5000 simulations.:  89%|████████▉ | 4470/5000 [00:30<00:03, 150.14it/s]Running 5000 simulations.:  90%|████████▉ | 4486/5000 [00:30<00:03, 150.83it/s]Running 5000 simulations.:  90%|█████████ | 4502/5000 [00:30<00:03, 151.15it/s]Running 5000 simulations.:  90%|█████████ | 4518/5000 [00:30<00:03, 150.58it/s]Running 5000 simulations.:  91%|█████████ | 4534/5000 [00:30<00:03, 149.11it/s]Running 5000 simulations.:  91%|█████████ | 4549/5000 [00:31<00:03, 149.11it/s]Running 5000 simulations.:  91%|█████████▏| 4564/5000 [00:31<00:02, 148.92it/s]Running 5000 simulations.:  92%|█████████▏| 4579/5000 [00:31<00:02, 147.51it/s]Running 5000 simulations.:  92%|█████████▏| 4594/5000 [00:31<00:02, 146.44it/s]Running 5000 simulations.:  92%|█████████▏| 4609/5000 [00:31<00:02, 147.26it/s]Running 5000 simulations.:  92%|█████████▏| 4624/5000 [00:31<00:02, 140.07it/s]Running 5000 simulations.:  93%|█████████▎| 4640/5000 [00:31<00:02, 143.50it/s]Running 5000 simulations.:  93%|█████████▎| 4656/5000 [00:31<00:02, 145.82it/s]Running 5000 simulations.:  93%|█████████▎| 4671/5000 [00:31<00:02, 146.85it/s]Running 5000 simulations.:  94%|█████████▎| 4687/5000 [00:31<00:02, 147.90it/s]Running 5000 simulations.:  94%|█████████▍| 4702/5000 [00:32<00:02, 148.25it/s]Running 5000 simulations.:  94%|█████████▍| 4717/5000 [00:32<00:01, 146.53it/s]Running 5000 simulations.:  95%|█████████▍| 4732/5000 [00:32<00:01, 146.59it/s]Running 5000 simulations.:  95%|█████████▍| 4747/5000 [00:32<00:01, 147.19it/s]Running 5000 simulations.:  95%|█████████▌| 4762/5000 [00:32<00:01, 147.91it/s]Running 5000 simulations.:  96%|█████████▌| 4778/5000 [00:32<00:01, 149.29it/s]Running 5000 simulations.:  96%|█████████▌| 4794/5000 [00:32<00:01, 149.90it/s]Running 5000 simulations.:  96%|█████████▌| 4810/5000 [00:32<00:01, 150.73it/s]Running 5000 simulations.:  97%|█████████▋| 4826/5000 [00:32<00:01, 150.65it/s]Running 5000 simulations.:  97%|█████████▋| 4842/5000 [00:33<00:01, 150.62it/s]Running 5000 simulations.:  97%|█████████▋| 4858/5000 [00:33<00:00, 150.38it/s]Running 5000 simulations.:  97%|█████████▋| 4874/5000 [00:33<00:00, 148.75it/s]Running 5000 simulations.:  98%|█████████▊| 4889/5000 [00:33<00:00, 147.46it/s]Running 5000 simulations.:  98%|█████████▊| 4904/5000 [00:33<00:00, 146.84it/s]Running 5000 simulations.:  98%|█████████▊| 4919/5000 [00:33<00:00, 146.57it/s]Running 5000 simulations.:  99%|█████████▊| 4935/5000 [00:33<00:00, 147.98it/s]Running 5000 simulations.:  99%|█████████▉| 4950/5000 [00:33<00:00, 148.38it/s]Running 5000 simulations.:  99%|█████████▉| 4966/5000 [00:33<00:00, 149.30it/s]Running 5000 simulations.: 100%|█████████▉| 4982/5000 [00:33<00:00, 150.08it/s]Running 5000 simulations.: 100%|█████████▉| 4998/5000 [00:34<00:00, 150.04it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:34<00:00, 146.67it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:31, 156.66it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:31, 157.64it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:31, 157.96it/s]Running 5000 simulations.:   1%|▏         | 64/5000 [00:00<00:31, 157.46it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:31, 156.90it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:31, 155.12it/s]Running 5000 simulations.:   2%|▏         | 112/5000 [00:00<00:31, 155.10it/s]Running 5000 simulations.:   3%|▎         | 128/5000 [00:00<00:31, 156.51it/s]Running 5000 simulations.:   3%|▎         | 144/5000 [00:00<00:31, 156.27it/s]Running 5000 simulations.:   3%|▎         | 160/5000 [00:01<00:31, 156.12it/s]Running 5000 simulations.:   4%|▎         | 176/5000 [00:01<00:30, 156.54it/s]Running 5000 simulations.:   4%|▍         | 192/5000 [00:01<00:30, 156.60it/s]Running 5000 simulations.:   4%|▍         | 208/5000 [00:01<00:30, 156.70it/s]Running 5000 simulations.:   4%|▍         | 224/5000 [00:01<00:30, 156.31it/s]Running 5000 simulations.:   5%|▍         | 240/5000 [00:01<00:30, 154.67it/s]Running 5000 simulations.:   5%|▌         | 258/5000 [00:01<00:29, 159.66it/s]Running 5000 simulations.:   6%|▌         | 276/5000 [00:01<00:28, 164.11it/s]Running 5000 simulations.:   6%|▌         | 294/5000 [00:01<00:28, 167.06it/s]Running 5000 simulations.:   6%|▌         | 311/5000 [00:01<00:28, 162.65it/s]Running 5000 simulations.:   7%|▋         | 328/5000 [00:02<00:28, 161.44it/s]Running 5000 simulations.:   7%|▋         | 345/5000 [00:02<00:29, 158.22it/s]Running 5000 simulations.:   7%|▋         | 361/5000 [00:02<00:29, 157.18it/s]Running 5000 simulations.:   8%|▊         | 377/5000 [00:02<00:29, 156.37it/s]Running 5000 simulations.:   8%|▊         | 393/5000 [00:02<00:29, 155.28it/s]Running 5000 simulations.:   8%|▊         | 409/5000 [00:02<00:29, 154.56it/s]Running 5000 simulations.:   8%|▊         | 425/5000 [00:02<00:29, 154.10it/s]Running 5000 simulations.:   9%|▉         | 441/5000 [00:02<00:29, 152.86it/s]Running 5000 simulations.:   9%|▉         | 457/5000 [00:02<00:29, 152.81it/s]Running 5000 simulations.:   9%|▉         | 473/5000 [00:03<00:29, 153.93it/s]Running 5000 simulations.:  10%|▉         | 489/5000 [00:03<00:29, 153.74it/s]Running 5000 simulations.:  10%|█         | 505/5000 [00:03<00:29, 153.88it/s]Running 5000 simulations.:  10%|█         | 521/5000 [00:03<00:29, 153.91it/s]Running 5000 simulations.:  11%|█         | 537/5000 [00:03<00:28, 154.14it/s]Running 5000 simulations.:  11%|█         | 553/5000 [00:03<00:28, 153.85it/s]Running 5000 simulations.:  11%|█▏        | 569/5000 [00:03<00:28, 153.53it/s]Running 5000 simulations.:  12%|█▏        | 585/5000 [00:03<00:28, 152.43it/s]Running 5000 simulations.:  12%|█▏        | 601/5000 [00:03<00:28, 151.94it/s]Running 5000 simulations.:  12%|█▏        | 617/5000 [00:03<00:28, 151.65it/s]Running 5000 simulations.:  13%|█▎        | 633/5000 [00:04<00:28, 150.73it/s]Running 5000 simulations.:  13%|█▎        | 649/5000 [00:04<00:29, 149.56it/s]Running 5000 simulations.:  13%|█▎        | 665/5000 [00:04<00:28, 150.41it/s]Running 5000 simulations.:  14%|█▎        | 681/5000 [00:04<00:28, 151.48it/s]Running 5000 simulations.:  14%|█▍        | 697/5000 [00:04<00:28, 152.01it/s]Running 5000 simulations.:  14%|█▍        | 713/5000 [00:04<00:28, 151.96it/s]Running 5000 simulations.:  15%|█▍        | 729/5000 [00:04<00:28, 150.86it/s]Running 5000 simulations.:  15%|█▍        | 745/5000 [00:04<00:28, 151.64it/s]Running 5000 simulations.:  15%|█▌        | 761/5000 [00:04<00:27, 152.59it/s]Running 5000 simulations.:  16%|█▌        | 777/5000 [00:05<00:27, 152.34it/s]Running 5000 simulations.:  16%|█▌        | 793/5000 [00:05<00:27, 152.25it/s]Running 5000 simulations.:  16%|█▌        | 809/5000 [00:05<00:27, 152.47it/s]Running 5000 simulations.:  16%|█▋        | 825/5000 [00:05<00:27, 152.22it/s]Running 5000 simulations.:  17%|█▋        | 841/5000 [00:05<00:27, 153.08it/s]Running 5000 simulations.:  17%|█▋        | 857/5000 [00:05<00:26, 153.47it/s]Running 5000 simulations.:  17%|█▋        | 873/5000 [00:05<00:26, 153.41it/s]Running 5000 simulations.:  18%|█▊        | 889/5000 [00:05<00:26, 153.22it/s]Running 5000 simulations.:  18%|█▊        | 905/5000 [00:05<00:26, 152.37it/s]Running 5000 simulations.:  18%|█▊        | 921/5000 [00:05<00:26, 151.96it/s]Running 5000 simulations.:  19%|█▊        | 937/5000 [00:06<00:26, 152.70it/s]Running 5000 simulations.:  19%|█▉        | 953/5000 [00:06<00:26, 152.89it/s]Running 5000 simulations.:  19%|█▉        | 969/5000 [00:06<00:26, 152.71it/s]Running 5000 simulations.:  20%|█▉        | 985/5000 [00:06<00:26, 152.67it/s]Running 5000 simulations.:  20%|██        | 1001/5000 [00:06<00:26, 152.06it/s]Running 5000 simulations.:  20%|██        | 1017/5000 [00:06<00:26, 151.31it/s]Running 5000 simulations.:  21%|██        | 1033/5000 [00:06<00:26, 151.12it/s]Running 5000 simulations.:  21%|██        | 1049/5000 [00:06<00:26, 150.89it/s]Running 5000 simulations.:  21%|██▏       | 1065/5000 [00:06<00:26, 150.36it/s]Running 5000 simulations.:  22%|██▏       | 1081/5000 [00:07<00:25, 151.07it/s]Running 5000 simulations.:  22%|██▏       | 1097/5000 [00:07<00:25, 151.72it/s]Running 5000 simulations.:  22%|██▏       | 1113/5000 [00:07<00:25, 151.82it/s]Running 5000 simulations.:  23%|██▎       | 1129/5000 [00:07<00:25, 150.76it/s]Running 5000 simulations.:  23%|██▎       | 1145/5000 [00:07<00:25, 150.67it/s]Running 5000 simulations.:  23%|██▎       | 1161/5000 [00:07<00:25, 151.19it/s]Running 5000 simulations.:  24%|██▎       | 1177/5000 [00:07<00:25, 150.79it/s]Running 5000 simulations.:  24%|██▍       | 1193/5000 [00:07<00:25, 150.85it/s]Running 5000 simulations.:  24%|██▍       | 1209/5000 [00:07<00:25, 151.30it/s]Running 5000 simulations.:  24%|██▍       | 1225/5000 [00:07<00:24, 151.96it/s]Running 5000 simulations.:  25%|██▍       | 1241/5000 [00:08<00:24, 152.40it/s]Running 5000 simulations.:  25%|██▌       | 1257/5000 [00:08<00:24, 152.97it/s]Running 5000 simulations.:  25%|██▌       | 1273/5000 [00:08<00:24, 152.54it/s]Running 5000 simulations.:  26%|██▌       | 1289/5000 [00:08<00:24, 153.32it/s]Running 5000 simulations.:  26%|██▌       | 1305/5000 [00:08<00:24, 152.81it/s]Running 5000 simulations.:  26%|██▋       | 1321/5000 [00:08<00:24, 152.59it/s]Running 5000 simulations.:  27%|██▋       | 1337/5000 [00:08<00:24, 152.49it/s]Running 5000 simulations.:  27%|██▋       | 1353/5000 [00:08<00:24, 151.27it/s]Running 5000 simulations.:  27%|██▋       | 1369/5000 [00:08<00:23, 151.75it/s]Running 5000 simulations.:  28%|██▊       | 1385/5000 [00:09<00:23, 153.53it/s]Running 5000 simulations.:  28%|██▊       | 1401/5000 [00:09<00:23, 151.68it/s]Running 5000 simulations.:  28%|██▊       | 1417/5000 [00:09<00:23, 151.84it/s]Running 5000 simulations.:  29%|██▊       | 1433/5000 [00:09<00:23, 151.95it/s]Running 5000 simulations.:  29%|██▉       | 1449/5000 [00:09<00:23, 152.07it/s]Running 5000 simulations.:  29%|██▉       | 1465/5000 [00:09<00:23, 152.53it/s]Running 5000 simulations.:  30%|██▉       | 1481/5000 [00:09<00:23, 152.21it/s]Running 5000 simulations.:  30%|██▉       | 1497/5000 [00:09<00:22, 152.66it/s]Running 5000 simulations.:  30%|███       | 1515/5000 [00:09<00:22, 158.24it/s]Running 5000 simulations.:  31%|███       | 1533/5000 [00:09<00:21, 162.66it/s]Running 5000 simulations.:  31%|███       | 1550/5000 [00:10<00:20, 164.35it/s]Running 5000 simulations.:  31%|███▏      | 1567/5000 [00:10<00:21, 160.19it/s]Running 5000 simulations.:  32%|███▏      | 1584/5000 [00:10<00:21, 159.25it/s]Running 5000 simulations.:  32%|███▏      | 1600/5000 [00:10<00:21, 156.87it/s]Running 5000 simulations.:  32%|███▏      | 1616/5000 [00:10<00:21, 155.91it/s]Running 5000 simulations.:  33%|███▎      | 1632/5000 [00:10<00:21, 154.78it/s]Running 5000 simulations.:  33%|███▎      | 1648/5000 [00:10<00:21, 154.45it/s]Running 5000 simulations.:  33%|███▎      | 1664/5000 [00:10<00:21, 154.20it/s]Running 5000 simulations.:  34%|███▎      | 1680/5000 [00:10<00:21, 153.85it/s]Running 5000 simulations.:  34%|███▍      | 1696/5000 [00:11<00:21, 154.20it/s]Running 5000 simulations.:  34%|███▍      | 1712/5000 [00:11<00:21, 153.45it/s]Running 5000 simulations.:  35%|███▍      | 1728/5000 [00:11<00:21, 152.36it/s]Running 5000 simulations.:  35%|███▍      | 1744/5000 [00:11<00:21, 152.40it/s]Running 5000 simulations.:  35%|███▌      | 1760/5000 [00:11<00:21, 151.99it/s]Running 5000 simulations.:  36%|███▌      | 1776/5000 [00:11<00:21, 152.11it/s]Running 5000 simulations.:  36%|███▌      | 1792/5000 [00:11<00:21, 152.19it/s]Running 5000 simulations.:  36%|███▌      | 1808/5000 [00:11<00:20, 152.19it/s]Running 5000 simulations.:  36%|███▋      | 1824/5000 [00:11<00:20, 151.63it/s]Running 5000 simulations.:  37%|███▋      | 1840/5000 [00:11<00:20, 151.00it/s]Running 5000 simulations.:  37%|███▋      | 1856/5000 [00:12<00:20, 150.64it/s]Running 5000 simulations.:  37%|███▋      | 1872/5000 [00:12<00:20, 150.25it/s]Running 5000 simulations.:  38%|███▊      | 1888/5000 [00:12<00:20, 149.84it/s]Running 5000 simulations.:  38%|███▊      | 1904/5000 [00:12<00:20, 150.49it/s]Running 5000 simulations.:  38%|███▊      | 1920/5000 [00:12<00:20, 150.93it/s]Running 5000 simulations.:  39%|███▊      | 1936/5000 [00:12<00:20, 150.94it/s]Running 5000 simulations.:  39%|███▉      | 1952/5000 [00:12<00:20, 150.72it/s]Running 5000 simulations.:  39%|███▉      | 1968/5000 [00:12<00:20, 150.25it/s]Running 5000 simulations.:  40%|███▉      | 1984/5000 [00:12<00:20, 150.07it/s]Running 5000 simulations.:  40%|████      | 2000/5000 [00:13<00:19, 150.02it/s]Running 5000 simulations.:  40%|████      | 2016/5000 [00:13<00:19, 149.75it/s]Running 5000 simulations.:  41%|████      | 2031/5000 [00:13<00:19, 148.73it/s]Running 5000 simulations.:  41%|████      | 2046/5000 [00:13<00:19, 148.72it/s]Running 5000 simulations.:  41%|████      | 2062/5000 [00:13<00:19, 150.32it/s]Running 5000 simulations.:  42%|████▏     | 2078/5000 [00:13<00:19, 150.34it/s]Running 5000 simulations.:  42%|████▏     | 2094/5000 [00:13<00:19, 150.20it/s]Running 5000 simulations.:  42%|████▏     | 2110/5000 [00:13<00:19, 149.96it/s]Running 5000 simulations.:  42%|████▎     | 2125/5000 [00:13<00:19, 149.18it/s]Running 5000 simulations.:  43%|████▎     | 2140/5000 [00:13<00:19, 149.08it/s]Running 5000 simulations.:  43%|████▎     | 2155/5000 [00:14<00:19, 149.17it/s]Running 5000 simulations.:  43%|████▎     | 2171/5000 [00:14<00:18, 149.95it/s]Running 5000 simulations.:  44%|████▎     | 2187/5000 [00:14<00:18, 149.94it/s]Running 5000 simulations.:  44%|████▍     | 2202/5000 [00:14<00:18, 148.78it/s]Running 5000 simulations.:  44%|████▍     | 2217/5000 [00:14<00:18, 148.79it/s]Running 5000 simulations.:  45%|████▍     | 2233/5000 [00:14<00:18, 150.09it/s]Running 5000 simulations.:  45%|████▍     | 2249/5000 [00:14<00:18, 149.71it/s]Running 5000 simulations.:  45%|████▌     | 2265/5000 [00:14<00:18, 150.24it/s]Running 5000 simulations.:  46%|████▌     | 2281/5000 [00:14<00:18, 150.18it/s]Running 5000 simulations.:  46%|████▌     | 2297/5000 [00:15<00:17, 150.66it/s]Running 5000 simulations.:  46%|████▋     | 2313/5000 [00:15<00:17, 150.84it/s]Running 5000 simulations.:  47%|████▋     | 2329/5000 [00:15<00:17, 150.98it/s]Running 5000 simulations.:  47%|████▋     | 2345/5000 [00:15<00:17, 150.48it/s]Running 5000 simulations.:  47%|████▋     | 2361/5000 [00:15<00:17, 149.88it/s]Running 5000 simulations.:  48%|████▊     | 2376/5000 [00:15<00:17, 149.66it/s]Running 5000 simulations.:  48%|████▊     | 2391/5000 [00:15<00:17, 149.22it/s]Running 5000 simulations.:  48%|████▊     | 2406/5000 [00:15<00:18, 140.68it/s]Running 5000 simulations.:  48%|████▊     | 2422/5000 [00:15<00:17, 143.81it/s]Running 5000 simulations.:  49%|████▉     | 2438/5000 [00:15<00:17, 146.19it/s]Running 5000 simulations.:  49%|████▉     | 2454/5000 [00:16<00:17, 147.52it/s]Running 5000 simulations.:  49%|████▉     | 2470/5000 [00:16<00:17, 148.23it/s]Running 5000 simulations.:  50%|████▉     | 2486/5000 [00:16<00:16, 148.92it/s]Running 5000 simulations.:  50%|█████     | 2501/5000 [00:16<00:16, 147.93it/s]Running 5000 simulations.:  50%|█████     | 2516/5000 [00:16<00:16, 148.07it/s]Running 5000 simulations.:  51%|█████     | 2532/5000 [00:16<00:16, 149.10it/s]Running 5000 simulations.:  51%|█████     | 2548/5000 [00:16<00:16, 149.70it/s]Running 5000 simulations.:  51%|█████▏    | 2564/5000 [00:16<00:16, 150.12it/s]Running 5000 simulations.:  52%|█████▏    | 2580/5000 [00:16<00:16, 150.15it/s]Running 5000 simulations.:  52%|█████▏    | 2596/5000 [00:17<00:16, 150.18it/s]Running 5000 simulations.:  52%|█████▏    | 2612/5000 [00:17<00:15, 150.17it/s]Running 5000 simulations.:  53%|█████▎    | 2628/5000 [00:17<00:15, 149.18it/s]Running 5000 simulations.:  53%|█████▎    | 2643/5000 [00:17<00:15, 147.44it/s]Running 5000 simulations.:  53%|█████▎    | 2658/5000 [00:17<00:15, 147.83it/s]Running 5000 simulations.:  53%|█████▎    | 2674/5000 [00:17<00:15, 149.24it/s]Running 5000 simulations.:  54%|█████▍    | 2690/5000 [00:17<00:15, 149.86it/s]Running 5000 simulations.:  54%|█████▍    | 2705/5000 [00:17<00:15, 149.58it/s]Running 5000 simulations.:  54%|█████▍    | 2721/5000 [00:17<00:15, 150.01it/s]Running 5000 simulations.:  55%|█████▍    | 2737/5000 [00:17<00:15, 150.63it/s]Running 5000 simulations.:  55%|█████▌    | 2753/5000 [00:18<00:14, 150.92it/s]Running 5000 simulations.:  55%|█████▌    | 2769/5000 [00:18<00:14, 150.31it/s]Running 5000 simulations.:  56%|█████▌    | 2785/5000 [00:18<00:14, 149.26it/s]Running 5000 simulations.:  56%|█████▌    | 2802/5000 [00:18<00:14, 154.08it/s]Running 5000 simulations.:  56%|█████▋    | 2819/5000 [00:18<00:13, 158.40it/s]Running 5000 simulations.:  57%|█████▋    | 2836/5000 [00:18<00:13, 161.37it/s]Running 5000 simulations.:  57%|█████▋    | 2853/5000 [00:18<00:13, 157.39it/s]Running 5000 simulations.:  57%|█████▋    | 2869/5000 [00:18<00:13, 154.41it/s]Running 5000 simulations.:  58%|█████▊    | 2885/5000 [00:18<00:13, 151.63it/s]Running 5000 simulations.:  58%|█████▊    | 2901/5000 [00:19<00:13, 151.10it/s]Running 5000 simulations.:  58%|█████▊    | 2917/5000 [00:19<00:13, 150.64it/s]Running 5000 simulations.:  59%|█████▊    | 2933/5000 [00:19<00:13, 150.76it/s]Running 5000 simulations.:  59%|█████▉    | 2949/5000 [00:19<00:13, 150.91it/s]Running 5000 simulations.:  59%|█████▉    | 2965/5000 [00:19<00:13, 150.63it/s]Running 5000 simulations.:  60%|█████▉    | 2981/5000 [00:19<00:13, 149.51it/s]Running 5000 simulations.:  60%|█████▉    | 2996/5000 [00:19<00:13, 149.43it/s]Running 5000 simulations.:  60%|██████    | 3012/5000 [00:19<00:13, 151.34it/s]Running 5000 simulations.:  61%|██████    | 3028/5000 [00:19<00:13, 150.80it/s]Running 5000 simulations.:  61%|██████    | 3044/5000 [00:19<00:12, 151.12it/s]Running 5000 simulations.:  61%|██████    | 3060/5000 [00:20<00:12, 150.10it/s]Running 5000 simulations.:  62%|██████▏   | 3076/5000 [00:20<00:12, 150.48it/s]Running 5000 simulations.:  62%|██████▏   | 3092/5000 [00:20<00:12, 150.66it/s]Running 5000 simulations.:  62%|██████▏   | 3108/5000 [00:20<00:12, 150.80it/s]Running 5000 simulations.:  62%|██████▏   | 3124/5000 [00:20<00:12, 150.56it/s]Running 5000 simulations.:  63%|██████▎   | 3140/5000 [00:20<00:12, 150.76it/s]Running 5000 simulations.:  63%|██████▎   | 3156/5000 [00:20<00:12, 150.43it/s]Running 5000 simulations.:  63%|██████▎   | 3172/5000 [00:20<00:12, 150.46it/s]Running 5000 simulations.:  64%|██████▍   | 3188/5000 [00:20<00:12, 150.75it/s]Running 5000 simulations.:  64%|██████▍   | 3204/5000 [00:21<00:11, 152.04it/s]Running 5000 simulations.:  64%|██████▍   | 3220/5000 [00:21<00:11, 152.47it/s]Running 5000 simulations.:  65%|██████▍   | 3236/5000 [00:21<00:11, 152.37it/s]Running 5000 simulations.:  65%|██████▌   | 3252/5000 [00:21<00:11, 151.85it/s]Running 5000 simulations.:  65%|██████▌   | 3268/5000 [00:21<00:11, 151.66it/s]Running 5000 simulations.:  66%|██████▌   | 3284/5000 [00:21<00:11, 151.56it/s]Running 5000 simulations.:  66%|██████▌   | 3300/5000 [00:21<00:11, 150.14it/s]Running 5000 simulations.:  66%|██████▋   | 3316/5000 [00:21<00:11, 149.95it/s]Running 5000 simulations.:  67%|██████▋   | 3332/5000 [00:21<00:11, 151.11it/s]Running 5000 simulations.:  67%|██████▋   | 3348/5000 [00:21<00:10, 151.14it/s]Running 5000 simulations.:  67%|██████▋   | 3364/5000 [00:22<00:10, 150.52it/s]Running 5000 simulations.:  68%|██████▊   | 3380/5000 [00:22<00:10, 150.46it/s]Running 5000 simulations.:  68%|██████▊   | 3396/5000 [00:22<00:10, 150.20it/s]Running 5000 simulations.:  68%|██████▊   | 3412/5000 [00:22<00:10, 150.53it/s]Running 5000 simulations.:  69%|██████▊   | 3428/5000 [00:22<00:10, 150.62it/s]Running 5000 simulations.:  69%|██████▉   | 3444/5000 [00:22<00:10, 149.28it/s]Running 5000 simulations.:  69%|██████▉   | 3459/5000 [00:22<00:10, 149.39it/s]Running 5000 simulations.:  70%|██████▉   | 3475/5000 [00:22<00:10, 149.75it/s]Running 5000 simulations.:  70%|██████▉   | 3490/5000 [00:22<00:10, 148.35it/s]Running 5000 simulations.:  70%|███████   | 3505/5000 [00:23<00:10, 145.89it/s]Running 5000 simulations.:  70%|███████   | 3520/5000 [00:23<00:10, 146.53it/s]Running 5000 simulations.:  71%|███████   | 3536/5000 [00:23<00:09, 147.98it/s]Running 5000 simulations.:  71%|███████   | 3552/5000 [00:23<00:09, 148.75it/s]Running 5000 simulations.:  71%|███████▏  | 3568/5000 [00:23<00:09, 149.06it/s]Running 5000 simulations.:  72%|███████▏  | 3583/5000 [00:23<00:09, 148.40it/s]Running 5000 simulations.:  72%|███████▏  | 3598/5000 [00:23<00:09, 147.96it/s]Running 5000 simulations.:  72%|███████▏  | 3614/5000 [00:23<00:09, 149.28it/s]Running 5000 simulations.:  73%|███████▎  | 3629/5000 [00:23<00:09, 149.13it/s]Running 5000 simulations.:  73%|███████▎  | 3645/5000 [00:23<00:09, 150.02it/s]Running 5000 simulations.:  73%|███████▎  | 3661/5000 [00:24<00:08, 150.17it/s]Running 5000 simulations.:  74%|███████▎  | 3677/5000 [00:24<00:08, 150.21it/s]Running 5000 simulations.:  74%|███████▍  | 3693/5000 [00:24<00:08, 150.47it/s]Running 5000 simulations.:  74%|███████▍  | 3709/5000 [00:24<00:08, 150.16it/s]Running 5000 simulations.:  74%|███████▍  | 3725/5000 [00:24<00:08, 150.02it/s]Running 5000 simulations.:  75%|███████▍  | 3741/5000 [00:24<00:08, 149.42it/s]Running 5000 simulations.:  75%|███████▌  | 3756/5000 [00:24<00:08, 149.19it/s]Running 5000 simulations.:  75%|███████▌  | 3772/5000 [00:24<00:08, 150.26it/s]Running 5000 simulations.:  76%|███████▌  | 3788/5000 [00:24<00:07, 151.50it/s]Running 5000 simulations.:  76%|███████▌  | 3804/5000 [00:25<00:07, 150.78it/s]Running 5000 simulations.:  76%|███████▋  | 3820/5000 [00:25<00:07, 151.26it/s]Running 5000 simulations.:  77%|███████▋  | 3836/5000 [00:25<00:07, 150.89it/s]Running 5000 simulations.:  77%|███████▋  | 3852/5000 [00:25<00:07, 151.40it/s]Running 5000 simulations.:  77%|███████▋  | 3868/5000 [00:25<00:07, 150.95it/s]Running 5000 simulations.:  78%|███████▊  | 3884/5000 [00:25<00:07, 151.00it/s]Running 5000 simulations.:  78%|███████▊  | 3900/5000 [00:25<00:07, 150.57it/s]Running 5000 simulations.:  78%|███████▊  | 3916/5000 [00:25<00:07, 150.27it/s]Running 5000 simulations.:  79%|███████▊  | 3932/5000 [00:25<00:07, 150.64it/s]Running 5000 simulations.:  79%|███████▉  | 3948/5000 [00:25<00:07, 150.27it/s]Running 5000 simulations.:  79%|███████▉  | 3964/5000 [00:26<00:06, 150.48it/s]Running 5000 simulations.:  80%|███████▉  | 3980/5000 [00:26<00:06, 150.63it/s]Running 5000 simulations.:  80%|███████▉  | 3996/5000 [00:26<00:06, 150.52it/s]Running 5000 simulations.:  80%|████████  | 4012/5000 [00:26<00:06, 152.13it/s]Running 5000 simulations.:  81%|████████  | 4028/5000 [00:26<00:06, 151.65it/s]Running 5000 simulations.:  81%|████████  | 4044/5000 [00:26<00:06, 151.52it/s]Running 5000 simulations.:  81%|████████  | 4060/5000 [00:26<00:06, 151.55it/s]Running 5000 simulations.:  82%|████████▏ | 4076/5000 [00:26<00:06, 151.84it/s]Running 5000 simulations.:  82%|████████▏ | 4092/5000 [00:26<00:06, 151.18it/s]Running 5000 simulations.:  82%|████████▏ | 4108/5000 [00:27<00:05, 152.45it/s]Running 5000 simulations.:  82%|████████▎ | 4125/5000 [00:27<00:05, 156.86it/s]Running 5000 simulations.:  83%|████████▎ | 4143/5000 [00:27<00:05, 160.86it/s]Running 5000 simulations.:  83%|████████▎ | 4160/5000 [00:27<00:05, 161.84it/s]Running 5000 simulations.:  84%|████████▎ | 4177/5000 [00:27<00:05, 157.71it/s]Running 5000 simulations.:  84%|████████▍ | 4193/5000 [00:27<00:05, 156.03it/s]Running 5000 simulations.:  84%|████████▍ | 4209/5000 [00:27<00:05, 155.62it/s]Running 5000 simulations.:  84%|████████▍ | 4225/5000 [00:27<00:05, 154.35it/s]Running 5000 simulations.:  85%|████████▍ | 4241/5000 [00:27<00:04, 153.40it/s]Running 5000 simulations.:  85%|████████▌ | 4257/5000 [00:27<00:04, 153.17it/s]Running 5000 simulations.:  85%|████████▌ | 4273/5000 [00:28<00:04, 152.95it/s]Running 5000 simulations.:  86%|████████▌ | 4289/5000 [00:28<00:04, 152.65it/s]Running 5000 simulations.:  86%|████████▌ | 4305/5000 [00:28<00:04, 152.21it/s]Running 5000 simulations.:  86%|████████▋ | 4321/5000 [00:28<00:04, 150.37it/s]Running 5000 simulations.:  87%|████████▋ | 4337/5000 [00:28<00:04, 150.13it/s]Running 5000 simulations.:  87%|████████▋ | 4353/5000 [00:28<00:04, 151.06it/s]Running 5000 simulations.:  87%|████████▋ | 4369/5000 [00:28<00:04, 151.27it/s]Running 5000 simulations.:  88%|████████▊ | 4385/5000 [00:28<00:04, 151.08it/s]Running 5000 simulations.:  88%|████████▊ | 4401/5000 [00:28<00:03, 150.82it/s]Running 5000 simulations.:  88%|████████▊ | 4417/5000 [00:29<00:03, 150.73it/s]Running 5000 simulations.:  89%|████████▊ | 4433/5000 [00:29<00:03, 151.01it/s]Running 5000 simulations.:  89%|████████▉ | 4449/5000 [00:29<00:03, 150.85it/s]Running 5000 simulations.:  89%|████████▉ | 4465/5000 [00:29<00:03, 150.99it/s]Running 5000 simulations.:  90%|████████▉ | 4481/5000 [00:29<00:03, 150.87it/s]Running 5000 simulations.:  90%|████████▉ | 4497/5000 [00:29<00:03, 149.68it/s]Running 5000 simulations.:  90%|█████████ | 4513/5000 [00:29<00:03, 150.58it/s]Running 5000 simulations.:  91%|█████████ | 4529/5000 [00:29<00:03, 150.87it/s]Running 5000 simulations.:  91%|█████████ | 4545/5000 [00:29<00:03, 150.50it/s]Running 5000 simulations.:  91%|█████████ | 4561/5000 [00:30<00:02, 150.22it/s]Running 5000 simulations.:  92%|█████████▏| 4577/5000 [00:30<00:02, 150.45it/s]Running 5000 simulations.:  92%|█████████▏| 4593/5000 [00:30<00:02, 150.45it/s]Running 5000 simulations.:  92%|█████████▏| 4609/5000 [00:30<00:02, 150.78it/s]Running 5000 simulations.:  92%|█████████▎| 4625/5000 [00:30<00:02, 150.69it/s]Running 5000 simulations.:  93%|█████████▎| 4641/5000 [00:30<00:02, 150.78it/s]Running 5000 simulations.:  93%|█████████▎| 4657/5000 [00:30<00:02, 151.55it/s]Running 5000 simulations.:  93%|█████████▎| 4673/5000 [00:30<00:02, 151.49it/s]Running 5000 simulations.:  94%|█████████▍| 4689/5000 [00:30<00:02, 150.99it/s]Running 5000 simulations.:  94%|█████████▍| 4705/5000 [00:30<00:01, 151.96it/s]Running 5000 simulations.:  94%|█████████▍| 4721/5000 [00:31<00:01, 151.83it/s]Running 5000 simulations.:  95%|█████████▍| 4737/5000 [00:31<00:01, 152.13it/s]Running 5000 simulations.:  95%|█████████▌| 4753/5000 [00:31<00:01, 151.94it/s]Running 5000 simulations.:  95%|█████████▌| 4769/5000 [00:31<00:01, 152.02it/s]Running 5000 simulations.:  96%|█████████▌| 4785/5000 [00:31<00:01, 151.90it/s]Running 5000 simulations.:  96%|█████████▌| 4801/5000 [00:31<00:01, 151.49it/s]Running 5000 simulations.:  96%|█████████▋| 4817/5000 [00:31<00:01, 151.77it/s]Running 5000 simulations.:  97%|█████████▋| 4833/5000 [00:31<00:01, 152.11it/s]Running 5000 simulations.:  97%|█████████▋| 4849/5000 [00:31<00:00, 151.53it/s]Running 5000 simulations.:  97%|█████████▋| 4865/5000 [00:32<00:00, 150.65it/s]Running 5000 simulations.:  98%|█████████▊| 4881/5000 [00:32<00:00, 151.06it/s]Running 5000 simulations.:  98%|█████████▊| 4897/5000 [00:32<00:00, 151.49it/s]Running 5000 simulations.:  98%|█████████▊| 4913/5000 [00:32<00:00, 151.19it/s]Running 5000 simulations.:  99%|█████████▊| 4929/5000 [00:32<00:00, 150.66it/s]Running 5000 simulations.:  99%|█████████▉| 4945/5000 [00:32<00:00, 151.53it/s]Running 5000 simulations.:  99%|█████████▉| 4961/5000 [00:32<00:00, 152.27it/s]Running 5000 simulations.: 100%|█████████▉| 4977/5000 [00:32<00:00, 152.31it/s]Running 5000 simulations.: 100%|█████████▉| 4993/5000 [00:32<00:00, 152.52it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:32<00:00, 151.92it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 25/5000 [00:00<00:19, 249.72it/s]Running 5000 simulations.:   1%|          | 50/5000 [00:00<00:19, 248.57it/s]Running 5000 simulations.:   2%|▏         | 75/5000 [00:00<00:19, 248.14it/s]Running 5000 simulations.:   2%|▏         | 100/5000 [00:00<00:19, 248.08it/s]Running 5000 simulations.:   2%|▎         | 125/5000 [00:00<00:19, 247.10it/s]Running 5000 simulations.:   3%|▎         | 150/5000 [00:00<00:19, 246.68it/s]Running 5000 simulations.:   4%|▎         | 175/5000 [00:00<00:19, 246.39it/s]Running 5000 simulations.:   4%|▍         | 200/5000 [00:00<00:19, 245.66it/s]Running 5000 simulations.:   4%|▍         | 225/5000 [00:00<00:19, 244.83it/s]Running 5000 simulations.:   5%|▍         | 249/5000 [00:01<00:19, 242.25it/s]Running 5000 simulations.:   5%|▌         | 273/5000 [00:01<00:19, 241.43it/s]Running 5000 simulations.:   6%|▌         | 298/5000 [00:01<00:19, 241.50it/s]Running 5000 simulations.:   6%|▋         | 323/5000 [00:01<00:19, 241.50it/s]Running 5000 simulations.:   7%|▋         | 348/5000 [00:01<00:19, 242.15it/s]Running 5000 simulations.:   7%|▋         | 373/5000 [00:01<00:19, 242.43it/s]Running 5000 simulations.:   8%|▊         | 398/5000 [00:01<00:19, 241.85it/s]Running 5000 simulations.:   8%|▊         | 423/5000 [00:01<00:19, 240.77it/s]Running 5000 simulations.:   9%|▉         | 448/5000 [00:01<00:19, 238.28it/s]Running 5000 simulations.:   9%|▉         | 473/5000 [00:01<00:18, 239.16it/s]Running 5000 simulations.:  10%|▉         | 498/5000 [00:02<00:18, 240.02it/s]Running 5000 simulations.:  10%|█         | 523/5000 [00:02<00:18, 240.65it/s]Running 5000 simulations.:  11%|█         | 548/5000 [00:02<00:18, 240.20it/s]Running 5000 simulations.:  11%|█▏        | 573/5000 [00:02<00:18, 240.75it/s]Running 5000 simulations.:  12%|█▏        | 598/5000 [00:02<00:18, 240.52it/s]Running 5000 simulations.:  12%|█▏        | 623/5000 [00:02<00:18, 239.96it/s]Running 5000 simulations.:  13%|█▎        | 647/5000 [00:02<00:18, 239.89it/s]Running 5000 simulations.:  13%|█▎        | 671/5000 [00:02<00:18, 239.86it/s]Running 5000 simulations.:  14%|█▍        | 696/5000 [00:02<00:17, 240.23it/s]Running 5000 simulations.:  14%|█▍        | 721/5000 [00:02<00:17, 239.26it/s]Running 5000 simulations.:  15%|█▍        | 745/5000 [00:03<00:17, 239.45it/s]Running 5000 simulations.:  15%|█▌        | 769/5000 [00:03<00:17, 239.20it/s]Running 5000 simulations.:  16%|█▌        | 793/5000 [00:03<00:17, 239.29it/s]Running 5000 simulations.:  16%|█▋        | 818/5000 [00:03<00:17, 239.51it/s]Running 5000 simulations.:  17%|█▋        | 842/5000 [00:03<00:17, 239.53it/s]Running 5000 simulations.:  17%|█▋        | 866/5000 [00:03<00:17, 239.08it/s]Running 5000 simulations.:  18%|█▊        | 890/5000 [00:03<00:17, 238.83it/s]Running 5000 simulations.:  18%|█▊        | 914/5000 [00:03<00:17, 238.82it/s]Running 5000 simulations.:  19%|█▉        | 938/5000 [00:03<00:16, 239.00it/s]Running 5000 simulations.:  19%|█▉        | 962/5000 [00:03<00:16, 239.10it/s]Running 5000 simulations.:  20%|█▉        | 986/5000 [00:04<00:16, 239.28it/s]Running 5000 simulations.:  20%|██        | 1010/5000 [00:04<00:16, 239.36it/s]Running 5000 simulations.:  21%|██        | 1034/5000 [00:04<00:16, 239.38it/s]Running 5000 simulations.:  21%|██        | 1058/5000 [00:04<00:16, 239.09it/s]Running 5000 simulations.:  22%|██▏       | 1082/5000 [00:04<00:16, 238.85it/s]Running 5000 simulations.:  22%|██▏       | 1106/5000 [00:04<00:16, 238.87it/s]Running 5000 simulations.:  23%|██▎       | 1130/5000 [00:04<00:16, 238.82it/s]Running 5000 simulations.:  23%|██▎       | 1155/5000 [00:04<00:16, 239.21it/s]Running 5000 simulations.:  24%|██▎       | 1180/5000 [00:04<00:15, 239.50it/s]Running 5000 simulations.:  24%|██▍       | 1204/5000 [00:05<00:15, 239.48it/s]Running 5000 simulations.:  25%|██▍       | 1229/5000 [00:05<00:15, 239.99it/s]Running 5000 simulations.:  25%|██▌       | 1253/5000 [00:05<00:15, 239.83it/s]Running 5000 simulations.:  26%|██▌       | 1277/5000 [00:05<00:15, 239.60it/s]Running 5000 simulations.:  26%|██▌       | 1302/5000 [00:05<00:15, 239.91it/s]Running 5000 simulations.:  27%|██▋       | 1327/5000 [00:05<00:15, 240.57it/s]Running 5000 simulations.:  27%|██▋       | 1352/5000 [00:05<00:15, 239.87it/s]Running 5000 simulations.:  28%|██▊       | 1376/5000 [00:05<00:15, 239.72it/s]Running 5000 simulations.:  28%|██▊       | 1401/5000 [00:05<00:15, 239.78it/s]Running 5000 simulations.:  28%|██▊       | 1425/5000 [00:05<00:14, 239.67it/s]Running 5000 simulations.:  29%|██▉       | 1450/5000 [00:06<00:14, 240.60it/s]Running 5000 simulations.:  30%|██▉       | 1475/5000 [00:06<00:14, 240.52it/s]Running 5000 simulations.:  30%|███       | 1500/5000 [00:06<00:14, 241.02it/s]Running 5000 simulations.:  30%|███       | 1525/5000 [00:06<00:14, 241.87it/s]Running 5000 simulations.:  31%|███       | 1550/5000 [00:06<00:14, 242.83it/s]Running 5000 simulations.:  32%|███▏      | 1575/5000 [00:06<00:14, 243.24it/s]Running 5000 simulations.:  32%|███▏      | 1600/5000 [00:06<00:13, 243.59it/s]Running 5000 simulations.:  32%|███▎      | 1625/5000 [00:06<00:13, 243.65it/s]Running 5000 simulations.:  33%|███▎      | 1650/5000 [00:06<00:13, 243.97it/s]Running 5000 simulations.:  34%|███▎      | 1675/5000 [00:06<00:13, 244.31it/s]Running 5000 simulations.:  34%|███▍      | 1700/5000 [00:07<00:13, 244.56it/s]Running 5000 simulations.:  34%|███▍      | 1725/5000 [00:07<00:13, 245.46it/s]Running 5000 simulations.:  35%|███▌      | 1750/5000 [00:07<00:13, 245.62it/s]Running 5000 simulations.:  36%|███▌      | 1775/5000 [00:07<00:13, 244.70it/s]Running 5000 simulations.:  36%|███▌      | 1800/5000 [00:07<00:13, 244.34it/s]Running 5000 simulations.:  36%|███▋      | 1825/5000 [00:07<00:12, 244.30it/s]Running 5000 simulations.:  37%|███▋      | 1850/5000 [00:07<00:12, 244.25it/s]Running 5000 simulations.:  38%|███▊      | 1875/5000 [00:07<00:12, 244.08it/s]Running 5000 simulations.:  38%|███▊      | 1900/5000 [00:07<00:12, 243.88it/s]Running 5000 simulations.:  38%|███▊      | 1925/5000 [00:07<00:12, 244.89it/s]Running 5000 simulations.:  39%|███▉      | 1950/5000 [00:08<00:12, 245.27it/s]Running 5000 simulations.:  40%|███▉      | 1975/5000 [00:08<00:12, 244.68it/s]Running 5000 simulations.:  40%|████      | 2000/5000 [00:08<00:12, 243.87it/s]Running 5000 simulations.:  40%|████      | 2025/5000 [00:08<00:12, 243.13it/s]Running 5000 simulations.:  41%|████      | 2050/5000 [00:08<00:12, 242.24it/s]Running 5000 simulations.:  42%|████▏     | 2075/5000 [00:08<00:12, 243.00it/s]Running 5000 simulations.:  42%|████▏     | 2100/5000 [00:08<00:11, 242.82it/s]Running 5000 simulations.:  42%|████▎     | 2125/5000 [00:08<00:11, 242.36it/s]Running 5000 simulations.:  43%|████▎     | 2150/5000 [00:08<00:11, 242.54it/s]Running 5000 simulations.:  44%|████▎     | 2175/5000 [00:08<00:11, 242.86it/s]Running 5000 simulations.:  44%|████▍     | 2200/5000 [00:09<00:11, 242.98it/s]Running 5000 simulations.:  44%|████▍     | 2225/5000 [00:09<00:11, 242.98it/s]Running 5000 simulations.:  45%|████▌     | 2250/5000 [00:09<00:11, 242.40it/s]Running 5000 simulations.:  46%|████▌     | 2275/5000 [00:09<00:11, 242.34it/s]Running 5000 simulations.:  46%|████▌     | 2300/5000 [00:09<00:11, 241.95it/s]Running 5000 simulations.:  46%|████▋     | 2325/5000 [00:09<00:11, 242.03it/s]Running 5000 simulations.:  47%|████▋     | 2350/5000 [00:09<00:10, 242.18it/s]Running 5000 simulations.:  48%|████▊     | 2375/5000 [00:09<00:10, 242.10it/s]Running 5000 simulations.:  48%|████▊     | 2400/5000 [00:09<00:10, 242.02it/s]Running 5000 simulations.:  48%|████▊     | 2425/5000 [00:10<00:10, 241.66it/s]Running 5000 simulations.:  49%|████▉     | 2450/5000 [00:10<00:10, 241.93it/s]Running 5000 simulations.:  50%|████▉     | 2475/5000 [00:10<00:10, 242.73it/s]Running 5000 simulations.:  50%|█████     | 2500/5000 [00:10<00:10, 242.67it/s]Running 5000 simulations.:  50%|█████     | 2525/5000 [00:10<00:10, 241.07it/s]Running 5000 simulations.:  51%|█████     | 2550/5000 [00:10<00:10, 237.65it/s]Running 5000 simulations.:  51%|█████▏    | 2574/5000 [00:10<00:10, 236.83it/s]Running 5000 simulations.:  52%|█████▏    | 2598/5000 [00:10<00:10, 236.98it/s]Running 5000 simulations.:  52%|█████▏    | 2622/5000 [00:10<00:09, 237.84it/s]Running 5000 simulations.:  53%|█████▎    | 2646/5000 [00:10<00:09, 238.03it/s]Running 5000 simulations.:  53%|█████▎    | 2670/5000 [00:11<00:09, 237.33it/s]Running 5000 simulations.:  54%|█████▍    | 2694/5000 [00:11<00:09, 237.95it/s]Running 5000 simulations.:  54%|█████▍    | 2718/5000 [00:11<00:09, 238.46it/s]Running 5000 simulations.:  55%|█████▍    | 2742/5000 [00:11<00:09, 238.47it/s]Running 5000 simulations.:  55%|█████▌    | 2766/5000 [00:11<00:09, 237.54it/s]Running 5000 simulations.:  56%|█████▌    | 2790/5000 [00:11<00:09, 236.85it/s]Running 5000 simulations.:  56%|█████▋    | 2814/5000 [00:11<00:09, 236.51it/s]Running 5000 simulations.:  57%|█████▋    | 2838/5000 [00:11<00:09, 236.02it/s]Running 5000 simulations.:  57%|█████▋    | 2862/5000 [00:11<00:09, 235.85it/s]Running 5000 simulations.:  58%|█████▊    | 2886/5000 [00:11<00:08, 235.96it/s]Running 5000 simulations.:  58%|█████▊    | 2910/5000 [00:12<00:08, 235.78it/s]Running 5000 simulations.:  59%|█████▊    | 2934/5000 [00:12<00:08, 235.56it/s]Running 5000 simulations.:  59%|█████▉    | 2958/5000 [00:12<00:08, 235.58it/s]Running 5000 simulations.:  60%|█████▉    | 2982/5000 [00:12<00:08, 235.39it/s]Running 5000 simulations.:  60%|██████    | 3006/5000 [00:12<00:08, 235.16it/s]Running 5000 simulations.:  61%|██████    | 3030/5000 [00:12<00:08, 235.12it/s]Running 5000 simulations.:  61%|██████    | 3054/5000 [00:12<00:08, 234.91it/s]Running 5000 simulations.:  62%|██████▏   | 3078/5000 [00:12<00:08, 235.19it/s]Running 5000 simulations.:  62%|██████▏   | 3102/5000 [00:12<00:08, 235.10it/s]Running 5000 simulations.:  63%|██████▎   | 3126/5000 [00:12<00:07, 235.31it/s]Running 5000 simulations.:  63%|██████▎   | 3150/5000 [00:13<00:07, 235.41it/s]Running 5000 simulations.:  63%|██████▎   | 3174/5000 [00:13<00:07, 235.12it/s]Running 5000 simulations.:  64%|██████▍   | 3198/5000 [00:13<00:07, 235.09it/s]Running 5000 simulations.:  64%|██████▍   | 3222/5000 [00:13<00:07, 235.33it/s]Running 5000 simulations.:  65%|██████▍   | 3246/5000 [00:13<00:07, 235.10it/s]Running 5000 simulations.:  65%|██████▌   | 3270/5000 [00:13<00:07, 235.67it/s]Running 5000 simulations.:  66%|██████▌   | 3294/5000 [00:13<00:07, 235.93it/s]Running 5000 simulations.:  66%|██████▋   | 3318/5000 [00:13<00:07, 236.10it/s]Running 5000 simulations.:  67%|██████▋   | 3342/5000 [00:13<00:07, 235.51it/s]Running 5000 simulations.:  67%|██████▋   | 3366/5000 [00:14<00:06, 235.31it/s]Running 5000 simulations.:  68%|██████▊   | 3390/5000 [00:14<00:06, 235.35it/s]Running 5000 simulations.:  68%|██████▊   | 3414/5000 [00:14<00:06, 235.89it/s]Running 5000 simulations.:  69%|██████▉   | 3438/5000 [00:14<00:06, 236.41it/s]Running 5000 simulations.:  69%|██████▉   | 3462/5000 [00:14<00:06, 236.33it/s]Running 5000 simulations.:  70%|██████▉   | 3486/5000 [00:14<00:06, 235.27it/s]Running 5000 simulations.:  70%|███████   | 3510/5000 [00:14<00:06, 235.33it/s]Running 5000 simulations.:  71%|███████   | 3534/5000 [00:14<00:06, 235.29it/s]Running 5000 simulations.:  71%|███████   | 3558/5000 [00:14<00:06, 235.92it/s]Running 5000 simulations.:  72%|███████▏  | 3582/5000 [00:14<00:06, 235.80it/s]Running 5000 simulations.:  72%|███████▏  | 3606/5000 [00:15<00:05, 235.46it/s]Running 5000 simulations.:  73%|███████▎  | 3630/5000 [00:15<00:05, 235.05it/s]Running 5000 simulations.:  73%|███████▎  | 3654/5000 [00:15<00:05, 235.76it/s]Running 5000 simulations.:  74%|███████▎  | 3678/5000 [00:15<00:05, 235.82it/s]Running 5000 simulations.:  74%|███████▍  | 3702/5000 [00:15<00:05, 235.43it/s]Running 5000 simulations.:  75%|███████▍  | 3726/5000 [00:15<00:05, 234.77it/s]Running 5000 simulations.:  75%|███████▌  | 3750/5000 [00:15<00:05, 235.02it/s]Running 5000 simulations.:  75%|███████▌  | 3774/5000 [00:15<00:05, 234.81it/s]Running 5000 simulations.:  76%|███████▌  | 3798/5000 [00:15<00:05, 234.60it/s]Running 5000 simulations.:  76%|███████▋  | 3822/5000 [00:15<00:05, 234.61it/s]Running 5000 simulations.:  77%|███████▋  | 3846/5000 [00:16<00:04, 234.73it/s]Running 5000 simulations.:  77%|███████▋  | 3870/5000 [00:16<00:04, 234.81it/s]Running 5000 simulations.:  78%|███████▊  | 3894/5000 [00:16<00:04, 235.05it/s]Running 5000 simulations.:  78%|███████▊  | 3918/5000 [00:16<00:04, 234.93it/s]Running 5000 simulations.:  79%|███████▉  | 3942/5000 [00:16<00:04, 236.02it/s]Running 5000 simulations.:  79%|███████▉  | 3966/5000 [00:16<00:04, 236.72it/s]Running 5000 simulations.:  80%|███████▉  | 3990/5000 [00:16<00:04, 236.59it/s]Running 5000 simulations.:  80%|████████  | 4014/5000 [00:16<00:04, 235.75it/s]Running 5000 simulations.:  81%|████████  | 4038/5000 [00:16<00:04, 235.64it/s]Running 5000 simulations.:  81%|████████  | 4062/5000 [00:16<00:03, 235.78it/s]Running 5000 simulations.:  82%|████████▏ | 4086/5000 [00:17<00:03, 234.73it/s]Running 5000 simulations.:  82%|████████▏ | 4110/5000 [00:17<00:03, 235.39it/s]Running 5000 simulations.:  83%|████████▎ | 4134/5000 [00:17<00:03, 234.98it/s]Running 5000 simulations.:  83%|████████▎ | 4159/5000 [00:17<00:03, 237.05it/s]Running 5000 simulations.:  84%|████████▎ | 4184/5000 [00:17<00:03, 239.30it/s]Running 5000 simulations.:  84%|████████▍ | 4209/5000 [00:17<00:03, 240.52it/s]Running 5000 simulations.:  85%|████████▍ | 4234/5000 [00:17<00:03, 241.84it/s]Running 5000 simulations.:  85%|████████▌ | 4259/5000 [00:17<00:03, 242.77it/s]Running 5000 simulations.:  86%|████████▌ | 4284/5000 [00:17<00:02, 243.83it/s]Running 5000 simulations.:  86%|████████▌ | 4309/5000 [00:17<00:02, 244.43it/s]Running 5000 simulations.:  87%|████████▋ | 4334/5000 [00:18<00:02, 245.05it/s]Running 5000 simulations.:  87%|████████▋ | 4359/5000 [00:18<00:02, 246.26it/s]Running 5000 simulations.:  88%|████████▊ | 4384/5000 [00:18<00:02, 247.02it/s]Running 5000 simulations.:  88%|████████▊ | 4409/5000 [00:18<00:02, 246.86it/s]Running 5000 simulations.:  89%|████████▊ | 4434/5000 [00:18<00:02, 242.72it/s]Running 5000 simulations.:  89%|████████▉ | 4459/5000 [00:18<00:02, 243.75it/s]Running 5000 simulations.:  90%|████████▉ | 4484/5000 [00:18<00:02, 245.51it/s]Running 5000 simulations.:  90%|█████████ | 4509/5000 [00:18<00:01, 246.56it/s]Running 5000 simulations.:  91%|█████████ | 4534/5000 [00:18<00:01, 246.94it/s]Running 5000 simulations.:  91%|█████████ | 4559/5000 [00:19<00:01, 247.33it/s]Running 5000 simulations.:  92%|█████████▏| 4584/5000 [00:19<00:01, 247.19it/s]Running 5000 simulations.:  92%|█████████▏| 4610/5000 [00:19<00:01, 248.07it/s]Running 5000 simulations.:  93%|█████████▎| 4636/5000 [00:19<00:01, 248.91it/s]Running 5000 simulations.:  93%|█████████▎| 4662/5000 [00:19<00:01, 249.36it/s]Running 5000 simulations.:  94%|█████████▎| 4687/5000 [00:19<00:01, 249.52it/s]Running 5000 simulations.:  94%|█████████▍| 4712/5000 [00:19<00:01, 249.51it/s]Running 5000 simulations.:  95%|█████████▍| 4738/5000 [00:19<00:01, 249.92it/s]Running 5000 simulations.:  95%|█████████▌| 4764/5000 [00:19<00:00, 250.02it/s]Running 5000 simulations.:  96%|█████████▌| 4790/5000 [00:19<00:00, 249.96it/s]Running 5000 simulations.:  96%|█████████▋| 4815/5000 [00:20<00:00, 249.32it/s]Running 5000 simulations.:  97%|█████████▋| 4840/5000 [00:20<00:00, 246.71it/s]Running 5000 simulations.:  97%|█████████▋| 4865/5000 [00:20<00:00, 242.55it/s]Running 5000 simulations.:  98%|█████████▊| 4890/5000 [00:20<00:00, 242.53it/s]Running 5000 simulations.:  98%|█████████▊| 4915/5000 [00:20<00:00, 244.29it/s]Running 5000 simulations.:  99%|█████████▉| 4940/5000 [00:20<00:00, 245.01it/s]Running 5000 simulations.:  99%|█████████▉| 4965/5000 [00:20<00:00, 245.59it/s]Running 5000 simulations.: 100%|█████████▉| 4990/5000 [00:20<00:00, 246.02it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:20<00:00, 240.51it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   1%|          | 26/5000 [00:00<00:19, 253.99it/s]Running 5000 simulations.:   1%|          | 52/5000 [00:00<00:19, 253.95it/s]Running 5000 simulations.:   2%|▏         | 78/5000 [00:00<00:19, 253.93it/s]Running 5000 simulations.:   2%|▏         | 104/5000 [00:00<00:19, 253.22it/s]Running 5000 simulations.:   3%|▎         | 130/5000 [00:00<00:19, 252.41it/s]Running 5000 simulations.:   3%|▎         | 156/5000 [00:00<00:19, 251.84it/s]Running 5000 simulations.:   4%|▎         | 182/5000 [00:00<00:19, 251.27it/s]Running 5000 simulations.:   4%|▍         | 207/5000 [00:00<00:19, 250.06it/s]Running 5000 simulations.:   5%|▍         | 233/5000 [00:00<00:19, 250.05it/s]Running 5000 simulations.:   5%|▌         | 258/5000 [00:01<00:18, 249.72it/s]Running 5000 simulations.:   6%|▌         | 284/5000 [00:01<00:18, 250.38it/s]Running 5000 simulations.:   6%|▌         | 309/5000 [00:01<00:18, 250.11it/s]Running 5000 simulations.:   7%|▋         | 334/5000 [00:01<00:18, 249.34it/s]Running 5000 simulations.:   7%|▋         | 359/5000 [00:01<00:18, 248.95it/s]Running 5000 simulations.:   8%|▊         | 384/5000 [00:01<00:19, 239.48it/s]Running 5000 simulations.:   8%|▊         | 409/5000 [00:01<00:18, 242.36it/s]Running 5000 simulations.:   9%|▊         | 435/5000 [00:01<00:18, 244.77it/s]Running 5000 simulations.:   9%|▉         | 460/5000 [00:01<00:18, 245.93it/s]Running 5000 simulations.:  10%|▉         | 485/5000 [00:01<00:18, 247.10it/s]Running 5000 simulations.:  10%|█         | 510/5000 [00:02<00:18, 247.28it/s]Running 5000 simulations.:  11%|█         | 535/5000 [00:02<00:18, 247.12it/s]Running 5000 simulations.:  11%|█         | 560/5000 [00:02<00:17, 246.91it/s]Running 5000 simulations.:  12%|█▏        | 585/5000 [00:02<00:17, 245.93it/s]Running 5000 simulations.:  12%|█▏        | 610/5000 [00:02<00:18, 242.93it/s]Running 5000 simulations.:  13%|█▎        | 635/5000 [00:02<00:18, 241.02it/s]Running 5000 simulations.:  13%|█▎        | 660/5000 [00:02<00:17, 241.55it/s]Running 5000 simulations.:  14%|█▎        | 685/5000 [00:02<00:17, 243.12it/s]Running 5000 simulations.:  14%|█▍        | 710/5000 [00:02<00:17, 243.73it/s]Running 5000 simulations.:  15%|█▍        | 735/5000 [00:02<00:17, 244.19it/s]Running 5000 simulations.:  15%|█▌        | 760/5000 [00:03<00:17, 245.31it/s]Running 5000 simulations.:  16%|█▌        | 785/5000 [00:03<00:17, 245.99it/s]Running 5000 simulations.:  16%|█▌        | 810/5000 [00:03<00:17, 246.01it/s]Running 5000 simulations.:  17%|█▋        | 835/5000 [00:03<00:16, 245.65it/s]Running 5000 simulations.:  17%|█▋        | 860/5000 [00:03<00:16, 246.09it/s]Running 5000 simulations.:  18%|█▊        | 885/5000 [00:03<00:16, 246.94it/s]Running 5000 simulations.:  18%|█▊        | 910/5000 [00:03<00:16, 247.62it/s]Running 5000 simulations.:  19%|█▊        | 935/5000 [00:03<00:16, 246.33it/s]Running 5000 simulations.:  19%|█▉        | 960/5000 [00:03<00:16, 246.10it/s]Running 5000 simulations.:  20%|█▉        | 985/5000 [00:03<00:16, 246.17it/s]Running 5000 simulations.:  20%|██        | 1010/5000 [00:04<00:16, 245.80it/s]Running 5000 simulations.:  21%|██        | 1035/5000 [00:04<00:16, 245.55it/s]Running 5000 simulations.:  21%|██        | 1060/5000 [00:04<00:16, 244.73it/s]Running 5000 simulations.:  22%|██▏       | 1085/5000 [00:04<00:15, 244.78it/s]Running 5000 simulations.:  22%|██▏       | 1110/5000 [00:04<00:15, 245.30it/s]Running 5000 simulations.:  23%|██▎       | 1135/5000 [00:04<00:15, 245.44it/s]Running 5000 simulations.:  23%|██▎       | 1160/5000 [00:04<00:15, 245.42it/s]Running 5000 simulations.:  24%|██▎       | 1185/5000 [00:04<00:15, 245.01it/s]Running 5000 simulations.:  24%|██▍       | 1210/5000 [00:04<00:15, 244.95it/s]Running 5000 simulations.:  25%|██▍       | 1235/5000 [00:05<00:15, 245.11it/s]Running 5000 simulations.:  25%|██▌       | 1260/5000 [00:05<00:15, 245.70it/s]Running 5000 simulations.:  26%|██▌       | 1285/5000 [00:05<00:15, 245.92it/s]Running 5000 simulations.:  26%|██▌       | 1310/5000 [00:05<00:15, 245.69it/s]Running 5000 simulations.:  27%|██▋       | 1335/5000 [00:05<00:14, 245.85it/s]Running 5000 simulations.:  27%|██▋       | 1360/5000 [00:05<00:14, 245.41it/s]Running 5000 simulations.:  28%|██▊       | 1385/5000 [00:05<00:14, 244.74it/s]Running 5000 simulations.:  28%|██▊       | 1410/5000 [00:05<00:14, 244.22it/s]Running 5000 simulations.:  29%|██▊       | 1435/5000 [00:05<00:14, 244.29it/s]Running 5000 simulations.:  29%|██▉       | 1460/5000 [00:05<00:14, 244.42it/s]Running 5000 simulations.:  30%|██▉       | 1485/5000 [00:06<00:14, 243.43it/s]Running 5000 simulations.:  30%|███       | 1510/5000 [00:06<00:14, 243.79it/s]Running 5000 simulations.:  31%|███       | 1535/5000 [00:06<00:14, 243.61it/s]Running 5000 simulations.:  31%|███       | 1560/5000 [00:06<00:14, 243.51it/s]Running 5000 simulations.:  32%|███▏      | 1585/5000 [00:06<00:14, 243.43it/s]Running 5000 simulations.:  32%|███▏      | 1610/5000 [00:06<00:13, 243.57it/s]Running 5000 simulations.:  33%|███▎      | 1635/5000 [00:06<00:13, 243.88it/s]Running 5000 simulations.:  33%|███▎      | 1660/5000 [00:06<00:13, 243.78it/s]Running 5000 simulations.:  34%|███▎      | 1685/5000 [00:06<00:13, 243.64it/s]Running 5000 simulations.:  34%|███▍      | 1710/5000 [00:06<00:13, 243.99it/s]Running 5000 simulations.:  35%|███▍      | 1735/5000 [00:07<00:13, 244.82it/s]Running 5000 simulations.:  35%|███▌      | 1760/5000 [00:07<00:13, 245.12it/s]Running 5000 simulations.:  36%|███▌      | 1785/5000 [00:07<00:13, 244.74it/s]Running 5000 simulations.:  36%|███▌      | 1810/5000 [00:07<00:13, 244.14it/s]Running 5000 simulations.:  37%|███▋      | 1835/5000 [00:07<00:12, 243.63it/s]Running 5000 simulations.:  37%|███▋      | 1860/5000 [00:07<00:12, 243.62it/s]Running 5000 simulations.:  38%|███▊      | 1885/5000 [00:07<00:12, 242.99it/s]Running 5000 simulations.:  38%|███▊      | 1910/5000 [00:07<00:12, 243.86it/s]Running 5000 simulations.:  39%|███▊      | 1935/5000 [00:07<00:12, 244.82it/s]Running 5000 simulations.:  39%|███▉      | 1960/5000 [00:07<00:12, 244.55it/s]Running 5000 simulations.:  40%|███▉      | 1985/5000 [00:08<00:12, 244.36it/s]Running 5000 simulations.:  40%|████      | 2010/5000 [00:08<00:12, 241.01it/s]Running 5000 simulations.:  41%|████      | 2035/5000 [00:08<00:12, 241.77it/s]Running 5000 simulations.:  41%|████      | 2060/5000 [00:08<00:12, 242.86it/s]Running 5000 simulations.:  42%|████▏     | 2085/5000 [00:08<00:11, 244.70it/s]Running 5000 simulations.:  42%|████▏     | 2111/5000 [00:08<00:11, 246.51it/s]Running 5000 simulations.:  43%|████▎     | 2136/5000 [00:08<00:11, 247.53it/s]Running 5000 simulations.:  43%|████▎     | 2162/5000 [00:08<00:11, 248.47it/s]Running 5000 simulations.:  44%|████▍     | 2188/5000 [00:08<00:11, 249.03it/s]Running 5000 simulations.:  44%|████▍     | 2213/5000 [00:09<00:11, 249.20it/s]Running 5000 simulations.:  45%|████▍     | 2239/5000 [00:09<00:11, 250.22it/s]Running 5000 simulations.:  45%|████▌     | 2265/5000 [00:09<00:10, 250.39it/s]Running 5000 simulations.:  46%|████▌     | 2291/5000 [00:09<00:10, 249.11it/s]Running 5000 simulations.:  46%|████▋     | 2317/5000 [00:09<00:10, 249.49it/s]Running 5000 simulations.:  47%|████▋     | 2342/5000 [00:09<00:10, 249.31it/s]Running 5000 simulations.:  47%|████▋     | 2367/5000 [00:09<00:10, 249.37it/s]Running 5000 simulations.:  48%|████▊     | 2392/5000 [00:09<00:10, 249.10it/s]Running 5000 simulations.:  48%|████▊     | 2417/5000 [00:09<00:10, 249.18it/s]Running 5000 simulations.:  49%|████▉     | 2442/5000 [00:09<00:10, 247.74it/s]Running 5000 simulations.:  49%|████▉     | 2467/5000 [00:10<00:10, 248.04it/s]Running 5000 simulations.:  50%|████▉     | 2492/5000 [00:10<00:10, 247.96it/s]Running 5000 simulations.:  50%|█████     | 2518/5000 [00:10<00:09, 248.65it/s]Running 5000 simulations.:  51%|█████     | 2544/5000 [00:10<00:09, 249.12it/s]Running 5000 simulations.:  51%|█████▏    | 2570/5000 [00:10<00:09, 249.37it/s]Running 5000 simulations.:  52%|█████▏    | 2596/5000 [00:10<00:09, 250.09it/s]Running 5000 simulations.:  52%|█████▏    | 2622/5000 [00:10<00:09, 250.81it/s]Running 5000 simulations.:  53%|█████▎    | 2648/5000 [00:10<00:09, 249.59it/s]Running 5000 simulations.:  53%|█████▎    | 2673/5000 [00:10<00:09, 248.98it/s]Running 5000 simulations.:  54%|█████▍    | 2698/5000 [00:10<00:09, 248.10it/s]Running 5000 simulations.:  54%|█████▍    | 2724/5000 [00:11<00:09, 249.38it/s]Running 5000 simulations.:  55%|█████▌    | 2750/5000 [00:11<00:08, 250.07it/s]Running 5000 simulations.:  56%|█████▌    | 2776/5000 [00:11<00:08, 251.34it/s]Running 5000 simulations.:  56%|█████▌    | 2802/5000 [00:11<00:08, 251.79it/s]Running 5000 simulations.:  57%|█████▋    | 2828/5000 [00:11<00:08, 251.18it/s]Running 5000 simulations.:  57%|█████▋    | 2854/5000 [00:11<00:08, 250.43it/s]Running 5000 simulations.:  58%|█████▊    | 2880/5000 [00:11<00:08, 250.58it/s]Running 5000 simulations.:  58%|█████▊    | 2906/5000 [00:11<00:08, 251.20it/s]Running 5000 simulations.:  59%|█████▊    | 2932/5000 [00:11<00:08, 250.79it/s]Running 5000 simulations.:  59%|█████▉    | 2958/5000 [00:11<00:08, 251.23it/s]Running 5000 simulations.:  60%|█████▉    | 2984/5000 [00:12<00:08, 251.03it/s]Running 5000 simulations.:  60%|██████    | 3010/5000 [00:12<00:07, 251.49it/s]Running 5000 simulations.:  61%|██████    | 3036/5000 [00:12<00:07, 252.02it/s]Running 5000 simulations.:  61%|██████    | 3062/5000 [00:12<00:07, 249.69it/s]Running 5000 simulations.:  62%|██████▏   | 3087/5000 [00:12<00:07, 245.21it/s]Running 5000 simulations.:  62%|██████▏   | 3112/5000 [00:12<00:07, 245.75it/s]Running 5000 simulations.:  63%|██████▎   | 3138/5000 [00:12<00:07, 247.43it/s]Running 5000 simulations.:  63%|██████▎   | 3164/5000 [00:12<00:07, 248.60it/s]Running 5000 simulations.:  64%|██████▍   | 3190/5000 [00:12<00:07, 249.64it/s]Running 5000 simulations.:  64%|██████▍   | 3216/5000 [00:13<00:07, 249.96it/s]Running 5000 simulations.:  65%|██████▍   | 3242/5000 [00:13<00:07, 249.73it/s]Running 5000 simulations.:  65%|██████▌   | 3267/5000 [00:13<00:06, 249.56it/s]Running 5000 simulations.:  66%|██████▌   | 3293/5000 [00:13<00:06, 250.17it/s]Running 5000 simulations.:  66%|██████▋   | 3319/5000 [00:13<00:06, 249.79it/s]Running 5000 simulations.:  67%|██████▋   | 3345/5000 [00:13<00:06, 250.15it/s]Running 5000 simulations.:  67%|██████▋   | 3371/5000 [00:13<00:06, 250.75it/s]Running 5000 simulations.:  68%|██████▊   | 3397/5000 [00:13<00:06, 250.91it/s]Running 5000 simulations.:  68%|██████▊   | 3423/5000 [00:13<00:06, 250.60it/s]Running 5000 simulations.:  69%|██████▉   | 3449/5000 [00:13<00:06, 250.63it/s]Running 5000 simulations.:  70%|██████▉   | 3475/5000 [00:14<00:06, 249.61it/s]Running 5000 simulations.:  70%|███████   | 3500/5000 [00:14<00:06, 248.02it/s]Running 5000 simulations.:  71%|███████   | 3526/5000 [00:14<00:05, 248.91it/s]Running 5000 simulations.:  71%|███████   | 3552/5000 [00:14<00:05, 249.95it/s]Running 5000 simulations.:  72%|███████▏  | 3577/5000 [00:14<00:05, 249.49it/s]Running 5000 simulations.:  72%|███████▏  | 3602/5000 [00:14<00:05, 248.49it/s]Running 5000 simulations.:  73%|███████▎  | 3628/5000 [00:14<00:05, 249.29it/s]Running 5000 simulations.:  73%|███████▎  | 3654/5000 [00:14<00:05, 250.69it/s]Running 5000 simulations.:  74%|███████▎  | 3680/5000 [00:14<00:05, 250.28it/s]Running 5000 simulations.:  74%|███████▍  | 3706/5000 [00:14<00:05, 251.42it/s]Running 5000 simulations.:  75%|███████▍  | 3732/5000 [00:15<00:05, 251.59it/s]Running 5000 simulations.:  75%|███████▌  | 3758/5000 [00:15<00:04, 251.83it/s]Running 5000 simulations.:  76%|███████▌  | 3784/5000 [00:15<00:04, 249.47it/s]Running 5000 simulations.:  76%|███████▌  | 3809/5000 [00:15<00:04, 246.49it/s]Running 5000 simulations.:  77%|███████▋  | 3835/5000 [00:15<00:04, 247.71it/s]Running 5000 simulations.:  77%|███████▋  | 3860/5000 [00:15<00:04, 248.04it/s]Running 5000 simulations.:  78%|███████▊  | 3885/5000 [00:15<00:04, 247.46it/s]Running 5000 simulations.:  78%|███████▊  | 3911/5000 [00:15<00:04, 248.54it/s]Running 5000 simulations.:  79%|███████▊  | 3937/5000 [00:15<00:04, 249.57it/s]Running 5000 simulations.:  79%|███████▉  | 3963/5000 [00:16<00:04, 250.29it/s]Running 5000 simulations.:  80%|███████▉  | 3989/5000 [00:16<00:04, 251.38it/s]Running 5000 simulations.:  80%|████████  | 4015/5000 [00:16<00:03, 251.97it/s]Running 5000 simulations.:  81%|████████  | 4041/5000 [00:16<00:03, 251.64it/s]Running 5000 simulations.:  81%|████████▏ | 4067/5000 [00:16<00:03, 251.56it/s]Running 5000 simulations.:  82%|████████▏ | 4093/5000 [00:16<00:03, 247.35it/s]Running 5000 simulations.:  82%|████████▏ | 4119/5000 [00:16<00:03, 248.70it/s]Running 5000 simulations.:  83%|████████▎ | 4145/5000 [00:16<00:03, 250.38it/s]Running 5000 simulations.:  83%|████████▎ | 4171/5000 [00:16<00:03, 250.42it/s]Running 5000 simulations.:  84%|████████▍ | 4197/5000 [00:16<00:03, 250.04it/s]Running 5000 simulations.:  84%|████████▍ | 4223/5000 [00:17<00:03, 250.39it/s]Running 5000 simulations.:  85%|████████▍ | 4249/5000 [00:17<00:02, 250.50it/s]Running 5000 simulations.:  86%|████████▌ | 4275/5000 [00:17<00:02, 250.09it/s]Running 5000 simulations.:  86%|████████▌ | 4301/5000 [00:17<00:02, 249.68it/s]Running 5000 simulations.:  87%|████████▋ | 4327/5000 [00:17<00:02, 250.03it/s]Running 5000 simulations.:  87%|████████▋ | 4353/5000 [00:17<00:02, 250.41it/s]Running 5000 simulations.:  88%|████████▊ | 4379/5000 [00:17<00:02, 251.47it/s]Running 5000 simulations.:  88%|████████▊ | 4405/5000 [00:17<00:02, 251.75it/s]Running 5000 simulations.:  89%|████████▊ | 4431/5000 [00:17<00:02, 252.69it/s]Running 5000 simulations.:  89%|████████▉ | 4457/5000 [00:17<00:02, 253.02it/s]Running 5000 simulations.:  90%|████████▉ | 4483/5000 [00:18<00:02, 253.20it/s]Running 5000 simulations.:  90%|█████████ | 4509/5000 [00:18<00:01, 253.49it/s]Running 5000 simulations.:  91%|█████████ | 4535/5000 [00:18<00:01, 253.47it/s]Running 5000 simulations.:  91%|█████████ | 4561/5000 [00:18<00:01, 254.25it/s]Running 5000 simulations.:  92%|█████████▏| 4587/5000 [00:18<00:01, 254.82it/s]Running 5000 simulations.:  92%|█████████▏| 4613/5000 [00:18<00:01, 254.75it/s]Running 5000 simulations.:  93%|█████████▎| 4639/5000 [00:18<00:01, 254.23it/s]Running 5000 simulations.:  93%|█████████▎| 4665/5000 [00:18<00:01, 253.82it/s]Running 5000 simulations.:  94%|█████████▍| 4691/5000 [00:18<00:01, 252.47it/s]Running 5000 simulations.:  94%|█████████▍| 4717/5000 [00:19<00:01, 250.12it/s]Running 5000 simulations.:  95%|█████████▍| 4743/5000 [00:19<00:01, 248.98it/s]Running 5000 simulations.:  95%|█████████▌| 4769/5000 [00:19<00:00, 251.52it/s]Running 5000 simulations.:  96%|█████████▌| 4795/5000 [00:19<00:00, 251.43it/s]Running 5000 simulations.:  96%|█████████▋| 4821/5000 [00:19<00:00, 252.63it/s]Running 5000 simulations.:  97%|█████████▋| 4847/5000 [00:19<00:00, 253.96it/s]Running 5000 simulations.:  97%|█████████▋| 4873/5000 [00:19<00:00, 254.92it/s]Running 5000 simulations.:  98%|█████████▊| 4899/5000 [00:19<00:00, 254.52it/s]Running 5000 simulations.:  98%|█████████▊| 4925/5000 [00:19<00:00, 254.65it/s]Running 5000 simulations.:  99%|█████████▉| 4951/5000 [00:19<00:00, 254.26it/s]Running 5000 simulations.: 100%|█████████▉| 4977/5000 [00:20<00:00, 254.07it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:20<00:00, 248.44it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   1%|          | 27/5000 [00:00<00:18, 263.32it/s]Running 5000 simulations.:   1%|          | 54/5000 [00:00<00:18, 262.95it/s]Running 5000 simulations.:   2%|▏         | 81/5000 [00:00<00:18, 262.74it/s]Running 5000 simulations.:   2%|▏         | 107/5000 [00:00<00:18, 261.65it/s]Running 5000 simulations.:   3%|▎         | 133/5000 [00:00<00:18, 260.68it/s]Running 5000 simulations.:   3%|▎         | 159/5000 [00:00<00:18, 260.02it/s]Running 5000 simulations.:   4%|▎         | 185/5000 [00:00<00:18, 258.88it/s]Running 5000 simulations.:   4%|▍         | 211/5000 [00:00<00:18, 257.90it/s]Running 5000 simulations.:   5%|▍         | 237/5000 [00:00<00:18, 257.63it/s]Running 5000 simulations.:   5%|▌         | 263/5000 [00:01<00:18, 257.23it/s]Running 5000 simulations.:   6%|▌         | 289/5000 [00:01<00:18, 256.59it/s]Running 5000 simulations.:   6%|▋         | 315/5000 [00:01<00:18, 255.76it/s]Running 5000 simulations.:   7%|▋         | 341/5000 [00:01<00:18, 255.12it/s]Running 5000 simulations.:   7%|▋         | 367/5000 [00:01<00:18, 254.93it/s]Running 5000 simulations.:   8%|▊         | 393/5000 [00:01<00:17, 256.17it/s]Running 5000 simulations.:   8%|▊         | 419/5000 [00:01<00:17, 257.05it/s]Running 5000 simulations.:   9%|▉         | 445/5000 [00:01<00:17, 257.83it/s]Running 5000 simulations.:   9%|▉         | 472/5000 [00:01<00:17, 258.60it/s]Running 5000 simulations.:  10%|▉         | 498/5000 [00:01<00:17, 258.91it/s]Running 5000 simulations.:  10%|█         | 524/5000 [00:02<00:17, 258.12it/s]Running 5000 simulations.:  11%|█         | 550/5000 [00:02<00:17, 257.79it/s]Running 5000 simulations.:  12%|█▏        | 576/5000 [00:02<00:17, 258.07it/s]Running 5000 simulations.:  12%|█▏        | 602/5000 [00:02<00:17, 258.06it/s]Running 5000 simulations.:  13%|█▎        | 628/5000 [00:02<00:16, 257.80it/s]Running 5000 simulations.:  13%|█▎        | 654/5000 [00:02<00:16, 257.95it/s]Running 5000 simulations.:  14%|█▎        | 680/5000 [00:02<00:16, 258.25it/s]Running 5000 simulations.:  14%|█▍        | 706/5000 [00:02<00:16, 257.67it/s]Running 5000 simulations.:  15%|█▍        | 732/5000 [00:02<00:16, 253.99it/s]Running 5000 simulations.:  15%|█▌        | 758/5000 [00:02<00:16, 254.42it/s]Running 5000 simulations.:  16%|█▌        | 784/5000 [00:03<00:16, 254.65it/s]Running 5000 simulations.:  16%|█▌        | 810/5000 [00:03<00:16, 253.96it/s]Running 5000 simulations.:  17%|█▋        | 836/5000 [00:03<00:16, 255.03it/s]Running 5000 simulations.:  17%|█▋        | 862/5000 [00:03<00:16, 254.50it/s]Running 5000 simulations.:  18%|█▊        | 888/5000 [00:03<00:16, 254.76it/s]Running 5000 simulations.:  18%|█▊        | 914/5000 [00:03<00:15, 255.50it/s]Running 5000 simulations.:  19%|█▉        | 940/5000 [00:03<00:15, 255.86it/s]Running 5000 simulations.:  19%|█▉        | 966/5000 [00:03<00:15, 255.48it/s]Running 5000 simulations.:  20%|█▉        | 992/5000 [00:03<00:15, 256.27it/s]Running 5000 simulations.:  20%|██        | 1018/5000 [00:03<00:15, 257.10it/s]Running 5000 simulations.:  21%|██        | 1044/5000 [00:04<00:15, 257.20it/s]Running 5000 simulations.:  21%|██▏       | 1070/5000 [00:04<00:15, 256.69it/s]Running 5000 simulations.:  22%|██▏       | 1096/5000 [00:04<00:15, 256.61it/s]Running 5000 simulations.:  22%|██▏       | 1122/5000 [00:04<00:15, 256.31it/s]Running 5000 simulations.:  23%|██▎       | 1148/5000 [00:04<00:15, 256.25it/s]Running 5000 simulations.:  23%|██▎       | 1174/5000 [00:04<00:14, 255.28it/s]Running 5000 simulations.:  24%|██▍       | 1200/5000 [00:04<00:14, 254.17it/s]Running 5000 simulations.:  25%|██▍       | 1226/5000 [00:04<00:14, 254.09it/s]Running 5000 simulations.:  25%|██▌       | 1252/5000 [00:04<00:14, 253.67it/s]Running 5000 simulations.:  26%|██▌       | 1278/5000 [00:04<00:14, 253.16it/s]Running 5000 simulations.:  26%|██▌       | 1304/5000 [00:05<00:14, 253.36it/s]Running 5000 simulations.:  27%|██▋       | 1330/5000 [00:05<00:14, 254.32it/s]Running 5000 simulations.:  27%|██▋       | 1356/5000 [00:05<00:14, 253.15it/s]Running 5000 simulations.:  28%|██▊       | 1382/5000 [00:05<00:14, 253.36it/s]Running 5000 simulations.:  28%|██▊       | 1408/5000 [00:05<00:14, 253.31it/s]Running 5000 simulations.:  29%|██▊       | 1434/5000 [00:05<00:14, 253.14it/s]Running 5000 simulations.:  29%|██▉       | 1460/5000 [00:05<00:13, 253.88it/s]Running 5000 simulations.:  30%|██▉       | 1486/5000 [00:05<00:13, 254.69it/s]Running 5000 simulations.:  30%|███       | 1512/5000 [00:05<00:13, 254.55it/s]Running 5000 simulations.:  31%|███       | 1538/5000 [00:06<00:13, 254.68it/s]Running 5000 simulations.:  31%|███▏      | 1564/5000 [00:06<00:13, 255.24it/s]Running 5000 simulations.:  32%|███▏      | 1590/5000 [00:06<00:13, 255.93it/s]Running 5000 simulations.:  32%|███▏      | 1616/5000 [00:06<00:13, 255.86it/s]Running 5000 simulations.:  33%|███▎      | 1642/5000 [00:06<00:13, 254.99it/s]Running 5000 simulations.:  33%|███▎      | 1668/5000 [00:06<00:13, 254.47it/s]Running 5000 simulations.:  34%|███▍      | 1694/5000 [00:06<00:12, 254.84it/s]Running 5000 simulations.:  34%|███▍      | 1720/5000 [00:06<00:12, 254.19it/s]Running 5000 simulations.:  35%|███▍      | 1746/5000 [00:06<00:12, 253.25it/s]Running 5000 simulations.:  35%|███▌      | 1772/5000 [00:06<00:12, 251.93it/s]Running 5000 simulations.:  36%|███▌      | 1798/5000 [00:07<00:12, 251.90it/s]Running 5000 simulations.:  36%|███▋      | 1824/5000 [00:07<00:12, 250.02it/s]Running 5000 simulations.:  37%|███▋      | 1850/5000 [00:07<00:12, 247.70it/s]Running 5000 simulations.:  38%|███▊      | 1876/5000 [00:07<00:12, 249.68it/s]Running 5000 simulations.:  38%|███▊      | 1902/5000 [00:07<00:12, 250.88it/s]Running 5000 simulations.:  39%|███▊      | 1928/5000 [00:07<00:12, 248.69it/s]Running 5000 simulations.:  39%|███▉      | 1953/5000 [00:07<00:12, 247.80it/s]Running 5000 simulations.:  40%|███▉      | 1979/5000 [00:07<00:12, 249.36it/s]Running 5000 simulations.:  40%|████      | 2005/5000 [00:07<00:11, 250.51it/s]Running 5000 simulations.:  41%|████      | 2031/5000 [00:07<00:11, 250.16it/s]Running 5000 simulations.:  41%|████      | 2057/5000 [00:08<00:11, 250.88it/s]Running 5000 simulations.:  42%|████▏     | 2083/5000 [00:08<00:11, 251.08it/s]Running 5000 simulations.:  42%|████▏     | 2109/5000 [00:08<00:11, 251.01it/s]Running 5000 simulations.:  43%|████▎     | 2135/5000 [00:08<00:11, 250.81it/s]Running 5000 simulations.:  43%|████▎     | 2161/5000 [00:08<00:11, 249.96it/s]Running 5000 simulations.:  44%|████▎     | 2187/5000 [00:08<00:11, 251.41it/s]Running 5000 simulations.:  44%|████▍     | 2213/5000 [00:08<00:11, 252.09it/s]Running 5000 simulations.:  45%|████▍     | 2239/5000 [00:08<00:10, 253.15it/s]Running 5000 simulations.:  45%|████▌     | 2265/5000 [00:08<00:10, 252.64it/s]Running 5000 simulations.:  46%|████▌     | 2291/5000 [00:08<00:10, 251.86it/s]Running 5000 simulations.:  46%|████▋     | 2317/5000 [00:09<00:10, 252.92it/s]Running 5000 simulations.:  47%|████▋     | 2343/5000 [00:09<00:10, 253.66it/s]Running 5000 simulations.:  47%|████▋     | 2369/5000 [00:09<00:10, 253.63it/s]Running 5000 simulations.:  48%|████▊     | 2395/5000 [00:09<00:10, 252.74it/s]Running 5000 simulations.:  48%|████▊     | 2421/5000 [00:09<00:10, 252.73it/s]Running 5000 simulations.:  49%|████▉     | 2447/5000 [00:09<00:10, 252.43it/s]Running 5000 simulations.:  49%|████▉     | 2473/5000 [00:09<00:10, 251.94it/s]Running 5000 simulations.:  50%|████▉     | 2499/5000 [00:09<00:09, 251.70it/s]Running 5000 simulations.:  50%|█████     | 2525/5000 [00:09<00:09, 250.67it/s]Running 5000 simulations.:  51%|█████     | 2551/5000 [00:10<00:09, 249.60it/s]Running 5000 simulations.:  52%|█████▏    | 2576/5000 [00:10<00:09, 248.69it/s]Running 5000 simulations.:  52%|█████▏    | 2602/5000 [00:10<00:09, 249.57it/s]Running 5000 simulations.:  53%|█████▎    | 2628/5000 [00:10<00:09, 251.07it/s]Running 5000 simulations.:  53%|█████▎    | 2654/5000 [00:10<00:09, 252.20it/s]Running 5000 simulations.:  54%|█████▎    | 2680/5000 [00:10<00:09, 251.87it/s]Running 5000 simulations.:  54%|█████▍    | 2706/5000 [00:10<00:09, 251.50it/s]Running 5000 simulations.:  55%|█████▍    | 2732/5000 [00:10<00:09, 251.60it/s]Running 5000 simulations.:  55%|█████▌    | 2758/5000 [00:10<00:08, 250.97it/s]Running 5000 simulations.:  56%|█████▌    | 2784/5000 [00:10<00:08, 251.28it/s]Running 5000 simulations.:  56%|█████▌    | 2810/5000 [00:11<00:08, 251.65it/s]Running 5000 simulations.:  57%|█████▋    | 2836/5000 [00:11<00:08, 252.28it/s]Running 5000 simulations.:  57%|█████▋    | 2862/5000 [00:11<00:08, 252.71it/s]Running 5000 simulations.:  58%|█████▊    | 2888/5000 [00:11<00:08, 251.09it/s]Running 5000 simulations.:  58%|█████▊    | 2914/5000 [00:11<00:08, 250.91it/s]Running 5000 simulations.:  59%|█████▉    | 2940/5000 [00:11<00:08, 251.11it/s]Running 5000 simulations.:  59%|█████▉    | 2966/5000 [00:11<00:08, 248.99it/s]Running 5000 simulations.:  60%|█████▉    | 2991/5000 [00:11<00:08, 248.61it/s]Running 5000 simulations.:  60%|██████    | 3017/5000 [00:11<00:07, 249.17it/s]Running 5000 simulations.:  61%|██████    | 3042/5000 [00:11<00:07, 248.92it/s]Running 5000 simulations.:  61%|██████▏   | 3068/5000 [00:12<00:07, 249.35it/s]Running 5000 simulations.:  62%|██████▏   | 3093/5000 [00:12<00:07, 248.66it/s]Running 5000 simulations.:  62%|██████▏   | 3119/5000 [00:12<00:07, 249.94it/s]Running 5000 simulations.:  63%|██████▎   | 3145/5000 [00:12<00:07, 251.23it/s]Running 5000 simulations.:  63%|██████▎   | 3171/5000 [00:12<00:07, 251.50it/s]Running 5000 simulations.:  64%|██████▍   | 3197/5000 [00:12<00:07, 252.08it/s]Running 5000 simulations.:  64%|██████▍   | 3223/5000 [00:12<00:07, 251.53it/s]Running 5000 simulations.:  65%|██████▍   | 3249/5000 [00:12<00:06, 251.90it/s]Running 5000 simulations.:  66%|██████▌   | 3275/5000 [00:12<00:06, 252.26it/s]Running 5000 simulations.:  66%|██████▌   | 3301/5000 [00:13<00:06, 251.68it/s]Running 5000 simulations.:  67%|██████▋   | 3327/5000 [00:13<00:06, 252.12it/s]Running 5000 simulations.:  67%|██████▋   | 3353/5000 [00:13<00:06, 252.37it/s]Running 5000 simulations.:  68%|██████▊   | 3379/5000 [00:13<00:06, 251.76it/s]Running 5000 simulations.:  68%|██████▊   | 3405/5000 [00:13<00:06, 252.16it/s]Running 5000 simulations.:  69%|██████▊   | 3431/5000 [00:13<00:06, 252.59it/s]Running 5000 simulations.:  69%|██████▉   | 3457/5000 [00:13<00:06, 252.70it/s]Running 5000 simulations.:  70%|██████▉   | 3483/5000 [00:13<00:06, 252.36it/s]Running 5000 simulations.:  70%|███████   | 3509/5000 [00:13<00:05, 250.99it/s]Running 5000 simulations.:  71%|███████   | 3535/5000 [00:13<00:05, 250.28it/s]Running 5000 simulations.:  71%|███████   | 3561/5000 [00:14<00:05, 251.25it/s]Running 5000 simulations.:  72%|███████▏  | 3587/5000 [00:14<00:05, 251.07it/s]Running 5000 simulations.:  72%|███████▏  | 3613/5000 [00:14<00:05, 250.84it/s]Running 5000 simulations.:  73%|███████▎  | 3639/5000 [00:14<00:05, 250.52it/s]Running 5000 simulations.:  73%|███████▎  | 3665/5000 [00:14<00:05, 251.21it/s]Running 5000 simulations.:  74%|███████▍  | 3691/5000 [00:14<00:05, 252.16it/s]Running 5000 simulations.:  74%|███████▍  | 3717/5000 [00:14<00:05, 252.10it/s]Running 5000 simulations.:  75%|███████▍  | 3743/5000 [00:14<00:04, 252.14it/s]Running 5000 simulations.:  75%|███████▌  | 3769/5000 [00:14<00:04, 250.57it/s]Running 5000 simulations.:  76%|███████▌  | 3795/5000 [00:14<00:04, 246.97it/s]Running 5000 simulations.:  76%|███████▋  | 3820/5000 [00:15<00:04, 247.37it/s]Running 5000 simulations.:  77%|███████▋  | 3846/5000 [00:15<00:04, 248.60it/s]Running 5000 simulations.:  77%|███████▋  | 3872/5000 [00:15<00:04, 249.05it/s]Running 5000 simulations.:  78%|███████▊  | 3898/5000 [00:15<00:04, 249.70it/s]Running 5000 simulations.:  78%|███████▊  | 3923/5000 [00:15<00:04, 246.82it/s]Running 5000 simulations.:  79%|███████▉  | 3948/5000 [00:15<00:04, 246.98it/s]Running 5000 simulations.:  79%|███████▉  | 3974/5000 [00:15<00:04, 248.69it/s]Running 5000 simulations.:  80%|████████  | 4000/5000 [00:15<00:04, 249.85it/s]Running 5000 simulations.:  81%|████████  | 4026/5000 [00:15<00:03, 249.93it/s]Running 5000 simulations.:  81%|████████  | 4052/5000 [00:16<00:03, 250.33it/s]Running 5000 simulations.:  82%|████████▏ | 4078/5000 [00:16<00:03, 250.04it/s]Running 5000 simulations.:  82%|████████▏ | 4104/5000 [00:16<00:03, 251.35it/s]Running 5000 simulations.:  83%|████████▎ | 4130/5000 [00:16<00:03, 252.26it/s]Running 5000 simulations.:  83%|████████▎ | 4156/5000 [00:16<00:03, 251.31it/s]Running 5000 simulations.:  84%|████████▎ | 4182/5000 [00:16<00:03, 251.48it/s]Running 5000 simulations.:  84%|████████▍ | 4208/5000 [00:16<00:03, 252.21it/s]Running 5000 simulations.:  85%|████████▍ | 4234/5000 [00:16<00:03, 251.44it/s]Running 5000 simulations.:  85%|████████▌ | 4260/5000 [00:16<00:02, 251.94it/s]Running 5000 simulations.:  86%|████████▌ | 4286/5000 [00:16<00:02, 251.06it/s]Running 5000 simulations.:  86%|████████▌ | 4312/5000 [00:17<00:02, 250.73it/s]Running 5000 simulations.:  87%|████████▋ | 4338/5000 [00:17<00:02, 251.19it/s]Running 5000 simulations.:  87%|████████▋ | 4364/5000 [00:17<00:02, 250.73it/s]Running 5000 simulations.:  88%|████████▊ | 4390/5000 [00:17<00:02, 251.29it/s]Running 5000 simulations.:  88%|████████▊ | 4416/5000 [00:17<00:02, 252.33it/s]Running 5000 simulations.:  89%|████████▉ | 4442/5000 [00:17<00:02, 252.72it/s]Running 5000 simulations.:  89%|████████▉ | 4468/5000 [00:17<00:02, 251.87it/s]Running 5000 simulations.:  90%|████████▉ | 4494/5000 [00:17<00:02, 251.86it/s]Running 5000 simulations.:  90%|█████████ | 4520/5000 [00:17<00:01, 252.55it/s]Running 5000 simulations.:  91%|█████████ | 4546/5000 [00:17<00:01, 253.54it/s]Running 5000 simulations.:  91%|█████████▏| 4572/5000 [00:18<00:01, 254.15it/s]Running 5000 simulations.:  92%|█████████▏| 4598/5000 [00:18<00:01, 255.14it/s]Running 5000 simulations.:  92%|█████████▏| 4624/5000 [00:18<00:01, 254.80it/s]Running 5000 simulations.:  93%|█████████▎| 4650/5000 [00:18<00:01, 254.39it/s]Running 5000 simulations.:  94%|█████████▎| 4676/5000 [00:18<00:01, 254.32it/s]Running 5000 simulations.:  94%|█████████▍| 4702/5000 [00:18<00:01, 254.25it/s]Running 5000 simulations.:  95%|█████████▍| 4728/5000 [00:18<00:01, 254.57it/s]Running 5000 simulations.:  95%|█████████▌| 4754/5000 [00:18<00:00, 254.26it/s]Running 5000 simulations.:  96%|█████████▌| 4780/5000 [00:18<00:00, 254.44it/s]Running 5000 simulations.:  96%|█████████▌| 4806/5000 [00:18<00:00, 254.93it/s]Running 5000 simulations.:  97%|█████████▋| 4832/5000 [00:19<00:00, 255.47it/s]Running 5000 simulations.:  97%|█████████▋| 4858/5000 [00:19<00:00, 254.04it/s]Running 5000 simulations.:  98%|█████████▊| 4884/5000 [00:19<00:00, 253.75it/s]Running 5000 simulations.:  98%|█████████▊| 4910/5000 [00:19<00:00, 253.74it/s]Running 5000 simulations.:  99%|█████████▊| 4936/5000 [00:19<00:00, 250.05it/s]Running 5000 simulations.:  99%|█████████▉| 4962/5000 [00:19<00:00, 250.50it/s]Running 5000 simulations.: 100%|█████████▉| 4988/5000 [00:19<00:00, 250.81it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:19<00:00, 252.89it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   1%|          | 27/5000 [00:00<00:18, 265.64it/s]Running 5000 simulations.:   1%|          | 53/5000 [00:00<00:18, 263.58it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:18, 263.68it/s]Running 5000 simulations.:   2%|▏         | 107/5000 [00:00<00:18, 263.03it/s]Running 5000 simulations.:   3%|▎         | 134/5000 [00:00<00:18, 262.44it/s]Running 5000 simulations.:   3%|▎         | 161/5000 [00:00<00:18, 261.76it/s]Running 5000 simulations.:   4%|▎         | 187/5000 [00:00<00:18, 260.57it/s]Running 5000 simulations.:   4%|▍         | 214/5000 [00:00<00:18, 260.45it/s]Running 5000 simulations.:   5%|▍         | 239/5000 [00:00<00:18, 256.36it/s]Running 5000 simulations.:   5%|▌         | 265/5000 [00:01<00:18, 256.43it/s]Running 5000 simulations.:   6%|▌         | 291/5000 [00:01<00:18, 256.45it/s]Running 5000 simulations.:   6%|▋         | 317/5000 [00:01<00:18, 256.55it/s]Running 5000 simulations.:   7%|▋         | 343/5000 [00:01<00:18, 256.47it/s]Running 5000 simulations.:   7%|▋         | 369/5000 [00:01<00:18, 256.20it/s]Running 5000 simulations.:   8%|▊         | 395/5000 [00:01<00:17, 256.29it/s]Running 5000 simulations.:   8%|▊         | 421/5000 [00:01<00:17, 256.53it/s]Running 5000 simulations.:   9%|▉         | 447/5000 [00:01<00:17, 257.12it/s]Running 5000 simulations.:   9%|▉         | 473/5000 [00:01<00:17, 253.10it/s]Running 5000 simulations.:  10%|▉         | 499/5000 [00:01<00:17, 251.00it/s]Running 5000 simulations.:  10%|█         | 525/5000 [00:02<00:17, 252.82it/s]Running 5000 simulations.:  11%|█         | 552/5000 [00:02<00:17, 255.13it/s]Running 5000 simulations.:  12%|█▏        | 578/5000 [00:02<00:17, 256.35it/s]Running 5000 simulations.:  12%|█▏        | 604/5000 [00:02<00:17, 255.93it/s]Running 5000 simulations.:  13%|█▎        | 630/5000 [00:02<00:17, 255.36it/s]Running 5000 simulations.:  13%|█▎        | 656/5000 [00:02<00:17, 254.65it/s]Running 5000 simulations.:  14%|█▎        | 682/5000 [00:02<00:16, 254.82it/s]Running 5000 simulations.:  14%|█▍        | 708/5000 [00:02<00:16, 254.78it/s]Running 5000 simulations.:  15%|█▍        | 734/5000 [00:02<00:16, 254.36it/s]Running 5000 simulations.:  15%|█▌        | 760/5000 [00:02<00:16, 254.63it/s]Running 5000 simulations.:  16%|█▌        | 786/5000 [00:03<00:16, 253.78it/s]Running 5000 simulations.:  16%|█▌        | 812/5000 [00:03<00:16, 250.85it/s]Running 5000 simulations.:  17%|█▋        | 838/5000 [00:03<00:16, 251.90it/s]Running 5000 simulations.:  17%|█▋        | 864/5000 [00:03<00:16, 253.95it/s]Running 5000 simulations.:  18%|█▊        | 890/5000 [00:03<00:16, 254.27it/s]Running 5000 simulations.:  18%|█▊        | 916/5000 [00:03<00:16, 252.82it/s]Running 5000 simulations.:  19%|█▉        | 942/5000 [00:03<00:16, 253.57it/s]Running 5000 simulations.:  19%|█▉        | 968/5000 [00:03<00:15, 253.77it/s]Running 5000 simulations.:  20%|█▉        | 994/5000 [00:03<00:15, 253.46it/s]Running 5000 simulations.:  20%|██        | 1020/5000 [00:03<00:15, 253.03it/s]Running 5000 simulations.:  21%|██        | 1046/5000 [00:04<00:15, 254.09it/s]Running 5000 simulations.:  21%|██▏       | 1072/5000 [00:04<00:15, 253.92it/s]Running 5000 simulations.:  22%|██▏       | 1098/5000 [00:04<00:15, 253.71it/s]Running 5000 simulations.:  22%|██▏       | 1124/5000 [00:04<00:15, 254.46it/s]Running 5000 simulations.:  23%|██▎       | 1150/5000 [00:04<00:15, 255.22it/s]Running 5000 simulations.:  24%|██▎       | 1176/5000 [00:04<00:14, 255.26it/s]Running 5000 simulations.:  24%|██▍       | 1202/5000 [00:04<00:14, 255.19it/s]Running 5000 simulations.:  25%|██▍       | 1228/5000 [00:04<00:14, 255.06it/s]Running 5000 simulations.:  25%|██▌       | 1254/5000 [00:04<00:14, 255.01it/s]Running 5000 simulations.:  26%|██▌       | 1280/5000 [00:05<00:14, 254.94it/s]Running 5000 simulations.:  26%|██▌       | 1306/5000 [00:05<00:14, 254.41it/s]Running 5000 simulations.:  27%|██▋       | 1332/5000 [00:05<00:14, 253.19it/s]Running 5000 simulations.:  27%|██▋       | 1358/5000 [00:05<00:14, 254.01it/s]Running 5000 simulations.:  28%|██▊       | 1384/5000 [00:05<00:14, 254.27it/s]Running 5000 simulations.:  28%|██▊       | 1410/5000 [00:05<00:14, 253.80it/s]Running 5000 simulations.:  29%|██▊       | 1436/5000 [00:05<00:14, 253.89it/s]Running 5000 simulations.:  29%|██▉       | 1462/5000 [00:05<00:13, 253.98it/s]Running 5000 simulations.:  30%|██▉       | 1488/5000 [00:05<00:13, 254.85it/s]Running 5000 simulations.:  30%|███       | 1514/5000 [00:05<00:13, 254.68it/s]Running 5000 simulations.:  31%|███       | 1540/5000 [00:06<00:13, 254.62it/s]Running 5000 simulations.:  31%|███▏      | 1566/5000 [00:06<00:13, 254.06it/s]Running 5000 simulations.:  32%|███▏      | 1592/5000 [00:06<00:13, 243.64it/s]Running 5000 simulations.:  32%|███▏      | 1618/5000 [00:06<00:13, 246.23it/s]Running 5000 simulations.:  33%|███▎      | 1644/5000 [00:06<00:13, 248.61it/s]Running 5000 simulations.:  33%|███▎      | 1670/5000 [00:06<00:13, 250.56it/s]Running 5000 simulations.:  34%|███▍      | 1696/5000 [00:06<00:13, 251.31it/s]Running 5000 simulations.:  34%|███▍      | 1722/5000 [00:06<00:13, 251.61it/s]Running 5000 simulations.:  35%|███▍      | 1748/5000 [00:06<00:12, 250.92it/s]Running 5000 simulations.:  35%|███▌      | 1774/5000 [00:06<00:12, 251.35it/s]Running 5000 simulations.:  36%|███▌      | 1800/5000 [00:07<00:12, 251.66it/s]Running 5000 simulations.:  37%|███▋      | 1826/5000 [00:07<00:12, 252.28it/s]Running 5000 simulations.:  37%|███▋      | 1852/5000 [00:07<00:12, 252.97it/s]Running 5000 simulations.:  38%|███▊      | 1878/5000 [00:07<00:12, 252.82it/s]Running 5000 simulations.:  38%|███▊      | 1904/5000 [00:07<00:12, 253.06it/s]Running 5000 simulations.:  39%|███▊      | 1930/5000 [00:07<00:12, 252.84it/s]Running 5000 simulations.:  39%|███▉      | 1956/5000 [00:07<00:12, 252.56it/s]Running 5000 simulations.:  40%|███▉      | 1982/5000 [00:07<00:11, 252.11it/s]Running 5000 simulations.:  40%|████      | 2008/5000 [00:07<00:11, 249.71it/s]Running 5000 simulations.:  41%|████      | 2034/5000 [00:08<00:11, 250.55it/s]Running 5000 simulations.:  41%|████      | 2060/5000 [00:08<00:11, 251.19it/s]Running 5000 simulations.:  42%|████▏     | 2086/5000 [00:08<00:11, 250.84it/s]Running 5000 simulations.:  42%|████▏     | 2112/5000 [00:08<00:11, 250.64it/s]Running 5000 simulations.:  43%|████▎     | 2138/5000 [00:08<00:11, 250.13it/s]Running 5000 simulations.:  43%|████▎     | 2164/5000 [00:08<00:11, 250.70it/s]Running 5000 simulations.:  44%|████▍     | 2190/5000 [00:08<00:11, 250.82it/s]Running 5000 simulations.:  44%|████▍     | 2216/5000 [00:08<00:11, 250.76it/s]Running 5000 simulations.:  45%|████▍     | 2242/5000 [00:08<00:10, 250.96it/s]Running 5000 simulations.:  45%|████▌     | 2268/5000 [00:08<00:10, 251.30it/s]Running 5000 simulations.:  46%|████▌     | 2294/5000 [00:09<00:10, 251.47it/s]Running 5000 simulations.:  46%|████▋     | 2320/5000 [00:09<00:10, 251.55it/s]Running 5000 simulations.:  47%|████▋     | 2346/5000 [00:09<00:10, 252.14it/s]Running 5000 simulations.:  47%|████▋     | 2372/5000 [00:09<00:10, 251.54it/s]Running 5000 simulations.:  48%|████▊     | 2398/5000 [00:09<00:10, 251.32it/s]Running 5000 simulations.:  48%|████▊     | 2424/5000 [00:09<00:10, 250.97it/s]Running 5000 simulations.:  49%|████▉     | 2450/5000 [00:09<00:10, 251.44it/s]Running 5000 simulations.:  50%|████▉     | 2476/5000 [00:09<00:09, 252.88it/s]Running 5000 simulations.:  50%|█████     | 2502/5000 [00:09<00:09, 253.56it/s]Running 5000 simulations.:  51%|█████     | 2528/5000 [00:09<00:09, 254.44it/s]Running 5000 simulations.:  51%|█████     | 2554/5000 [00:10<00:09, 253.17it/s]Running 5000 simulations.:  52%|█████▏    | 2580/5000 [00:10<00:09, 254.07it/s]Running 5000 simulations.:  52%|█████▏    | 2606/5000 [00:10<00:09, 253.31it/s]Running 5000 simulations.:  53%|█████▎    | 2632/5000 [00:10<00:09, 253.48it/s]Running 5000 simulations.:  53%|█████▎    | 2658/5000 [00:10<00:09, 252.96it/s]Running 5000 simulations.:  54%|█████▎    | 2684/5000 [00:10<00:09, 252.92it/s]Running 5000 simulations.:  54%|█████▍    | 2710/5000 [00:10<00:09, 251.71it/s]Running 5000 simulations.:  55%|█████▍    | 2736/5000 [00:10<00:09, 251.38it/s]Running 5000 simulations.:  55%|█████▌    | 2762/5000 [00:10<00:08, 250.86it/s]Running 5000 simulations.:  56%|█████▌    | 2788/5000 [00:11<00:08, 249.42it/s]Running 5000 simulations.:  56%|█████▋    | 2814/5000 [00:11<00:08, 249.77it/s]Running 5000 simulations.:  57%|█████▋    | 2840/5000 [00:11<00:08, 250.35it/s]Running 5000 simulations.:  57%|█████▋    | 2866/5000 [00:11<00:08, 250.88it/s]Running 5000 simulations.:  58%|█████▊    | 2892/5000 [00:11<00:08, 251.49it/s]Running 5000 simulations.:  58%|█████▊    | 2918/5000 [00:11<00:08, 252.65it/s]Running 5000 simulations.:  59%|█████▉    | 2944/5000 [00:11<00:08, 251.70it/s]Running 5000 simulations.:  59%|█████▉    | 2970/5000 [00:11<00:08, 249.28it/s]Running 5000 simulations.:  60%|█████▉    | 2995/5000 [00:11<00:08, 247.08it/s]Running 5000 simulations.:  60%|██████    | 3020/5000 [00:11<00:07, 247.75it/s]Running 5000 simulations.:  61%|██████    | 3046/5000 [00:12<00:07, 248.74it/s]Running 5000 simulations.:  61%|██████▏   | 3072/5000 [00:12<00:07, 249.43it/s]Running 5000 simulations.:  62%|██████▏   | 3098/5000 [00:12<00:07, 249.75it/s]Running 5000 simulations.:  62%|██████▏   | 3124/5000 [00:12<00:07, 250.04it/s]Running 5000 simulations.:  63%|██████▎   | 3150/5000 [00:12<00:07, 250.83it/s]Running 5000 simulations.:  64%|██████▎   | 3176/5000 [00:12<00:07, 250.81it/s]Running 5000 simulations.:  64%|██████▍   | 3202/5000 [00:12<00:07, 250.63it/s]Running 5000 simulations.:  65%|██████▍   | 3228/5000 [00:12<00:07, 250.64it/s]Running 5000 simulations.:  65%|██████▌   | 3254/5000 [00:12<00:06, 250.61it/s]Running 5000 simulations.:  66%|██████▌   | 3280/5000 [00:12<00:06, 250.80it/s]Running 5000 simulations.:  66%|██████▌   | 3306/5000 [00:13<00:06, 250.71it/s]Running 5000 simulations.:  67%|██████▋   | 3332/5000 [00:13<00:06, 250.27it/s]Running 5000 simulations.:  67%|██████▋   | 3358/5000 [00:13<00:06, 248.99it/s]Running 5000 simulations.:  68%|██████▊   | 3384/5000 [00:13<00:06, 249.91it/s]Running 5000 simulations.:  68%|██████▊   | 3410/5000 [00:13<00:06, 249.98it/s]Running 5000 simulations.:  69%|██████▊   | 3436/5000 [00:13<00:06, 251.25it/s]Running 5000 simulations.:  69%|██████▉   | 3462/5000 [00:13<00:06, 251.78it/s]Running 5000 simulations.:  70%|██████▉   | 3488/5000 [00:13<00:09, 167.95it/s]Running 5000 simulations.:  70%|███████   | 3514/5000 [00:14<00:07, 186.85it/s]Running 5000 simulations.:  71%|███████   | 3540/5000 [00:14<00:07, 202.27it/s]Running 5000 simulations.:  71%|███████▏  | 3565/5000 [00:14<00:06, 214.47it/s]Running 5000 simulations.:  72%|███████▏  | 3591/5000 [00:14<00:06, 224.63it/s]Running 5000 simulations.:  72%|███████▏  | 3617/5000 [00:14<00:05, 231.75it/s]Running 5000 simulations.:  73%|███████▎  | 3643/5000 [00:14<00:05, 237.84it/s]Running 5000 simulations.:  73%|███████▎  | 3669/5000 [00:14<00:05, 242.65it/s]Running 5000 simulations.:  74%|███████▍  | 3695/5000 [00:14<00:05, 246.35it/s]Running 5000 simulations.:  74%|███████▍  | 3721/5000 [00:14<00:05, 246.40it/s]Running 5000 simulations.:  75%|███████▍  | 3747/5000 [00:14<00:05, 247.72it/s]Running 5000 simulations.:  75%|███████▌  | 3773/5000 [00:15<00:04, 248.54it/s]Running 5000 simulations.:  76%|███████▌  | 3799/5000 [00:15<00:04, 249.04it/s]Running 5000 simulations.:  76%|███████▋  | 3825/5000 [00:15<00:04, 249.49it/s]Running 5000 simulations.:  77%|███████▋  | 3851/5000 [00:15<00:04, 250.12it/s]Running 5000 simulations.:  78%|███████▊  | 3877/5000 [00:15<00:04, 250.44it/s]Running 5000 simulations.:  78%|███████▊  | 3903/5000 [00:15<00:04, 251.07it/s]Running 5000 simulations.:  79%|███████▊  | 3929/5000 [00:15<00:04, 250.95it/s]Running 5000 simulations.:  79%|███████▉  | 3955/5000 [00:15<00:04, 251.38it/s]Running 5000 simulations.:  80%|███████▉  | 3981/5000 [00:15<00:04, 250.77it/s]Running 5000 simulations.:  80%|████████  | 4007/5000 [00:16<00:03, 251.02it/s]Running 5000 simulations.:  81%|████████  | 4033/5000 [00:16<00:03, 251.39it/s]Running 5000 simulations.:  81%|████████  | 4059/5000 [00:16<00:03, 251.36it/s]Running 5000 simulations.:  82%|████████▏ | 4085/5000 [00:16<00:03, 251.68it/s]Running 5000 simulations.:  82%|████████▏ | 4111/5000 [00:16<00:03, 251.51it/s]Running 5000 simulations.:  83%|████████▎ | 4137/5000 [00:16<00:03, 250.91it/s]Running 5000 simulations.:  83%|████████▎ | 4163/5000 [00:16<00:03, 251.35it/s]Running 5000 simulations.:  84%|████████▍ | 4189/5000 [00:16<00:03, 251.23it/s]Running 5000 simulations.:  84%|████████▍ | 4215/5000 [00:16<00:03, 251.45it/s]Running 5000 simulations.:  85%|████████▍ | 4241/5000 [00:16<00:03, 249.53it/s]Running 5000 simulations.:  85%|████████▌ | 4266/5000 [00:17<00:02, 247.02it/s]Running 5000 simulations.:  86%|████████▌ | 4292/5000 [00:17<00:02, 248.75it/s]Running 5000 simulations.:  86%|████████▋ | 4318/5000 [00:17<00:02, 249.49it/s]Running 5000 simulations.:  87%|████████▋ | 4344/5000 [00:17<00:02, 250.07it/s]Running 5000 simulations.:  87%|████████▋ | 4370/5000 [00:17<00:02, 251.51it/s]Running 5000 simulations.:  88%|████████▊ | 4396/5000 [00:17<00:02, 251.49it/s]Running 5000 simulations.:  88%|████████▊ | 4422/5000 [00:17<00:02, 252.88it/s]Running 5000 simulations.:  89%|████████▉ | 4448/5000 [00:17<00:02, 253.85it/s]Running 5000 simulations.:  89%|████████▉ | 4474/5000 [00:17<00:02, 254.20it/s]Running 5000 simulations.:  90%|█████████ | 4500/5000 [00:17<00:01, 254.10it/s]Running 5000 simulations.:  91%|█████████ | 4526/5000 [00:18<00:01, 250.71it/s]Running 5000 simulations.:  91%|█████████ | 4552/5000 [00:18<00:01, 251.25it/s]Running 5000 simulations.:  92%|█████████▏| 4578/5000 [00:18<00:01, 251.19it/s]Running 5000 simulations.:  92%|█████████▏| 4604/5000 [00:18<00:01, 252.57it/s]Running 5000 simulations.:  93%|█████████▎| 4630/5000 [00:18<00:01, 254.38it/s]Running 5000 simulations.:  93%|█████████▎| 4656/5000 [00:18<00:01, 255.09it/s]Running 5000 simulations.:  94%|█████████▎| 4682/5000 [00:18<00:01, 254.23it/s]Running 5000 simulations.:  94%|█████████▍| 4708/5000 [00:18<00:01, 254.77it/s]Running 5000 simulations.:  95%|█████████▍| 4734/5000 [00:18<00:01, 255.62it/s]Running 5000 simulations.:  95%|█████████▌| 4760/5000 [00:19<00:00, 255.22it/s]Running 5000 simulations.:  96%|█████████▌| 4786/5000 [00:19<00:00, 255.75it/s]Running 5000 simulations.:  96%|█████████▌| 4812/5000 [00:19<00:00, 254.82it/s]Running 5000 simulations.:  97%|█████████▋| 4838/5000 [00:19<00:00, 254.60it/s]Running 5000 simulations.:  97%|█████████▋| 4864/5000 [00:19<00:00, 254.58it/s]Running 5000 simulations.:  98%|█████████▊| 4890/5000 [00:19<00:00, 254.52it/s]Running 5000 simulations.:  98%|█████████▊| 4916/5000 [00:19<00:00, 255.06it/s]Running 5000 simulations.:  99%|█████████▉| 4942/5000 [00:19<00:00, 254.96it/s]Running 5000 simulations.:  99%|█████████▉| 4968/5000 [00:19<00:00, 255.00it/s]Running 5000 simulations.: 100%|█████████▉| 4994/5000 [00:19<00:00, 254.20it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:19<00:00, 250.50it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   1%|          | 26/5000 [00:00<00:19, 256.66it/s]Running 5000 simulations.:   1%|          | 52/5000 [00:00<00:19, 257.57it/s]Running 5000 simulations.:   2%|▏         | 78/5000 [00:00<00:19, 258.03it/s]Running 5000 simulations.:   2%|▏         | 104/5000 [00:00<00:18, 258.43it/s]Running 5000 simulations.:   3%|▎         | 130/5000 [00:00<00:18, 257.34it/s]Running 5000 simulations.:   3%|▎         | 156/5000 [00:00<00:18, 256.16it/s]Running 5000 simulations.:   4%|▎         | 181/5000 [00:00<00:19, 253.01it/s]Running 5000 simulations.:   4%|▍         | 207/5000 [00:00<00:18, 253.39it/s]Running 5000 simulations.:   5%|▍         | 234/5000 [00:00<00:18, 255.52it/s]Running 5000 simulations.:   5%|▌         | 260/5000 [00:01<00:18, 256.26it/s]Running 5000 simulations.:   6%|▌         | 286/5000 [00:01<00:18, 254.54it/s]Running 5000 simulations.:   6%|▌         | 312/5000 [00:01<00:18, 253.36it/s]Running 5000 simulations.:   7%|▋         | 338/5000 [00:01<00:18, 253.04it/s]Running 5000 simulations.:   7%|▋         | 364/5000 [00:01<00:18, 254.96it/s]Running 5000 simulations.:   8%|▊         | 390/5000 [00:01<00:18, 254.28it/s]Running 5000 simulations.:   8%|▊         | 416/5000 [00:01<00:17, 255.40it/s]Running 5000 simulations.:   9%|▉         | 442/5000 [00:01<00:17, 253.72it/s]Running 5000 simulations.:   9%|▉         | 468/5000 [00:01<00:17, 253.40it/s]Running 5000 simulations.:  10%|▉         | 494/5000 [00:01<00:17, 253.74it/s]Running 5000 simulations.:  10%|█         | 520/5000 [00:02<00:17, 255.20it/s]Running 5000 simulations.:  11%|█         | 546/5000 [00:02<00:17, 251.81it/s]Running 5000 simulations.:  11%|█▏        | 572/5000 [00:02<00:17, 249.14it/s]Running 5000 simulations.:  12%|█▏        | 597/5000 [00:02<00:17, 248.43it/s]Running 5000 simulations.:  12%|█▏        | 623/5000 [00:02<00:17, 250.07it/s]Running 5000 simulations.:  13%|█▎        | 649/5000 [00:02<00:17, 250.17it/s]Running 5000 simulations.:  14%|█▎        | 675/5000 [00:02<00:17, 250.58it/s]Running 5000 simulations.:  14%|█▍        | 701/5000 [00:02<00:17, 251.03it/s]Running 5000 simulations.:  15%|█▍        | 727/5000 [00:02<00:16, 252.05it/s]Running 5000 simulations.:  15%|█▌        | 753/5000 [00:02<00:16, 252.50it/s]Running 5000 simulations.:  16%|█▌        | 779/5000 [00:03<00:16, 252.83it/s]Running 5000 simulations.:  16%|█▌        | 805/5000 [00:03<00:16, 251.66it/s]Running 5000 simulations.:  17%|█▋        | 831/5000 [00:03<00:16, 251.15it/s]Running 5000 simulations.:  17%|█▋        | 857/5000 [00:03<00:16, 248.71it/s]Running 5000 simulations.:  18%|█▊        | 883/5000 [00:03<00:16, 249.97it/s]Running 5000 simulations.:  18%|█▊        | 909/5000 [00:03<00:16, 251.38it/s]Running 5000 simulations.:  19%|█▊        | 935/5000 [00:03<00:16, 249.93it/s]Running 5000 simulations.:  19%|█▉        | 961/5000 [00:03<00:16, 249.94it/s]Running 5000 simulations.:  20%|█▉        | 986/5000 [00:03<00:16, 249.09it/s]Running 5000 simulations.:  20%|██        | 1012/5000 [00:04<00:15, 249.84it/s]Running 5000 simulations.:  21%|██        | 1037/5000 [00:04<00:15, 249.82it/s]Running 5000 simulations.:  21%|██        | 1062/5000 [00:04<00:15, 249.84it/s]Running 5000 simulations.:  22%|██▏       | 1087/5000 [00:04<00:15, 247.82it/s]Running 5000 simulations.:  22%|██▏       | 1112/5000 [00:04<00:15, 246.67it/s]Running 5000 simulations.:  23%|██▎       | 1137/5000 [00:04<00:15, 246.98it/s]Running 5000 simulations.:  23%|██▎       | 1162/5000 [00:04<00:15, 247.56it/s]Running 5000 simulations.:  24%|██▍       | 1188/5000 [00:04<00:15, 248.37it/s]Running 5000 simulations.:  24%|██▍       | 1213/5000 [00:04<00:15, 248.30it/s]Running 5000 simulations.:  25%|██▍       | 1238/5000 [00:04<00:15, 246.30it/s]Running 5000 simulations.:  25%|██▌       | 1263/5000 [00:05<00:15, 245.66it/s]Running 5000 simulations.:  26%|██▌       | 1288/5000 [00:05<00:15, 244.94it/s]Running 5000 simulations.:  26%|██▋       | 1313/5000 [00:05<00:15, 243.90it/s]Running 5000 simulations.:  27%|██▋       | 1338/5000 [00:05<00:14, 244.53it/s]Running 5000 simulations.:  27%|██▋       | 1363/5000 [00:05<00:14, 245.08it/s]Running 5000 simulations.:  28%|██▊       | 1388/5000 [00:05<00:14, 244.85it/s]Running 5000 simulations.:  28%|██▊       | 1413/5000 [00:05<00:14, 244.86it/s]Running 5000 simulations.:  29%|██▉       | 1438/5000 [00:05<00:14, 244.23it/s]Running 5000 simulations.:  29%|██▉       | 1463/5000 [00:05<00:14, 243.56it/s]Running 5000 simulations.:  30%|██▉       | 1488/5000 [00:05<00:14, 244.07it/s]Running 5000 simulations.:  30%|███       | 1513/5000 [00:06<00:14, 245.62it/s]Running 5000 simulations.:  31%|███       | 1538/5000 [00:06<00:14, 246.48it/s]Running 5000 simulations.:  31%|███▏      | 1563/5000 [00:06<00:14, 245.47it/s]Running 5000 simulations.:  32%|███▏      | 1588/5000 [00:06<00:13, 245.98it/s]Running 5000 simulations.:  32%|███▏      | 1613/5000 [00:06<00:13, 244.34it/s]Running 5000 simulations.:  33%|███▎      | 1638/5000 [00:06<00:14, 239.65it/s]Running 5000 simulations.:  33%|███▎      | 1663/5000 [00:06<00:13, 240.54it/s]Running 5000 simulations.:  34%|███▍      | 1688/5000 [00:06<00:13, 242.73it/s]Running 5000 simulations.:  34%|███▍      | 1713/5000 [00:06<00:13, 244.27it/s]Running 5000 simulations.:  35%|███▍      | 1738/5000 [00:06<00:13, 244.18it/s]Running 5000 simulations.:  35%|███▌      | 1763/5000 [00:07<00:13, 243.64it/s]Running 5000 simulations.:  36%|███▌      | 1788/5000 [00:07<00:13, 243.54it/s]Running 5000 simulations.:  36%|███▋      | 1813/5000 [00:07<00:13, 242.97it/s]Running 5000 simulations.:  37%|███▋      | 1838/5000 [00:07<00:12, 243.61it/s]Running 5000 simulations.:  37%|███▋      | 1863/5000 [00:07<00:12, 245.17it/s]Running 5000 simulations.:  38%|███▊      | 1889/5000 [00:07<00:12, 246.89it/s]Running 5000 simulations.:  38%|███▊      | 1914/5000 [00:07<00:12, 247.69it/s]Running 5000 simulations.:  39%|███▉      | 1939/5000 [00:07<00:12, 245.73it/s]Running 5000 simulations.:  39%|███▉      | 1964/5000 [00:07<00:12, 244.43it/s]Running 5000 simulations.:  40%|███▉      | 1989/5000 [00:07<00:12, 243.96it/s]Running 5000 simulations.:  40%|████      | 2014/5000 [00:08<00:12, 244.87it/s]Running 5000 simulations.:  41%|████      | 2039/5000 [00:08<00:12, 246.37it/s]Running 5000 simulations.:  41%|████▏     | 2065/5000 [00:08<00:11, 247.21it/s]Running 5000 simulations.:  42%|████▏     | 2090/5000 [00:08<00:11, 244.53it/s]Running 5000 simulations.:  42%|████▏     | 2115/5000 [00:08<00:11, 244.54it/s]Running 5000 simulations.:  43%|████▎     | 2140/5000 [00:08<00:11, 244.44it/s]Running 5000 simulations.:  43%|████▎     | 2165/5000 [00:08<00:11, 244.13it/s]Running 5000 simulations.:  44%|████▍     | 2190/5000 [00:08<00:11, 243.83it/s]Running 5000 simulations.:  44%|████▍     | 2215/5000 [00:08<00:11, 243.63it/s]Running 5000 simulations.:  45%|████▍     | 2240/5000 [00:09<00:11, 243.33it/s]Running 5000 simulations.:  45%|████▌     | 2265/5000 [00:09<00:11, 243.43it/s]Running 5000 simulations.:  46%|████▌     | 2290/5000 [00:09<00:11, 244.12it/s]Running 5000 simulations.:  46%|████▋     | 2315/5000 [00:09<00:10, 245.53it/s]Running 5000 simulations.:  47%|████▋     | 2340/5000 [00:09<00:10, 244.16it/s]Running 5000 simulations.:  47%|████▋     | 2365/5000 [00:09<00:10, 244.44it/s]Running 5000 simulations.:  48%|████▊     | 2390/5000 [00:09<00:10, 244.85it/s]Running 5000 simulations.:  48%|████▊     | 2415/5000 [00:09<00:10, 244.27it/s]Running 5000 simulations.:  49%|████▉     | 2440/5000 [00:09<00:10, 244.12it/s]Running 5000 simulations.:  49%|████▉     | 2465/5000 [00:09<00:10, 245.06it/s]Running 5000 simulations.:  50%|████▉     | 2490/5000 [00:10<00:10, 244.52it/s]Running 5000 simulations.:  50%|█████     | 2515/5000 [00:10<00:10, 245.56it/s]Running 5000 simulations.:  51%|█████     | 2540/5000 [00:10<00:10, 244.07it/s]Running 5000 simulations.:  51%|█████▏    | 2565/5000 [00:10<00:09, 244.08it/s]Running 5000 simulations.:  52%|█████▏    | 2590/5000 [00:10<00:09, 242.90it/s]Running 5000 simulations.:  52%|█████▏    | 2615/5000 [00:10<00:09, 243.37it/s]Running 5000 simulations.:  53%|█████▎    | 2640/5000 [00:10<00:09, 244.37it/s]Running 5000 simulations.:  53%|█████▎    | 2665/5000 [00:10<00:09, 245.01it/s]Running 5000 simulations.:  54%|█████▍    | 2690/5000 [00:10<00:09, 243.70it/s]Running 5000 simulations.:  54%|█████▍    | 2715/5000 [00:10<00:09, 242.23it/s]Running 5000 simulations.:  55%|█████▍    | 2740/5000 [00:11<00:09, 242.03it/s]Running 5000 simulations.:  55%|█████▌    | 2765/5000 [00:11<00:09, 242.69it/s]Running 5000 simulations.:  56%|█████▌    | 2790/5000 [00:11<00:09, 243.21it/s]Running 5000 simulations.:  56%|█████▋    | 2815/5000 [00:11<00:09, 241.87it/s]Running 5000 simulations.:  57%|█████▋    | 2840/5000 [00:11<00:08, 241.26it/s]Running 5000 simulations.:  57%|█████▋    | 2865/5000 [00:11<00:08, 240.64it/s]Running 5000 simulations.:  58%|█████▊    | 2890/5000 [00:11<00:08, 240.34it/s]Running 5000 simulations.:  58%|█████▊    | 2915/5000 [00:11<00:08, 241.41it/s]Running 5000 simulations.:  59%|█████▉    | 2940/5000 [00:11<00:08, 242.03it/s]Running 5000 simulations.:  59%|█████▉    | 2965/5000 [00:12<00:08, 241.66it/s]Running 5000 simulations.:  60%|█████▉    | 2990/5000 [00:12<00:08, 241.42it/s]Running 5000 simulations.:  60%|██████    | 3015/5000 [00:12<00:08, 241.23it/s]Running 5000 simulations.:  61%|██████    | 3040/5000 [00:12<00:08, 241.16it/s]Running 5000 simulations.:  61%|██████▏   | 3065/5000 [00:12<00:08, 241.50it/s]Running 5000 simulations.:  62%|██████▏   | 3090/5000 [00:12<00:07, 239.79it/s]Running 5000 simulations.:  62%|██████▏   | 3115/5000 [00:12<00:07, 240.43it/s]Running 5000 simulations.:  63%|██████▎   | 3140/5000 [00:12<00:07, 241.95it/s]Running 5000 simulations.:  63%|██████▎   | 3165/5000 [00:12<00:07, 243.02it/s]Running 5000 simulations.:  64%|██████▍   | 3190/5000 [00:12<00:07, 240.72it/s]Running 5000 simulations.:  64%|██████▍   | 3215/5000 [00:13<00:07, 240.00it/s]Running 5000 simulations.:  65%|██████▍   | 3240/5000 [00:13<00:07, 239.84it/s]Running 5000 simulations.:  65%|██████▌   | 3265/5000 [00:13<00:07, 241.33it/s]Running 5000 simulations.:  66%|██████▌   | 3290/5000 [00:13<00:07, 241.74it/s]Running 5000 simulations.:  66%|██████▋   | 3315/5000 [00:13<00:06, 242.91it/s]Running 5000 simulations.:  67%|██████▋   | 3340/5000 [00:13<00:06, 242.87it/s]Running 5000 simulations.:  67%|██████▋   | 3365/5000 [00:13<00:06, 242.74it/s]Running 5000 simulations.:  68%|██████▊   | 3390/5000 [00:13<00:06, 243.47it/s]Running 5000 simulations.:  68%|██████▊   | 3415/5000 [00:13<00:06, 243.58it/s]Running 5000 simulations.:  69%|██████▉   | 3440/5000 [00:13<00:06, 244.78it/s]Running 5000 simulations.:  69%|██████▉   | 3465/5000 [00:14<00:06, 243.94it/s]Running 5000 simulations.:  70%|██████▉   | 3490/5000 [00:14<00:06, 242.85it/s]Running 5000 simulations.:  70%|███████   | 3515/5000 [00:14<00:06, 241.51it/s]Running 5000 simulations.:  71%|███████   | 3540/5000 [00:14<00:06, 241.33it/s]Running 5000 simulations.:  71%|███████▏  | 3565/5000 [00:14<00:05, 241.69it/s]Running 5000 simulations.:  72%|███████▏  | 3590/5000 [00:14<00:05, 243.44it/s]Running 5000 simulations.:  72%|███████▏  | 3615/5000 [00:14<00:05, 241.56it/s]Running 5000 simulations.:  73%|███████▎  | 3640/5000 [00:14<00:05, 241.19it/s]Running 5000 simulations.:  73%|███████▎  | 3665/5000 [00:14<00:05, 241.66it/s]Running 5000 simulations.:  74%|███████▍  | 3690/5000 [00:15<00:05, 241.57it/s]Running 5000 simulations.:  74%|███████▍  | 3715/5000 [00:15<00:05, 242.09it/s]Running 5000 simulations.:  75%|███████▍  | 3740/5000 [00:15<00:05, 242.05it/s]Running 5000 simulations.:  75%|███████▌  | 3765/5000 [00:15<00:05, 240.96it/s]Running 5000 simulations.:  76%|███████▌  | 3790/5000 [00:15<00:05, 241.39it/s]Running 5000 simulations.:  76%|███████▋  | 3815/5000 [00:15<00:04, 241.71it/s]Running 5000 simulations.:  77%|███████▋  | 3840/5000 [00:15<00:04, 242.45it/s]Running 5000 simulations.:  77%|███████▋  | 3865/5000 [00:15<00:04, 242.97it/s]Running 5000 simulations.:  78%|███████▊  | 3890/5000 [00:15<00:04, 242.70it/s]Running 5000 simulations.:  78%|███████▊  | 3915/5000 [00:15<00:04, 242.28it/s]Running 5000 simulations.:  79%|███████▉  | 3940/5000 [00:16<00:04, 241.62it/s]Running 5000 simulations.:  79%|███████▉  | 3965/5000 [00:16<00:04, 241.73it/s]Running 5000 simulations.:  80%|███████▉  | 3990/5000 [00:16<00:04, 242.06it/s]Running 5000 simulations.:  80%|████████  | 4015/5000 [00:16<00:04, 242.33it/s]Running 5000 simulations.:  81%|████████  | 4040/5000 [00:16<00:03, 241.22it/s]Running 5000 simulations.:  81%|████████▏ | 4065/5000 [00:16<00:03, 239.36it/s]Running 5000 simulations.:  82%|████████▏ | 4089/5000 [00:16<00:03, 237.58it/s]Running 5000 simulations.:  82%|████████▏ | 4114/5000 [00:16<00:03, 239.57it/s]Running 5000 simulations.:  83%|████████▎ | 4139/5000 [00:16<00:03, 242.04it/s]Running 5000 simulations.:  83%|████████▎ | 4164/5000 [00:16<00:03, 243.37it/s]Running 5000 simulations.:  84%|████████▍ | 4189/5000 [00:17<00:03, 242.29it/s]Running 5000 simulations.:  84%|████████▍ | 4214/5000 [00:17<00:03, 241.79it/s]Running 5000 simulations.:  85%|████████▍ | 4239/5000 [00:17<00:03, 241.19it/s]Running 5000 simulations.:  85%|████████▌ | 4264/5000 [00:17<00:03, 242.40it/s]Running 5000 simulations.:  86%|████████▌ | 4289/5000 [00:17<00:02, 242.33it/s]Running 5000 simulations.:  86%|████████▋ | 4314/5000 [00:17<00:02, 241.59it/s]Running 5000 simulations.:  87%|████████▋ | 4339/5000 [00:17<00:02, 242.49it/s]Running 5000 simulations.:  87%|████████▋ | 4364/5000 [00:17<00:02, 244.35it/s]Running 5000 simulations.:  88%|████████▊ | 4389/5000 [00:17<00:02, 244.37it/s]Running 5000 simulations.:  88%|████████▊ | 4414/5000 [00:17<00:02, 243.85it/s]Running 5000 simulations.:  89%|████████▉ | 4439/5000 [00:18<00:02, 243.16it/s]Running 5000 simulations.:  89%|████████▉ | 4464/5000 [00:18<00:02, 244.38it/s]Running 5000 simulations.:  90%|████████▉ | 4489/5000 [00:18<00:02, 244.65it/s]Running 5000 simulations.:  90%|█████████ | 4515/5000 [00:18<00:01, 246.88it/s]Running 5000 simulations.:  91%|█████████ | 4540/5000 [00:18<00:01, 246.24it/s]Running 5000 simulations.:  91%|█████████▏| 4565/5000 [00:18<00:01, 246.82it/s]Running 5000 simulations.:  92%|█████████▏| 4590/5000 [00:18<00:01, 247.24it/s]Running 5000 simulations.:  92%|█████████▏| 4615/5000 [00:18<00:01, 247.53it/s]Running 5000 simulations.:  93%|█████████▎| 4640/5000 [00:18<00:01, 248.10it/s]Running 5000 simulations.:  93%|█████████▎| 4666/5000 [00:19<00:01, 248.86it/s]Running 5000 simulations.:  94%|█████████▍| 4691/5000 [00:19<00:01, 245.64it/s]Running 5000 simulations.:  94%|█████████▍| 4716/5000 [00:19<00:01, 245.29it/s]Running 5000 simulations.:  95%|█████████▍| 4741/5000 [00:19<00:01, 244.99it/s]Running 5000 simulations.:  95%|█████████▌| 4766/5000 [00:19<00:00, 245.28it/s]Running 5000 simulations.:  96%|█████████▌| 4791/5000 [00:19<00:00, 245.79it/s]Running 5000 simulations.:  96%|█████████▋| 4816/5000 [00:19<00:00, 245.54it/s]Running 5000 simulations.:  97%|█████████▋| 4841/5000 [00:19<00:00, 245.64it/s]Running 5000 simulations.:  97%|█████████▋| 4866/5000 [00:19<00:00, 245.22it/s]Running 5000 simulations.:  98%|█████████▊| 4891/5000 [00:19<00:00, 244.48it/s]Running 5000 simulations.:  98%|█████████▊| 4916/5000 [00:20<00:00, 244.03it/s]Running 5000 simulations.:  99%|█████████▉| 4941/5000 [00:20<00:00, 244.58it/s]Running 5000 simulations.:  99%|█████████▉| 4966/5000 [00:20<00:00, 244.22it/s]Running 5000 simulations.: 100%|█████████▉| 4991/5000 [00:20<00:00, 243.75it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:20<00:00, 245.34it/s]
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 140389.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18828it [00:00, 129822.46it/s]           Drawing 10000 posterior samples: 18828it [00:00, 129388.97it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 132261.11it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19994it [00:00, 140865.06it/s]           Drawing 10000 posterior samples: 19994it [00:00, 140415.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17629it [00:00, 125526.93it/s]           Drawing 10000 posterior samples: 17629it [00:00, 125090.95it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18092it [00:00, 127156.35it/s]           Drawing 10000 posterior samples: 18092it [00:00, 126696.92it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 142251.65it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 140764.04it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159612.15it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160324.75it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19996it [00:00, 159800.06it/s]           Drawing 10000 posterior samples: 19996it [00:00, 159233.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158742.26it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 157597.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159722.16it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19997it [00:00, 159765.43it/s]           Drawing 10000 posterior samples: 19997it [00:00, 159210.75it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19680it [00:00, 158920.73it/s]           Drawing 10000 posterior samples: 19680it [00:00, 158332.09it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159719.12it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160991.83it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17846it [00:00, 144758.44it/s]           Drawing 10000 posterior samples: 17846it [00:00, 144220.69it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159493.79it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159335.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161046.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160338.85it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159876.80it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159445.90it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 156490.45it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161606.55it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161418.10it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161100.04it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16390it [00:00, 133049.17it/s]           Drawing 10000 posterior samples: 16390it [00:00, 132487.61it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162121.89it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19811it [00:00, 159523.73it/s]           Drawing 10000 posterior samples: 19811it [00:00, 158843.99it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162053.61it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161191.05it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18438it [00:00, 149231.46it/s]           Drawing 10000 posterior samples: 18438it [00:00, 148719.48it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18689it [00:00, 149039.54it/s]           Drawing 10000 posterior samples: 18689it [00:00, 148478.60it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160573.95it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 155547.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 157539.05it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158749.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19992it [00:00, 156436.90it/s]           Drawing 10000 posterior samples: 19992it [00:00, 155801.79it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 157954.94it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161599.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160275.13it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160186.99it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19814it [00:00, 160263.85it/s]           Drawing 10000 posterior samples: 19814it [00:00, 159580.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 157981.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 149802.46it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19741it [00:00, 149564.68it/s]           Drawing 10000 posterior samples: 19741it [00:00, 148946.41it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 148418.40it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160075.11it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 145653.76it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 152883.17it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160515.57it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160684.06it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160827.62it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159280.59it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159596.97it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158576.02it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18667it [00:00, 149109.33it/s]           Drawing 10000 posterior samples: 18667it [00:00, 148494.80it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18408it [00:00, 146617.45it/s]           Drawing 10000 posterior samples: 18408it [00:00, 146012.99it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18251it [00:00, 145565.99it/s]           Drawing 10000 posterior samples: 18251it [00:00, 144976.58it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18331it [00:00, 142829.41it/s]           Drawing 10000 posterior samples: 18331it [00:00, 142267.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18368it [00:00, 145153.28it/s]           Drawing 10000 posterior samples: 18368it [00:00, 144552.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 13186it [00:00, 104451.83it/s]           Drawing 10000 posterior samples: 13186it [00:00, 103993.22it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18398it [00:00, 142956.56it/s]           Drawing 10000 posterior samples: 18398it [00:00, 142277.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18319it [00:00, 147298.48it/s]           Drawing 10000 posterior samples: 18319it [00:00, 146705.06it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18320it [00:00, 138123.30it/s]           Drawing 10000 posterior samples: 18320it [00:00, 137577.75it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18318it [00:00, 147628.64it/s]           Drawing 10000 posterior samples: 18318it [00:00, 147051.10it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18302it [00:00, 148998.74it/s]           Drawing 10000 posterior samples: 18302it [00:00, 148433.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18311it [00:00, 147399.46it/s]           Drawing 10000 posterior samples: 18311it [00:00, 146834.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18338it [00:00, 146840.96it/s]           Drawing 10000 posterior samples: 18338it [00:00, 146205.67it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18304it [00:00, 145502.02it/s]           Drawing 10000 posterior samples: 18304it [00:00, 144951.49it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18305it [00:00, 135694.21it/s]           Drawing 10000 posterior samples: 18305it [00:00, 135241.74it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18452it [00:00, 148359.65it/s]           Drawing 10000 posterior samples: 18452it [00:00, 147800.08it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18288it [00:00, 146275.82it/s]           Drawing 10000 posterior samples: 18288it [00:00, 145732.23it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18349it [00:00, 145305.93it/s]           Drawing 10000 posterior samples: 18349it [00:00, 144739.16it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18305it [00:00, 145659.59it/s]           Drawing 10000 posterior samples: 18305it [00:00, 145113.11it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18341it [00:00, 145639.36it/s]           Drawing 10000 posterior samples: 18341it [00:00, 145070.29it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18385it [00:00, 147350.09it/s]           Drawing 10000 posterior samples: 18385it [00:00, 146763.40it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18433it [00:00, 148258.61it/s]           Drawing 10000 posterior samples: 18433it [00:00, 147634.64it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18285it [00:00, 146578.03it/s]           Drawing 10000 posterior samples: 18285it [00:00, 146017.09it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18359it [00:00, 146205.79it/s]           Drawing 10000 posterior samples: 18359it [00:00, 145686.58it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18360it [00:00, 142941.98it/s]           Drawing 10000 posterior samples: 18360it [00:00, 142409.34it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18356it [00:00, 140550.30it/s]           Drawing 10000 posterior samples: 18356it [00:00, 140053.02it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18314it [00:00, 146558.96it/s]           Drawing 10000 posterior samples: 18314it [00:00, 146004.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18363it [00:00, 141683.77it/s]           Drawing 10000 posterior samples: 18363it [00:00, 141147.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18355it [00:00, 139953.73it/s]           Drawing 10000 posterior samples: 18355it [00:00, 139442.69it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18293it [00:00, 136862.35it/s]           Drawing 10000 posterior samples: 18293it [00:00, 136255.46it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18384it [00:00, 137626.90it/s]           Drawing 10000 posterior samples: 18384it [00:00, 137121.02it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158864.32it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19557it [00:00, 156552.75it/s]           Drawing 10000 posterior samples: 19557it [00:00, 155874.17it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159098.73it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158190.26it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17337it [00:00, 139315.67it/s]           Drawing 10000 posterior samples: 17337it [00:00, 138786.80it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18718it [00:00, 148496.35it/s]           Drawing 10000 posterior samples: 18718it [00:00, 147943.41it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159264.26it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159830.50it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158957.64it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158863.12it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19995it [00:00, 160853.72it/s]           Drawing 10000 posterior samples: 19995it [00:00, 160218.53it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159965.22it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160408.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159959.12it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159398.63it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19797it [00:00, 157038.61it/s]           Drawing 10000 posterior samples: 19797it [00:00, 156326.09it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160663.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 158427.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19685it [00:00, 158366.24it/s]           Drawing 10000 posterior samples: 19685it [00:00, 157710.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159688.11it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159643.13it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159519.88it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160621.30it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 154226.17it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 152378.30it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 154162.11it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 153613.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 153892.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 142260.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18067it [00:00, 138888.41it/s]           Drawing 10000 posterior samples: 18067it [00:00, 138311.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 152916.06it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19167it [00:00, 147839.07it/s]           Drawing 10000 posterior samples: 19167it [00:00, 147280.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 157326.91it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19996it [00:00, 156941.06it/s]           Drawing 10000 posterior samples: 19996it [00:00, 156352.69it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18521it [00:00, 147094.30it/s]           Drawing 10000 posterior samples: 18521it [00:00, 146601.57it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18219it [00:00, 144333.68it/s]           Drawing 10000 posterior samples: 18219it [00:00, 143678.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159422.26it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 141851.37it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162447.77it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160332.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19994it [00:00, 162248.85it/s]           Drawing 10000 posterior samples: 19994it [00:00, 161488.69it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160250.64it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163201.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161285.89it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162013.55it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19902it [00:00, 164736.35it/s]           Drawing 10000 posterior samples: 19902it [00:00, 164022.60it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 156642.99it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 157913.91it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19205it [00:00, 153541.89it/s]           Drawing 10000 posterior samples: 19205it [00:00, 152921.60it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163214.56it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160725.32it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 161892.23it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 142681.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163495.76it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163817.60it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163638.65it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162776.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 165700.90it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164641.99it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17626it [00:00, 145667.64it/s]           Drawing 10000 posterior samples: 17626it [00:00, 145043.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18273it [00:00, 149848.70it/s]           Drawing 10000 posterior samples: 18273it [00:00, 149170.03it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18079it [00:00, 150140.33it/s]           Drawing 10000 posterior samples: 18079it [00:00, 149436.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18221it [00:00, 150983.87it/s]           Drawing 10000 posterior samples: 18221it [00:00, 150329.60it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18189it [00:00, 147671.50it/s]           Drawing 10000 posterior samples: 18189it [00:00, 147011.88it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples:  59%|█████▉    | 5927/10000 [00:00<00:00, 46387.54it/s]Drawing 10000 posterior samples: 11929it [00:00, 47214.21it/s]                          Drawing 10000 posterior samples: 11929it [00:00, 47677.76it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18250it [00:00, 147528.79it/s]           Drawing 10000 posterior samples: 18250it [00:00, 146890.10it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18247it [00:00, 148198.32it/s]           Drawing 10000 posterior samples: 18247it [00:00, 147470.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18202it [00:00, 147767.89it/s]           Drawing 10000 posterior samples: 18202it [00:00, 147012.13it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18327it [00:00, 149574.17it/s]           Drawing 10000 posterior samples: 18327it [00:00, 148936.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18309it [00:00, 150394.16it/s]           Drawing 10000 posterior samples: 18309it [00:00, 149578.32it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18305it [00:00, 147455.59it/s]           Drawing 10000 posterior samples: 18305it [00:00, 146826.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18250it [00:00, 135414.30it/s]           Drawing 10000 posterior samples: 18250it [00:00, 134659.62it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18271it [00:00, 140188.91it/s]           Drawing 10000 posterior samples: 18271it [00:00, 139526.06it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18277it [00:00, 147959.78it/s]           Drawing 10000 posterior samples: 18277it [00:00, 147310.39it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18285it [00:00, 140407.80it/s]           Drawing 10000 posterior samples: 18285it [00:00, 139801.21it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18258it [00:00, 150111.64it/s]           Drawing 10000 posterior samples: 18258it [00:00, 149365.03it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18206it [00:00, 149319.12it/s]           Drawing 10000 posterior samples: 18206it [00:00, 148361.76it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18323it [00:00, 150944.00it/s]           Drawing 10000 posterior samples: 18323it [00:00, 150208.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18233it [00:00, 150769.07it/s]           Drawing 10000 posterior samples: 18233it [00:00, 150123.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18244it [00:00, 150554.60it/s]           Drawing 10000 posterior samples: 18244it [00:00, 149910.73it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18298it [00:00, 150318.52it/s]           Drawing 10000 posterior samples: 18298it [00:00, 149585.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18223it [00:00, 150123.15it/s]           Drawing 10000 posterior samples: 18223it [00:00, 149495.96it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18315it [00:00, 150782.74it/s]           Drawing 10000 posterior samples: 18315it [00:00, 150117.99it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18331it [00:00, 152414.17it/s]           Drawing 10000 posterior samples: 18331it [00:00, 151701.75it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18260it [00:00, 149793.06it/s]           Drawing 10000 posterior samples: 18260it [00:00, 149152.74it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18326it [00:00, 151699.29it/s]           Drawing 10000 posterior samples: 18326it [00:00, 151034.87it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18286it [00:00, 151240.72it/s]           Drawing 10000 posterior samples: 18286it [00:00, 150587.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18257it [00:00, 150951.75it/s]           Drawing 10000 posterior samples: 18257it [00:00, 150296.39it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18254it [00:00, 151878.73it/s]           Drawing 10000 posterior samples: 18254it [00:00, 151144.45it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18284it [00:00, 152014.65it/s]           Drawing 10000 posterior samples: 18284it [00:00, 151250.43it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18383it [00:00, 141924.24it/s]           Drawing 10000 posterior samples: 18383it [00:00, 141326.44it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18071it [00:00, 149713.43it/s]           Drawing 10000 posterior samples: 18071it [00:00, 149039.87it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18402it [00:00, 152701.80it/s]           Drawing 10000 posterior samples: 18402it [00:00, 152003.21it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18367it [00:00, 152992.21it/s]           Drawing 10000 posterior samples: 18367it [00:00, 152311.92it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 15580it [00:00, 129866.23it/s]           Drawing 10000 posterior samples: 15580it [00:00, 129284.02it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18331it [00:00, 152082.85it/s]           Drawing 10000 posterior samples: 18331it [00:00, 151425.09it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18401it [00:00, 152264.23it/s]           Drawing 10000 posterior samples: 18401it [00:00, 151579.71it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18364it [00:00, 147056.17it/s]           Drawing 10000 posterior samples: 18364it [00:00, 146400.16it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18392it [00:00, 147293.05it/s]           Drawing 10000 posterior samples: 18392it [00:00, 146632.02it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18348it [00:00, 152269.97it/s]           Drawing 10000 posterior samples: 18348it [00:00, 151600.15it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18360it [00:00, 151849.77it/s]           Drawing 10000 posterior samples: 18360it [00:00, 151069.00it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18343it [00:00, 150607.37it/s]           Drawing 10000 posterior samples: 18343it [00:00, 149931.15it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18370it [00:00, 151694.07it/s]           Drawing 10000 posterior samples: 18370it [00:00, 151039.57it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18389it [00:00, 151393.15it/s]           Drawing 10000 posterior samples: 18389it [00:00, 150722.17it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18339it [00:00, 151038.43it/s]           Drawing 10000 posterior samples: 18339it [00:00, 150393.76it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18414it [00:00, 152916.34it/s]           Drawing 10000 posterior samples: 18414it [00:00, 152242.35it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18012it [00:00, 149113.30it/s]           Drawing 10000 posterior samples: 18012it [00:00, 148494.87it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18457it [00:00, 151987.29it/s]           Drawing 10000 posterior samples: 18457it [00:00, 151328.02it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18374it [00:00, 150109.65it/s]           Drawing 10000 posterior samples: 18374it [00:00, 149450.88it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18399it [00:00, 150712.24it/s]           Drawing 10000 posterior samples: 18399it [00:00, 150032.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18352it [00:00, 151217.95it/s]           Drawing 10000 posterior samples: 18352it [00:00, 150558.36it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18388it [00:00, 150084.87it/s]           Drawing 10000 posterior samples: 18388it [00:00, 149438.69it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18323it [00:00, 148050.70it/s]           Drawing 10000 posterior samples: 18323it [00:00, 147443.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18333it [00:00, 150313.70it/s]           Drawing 10000 posterior samples: 18333it [00:00, 149653.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18273it [00:00, 148615.82it/s]           Drawing 10000 posterior samples: 18273it [00:00, 147945.67it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18441it [00:00, 136538.04it/s]           Drawing 10000 posterior samples: 18441it [00:00, 136006.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18391it [00:00, 150288.63it/s]           Drawing 10000 posterior samples: 18391it [00:00, 149641.10it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18398it [00:00, 150704.35it/s]           Drawing 10000 posterior samples: 18398it [00:00, 150062.53it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18333it [00:00, 146650.29it/s]           Drawing 10000 posterior samples: 18333it [00:00, 146013.43it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18359it [00:00, 149890.46it/s]           Drawing 10000 posterior samples: 18359it [00:00, 149200.41it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18317it [00:00, 149791.31it/s]           Drawing 10000 posterior samples: 18317it [00:00, 149109.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17989it [00:00, 147882.32it/s]           Drawing 10000 posterior samples: 17989it [00:00, 147254.30it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18333it [00:00, 149780.81it/s]           Drawing 10000 posterior samples: 18333it [00:00, 149127.52it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18351it [00:00, 149455.97it/s]           Drawing 10000 posterior samples: 18351it [00:00, 148809.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17244it [00:00, 141111.81it/s]           Drawing 10000 posterior samples: 17244it [00:00, 140507.62it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18314it [00:00, 149933.41it/s]           Drawing 10000 posterior samples: 18314it [00:00, 149263.60it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18303it [00:00, 151318.36it/s]           Drawing 10000 posterior samples: 18303it [00:00, 150633.98it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18312it [00:00, 150881.24it/s]           Drawing 10000 posterior samples: 18312it [00:00, 150211.11it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18313it [00:00, 151141.85it/s]           Drawing 10000 posterior samples: 18313it [00:00, 150440.27it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18241it [00:00, 142872.37it/s]           Drawing 10000 posterior samples: 18241it [00:00, 142275.91it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18224it [00:00, 149348.18it/s]           Drawing 10000 posterior samples: 18224it [00:00, 148702.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18331it [00:00, 150778.91it/s]           Drawing 10000 posterior samples: 18331it [00:00, 150134.42it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18309it [00:00, 145820.34it/s]           Drawing 10000 posterior samples: 18309it [00:00, 145184.61it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18311it [00:00, 148660.54it/s]           Drawing 10000 posterior samples: 18311it [00:00, 148017.90it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18298it [00:00, 150272.90it/s]           Drawing 10000 posterior samples: 18298it [00:00, 149642.06it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18304it [00:00, 149176.01it/s]           Drawing 10000 posterior samples: 18304it [00:00, 148541.61it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18332it [00:00, 149259.20it/s]           Drawing 10000 posterior samples: 18332it [00:00, 148624.76it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18327it [00:00, 145774.64it/s]           Drawing 10000 posterior samples: 18327it [00:00, 145215.88it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18353it [00:00, 148570.44it/s]           Drawing 10000 posterior samples: 18353it [00:00, 147985.22it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18300it [00:00, 150078.63it/s]           Drawing 10000 posterior samples: 18300it [00:00, 149467.24it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18334it [00:00, 145297.42it/s]           Drawing 10000 posterior samples: 18334it [00:00, 144733.53it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18309it [00:00, 142478.28it/s]           Drawing 10000 posterior samples: 18309it [00:00, 141946.03it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18318it [00:00, 148268.51it/s]           Drawing 10000 posterior samples: 18318it [00:00, 147642.54it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18343it [00:00, 147787.24it/s]           Drawing 10000 posterior samples: 18343it [00:00, 147236.30it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18294it [00:00, 149479.65it/s]           Drawing 10000 posterior samples: 18294it [00:00, 148912.80it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18289it [00:00, 151329.39it/s]           Drawing 10000 posterior samples: 18289it [00:00, 150711.57it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18284it [00:00, 151687.51it/s]           Drawing 10000 posterior samples: 18284it [00:00, 151101.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18300it [00:00, 150829.77it/s]           Drawing 10000 posterior samples: 18300it [00:00, 150248.73it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18328it [00:00, 151413.13it/s]           Drawing 10000 posterior samples: 18328it [00:00, 150758.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18376it [00:00, 150479.76it/s]           Drawing 10000 posterior samples: 18376it [00:00, 149909.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18422it [00:00, 150650.27it/s]           Drawing 10000 posterior samples: 18422it [00:00, 149964.32it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17042it [00:00, 139245.81it/s]           Drawing 10000 posterior samples: 17042it [00:00, 138690.59it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18414it [00:00, 150320.78it/s]           Drawing 10000 posterior samples: 18414it [00:00, 149755.62it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18423it [00:00, 150228.46it/s]           Drawing 10000 posterior samples: 18423it [00:00, 149648.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16164it [00:00, 132183.39it/s]           Drawing 10000 posterior samples: 16164it [00:00, 131685.81it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18429it [00:00, 151515.77it/s]           Drawing 10000 posterior samples: 18429it [00:00, 150920.26it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18439it [00:00, 150387.29it/s]           Drawing 10000 posterior samples: 18439it [00:00, 149676.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18445it [00:00, 149616.96it/s]           Drawing 10000 posterior samples: 18445it [00:00, 148972.76it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18304it [00:00, 149819.37it/s]           Drawing 10000 posterior samples: 18304it [00:00, 149249.39it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18357it [00:00, 149382.13it/s]           Drawing 10000 posterior samples: 18357it [00:00, 148801.85it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18437it [00:00, 149838.08it/s]           Drawing 10000 posterior samples: 18437it [00:00, 149256.77it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18443it [00:00, 149600.16it/s]           Drawing 10000 posterior samples: 18443it [00:00, 149031.51it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18374it [00:00, 148678.46it/s]           Drawing 10000 posterior samples: 18374it [00:00, 148079.67it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18424it [00:00, 149466.85it/s]           Drawing 10000 posterior samples: 18424it [00:00, 148890.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18469it [00:00, 151914.91it/s]           Drawing 10000 posterior samples: 18469it [00:00, 151314.60it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18371it [00:00, 151183.53it/s]           Drawing 10000 posterior samples: 18371it [00:00, 150576.11it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17899it [00:00, 114677.84it/s]           Drawing 10000 posterior samples: 17899it [00:00, 114339.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18417it [00:00, 150515.18it/s]           Drawing 10000 posterior samples: 18417it [00:00, 149915.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18418it [00:00, 151185.88it/s]           Drawing 10000 posterior samples: 18418it [00:00, 150523.94it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18424it [00:00, 150289.21it/s]           Drawing 10000 posterior samples: 18424it [00:00, 149721.69it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18346it [00:00, 149977.88it/s]           Drawing 10000 posterior samples: 18346it [00:00, 149337.53it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18373it [00:00, 149244.78it/s]           Drawing 10000 posterior samples: 18373it [00:00, 148586.95it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18405it [00:00, 150547.64it/s]           Drawing 10000 posterior samples: 18405it [00:00, 149954.28it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18383it [00:00, 151242.52it/s]           Drawing 10000 posterior samples: 18383it [00:00, 150569.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18419it [00:00, 150832.18it/s]           Drawing 10000 posterior samples: 18419it [00:00, 150160.52it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18409it [00:00, 151084.80it/s]           Drawing 10000 posterior samples: 18409it [00:00, 150480.88it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18147it [00:00, 146779.61it/s]           Drawing 10000 posterior samples: 18147it [00:00, 146123.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18378it [00:00, 150491.44it/s]           Drawing 10000 posterior samples: 18378it [00:00, 149936.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18454it [00:00, 150571.70it/s]           Drawing 10000 posterior samples: 18454it [00:00, 149976.53it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18380it [00:00, 148356.08it/s]           Drawing 10000 posterior samples: 18380it [00:00, 147698.64it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163416.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18824it [00:00, 155496.04it/s]           Drawing 10000 posterior samples: 18824it [00:00, 154831.89it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162145.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162955.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18542it [00:00, 151636.33it/s]           Drawing 10000 posterior samples: 18542it [00:00, 150896.08it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18332it [00:00, 150607.27it/s]           Drawing 10000 posterior samples: 18332it [00:00, 149904.92it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163505.33it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162734.55it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163346.13it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164417.39it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19979it [00:00, 162940.46it/s]           Drawing 10000 posterior samples: 19979it [00:00, 162282.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162612.78it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162139.44it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163768.35it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 162215.31it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19632it [00:00, 161390.84it/s]           Drawing 10000 posterior samples: 19632it [00:00, 160769.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19898it [00:00, 162838.08it/s]           Drawing 10000 posterior samples: 19898it [00:00, 162123.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163108.56it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19755it [00:00, 162394.68it/s]           Drawing 10000 posterior samples: 19755it [00:00, 161775.77it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 164869.14it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 163622.05it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160404.46it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160672.37it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159276.36it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 156931.34it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159620.05it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19985it [00:00, 159072.03it/s]           Drawing 10000 posterior samples: 19985it [00:00, 158477.46it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 159376.83it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 160367.66it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17578it [00:00, 140232.42it/s]           Drawing 10000 posterior samples: 17578it [00:00, 139645.95it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
30
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Neural network successfully converged after 268 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Neural network successfully converged after 338 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Neural network successfully converged after 276 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Training neural network. Epochs trained:  340Training neural network. Epochs trained:  341Training neural network. Epochs trained:  342Training neural network. Epochs trained:  343Training neural network. Epochs trained:  344Training neural network. Epochs trained:  345Training neural network. Epochs trained:  346Training neural network. Epochs trained:  347Training neural network. Epochs trained:  348Training neural network. Epochs trained:  349Training neural network. Epochs trained:  350Training neural network. Epochs trained:  351Training neural network. Epochs trained:  352Training neural network. Epochs trained:  353Training neural network. Epochs trained:  354Training neural network. Epochs trained:  355Training neural network. Epochs trained:  356Training neural network. Epochs trained:  357Training neural network. Epochs trained:  358Training neural network. Epochs trained:  359Training neural network. Epochs trained:  360Training neural network. Epochs trained:  361Training neural network. Epochs trained:  362Training neural network. Epochs trained:  363Training neural network. Epochs trained:  364Training neural network. Epochs trained:  365Training neural network. Epochs trained:  366Training neural network. Epochs trained:  367Training neural network. Epochs trained:  368Training neural network. Epochs trained:  369Training neural network. Epochs trained:  370Training neural network. Epochs trained:  371Training neural network. Epochs trained:  372Training neural network. Epochs trained:  373Training neural network. Epochs trained:  374Training neural network. Epochs trained:  375Training neural network. Epochs trained:  376Training neural network. Epochs trained:  377Training neural network. Epochs trained:  378Training neural network. Epochs trained:  379Training neural network. Epochs trained:  380Training neural network. Epochs trained:  381Training neural network. Epochs trained:  382Training neural network. Epochs trained:  383Training neural network. Epochs trained:  384Training neural network. Epochs trained:  385Training neural network. Epochs trained:  386Training neural network. Epochs trained:  387Training neural network. Epochs trained:  388Training neural network. Epochs trained:  389Training neural network. Epochs trained:  390Training neural network. Epochs trained:  391Training neural network. Epochs trained:  392Training neural network. Epochs trained:  393Training neural network. Epochs trained:  394Training neural network. Epochs trained:  395Training neural network. Epochs trained:  396Training neural network. Epochs trained:  397Training neural network. Epochs trained:  398Training neural network. Epochs trained:  399Training neural network. Epochs trained:  400Training neural network. Epochs trained:  401Neural network successfully converged after 401 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Neural network successfully converged after 288 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Neural network successfully converged after 230 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Neural network successfully converged after 250 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Neural network successfully converged after 290 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Neural network successfully converged after 271 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Neural network successfully converged after 335 epochs.
log prob true 7.0774636
log prob true 6.528909
log prob true 6.6593494
log prob true 6.2703605
log prob true 6.003746
log prob true 6.4439316
log prob true 6.0905766
log prob true 6.862771
log prob true 6.267847
log prob true 6.7560816
log prob true 6.354087
log prob true 6.592291
log prob true 7.0696573
log prob true 6.5422916
log prob true 6.2223034
log prob true 6.1364584
log prob true 6.33114
log prob true 6.570028
log prob true 6.213427
log prob true 6.8107915
log prob true 6.5821915
log prob true 6.555478
log prob true 6.76835
log prob true 6.487831
log prob true 6.868464
log prob true 6.4825873
log prob true 6.0247126
log prob true 6.375981
log prob true 6.9509993
log prob true 5.8387294
log prob true 7.20008
log prob true 6.738972
log prob true 6.9832225
log prob true 6.763261
log prob true 6.289208
log prob true 6.4284606
log prob true 6.472775
log prob true 7.0247097
log prob true 6.6688247
log prob true 6.813327
log prob true 6.6560397
log prob true 6.916509
log prob true 7.3683505
log prob true 7.0051208
log prob true 6.4540052
log prob true 6.2100167
log prob true 6.3809223
log prob true 6.8826456
log prob true 6.477663
log prob true 7.1161647
log prob true 7.086023
log prob true 6.750465
log prob true 7.1151705
log prob true 6.5583434
log prob true 6.8502736
log prob true 6.81484
log prob true 6.1512623
log prob true 6.530843
log prob true 6.960272
log prob true 5.8748565
log prob true 4.1669035
log prob true 3.672757
log prob true 3.913234
log prob true 2.8049371
log prob true 3.2210367
log prob true 3.0692391
log prob true 3.3670824
log prob true 4.0999703
log prob true 3.526341
log prob true 3.5293925
log prob true 2.834056
log prob true 3.8128889
log prob true 4.171069
log prob true 3.8460329
log prob true 2.997396
log prob true 3.0109568
log prob true 3.5014684
log prob true 3.8569276
log prob true 2.3445323
log prob true 4.089415
log prob true 3.7817633
log prob true 3.6695526
log prob true 4.081798
log prob true 3.6101727
log prob true 4.197319
log prob true 3.8344176
log prob true 2.9958105
log prob true 3.454501
log prob true 3.934257
log prob true 2.2543068
log prob true 7.431312
log prob true 6.821468
log prob true 7.150191
log prob true 6.829068
log prob true 6.644145
log prob true 6.603501
log prob true 6.675315
log prob true 7.103856
log prob true 6.828805
log prob true 6.9698024
log prob true 6.680638
log prob true 7.043157
log prob true 7.4198084
log prob true 7.1561766
log prob true 6.5977054
log prob true 6.316032
log prob true 6.7238746
log prob true 7.03334
log prob true 6.5743055
log prob true 7.2133236
log prob true 7.1840706
log prob true 6.965883
log prob true 7.3099866
log prob true 6.769178
log prob true 7.027504
log prob true 6.947132
log prob true 6.2624807
log prob true 6.720133
log prob true 7.1425915
log prob true 6.6242237
log prob true 7.2089596
log prob true 6.592433
log prob true 6.91196
log prob true 6.5125194
log prob true 5.935032
log prob true 6.5103397
log prob true 6.3355165
log prob true 6.995169
log prob true 6.439286
log prob true 6.8022056
log prob true 6.4915776
log prob true 6.833773
log prob true 7.316275
log prob true 6.811306
log prob true 6.332852
log prob true 5.677279
log prob true 6.3282886
log prob true 6.518707
log prob true 6.3986387
log prob true 6.987857
log prob true 6.941636
log prob true 6.7704625
log prob true 7.1612353
log prob true 6.521114
log prob true 6.88447
log prob true 6.680019
log prob true 6.0588036
log prob true 6.4652567
log prob true 6.972127
log prob true 6.276694
log prob true 3.6844819
log prob true 3.7607946
log prob true 3.9859762
log prob true 2.8736827
log prob true 4.2218213
log prob true 2.9695482
log prob true 3.448005
log prob true 3.8427007
log prob true 3.5143733
log prob true 3.475543
log prob true 3.0509448
log prob true 3.976107
log prob true 4.1244726
log prob true 3.6656508
log prob true 2.9797244
log prob true 2.8764796
log prob true 3.608393
log prob true 3.9085736
log prob true 2.7637477
log prob true 4.037888
log prob true 3.7704008
log prob true 3.6792092
log prob true 4.090503
log prob true 3.6799328
log prob true 4.097545
log prob true 3.8554714
log prob true 3.0314558
log prob true 3.3861787
log prob true 4.0260963
log prob true 1.6821306
log prob true 4.260085
log prob true 3.561927
log prob true 4.0958524
log prob true 2.7332637
log prob true 2.6669586
log prob true 3.0000432
log prob true 3.4493566
log prob true 4.0212417
log prob true 3.613144
log prob true 3.720938
log prob true 3.1215506
log prob true 3.9223514
log prob true 4.3769784
log prob true 3.8988008
log prob true 3.2013268
log prob true 2.710386
log prob true 3.139209
log prob true 3.4917517
log prob true 3.0243459
log prob true 4.1086617
log prob true 4.0858817
log prob true 3.7756422
log prob true 4.253808
log prob true 3.6843705
log prob true 3.9857585
log prob true 3.9056146
log prob true 2.888507
log prob true 3.4828217
log prob true 4.1492496
log prob true 2.054489
log prob true 4.3218455
log prob true 3.8647714
log prob true 4.182872
log prob true 2.849692
log prob true 3.1787531
log prob true 2.8900492
log prob true 3.659156
log prob true 4.1601253
log prob true 3.6956642
log prob true 3.605457
log prob true 3.0864444
log prob true 4.0392113
log prob true 4.0503945
log prob true 3.9494047
log prob true 3.0304267
log prob true 2.9673038
log prob true 3.6680436
log prob true 3.9917269
log prob true 2.4108303
log prob true 4.2120037
log prob true 3.813123
log prob true 3.4110298
log prob true 4.1487446
log prob true 3.8530114
log prob true 4.206533
log prob true 4.09059
log prob true 3.1206017
log prob true 3.5878167
log prob true 3.9693255
log prob true 2.158761
log prob true 4.2128005
log prob true 3.8166296
log prob true 3.9319727
log prob true 2.2548974
log prob true 2.7998757
log prob true 2.6726627
log prob true 3.3283281
log prob true 4.040104
log prob true 3.465416
log prob true 3.666111
log prob true 3.0237372
log prob true 3.8909564
log prob true 4.0337844
log prob true 3.8521142
log prob true 2.7477417
log prob true 2.5985107
log prob true 3.2789326
log prob true 3.5795224
log prob true 1.6721625
log prob true 3.980669
log prob true 3.6926732
log prob true 3.4101164
log prob true 4.0121
log prob true 3.5772002
log prob true 3.9319725
log prob true 3.8415682
log prob true 2.9411378
log prob true 3.3570063
log prob true 3.9592462
log prob true 1.6892668
log prob true 7.1080756
log prob true 6.4770656
log prob true 6.402065
log prob true 6.489878
log prob true 5.384938
log prob true 6.1840973
log prob true 6.093477
log prob true 6.8206563
log prob true 6.3406014
log prob true 6.713964
log prob true 6.4643593
log prob true 6.567579
log prob true 7.1016393
log prob true 6.5406613
log prob true 6.1553993
log prob true 6.003631
log prob true 6.3108335
log prob true 6.419158
log prob true 6.2807474
log prob true 6.8149896
log prob true 6.699772
log prob true 6.5436177
log prob true 6.9208198
log prob true 6.2281075
log prob true 6.720886
log prob true 6.4447637
log prob true 5.830212
log prob true 6.3821425
log prob true 6.7905674
log prob true 5.6465597
script complete
