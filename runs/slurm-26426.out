Loading parflow-ml/latest
  Loading requirement: openmpi/gcc/4.1.0 parflow/3.9.0 gdal/3.2.1
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  warnings.warn(
/home/qh8373/SBI_TAYLOR/sbi_taylor/scripts/05_utils/sbiutils.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(y_out)
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 17/5000 [00:00<00:31, 160.40it/s]Running 5000 simulations.:   1%|          | 34/5000 [00:00<00:30, 160.55it/s]Running 5000 simulations.:   1%|          | 51/5000 [00:00<00:30, 160.46it/s]Running 5000 simulations.:   1%|▏         | 67/5000 [00:00<00:30, 160.22it/s]Running 5000 simulations.:   2%|▏         | 83/5000 [00:00<00:30, 160.06it/s]Running 5000 simulations.:   2%|▏         | 99/5000 [00:00<00:30, 159.90it/s]Running 5000 simulations.:   2%|▏         | 116/5000 [00:00<00:30, 159.99it/s]Running 5000 simulations.:   3%|▎         | 132/5000 [00:00<00:30, 159.50it/s]Running 5000 simulations.:   3%|▎         | 148/5000 [00:00<00:30, 159.15it/s]Running 5000 simulations.:   3%|▎         | 164/5000 [00:01<00:30, 158.28it/s]Running 5000 simulations.:   4%|▎         | 180/5000 [00:01<00:30, 157.75it/s]Running 5000 simulations.:   4%|▍         | 196/5000 [00:01<00:30, 157.85it/s]Running 5000 simulations.:   4%|▍         | 212/5000 [00:01<00:30, 157.89it/s]Running 5000 simulations.:   5%|▍         | 228/5000 [00:01<00:30, 157.71it/s]Running 5000 simulations.:   5%|▍         | 244/5000 [00:01<00:30, 157.23it/s]Running 5000 simulations.:   5%|▌         | 260/5000 [00:01<00:30, 157.33it/s]Running 5000 simulations.:   6%|▌         | 276/5000 [00:01<00:30, 157.20it/s]Running 5000 simulations.:   6%|▌         | 292/5000 [00:01<00:29, 157.65it/s]Running 5000 simulations.:   6%|▌         | 308/5000 [00:01<00:29, 157.38it/s]Running 5000 simulations.:   6%|▋         | 324/5000 [00:02<00:29, 157.17it/s]Running 5000 simulations.:   7%|▋         | 340/5000 [00:02<00:29, 156.92it/s]Running 5000 simulations.:   7%|▋         | 356/5000 [00:02<00:29, 156.58it/s]Running 5000 simulations.:   7%|▋         | 372/5000 [00:02<00:29, 155.86it/s]Running 5000 simulations.:   8%|▊         | 388/5000 [00:02<00:29, 155.32it/s]Running 5000 simulations.:   8%|▊         | 404/5000 [00:02<00:29, 155.36it/s]Running 5000 simulations.:   8%|▊         | 420/5000 [00:02<00:29, 154.87it/s]Running 5000 simulations.:   9%|▊         | 436/5000 [00:02<00:29, 155.07it/s]Running 5000 simulations.:   9%|▉         | 452/5000 [00:02<00:29, 155.81it/s]Running 5000 simulations.:   9%|▉         | 468/5000 [00:02<00:29, 155.47it/s]Running 5000 simulations.:  10%|▉         | 484/5000 [00:03<00:29, 154.91it/s]Running 5000 simulations.:  10%|█         | 500/5000 [00:03<00:29, 154.67it/s]Running 5000 simulations.:  10%|█         | 516/5000 [00:03<00:29, 154.44it/s]Running 5000 simulations.:  11%|█         | 532/5000 [00:03<00:29, 153.83it/s]Running 5000 simulations.:  11%|█         | 548/5000 [00:03<00:29, 153.50it/s]Running 5000 simulations.:  11%|█▏        | 564/5000 [00:03<00:28, 153.79it/s]Running 5000 simulations.:  12%|█▏        | 580/5000 [00:03<00:28, 153.67it/s]Running 5000 simulations.:  12%|█▏        | 596/5000 [00:03<00:28, 153.10it/s]Running 5000 simulations.:  12%|█▏        | 612/5000 [00:03<00:28, 152.64it/s]Running 5000 simulations.:  13%|█▎        | 628/5000 [00:04<00:28, 152.61it/s]Running 5000 simulations.:  13%|█▎        | 644/5000 [00:04<00:28, 152.80it/s]Running 5000 simulations.:  13%|█▎        | 660/5000 [00:04<00:28, 153.37it/s]Running 5000 simulations.:  14%|█▎        | 676/5000 [00:04<00:28, 153.31it/s]Running 5000 simulations.:  14%|█▍        | 692/5000 [00:04<00:28, 153.75it/s]Running 5000 simulations.:  14%|█▍        | 708/5000 [00:04<00:28, 152.75it/s]Running 5000 simulations.:  14%|█▍        | 724/5000 [00:04<00:28, 151.25it/s]Running 5000 simulations.:  15%|█▍        | 740/5000 [00:04<00:28, 151.52it/s]Running 5000 simulations.:  15%|█▌        | 756/5000 [00:04<00:27, 151.92it/s]Running 5000 simulations.:  15%|█▌        | 772/5000 [00:04<00:27, 152.17it/s]Running 5000 simulations.:  16%|█▌        | 788/5000 [00:05<00:27, 151.98it/s]Running 5000 simulations.:  16%|█▌        | 804/5000 [00:05<00:27, 151.71it/s]Running 5000 simulations.:  16%|█▋        | 820/5000 [00:05<00:27, 150.97it/s]Running 5000 simulations.:  17%|█▋        | 836/5000 [00:05<00:27, 150.61it/s]Running 5000 simulations.:  17%|█▋        | 852/5000 [00:05<00:27, 150.97it/s]Running 5000 simulations.:  17%|█▋        | 868/5000 [00:05<00:27, 151.38it/s]Running 5000 simulations.:  18%|█▊        | 884/5000 [00:05<00:27, 152.00it/s]Running 5000 simulations.:  18%|█▊        | 900/5000 [00:05<00:26, 152.38it/s]Running 5000 simulations.:  18%|█▊        | 916/5000 [00:05<00:27, 151.25it/s]Running 5000 simulations.:  19%|█▊        | 932/5000 [00:06<00:26, 152.18it/s]Running 5000 simulations.:  19%|█▉        | 948/5000 [00:06<00:26, 152.81it/s]Running 5000 simulations.:  19%|█▉        | 964/5000 [00:06<00:26, 153.38it/s]Running 5000 simulations.:  20%|█▉        | 980/5000 [00:06<00:26, 153.96it/s]Running 5000 simulations.:  20%|█▉        | 996/5000 [00:06<00:25, 154.01it/s]Running 5000 simulations.:  20%|██        | 1012/5000 [00:06<00:25, 153.52it/s]Running 5000 simulations.:  21%|██        | 1028/5000 [00:06<00:25, 153.56it/s]Running 5000 simulations.:  21%|██        | 1044/5000 [00:06<00:25, 153.53it/s]Running 5000 simulations.:  21%|██        | 1060/5000 [00:06<00:26, 150.28it/s]Running 5000 simulations.:  22%|██▏       | 1076/5000 [00:06<00:25, 151.39it/s]Running 5000 simulations.:  22%|██▏       | 1092/5000 [00:07<00:25, 152.08it/s]Running 5000 simulations.:  22%|██▏       | 1108/5000 [00:07<00:25, 152.62it/s]Running 5000 simulations.:  22%|██▏       | 1124/5000 [00:07<00:25, 153.04it/s]Running 5000 simulations.:  23%|██▎       | 1140/5000 [00:07<00:25, 152.54it/s]Running 5000 simulations.:  23%|██▎       | 1156/5000 [00:07<00:25, 152.46it/s]Running 5000 simulations.:  23%|██▎       | 1172/5000 [00:07<00:25, 152.51it/s]Running 5000 simulations.:  24%|██▍       | 1188/5000 [00:07<00:25, 152.17it/s]Running 5000 simulations.:  24%|██▍       | 1204/5000 [00:07<00:24, 151.94it/s]Running 5000 simulations.:  24%|██▍       | 1220/5000 [00:07<00:24, 152.27it/s]Running 5000 simulations.:  25%|██▍       | 1236/5000 [00:08<00:24, 152.47it/s]Running 5000 simulations.:  25%|██▌       | 1252/5000 [00:08<00:24, 151.91it/s]Running 5000 simulations.:  25%|██▌       | 1268/5000 [00:08<00:24, 151.71it/s]Running 5000 simulations.:  26%|██▌       | 1284/5000 [00:08<00:24, 151.71it/s]Running 5000 simulations.:  26%|██▌       | 1300/5000 [00:08<00:24, 151.65it/s]Running 5000 simulations.:  26%|██▋       | 1316/5000 [00:08<00:24, 151.96it/s]Running 5000 simulations.:  27%|██▋       | 1332/5000 [00:08<00:24, 151.89it/s]Running 5000 simulations.:  27%|██▋       | 1348/5000 [00:08<00:24, 151.85it/s]Running 5000 simulations.:  27%|██▋       | 1364/5000 [00:08<00:23, 152.09it/s]Running 5000 simulations.:  28%|██▊       | 1380/5000 [00:08<00:23, 152.50it/s]Running 5000 simulations.:  28%|██▊       | 1396/5000 [00:09<00:23, 152.54it/s]Running 5000 simulations.:  28%|██▊       | 1412/5000 [00:09<00:23, 152.73it/s]Running 5000 simulations.:  29%|██▊       | 1428/5000 [00:09<00:23, 153.41it/s]Running 5000 simulations.:  29%|██▉       | 1444/5000 [00:09<00:23, 153.03it/s]Running 5000 simulations.:  29%|██▉       | 1460/5000 [00:09<00:23, 152.08it/s]Running 5000 simulations.:  30%|██▉       | 1476/5000 [00:09<00:23, 151.54it/s]Running 5000 simulations.:  30%|██▉       | 1492/5000 [00:09<00:23, 150.35it/s]Running 5000 simulations.:  30%|███       | 1508/5000 [00:09<00:23, 150.18it/s]Running 5000 simulations.:  30%|███       | 1524/5000 [00:09<00:23, 150.60it/s]Running 5000 simulations.:  31%|███       | 1540/5000 [00:10<00:23, 150.41it/s]Running 5000 simulations.:  31%|███       | 1556/5000 [00:10<00:22, 150.10it/s]Running 5000 simulations.:  31%|███▏      | 1572/5000 [00:10<00:23, 148.85it/s]Running 5000 simulations.:  32%|███▏      | 1587/5000 [00:10<00:22, 149.11it/s]Running 5000 simulations.:  32%|███▏      | 1603/5000 [00:10<00:22, 149.84it/s]Running 5000 simulations.:  32%|███▏      | 1618/5000 [00:10<00:22, 149.71it/s]Running 5000 simulations.:  33%|███▎      | 1634/5000 [00:10<00:22, 150.11it/s]Running 5000 simulations.:  33%|███▎      | 1650/5000 [00:10<00:22, 150.69it/s]Running 5000 simulations.:  33%|███▎      | 1666/5000 [00:10<00:22, 151.08it/s]Running 5000 simulations.:  34%|███▎      | 1682/5000 [00:10<00:21, 151.08it/s]Running 5000 simulations.:  34%|███▍      | 1698/5000 [00:11<00:21, 150.17it/s]Running 5000 simulations.:  34%|███▍      | 1714/5000 [00:11<00:21, 150.43it/s]Running 5000 simulations.:  35%|███▍      | 1730/5000 [00:11<00:21, 151.27it/s]Running 5000 simulations.:  35%|███▍      | 1746/5000 [00:11<00:21, 151.11it/s]Running 5000 simulations.:  35%|███▌      | 1762/5000 [00:11<00:21, 151.26it/s]Running 5000 simulations.:  36%|███▌      | 1778/5000 [00:11<00:21, 151.58it/s]Running 5000 simulations.:  36%|███▌      | 1794/5000 [00:11<00:21, 151.40it/s]Running 5000 simulations.:  36%|███▌      | 1810/5000 [00:11<00:21, 151.28it/s]Running 5000 simulations.:  37%|███▋      | 1826/5000 [00:11<00:20, 151.46it/s]Running 5000 simulations.:  37%|███▋      | 1842/5000 [00:12<00:21, 150.35it/s]Running 5000 simulations.:  37%|███▋      | 1858/5000 [00:12<00:21, 148.57it/s]Running 5000 simulations.:  37%|███▋      | 1874/5000 [00:12<00:20, 149.52it/s]Running 5000 simulations.:  38%|███▊      | 1890/5000 [00:12<00:20, 150.40it/s]Running 5000 simulations.:  38%|███▊      | 1906/5000 [00:12<00:20, 150.89it/s]Running 5000 simulations.:  38%|███▊      | 1922/5000 [00:12<00:20, 150.75it/s]Running 5000 simulations.:  39%|███▉      | 1938/5000 [00:12<00:20, 151.19it/s]Running 5000 simulations.:  39%|███▉      | 1954/5000 [00:12<00:20, 151.09it/s]Running 5000 simulations.:  39%|███▉      | 1970/5000 [00:12<00:20, 151.16it/s]Running 5000 simulations.:  40%|███▉      | 1986/5000 [00:12<00:19, 150.86it/s]Running 5000 simulations.:  40%|████      | 2002/5000 [00:13<00:19, 150.49it/s]Running 5000 simulations.:  40%|████      | 2018/5000 [00:13<00:19, 150.55it/s]Running 5000 simulations.:  41%|████      | 2034/5000 [00:13<00:19, 150.51it/s]Running 5000 simulations.:  41%|████      | 2050/5000 [00:13<00:19, 150.45it/s]Running 5000 simulations.:  41%|████▏     | 2066/5000 [00:13<00:19, 150.69it/s]Running 5000 simulations.:  42%|████▏     | 2082/5000 [00:13<00:19, 151.23it/s]Running 5000 simulations.:  42%|████▏     | 2098/5000 [00:13<00:19, 151.11it/s]Running 5000 simulations.:  42%|████▏     | 2114/5000 [00:13<00:19, 151.06it/s]Running 5000 simulations.:  43%|████▎     | 2130/5000 [00:13<00:18, 151.47it/s]Running 5000 simulations.:  43%|████▎     | 2146/5000 [00:14<00:18, 151.62it/s]Running 5000 simulations.:  43%|████▎     | 2162/5000 [00:14<00:18, 151.84it/s]Running 5000 simulations.:  44%|████▎     | 2178/5000 [00:14<00:18, 151.81it/s]Running 5000 simulations.:  44%|████▍     | 2194/5000 [00:14<00:18, 151.99it/s]Running 5000 simulations.:  44%|████▍     | 2210/5000 [00:14<00:18, 150.95it/s]Running 5000 simulations.:  45%|████▍     | 2226/5000 [00:14<00:18, 149.98it/s]Running 5000 simulations.:  45%|████▍     | 2242/5000 [00:14<00:18, 150.68it/s]Running 5000 simulations.:  45%|████▌     | 2258/5000 [00:14<00:18, 151.41it/s]Running 5000 simulations.:  45%|████▌     | 2274/5000 [00:14<00:17, 151.96it/s]Running 5000 simulations.:  46%|████▌     | 2290/5000 [00:14<00:17, 151.93it/s]Running 5000 simulations.:  46%|████▌     | 2306/5000 [00:15<00:17, 151.02it/s]Running 5000 simulations.:  46%|████▋     | 2322/5000 [00:15<00:17, 149.71it/s]Running 5000 simulations.:  47%|████▋     | 2337/5000 [00:15<00:18, 146.95it/s]Running 5000 simulations.:  47%|████▋     | 2352/5000 [00:15<00:18, 144.52it/s]Running 5000 simulations.:  47%|████▋     | 2367/5000 [00:15<00:18, 142.48it/s]Running 5000 simulations.:  48%|████▊     | 2382/5000 [00:15<00:18, 141.46it/s]Running 5000 simulations.:  48%|████▊     | 2397/5000 [00:15<00:18, 141.06it/s]Running 5000 simulations.:  48%|████▊     | 2412/5000 [00:15<00:18, 140.58it/s]Running 5000 simulations.:  49%|████▊     | 2427/5000 [00:15<00:18, 140.20it/s]Running 5000 simulations.:  49%|████▉     | 2442/5000 [00:16<00:18, 139.88it/s]Running 5000 simulations.:  49%|████▉     | 2457/5000 [00:16<00:18, 140.06it/s]Running 5000 simulations.:  49%|████▉     | 2472/5000 [00:16<00:18, 139.99it/s]Running 5000 simulations.:  50%|████▉     | 2487/5000 [00:16<00:17, 139.97it/s]Running 5000 simulations.:  50%|█████     | 2502/5000 [00:16<00:17, 139.97it/s]Running 5000 simulations.:  50%|█████     | 2516/5000 [00:16<00:17, 139.65it/s]Running 5000 simulations.:  51%|█████     | 2530/5000 [00:16<00:17, 139.22it/s]Running 5000 simulations.:  51%|█████     | 2544/5000 [00:16<00:17, 139.35it/s]Running 5000 simulations.:  51%|█████     | 2559/5000 [00:16<00:17, 139.81it/s]Running 5000 simulations.:  51%|█████▏    | 2574/5000 [00:17<00:17, 139.99it/s]Running 5000 simulations.:  52%|█████▏    | 2588/5000 [00:17<00:17, 139.83it/s]Running 5000 simulations.:  52%|█████▏    | 2603/5000 [00:17<00:17, 140.42it/s]Running 5000 simulations.:  52%|█████▏    | 2618/5000 [00:17<00:16, 140.43it/s]Running 5000 simulations.:  53%|█████▎    | 2633/5000 [00:17<00:16, 140.69it/s]Running 5000 simulations.:  53%|█████▎    | 2648/5000 [00:17<00:16, 140.90it/s]Running 5000 simulations.:  53%|█████▎    | 2663/5000 [00:17<00:16, 141.08it/s]Running 5000 simulations.:  54%|█████▎    | 2678/5000 [00:17<00:16, 140.56it/s]Running 5000 simulations.:  54%|█████▍    | 2693/5000 [00:17<00:16, 139.63it/s]Running 5000 simulations.:  54%|█████▍    | 2708/5000 [00:17<00:16, 139.78it/s]Running 5000 simulations.:  54%|█████▍    | 2723/5000 [00:18<00:16, 140.14it/s]Running 5000 simulations.:  55%|█████▍    | 2738/5000 [00:18<00:16, 139.84it/s]Running 5000 simulations.:  55%|█████▌    | 2752/5000 [00:18<00:16, 139.13it/s]Running 5000 simulations.:  55%|█████▌    | 2767/5000 [00:18<00:15, 141.10it/s]Running 5000 simulations.:  56%|█████▌    | 2782/5000 [00:18<00:15, 143.29it/s]Running 5000 simulations.:  56%|█████▌    | 2797/5000 [00:18<00:15, 144.92it/s]Running 5000 simulations.:  56%|█████▌    | 2812/5000 [00:18<00:14, 146.16it/s]Running 5000 simulations.:  57%|█████▋    | 2827/5000 [00:18<00:14, 147.22it/s]Running 5000 simulations.:  57%|█████▋    | 2842/5000 [00:18<00:14, 147.75it/s]Running 5000 simulations.:  57%|█████▋    | 2857/5000 [00:18<00:14, 148.18it/s]Running 5000 simulations.:  57%|█████▋    | 2872/5000 [00:19<00:14, 148.30it/s]Running 5000 simulations.:  58%|█████▊    | 2887/5000 [00:19<00:14, 148.30it/s]Running 5000 simulations.:  58%|█████▊    | 2902/5000 [00:19<00:14, 148.36it/s]Running 5000 simulations.:  58%|█████▊    | 2917/5000 [00:19<00:14, 148.25it/s]Running 5000 simulations.:  59%|█████▊    | 2932/5000 [00:19<00:13, 148.39it/s]Running 5000 simulations.:  59%|█████▉    | 2948/5000 [00:19<00:13, 148.97it/s]Running 5000 simulations.:  59%|█████▉    | 2963/5000 [00:19<00:13, 148.89it/s]Running 5000 simulations.:  60%|█████▉    | 2978/5000 [00:19<00:13, 148.56it/s]Running 5000 simulations.:  60%|█████▉    | 2993/5000 [00:19<00:13, 148.32it/s]Running 5000 simulations.:  60%|██████    | 3008/5000 [00:20<00:13, 148.49it/s]Running 5000 simulations.:  60%|██████    | 3023/5000 [00:20<00:13, 148.63it/s]Running 5000 simulations.:  61%|██████    | 3038/5000 [00:20<00:13, 148.80it/s]Running 5000 simulations.:  61%|██████    | 3054/5000 [00:20<00:13, 149.28it/s]Running 5000 simulations.:  61%|██████▏   | 3070/5000 [00:20<00:12, 149.53it/s]Running 5000 simulations.:  62%|██████▏   | 3085/5000 [00:20<00:12, 149.57it/s]Running 5000 simulations.:  62%|██████▏   | 3100/5000 [00:20<00:12, 149.27it/s]Running 5000 simulations.:  62%|██████▏   | 3115/5000 [00:20<00:12, 149.08it/s]Running 5000 simulations.:  63%|██████▎   | 3130/5000 [00:20<00:12, 148.91it/s]Running 5000 simulations.:  63%|██████▎   | 3146/5000 [00:20<00:12, 149.43it/s]Running 5000 simulations.:  63%|██████▎   | 3161/5000 [00:21<00:12, 148.67it/s]Running 5000 simulations.:  64%|██████▎   | 3176/5000 [00:21<00:12, 148.88it/s]Running 5000 simulations.:  64%|██████▍   | 3191/5000 [00:21<00:12, 148.29it/s]Running 5000 simulations.:  64%|██████▍   | 3206/5000 [00:21<00:12, 148.08it/s]Running 5000 simulations.:  64%|██████▍   | 3222/5000 [00:21<00:11, 148.65it/s]Running 5000 simulations.:  65%|██████▍   | 3238/5000 [00:21<00:11, 148.99it/s]Running 5000 simulations.:  65%|██████▌   | 3253/5000 [00:21<00:11, 148.35it/s]Running 5000 simulations.:  65%|██████▌   | 3268/5000 [00:21<00:11, 147.99it/s]Running 5000 simulations.:  66%|██████▌   | 3283/5000 [00:21<00:11, 148.23it/s]Running 5000 simulations.:  66%|██████▌   | 3298/5000 [00:21<00:11, 148.56it/s]Running 5000 simulations.:  66%|██████▋   | 3313/5000 [00:22<00:11, 148.87it/s]Running 5000 simulations.:  67%|██████▋   | 3329/5000 [00:22<00:11, 149.27it/s]Running 5000 simulations.:  67%|██████▋   | 3344/5000 [00:22<00:11, 148.86it/s]Running 5000 simulations.:  67%|██████▋   | 3359/5000 [00:22<00:11, 148.87it/s]Running 5000 simulations.:  67%|██████▋   | 3374/5000 [00:22<00:10, 148.97it/s]Running 5000 simulations.:  68%|██████▊   | 3389/5000 [00:22<00:10, 149.21it/s]Running 5000 simulations.:  68%|██████▊   | 3404/5000 [00:22<00:10, 148.78it/s]Running 5000 simulations.:  68%|██████▊   | 3419/5000 [00:22<00:10, 148.42it/s]Running 5000 simulations.:  69%|██████▊   | 3434/5000 [00:22<00:10, 147.99it/s]Running 5000 simulations.:  69%|██████▉   | 3449/5000 [00:22<00:10, 147.33it/s]Running 5000 simulations.:  69%|██████▉   | 3464/5000 [00:23<00:10, 146.47it/s]Running 5000 simulations.:  70%|██████▉   | 3479/5000 [00:23<00:10, 146.81it/s]Running 5000 simulations.:  70%|██████▉   | 3494/5000 [00:23<00:10, 147.28it/s]Running 5000 simulations.:  70%|███████   | 3509/5000 [00:23<00:10, 145.59it/s]Running 5000 simulations.:  70%|███████   | 3524/5000 [00:23<00:10, 144.64it/s]Running 5000 simulations.:  71%|███████   | 3539/5000 [00:23<00:10, 144.25it/s]Running 5000 simulations.:  71%|███████   | 3554/5000 [00:23<00:10, 144.24it/s]Running 5000 simulations.:  71%|███████▏  | 3569/5000 [00:23<00:09, 143.89it/s]Running 5000 simulations.:  72%|███████▏  | 3584/5000 [00:23<00:09, 143.52it/s]Running 5000 simulations.:  72%|███████▏  | 3599/5000 [00:24<00:09, 143.54it/s]Running 5000 simulations.:  72%|███████▏  | 3614/5000 [00:24<00:09, 144.21it/s]Running 5000 simulations.:  73%|███████▎  | 3629/5000 [00:24<00:09, 144.49it/s]Running 5000 simulations.:  73%|███████▎  | 3644/5000 [00:24<00:09, 143.64it/s]Running 5000 simulations.:  73%|███████▎  | 3659/5000 [00:24<00:09, 140.18it/s]Running 5000 simulations.:  73%|███████▎  | 3674/5000 [00:24<00:09, 140.81it/s]Running 5000 simulations.:  74%|███████▍  | 3689/5000 [00:24<00:09, 141.71it/s]Running 5000 simulations.:  74%|███████▍  | 3704/5000 [00:24<00:09, 143.22it/s]Running 5000 simulations.:  74%|███████▍  | 3721/5000 [00:24<00:08, 147.92it/s]Running 5000 simulations.:  75%|███████▍  | 3738/5000 [00:24<00:08, 151.60it/s]Running 5000 simulations.:  75%|███████▌  | 3754/5000 [00:25<00:08, 148.79it/s]Running 5000 simulations.:  75%|███████▌  | 3769/5000 [00:25<00:08, 147.72it/s]Running 5000 simulations.:  76%|███████▌  | 3784/5000 [00:25<00:08, 146.26it/s]Running 5000 simulations.:  76%|███████▌  | 3799/5000 [00:25<00:08, 145.05it/s]Running 5000 simulations.:  76%|███████▋  | 3814/5000 [00:25<00:08, 144.55it/s]Running 5000 simulations.:  77%|███████▋  | 3829/5000 [00:25<00:08, 143.10it/s]Running 5000 simulations.:  77%|███████▋  | 3844/5000 [00:25<00:08, 141.21it/s]Running 5000 simulations.:  77%|███████▋  | 3859/5000 [00:25<00:08, 139.78it/s]Running 5000 simulations.:  77%|███████▋  | 3873/5000 [00:25<00:08, 134.21it/s]Running 5000 simulations.:  78%|███████▊  | 3887/5000 [00:26<00:08, 130.74it/s]Running 5000 simulations.:  78%|███████▊  | 3901/5000 [00:26<00:08, 128.46it/s]Running 5000 simulations.:  78%|███████▊  | 3914/5000 [00:26<00:08, 121.34it/s]Running 5000 simulations.:  79%|███████▊  | 3928/5000 [00:26<00:08, 126.11it/s]Running 5000 simulations.:  79%|███████▉  | 3942/5000 [00:26<00:08, 129.28it/s]Running 5000 simulations.:  79%|███████▉  | 3956/5000 [00:26<00:07, 131.62it/s]Running 5000 simulations.:  79%|███████▉  | 3970/5000 [00:26<00:07, 133.95it/s]Running 5000 simulations.:  80%|███████▉  | 3984/5000 [00:26<00:07, 135.62it/s]Running 5000 simulations.:  80%|███████▉  | 3998/5000 [00:26<00:07, 134.93it/s]Running 5000 simulations.:  80%|████████  | 4013/5000 [00:26<00:07, 136.44it/s]Running 5000 simulations.:  81%|████████  | 4028/5000 [00:27<00:07, 137.68it/s]Running 5000 simulations.:  81%|████████  | 4042/5000 [00:27<00:07, 135.95it/s]Running 5000 simulations.:  81%|████████  | 4056/5000 [00:27<00:06, 136.56it/s]Running 5000 simulations.:  81%|████████▏ | 4070/5000 [00:27<00:06, 137.15it/s]Running 5000 simulations.:  82%|████████▏ | 4084/5000 [00:27<00:06, 137.26it/s]Running 5000 simulations.:  82%|████████▏ | 4098/5000 [00:27<00:06, 137.22it/s]Running 5000 simulations.:  82%|████████▏ | 4112/5000 [00:27<00:06, 137.73it/s]Running 5000 simulations.:  83%|████████▎ | 4126/5000 [00:27<00:06, 138.13it/s]Running 5000 simulations.:  83%|████████▎ | 4140/5000 [00:27<00:06, 138.05it/s]Running 5000 simulations.:  83%|████████▎ | 4154/5000 [00:28<00:06, 137.78it/s]Running 5000 simulations.:  83%|████████▎ | 4168/5000 [00:28<00:06, 137.65it/s]Running 5000 simulations.:  84%|████████▎ | 4182/5000 [00:28<00:05, 137.93it/s]Running 5000 simulations.:  84%|████████▍ | 4196/5000 [00:28<00:05, 138.08it/s]Running 5000 simulations.:  84%|████████▍ | 4210/5000 [00:28<00:05, 136.39it/s]Running 5000 simulations.:  84%|████████▍ | 4224/5000 [00:28<00:05, 135.61it/s]Running 5000 simulations.:  85%|████████▍ | 4238/5000 [00:28<00:05, 135.45it/s]Running 5000 simulations.:  85%|████████▌ | 4252/5000 [00:28<00:05, 135.88it/s]Running 5000 simulations.:  85%|████████▌ | 4266/5000 [00:28<00:05, 135.92it/s]Running 5000 simulations.:  86%|████████▌ | 4280/5000 [00:28<00:05, 136.10it/s]Running 5000 simulations.:  86%|████████▌ | 4294/5000 [00:29<00:05, 135.72it/s]Running 5000 simulations.:  86%|████████▌ | 4308/5000 [00:29<00:05, 135.56it/s]Running 5000 simulations.:  86%|████████▋ | 4322/5000 [00:29<00:05, 134.94it/s]Running 5000 simulations.:  87%|████████▋ | 4336/5000 [00:29<00:04, 135.04it/s]Running 5000 simulations.:  87%|████████▋ | 4350/5000 [00:29<00:04, 135.90it/s]Running 5000 simulations.:  87%|████████▋ | 4364/5000 [00:29<00:04, 136.17it/s]Running 5000 simulations.:  88%|████████▊ | 4378/5000 [00:29<00:04, 135.65it/s]Running 5000 simulations.:  88%|████████▊ | 4392/5000 [00:29<00:04, 135.50it/s]Running 5000 simulations.:  88%|████████▊ | 4406/5000 [00:29<00:04, 135.11it/s]Running 5000 simulations.:  88%|████████▊ | 4420/5000 [00:29<00:04, 134.88it/s]Running 5000 simulations.:  89%|████████▊ | 4434/5000 [00:30<00:04, 135.10it/s]Running 5000 simulations.:  89%|████████▉ | 4448/5000 [00:30<00:04, 135.71it/s]Running 5000 simulations.:  89%|████████▉ | 4462/5000 [00:30<00:03, 136.30it/s]Running 5000 simulations.:  90%|████████▉ | 4476/5000 [00:30<00:03, 136.34it/s]Running 5000 simulations.:  90%|████████▉ | 4490/5000 [00:30<00:03, 135.80it/s]Running 5000 simulations.:  90%|█████████ | 4504/5000 [00:30<00:03, 134.94it/s]Running 5000 simulations.:  90%|█████████ | 4518/5000 [00:30<00:03, 135.06it/s]Running 5000 simulations.:  91%|█████████ | 4532/5000 [00:30<00:03, 134.18it/s]Running 5000 simulations.:  91%|█████████ | 4546/5000 [00:30<00:03, 133.98it/s]Running 5000 simulations.:  91%|█████████ | 4560/5000 [00:31<00:03, 133.44it/s]Running 5000 simulations.:  91%|█████████▏| 4574/5000 [00:31<00:03, 133.37it/s]Running 5000 simulations.:  92%|█████████▏| 4588/5000 [00:31<00:03, 133.58it/s]Running 5000 simulations.:  92%|█████████▏| 4602/5000 [00:31<00:02, 133.50it/s]Running 5000 simulations.:  92%|█████████▏| 4616/5000 [00:31<00:02, 134.11it/s]Running 5000 simulations.:  93%|█████████▎| 4630/5000 [00:31<00:02, 134.71it/s]Running 5000 simulations.:  93%|█████████▎| 4644/5000 [00:31<00:02, 134.83it/s]Running 5000 simulations.:  93%|█████████▎| 4658/5000 [00:31<00:02, 134.89it/s]Running 5000 simulations.:  93%|█████████▎| 4672/5000 [00:31<00:02, 132.61it/s]Running 5000 simulations.:  94%|█████████▎| 4686/5000 [00:31<00:02, 128.57it/s]Running 5000 simulations.:  94%|█████████▍| 4700/5000 [00:32<00:02, 130.99it/s]Running 5000 simulations.:  94%|█████████▍| 4714/5000 [00:32<00:02, 132.37it/s]Running 5000 simulations.:  95%|█████████▍| 4728/5000 [00:32<00:02, 133.54it/s]Running 5000 simulations.:  95%|█████████▍| 4742/5000 [00:32<00:01, 135.37it/s]Running 5000 simulations.:  95%|█████████▌| 4757/5000 [00:32<00:01, 137.31it/s]Running 5000 simulations.:  95%|█████████▌| 4772/5000 [00:32<00:01, 138.86it/s]Running 5000 simulations.:  96%|█████████▌| 4787/5000 [00:32<00:01, 139.99it/s]Running 5000 simulations.:  96%|█████████▌| 4802/5000 [00:32<00:01, 141.01it/s]Running 5000 simulations.:  96%|█████████▋| 4817/5000 [00:32<00:01, 141.60it/s]Running 5000 simulations.:  97%|█████████▋| 4832/5000 [00:33<00:01, 142.21it/s]Running 5000 simulations.:  97%|█████████▋| 4847/5000 [00:33<00:01, 142.39it/s]Running 5000 simulations.:  97%|█████████▋| 4862/5000 [00:33<00:00, 140.87it/s]Running 5000 simulations.:  98%|█████████▊| 4877/5000 [00:33<00:00, 140.71it/s]Running 5000 simulations.:  98%|█████████▊| 4892/5000 [00:33<00:00, 139.76it/s]Running 5000 simulations.:  98%|█████████▊| 4906/5000 [00:33<00:00, 138.32it/s]Running 5000 simulations.:  98%|█████████▊| 4920/5000 [00:33<00:00, 137.51it/s]Running 5000 simulations.:  99%|█████████▊| 4934/5000 [00:33<00:00, 137.50it/s]Running 5000 simulations.:  99%|█████████▉| 4948/5000 [00:33<00:00, 137.08it/s]Running 5000 simulations.:  99%|█████████▉| 4962/5000 [00:33<00:00, 137.85it/s]Running 5000 simulations.: 100%|█████████▉| 4976/5000 [00:34<00:00, 137.15it/s]Running 5000 simulations.: 100%|█████████▉| 4990/5000 [00:34<00:00, 136.56it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:34<00:00, 146.04it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 15/5000 [00:00<00:33, 149.44it/s]Running 5000 simulations.:   1%|          | 30/5000 [00:00<00:33, 149.57it/s]Running 5000 simulations.:   1%|          | 45/5000 [00:00<00:33, 149.40it/s]Running 5000 simulations.:   1%|          | 60/5000 [00:00<00:33, 149.28it/s]Running 5000 simulations.:   2%|▏         | 75/5000 [00:00<00:33, 149.09it/s]Running 5000 simulations.:   2%|▏         | 90/5000 [00:00<00:33, 148.64it/s]Running 5000 simulations.:   2%|▏         | 104/5000 [00:00<00:33, 145.93it/s]Running 5000 simulations.:   2%|▏         | 118/5000 [00:00<00:34, 143.04it/s]Running 5000 simulations.:   3%|▎         | 132/5000 [00:00<00:34, 140.60it/s]Running 5000 simulations.:   3%|▎         | 146/5000 [00:01<00:34, 138.96it/s]Running 5000 simulations.:   3%|▎         | 161/5000 [00:01<00:34, 139.67it/s]Running 5000 simulations.:   4%|▎         | 176/5000 [00:01<00:34, 141.63it/s]Running 5000 simulations.:   4%|▍         | 191/5000 [00:01<00:33, 142.99it/s]Running 5000 simulations.:   4%|▍         | 206/5000 [00:01<00:33, 143.94it/s]Running 5000 simulations.:   4%|▍         | 221/5000 [00:01<00:32, 144.93it/s]Running 5000 simulations.:   5%|▍         | 236/5000 [00:01<00:32, 145.64it/s]Running 5000 simulations.:   5%|▌         | 251/5000 [00:01<00:32, 145.66it/s]Running 5000 simulations.:   5%|▌         | 266/5000 [00:01<00:32, 145.87it/s]Running 5000 simulations.:   6%|▌         | 281/5000 [00:01<00:32, 145.99it/s]Running 5000 simulations.:   6%|▌         | 296/5000 [00:02<00:32, 145.36it/s]Running 5000 simulations.:   6%|▌         | 311/5000 [00:02<00:32, 144.96it/s]Running 5000 simulations.:   7%|▋         | 326/5000 [00:02<00:32, 144.96it/s]Running 5000 simulations.:   7%|▋         | 341/5000 [00:02<00:32, 145.27it/s]Running 5000 simulations.:   7%|▋         | 356/5000 [00:02<00:31, 145.34it/s]Running 5000 simulations.:   7%|▋         | 371/5000 [00:02<00:31, 145.49it/s]Running 5000 simulations.:   8%|▊         | 386/5000 [00:02<00:31, 145.11it/s]Running 5000 simulations.:   8%|▊         | 401/5000 [00:02<00:31, 144.95it/s]Running 5000 simulations.:   8%|▊         | 416/5000 [00:02<00:31, 144.68it/s]Running 5000 simulations.:   9%|▊         | 431/5000 [00:02<00:31, 144.28it/s]Running 5000 simulations.:   9%|▉         | 446/5000 [00:03<00:31, 144.02it/s]Running 5000 simulations.:   9%|▉         | 461/5000 [00:03<00:31, 144.38it/s]Running 5000 simulations.:  10%|▉         | 476/5000 [00:03<00:31, 144.35it/s]Running 5000 simulations.:  10%|▉         | 491/5000 [00:03<00:31, 144.16it/s]Running 5000 simulations.:  10%|█         | 506/5000 [00:03<00:30, 144.99it/s]Running 5000 simulations.:  10%|█         | 521/5000 [00:03<00:30, 145.07it/s]Running 5000 simulations.:  11%|█         | 536/5000 [00:03<00:30, 144.37it/s]Running 5000 simulations.:  11%|█         | 551/5000 [00:03<00:31, 140.95it/s]Running 5000 simulations.:  11%|█▏        | 566/5000 [00:03<00:32, 138.31it/s]Running 5000 simulations.:  12%|█▏        | 580/5000 [00:04<00:32, 136.59it/s]Running 5000 simulations.:  12%|█▏        | 594/5000 [00:04<00:32, 135.65it/s]Running 5000 simulations.:  12%|█▏        | 608/5000 [00:04<00:32, 134.42it/s]Running 5000 simulations.:  12%|█▏        | 622/5000 [00:04<00:32, 133.49it/s]Running 5000 simulations.:  13%|█▎        | 636/5000 [00:04<00:32, 132.88it/s]Running 5000 simulations.:  13%|█▎        | 650/5000 [00:04<00:32, 132.59it/s]Running 5000 simulations.:  13%|█▎        | 664/5000 [00:04<00:32, 132.77it/s]Running 5000 simulations.:  14%|█▎        | 678/5000 [00:04<00:32, 132.47it/s]Running 5000 simulations.:  14%|█▍        | 692/5000 [00:04<00:32, 132.86it/s]Running 5000 simulations.:  14%|█▍        | 706/5000 [00:04<00:31, 134.20it/s]Running 5000 simulations.:  14%|█▍        | 721/5000 [00:05<00:31, 137.81it/s]Running 5000 simulations.:  15%|█▍        | 736/5000 [00:05<00:30, 140.05it/s]Running 5000 simulations.:  15%|█▌        | 751/5000 [00:05<00:29, 141.86it/s]Running 5000 simulations.:  15%|█▌        | 766/5000 [00:05<00:29, 143.01it/s]Running 5000 simulations.:  16%|█▌        | 781/5000 [00:05<00:29, 144.27it/s]Running 5000 simulations.:  16%|█▌        | 796/5000 [00:05<00:29, 143.90it/s]Running 5000 simulations.:  16%|█▌        | 811/5000 [00:05<00:28, 144.60it/s]Running 5000 simulations.:  17%|█▋        | 826/5000 [00:05<00:28, 145.37it/s]Running 5000 simulations.:  17%|█▋        | 841/5000 [00:05<00:28, 145.68it/s]Running 5000 simulations.:  17%|█▋        | 856/5000 [00:06<00:28, 146.10it/s]Running 5000 simulations.:  17%|█▋        | 871/5000 [00:06<00:28, 145.96it/s]Running 5000 simulations.:  18%|█▊        | 886/5000 [00:06<00:28, 145.97it/s]Running 5000 simulations.:  18%|█▊        | 901/5000 [00:06<00:28, 146.34it/s]Running 5000 simulations.:  18%|█▊        | 916/5000 [00:06<00:27, 146.56it/s]Running 5000 simulations.:  19%|█▊        | 931/5000 [00:06<00:27, 146.71it/s]Running 5000 simulations.:  19%|█▉        | 946/5000 [00:06<00:27, 146.80it/s]Running 5000 simulations.:  19%|█▉        | 961/5000 [00:06<00:27, 146.95it/s]Running 5000 simulations.:  20%|█▉        | 976/5000 [00:06<00:27, 147.01it/s]Running 5000 simulations.:  20%|█▉        | 991/5000 [00:06<00:27, 146.96it/s]Running 5000 simulations.:  20%|██        | 1006/5000 [00:07<00:27, 147.00it/s]Running 5000 simulations.:  20%|██        | 1021/5000 [00:07<00:27, 146.65it/s]Running 5000 simulations.:  21%|██        | 1036/5000 [00:07<00:27, 145.54it/s]Running 5000 simulations.:  21%|██        | 1051/5000 [00:07<00:27, 145.84it/s]Running 5000 simulations.:  21%|██▏       | 1066/5000 [00:07<00:26, 146.33it/s]Running 5000 simulations.:  22%|██▏       | 1081/5000 [00:07<00:26, 146.29it/s]Running 5000 simulations.:  22%|██▏       | 1096/5000 [00:07<00:26, 145.61it/s]Running 5000 simulations.:  22%|██▏       | 1111/5000 [00:07<00:26, 144.65it/s]Running 5000 simulations.:  23%|██▎       | 1126/5000 [00:07<00:26, 143.96it/s]Running 5000 simulations.:  23%|██▎       | 1141/5000 [00:07<00:26, 144.18it/s]Running 5000 simulations.:  23%|██▎       | 1156/5000 [00:08<00:26, 143.28it/s]Running 5000 simulations.:  23%|██▎       | 1171/5000 [00:08<00:26, 143.28it/s]Running 5000 simulations.:  24%|██▎       | 1186/5000 [00:08<00:26, 142.84it/s]Running 5000 simulations.:  24%|██▍       | 1201/5000 [00:08<00:26, 142.73it/s]Running 5000 simulations.:  24%|██▍       | 1216/5000 [00:08<00:26, 142.49it/s]Running 5000 simulations.:  25%|██▍       | 1231/5000 [00:08<00:26, 142.89it/s]Running 5000 simulations.:  25%|██▍       | 1246/5000 [00:08<00:26, 142.84it/s]Running 5000 simulations.:  25%|██▌       | 1261/5000 [00:08<00:26, 143.19it/s]Running 5000 simulations.:  26%|██▌       | 1276/5000 [00:08<00:26, 143.07it/s]Running 5000 simulations.:  26%|██▌       | 1291/5000 [00:09<00:25, 143.23it/s]Running 5000 simulations.:  26%|██▌       | 1306/5000 [00:09<00:25, 143.22it/s]Running 5000 simulations.:  26%|██▋       | 1321/5000 [00:09<00:25, 142.74it/s]Running 5000 simulations.:  27%|██▋       | 1336/5000 [00:09<00:25, 141.26it/s]Running 5000 simulations.:  27%|██▋       | 1351/5000 [00:09<00:26, 140.34it/s]Running 5000 simulations.:  27%|██▋       | 1366/5000 [00:09<00:25, 140.18it/s]Running 5000 simulations.:  28%|██▊       | 1381/5000 [00:09<00:25, 139.99it/s]Running 5000 simulations.:  28%|██▊       | 1396/5000 [00:09<00:25, 139.94it/s]Running 5000 simulations.:  28%|██▊       | 1410/5000 [00:09<00:25, 139.48it/s]Running 5000 simulations.:  28%|██▊       | 1424/5000 [00:09<00:25, 139.28it/s]Running 5000 simulations.:  29%|██▉       | 1438/5000 [00:10<00:25, 139.43it/s]Running 5000 simulations.:  29%|██▉       | 1452/5000 [00:10<00:25, 139.19it/s]Running 5000 simulations.:  29%|██▉       | 1467/5000 [00:10<00:25, 139.53it/s]Running 5000 simulations.:  30%|██▉       | 1481/5000 [00:10<00:25, 139.65it/s]Running 5000 simulations.:  30%|██▉       | 1496/5000 [00:10<00:25, 139.96it/s]Running 5000 simulations.:  30%|███       | 1511/5000 [00:10<00:24, 140.29it/s]Running 5000 simulations.:  31%|███       | 1526/5000 [00:10<00:24, 140.20it/s]Running 5000 simulations.:  31%|███       | 1541/5000 [00:10<00:24, 140.08it/s]Running 5000 simulations.:  31%|███       | 1556/5000 [00:10<00:24, 139.91it/s]Running 5000 simulations.:  31%|███▏      | 1570/5000 [00:11<00:24, 139.36it/s]Running 5000 simulations.:  32%|███▏      | 1584/5000 [00:11<00:24, 139.13it/s]Running 5000 simulations.:  32%|███▏      | 1599/5000 [00:11<00:24, 140.59it/s]Running 5000 simulations.:  32%|███▏      | 1614/5000 [00:11<00:23, 142.14it/s]Running 5000 simulations.:  33%|███▎      | 1629/5000 [00:11<00:23, 142.40it/s]Running 5000 simulations.:  33%|███▎      | 1644/5000 [00:11<00:23, 142.83it/s]Running 5000 simulations.:  33%|███▎      | 1659/5000 [00:11<00:23, 143.21it/s]Running 5000 simulations.:  33%|███▎      | 1674/5000 [00:11<00:23, 143.44it/s]Running 5000 simulations.:  34%|███▍      | 1689/5000 [00:11<00:23, 143.73it/s]Running 5000 simulations.:  34%|███▍      | 1704/5000 [00:11<00:22, 143.89it/s]Running 5000 simulations.:  34%|███▍      | 1719/5000 [00:12<00:22, 144.27it/s]Running 5000 simulations.:  35%|███▍      | 1734/5000 [00:12<00:22, 144.46it/s]Running 5000 simulations.:  35%|███▍      | 1749/5000 [00:12<00:22, 144.65it/s]Running 5000 simulations.:  35%|███▌      | 1764/5000 [00:12<00:22, 144.74it/s]Running 5000 simulations.:  36%|███▌      | 1779/5000 [00:12<00:22, 144.99it/s]Running 5000 simulations.:  36%|███▌      | 1794/5000 [00:12<00:22, 145.46it/s]Running 5000 simulations.:  36%|███▌      | 1809/5000 [00:12<00:21, 145.93it/s]Running 5000 simulations.:  36%|███▋      | 1824/5000 [00:12<00:21, 145.73it/s]Running 5000 simulations.:  37%|███▋      | 1839/5000 [00:12<00:22, 141.06it/s]Running 5000 simulations.:  37%|███▋      | 1854/5000 [00:12<00:22, 137.87it/s]Running 5000 simulations.:  37%|███▋      | 1868/5000 [00:13<00:23, 135.86it/s]Running 5000 simulations.:  38%|███▊      | 1882/5000 [00:13<00:23, 134.58it/s]Running 5000 simulations.:  38%|███▊      | 1896/5000 [00:13<00:23, 133.59it/s]Running 5000 simulations.:  38%|███▊      | 1910/5000 [00:13<00:23, 132.38it/s]Running 5000 simulations.:  38%|███▊      | 1924/5000 [00:13<00:23, 132.43it/s]Running 5000 simulations.:  39%|███▉      | 1938/5000 [00:13<00:23, 131.68it/s]Running 5000 simulations.:  39%|███▉      | 1952/5000 [00:13<00:23, 131.73it/s]Running 5000 simulations.:  39%|███▉      | 1966/5000 [00:13<00:23, 131.77it/s]Running 5000 simulations.:  40%|███▉      | 1980/5000 [00:13<00:22, 131.57it/s]Running 5000 simulations.:  40%|███▉      | 1995/5000 [00:14<00:22, 135.78it/s]Running 5000 simulations.:  40%|████      | 2010/5000 [00:14<00:21, 138.65it/s]Running 5000 simulations.:  40%|████      | 2025/5000 [00:14<00:21, 140.78it/s]Running 5000 simulations.:  41%|████      | 2040/5000 [00:14<00:20, 143.01it/s]Running 5000 simulations.:  41%|████      | 2055/5000 [00:14<00:20, 144.21it/s]Running 5000 simulations.:  41%|████▏     | 2070/5000 [00:14<00:20, 144.59it/s]Running 5000 simulations.:  42%|████▏     | 2085/5000 [00:14<00:20, 144.74it/s]Running 5000 simulations.:  42%|████▏     | 2100/5000 [00:14<00:19, 145.35it/s]Running 5000 simulations.:  42%|████▏     | 2115/5000 [00:14<00:19, 145.68it/s]Running 5000 simulations.:  43%|████▎     | 2130/5000 [00:14<00:19, 145.99it/s]Running 5000 simulations.:  43%|████▎     | 2145/5000 [00:15<00:19, 145.84it/s]Running 5000 simulations.:  43%|████▎     | 2160/5000 [00:15<00:19, 145.61it/s]Running 5000 simulations.:  44%|████▎     | 2175/5000 [00:15<00:19, 145.46it/s]Running 5000 simulations.:  44%|████▍     | 2190/5000 [00:15<00:19, 145.23it/s]Running 5000 simulations.:  44%|████▍     | 2205/5000 [00:15<00:19, 145.33it/s]Running 5000 simulations.:  44%|████▍     | 2220/5000 [00:15<00:19, 145.26it/s]Running 5000 simulations.:  45%|████▍     | 2235/5000 [00:15<00:18, 145.69it/s]Running 5000 simulations.:  45%|████▌     | 2250/5000 [00:15<00:18, 145.37it/s]Running 5000 simulations.:  45%|████▌     | 2265/5000 [00:15<00:18, 146.10it/s]Running 5000 simulations.:  46%|████▌     | 2280/5000 [00:16<00:18, 146.26it/s]Running 5000 simulations.:  46%|████▌     | 2295/5000 [00:16<00:18, 146.03it/s]Running 5000 simulations.:  46%|████▌     | 2310/5000 [00:16<00:18, 145.33it/s]Running 5000 simulations.:  46%|████▋     | 2325/5000 [00:16<00:18, 145.36it/s]Running 5000 simulations.:  47%|████▋     | 2340/5000 [00:16<00:18, 145.30it/s]Running 5000 simulations.:  47%|████▋     | 2355/5000 [00:16<00:18, 145.65it/s]Running 5000 simulations.:  47%|████▋     | 2370/5000 [00:16<00:18, 145.82it/s]Running 5000 simulations.:  48%|████▊     | 2385/5000 [00:16<00:17, 146.09it/s]Running 5000 simulations.:  48%|████▊     | 2400/5000 [00:16<00:17, 145.71it/s]Running 5000 simulations.:  48%|████▊     | 2415/5000 [00:16<00:17, 145.36it/s]Running 5000 simulations.:  49%|████▊     | 2430/5000 [00:17<00:17, 145.58it/s]Running 5000 simulations.:  49%|████▉     | 2445/5000 [00:17<00:17, 145.92it/s]Running 5000 simulations.:  49%|████▉     | 2460/5000 [00:17<00:17, 145.30it/s]Running 5000 simulations.:  50%|████▉     | 2475/5000 [00:17<00:17, 145.71it/s]Running 5000 simulations.:  50%|████▉     | 2490/5000 [00:17<00:17, 145.43it/s]Running 5000 simulations.:  50%|█████     | 2505/5000 [00:17<00:17, 145.77it/s]Running 5000 simulations.:  50%|█████     | 2520/5000 [00:17<00:17, 145.79it/s]Running 5000 simulations.:  51%|█████     | 2535/5000 [00:17<00:16, 145.94it/s]Running 5000 simulations.:  51%|█████     | 2550/5000 [00:17<00:16, 145.59it/s]Running 5000 simulations.:  51%|█████▏    | 2565/5000 [00:17<00:16, 145.23it/s]Running 5000 simulations.:  52%|█████▏    | 2580/5000 [00:18<00:16, 145.26it/s]Running 5000 simulations.:  52%|█████▏    | 2595/5000 [00:18<00:16, 145.24it/s]Running 5000 simulations.:  52%|█████▏    | 2610/5000 [00:18<00:16, 145.08it/s]Running 5000 simulations.:  52%|█████▎    | 2625/5000 [00:18<00:16, 144.88it/s]Running 5000 simulations.:  53%|█████▎    | 2640/5000 [00:18<00:16, 144.91it/s]Running 5000 simulations.:  53%|█████▎    | 2655/5000 [00:18<00:16, 145.24it/s]Running 5000 simulations.:  53%|█████▎    | 2670/5000 [00:18<00:16, 144.76it/s]Running 5000 simulations.:  54%|█████▎    | 2685/5000 [00:18<00:16, 144.47it/s]Running 5000 simulations.:  54%|█████▍    | 2700/5000 [00:18<00:15, 144.66it/s]Running 5000 simulations.:  54%|█████▍    | 2715/5000 [00:19<00:15, 145.28it/s]Running 5000 simulations.:  55%|█████▍    | 2730/5000 [00:19<00:15, 145.72it/s]Running 5000 simulations.:  55%|█████▍    | 2745/5000 [00:19<00:15, 145.79it/s]Running 5000 simulations.:  55%|█████▌    | 2760/5000 [00:19<00:15, 145.37it/s]Running 5000 simulations.:  56%|█████▌    | 2775/5000 [00:19<00:15, 145.10it/s]Running 5000 simulations.:  56%|█████▌    | 2790/5000 [00:19<00:15, 145.04it/s]Running 5000 simulations.:  56%|█████▌    | 2805/5000 [00:19<00:15, 145.00it/s]Running 5000 simulations.:  56%|█████▋    | 2820/5000 [00:19<00:15, 144.85it/s]Running 5000 simulations.:  57%|█████▋    | 2835/5000 [00:19<00:14, 144.74it/s]Running 5000 simulations.:  57%|█████▋    | 2850/5000 [00:19<00:14, 145.46it/s]Running 5000 simulations.:  57%|█████▋    | 2865/5000 [00:20<00:14, 145.37it/s]Running 5000 simulations.:  58%|█████▊    | 2880/5000 [00:20<00:14, 145.24it/s]Running 5000 simulations.:  58%|█████▊    | 2895/5000 [00:20<00:14, 144.21it/s]Running 5000 simulations.:  58%|█████▊    | 2910/5000 [00:20<00:14, 143.92it/s]Running 5000 simulations.:  58%|█████▊    | 2925/5000 [00:20<00:14, 144.16it/s]Running 5000 simulations.:  59%|█████▉    | 2940/5000 [00:20<00:14, 143.79it/s]Running 5000 simulations.:  59%|█████▉    | 2955/5000 [00:20<00:14, 142.74it/s]Running 5000 simulations.:  59%|█████▉    | 2970/5000 [00:20<00:14, 141.51it/s]Running 5000 simulations.:  60%|█████▉    | 2985/5000 [00:20<00:14, 141.11it/s]Running 5000 simulations.:  60%|██████    | 3000/5000 [00:20<00:14, 141.96it/s]Running 5000 simulations.:  60%|██████    | 3015/5000 [00:21<00:14, 141.54it/s]Running 5000 simulations.:  61%|██████    | 3030/5000 [00:21<00:13, 141.37it/s]Running 5000 simulations.:  61%|██████    | 3045/5000 [00:21<00:13, 141.30it/s]Running 5000 simulations.:  61%|██████    | 3060/5000 [00:21<00:13, 141.14it/s]Running 5000 simulations.:  62%|██████▏   | 3075/5000 [00:21<00:13, 141.41it/s]Running 5000 simulations.:  62%|██████▏   | 3090/5000 [00:21<00:13, 141.48it/s]Running 5000 simulations.:  62%|██████▏   | 3105/5000 [00:21<00:13, 141.22it/s]Running 5000 simulations.:  62%|██████▏   | 3120/5000 [00:21<00:13, 141.54it/s]Running 5000 simulations.:  63%|██████▎   | 3135/5000 [00:21<00:13, 141.94it/s]Running 5000 simulations.:  63%|██████▎   | 3150/5000 [00:22<00:13, 142.02it/s]Running 5000 simulations.:  63%|██████▎   | 3165/5000 [00:22<00:12, 142.58it/s]Running 5000 simulations.:  64%|██████▎   | 3180/5000 [00:22<00:12, 142.85it/s]Running 5000 simulations.:  64%|██████▍   | 3195/5000 [00:22<00:12, 143.62it/s]Running 5000 simulations.:  64%|██████▍   | 3212/5000 [00:22<00:12, 148.69it/s]Running 5000 simulations.:  65%|██████▍   | 3229/5000 [00:22<00:11, 152.02it/s]Running 5000 simulations.:  65%|██████▍   | 3245/5000 [00:22<00:11, 149.49it/s]Running 5000 simulations.:  65%|██████▌   | 3260/5000 [00:22<00:11, 146.94it/s]Running 5000 simulations.:  66%|██████▌   | 3275/5000 [00:22<00:11, 144.58it/s]Running 5000 simulations.:  66%|██████▌   | 3290/5000 [00:22<00:11, 143.76it/s]Running 5000 simulations.:  66%|██████▌   | 3305/5000 [00:23<00:11, 144.26it/s]Running 5000 simulations.:  66%|██████▋   | 3320/5000 [00:23<00:11, 144.22it/s]Running 5000 simulations.:  67%|██████▋   | 3335/5000 [00:23<00:11, 144.29it/s]Running 5000 simulations.:  67%|██████▋   | 3350/5000 [00:23<00:11, 144.61it/s]Running 5000 simulations.:  67%|██████▋   | 3365/5000 [00:23<00:11, 139.00it/s]Running 5000 simulations.:  68%|██████▊   | 3379/5000 [00:23<00:12, 134.97it/s]Running 5000 simulations.:  68%|██████▊   | 3393/5000 [00:23<00:12, 132.23it/s]Running 5000 simulations.:  68%|██████▊   | 3407/5000 [00:23<00:12, 131.38it/s]Running 5000 simulations.:  68%|██████▊   | 3421/5000 [00:23<00:12, 130.93it/s]Running 5000 simulations.:  69%|██████▊   | 3435/5000 [00:24<00:12, 129.82it/s]Running 5000 simulations.:  69%|██████▉   | 3449/5000 [00:24<00:12, 128.92it/s]Running 5000 simulations.:  69%|██████▉   | 3462/5000 [00:24<00:11, 129.12it/s]Running 5000 simulations.:  70%|██████▉   | 3476/5000 [00:24<00:11, 130.18it/s]Running 5000 simulations.:  70%|██████▉   | 3490/5000 [00:24<00:11, 131.15it/s]Running 5000 simulations.:  70%|███████   | 3504/5000 [00:24<00:11, 133.04it/s]Running 5000 simulations.:  70%|███████   | 3518/5000 [00:24<00:11, 133.98it/s]Running 5000 simulations.:  71%|███████   | 3532/5000 [00:24<00:10, 134.03it/s]Running 5000 simulations.:  71%|███████   | 3546/5000 [00:24<00:10, 132.75it/s]Running 5000 simulations.:  71%|███████   | 3560/5000 [00:25<00:10, 132.70it/s]Running 5000 simulations.:  71%|███████▏  | 3574/5000 [00:25<00:10, 132.06it/s]Running 5000 simulations.:  72%|███████▏  | 3588/5000 [00:25<00:10, 132.09it/s]Running 5000 simulations.:  72%|███████▏  | 3602/5000 [00:25<00:10, 131.72it/s]Running 5000 simulations.:  72%|███████▏  | 3616/5000 [00:25<00:10, 131.53it/s]Running 5000 simulations.:  73%|███████▎  | 3630/5000 [00:25<00:10, 131.20it/s]Running 5000 simulations.:  73%|███████▎  | 3644/5000 [00:25<00:10, 131.50it/s]Running 5000 simulations.:  73%|███████▎  | 3658/5000 [00:25<00:10, 131.47it/s]Running 5000 simulations.:  73%|███████▎  | 3673/5000 [00:25<00:09, 135.24it/s]Running 5000 simulations.:  74%|███████▍  | 3688/5000 [00:25<00:09, 137.86it/s]Running 5000 simulations.:  74%|███████▍  | 3703/5000 [00:26<00:09, 140.14it/s]Running 5000 simulations.:  74%|███████▍  | 3718/5000 [00:26<00:09, 141.28it/s]Running 5000 simulations.:  75%|███████▍  | 3733/5000 [00:26<00:08, 142.15it/s]Running 5000 simulations.:  75%|███████▍  | 3748/5000 [00:26<00:08, 142.77it/s]Running 5000 simulations.:  75%|███████▌  | 3763/5000 [00:26<00:08, 142.93it/s]Running 5000 simulations.:  76%|███████▌  | 3778/5000 [00:26<00:08, 143.15it/s]Running 5000 simulations.:  76%|███████▌  | 3793/5000 [00:26<00:08, 143.52it/s]Running 5000 simulations.:  76%|███████▌  | 3808/5000 [00:26<00:08, 143.99it/s]Running 5000 simulations.:  76%|███████▋  | 3823/5000 [00:26<00:08, 144.11it/s]Running 5000 simulations.:  77%|███████▋  | 3838/5000 [00:27<00:08, 144.31it/s]Running 5000 simulations.:  77%|███████▋  | 3853/5000 [00:27<00:07, 144.57it/s]Running 5000 simulations.:  77%|███████▋  | 3868/5000 [00:27<00:07, 144.59it/s]Running 5000 simulations.:  78%|███████▊  | 3883/5000 [00:27<00:07, 144.39it/s]Running 5000 simulations.:  78%|███████▊  | 3898/5000 [00:27<00:07, 144.33it/s]Running 5000 simulations.:  78%|███████▊  | 3913/5000 [00:27<00:07, 144.33it/s]Running 5000 simulations.:  79%|███████▊  | 3928/5000 [00:27<00:07, 144.02it/s]Running 5000 simulations.:  79%|███████▉  | 3943/5000 [00:27<00:07, 138.20it/s]Running 5000 simulations.:  79%|███████▉  | 3957/5000 [00:27<00:07, 137.62it/s]Running 5000 simulations.:  79%|███████▉  | 3972/5000 [00:27<00:07, 139.23it/s]Running 5000 simulations.:  80%|███████▉  | 3987/5000 [00:28<00:07, 140.63it/s]Running 5000 simulations.:  80%|████████  | 4002/5000 [00:28<00:07, 141.57it/s]Running 5000 simulations.:  80%|████████  | 4017/5000 [00:28<00:06, 141.69it/s]Running 5000 simulations.:  81%|████████  | 4032/5000 [00:28<00:06, 141.68it/s]Running 5000 simulations.:  81%|████████  | 4047/5000 [00:28<00:06, 142.32it/s]Running 5000 simulations.:  81%|████████  | 4062/5000 [00:28<00:06, 142.51it/s]Running 5000 simulations.:  82%|████████▏ | 4077/5000 [00:28<00:06, 142.97it/s]Running 5000 simulations.:  82%|████████▏ | 4092/5000 [00:28<00:06, 142.84it/s]Running 5000 simulations.:  82%|████████▏ | 4107/5000 [00:28<00:06, 142.84it/s]Running 5000 simulations.:  82%|████████▏ | 4122/5000 [00:29<00:06, 142.77it/s]Running 5000 simulations.:  83%|████████▎ | 4137/5000 [00:29<00:06, 142.93it/s]Running 5000 simulations.:  83%|████████▎ | 4152/5000 [00:29<00:05, 143.47it/s]Running 5000 simulations.:  83%|████████▎ | 4167/5000 [00:29<00:05, 143.49it/s]Running 5000 simulations.:  84%|████████▎ | 4182/5000 [00:29<00:05, 143.65it/s]Running 5000 simulations.:  84%|████████▍ | 4197/5000 [00:29<00:05, 143.80it/s]Running 5000 simulations.:  84%|████████▍ | 4212/5000 [00:29<00:05, 143.74it/s]Running 5000 simulations.:  85%|████████▍ | 4227/5000 [00:29<00:05, 143.58it/s]Running 5000 simulations.:  85%|████████▍ | 4242/5000 [00:29<00:05, 143.44it/s]Running 5000 simulations.:  85%|████████▌ | 4257/5000 [00:29<00:05, 143.21it/s]Running 5000 simulations.:  85%|████████▌ | 4272/5000 [00:30<00:05, 143.51it/s]Running 5000 simulations.:  86%|████████▌ | 4287/5000 [00:30<00:04, 143.88it/s]Running 5000 simulations.:  86%|████████▌ | 4302/5000 [00:30<00:04, 144.00it/s]Running 5000 simulations.:  86%|████████▋ | 4317/5000 [00:30<00:04, 144.30it/s]Running 5000 simulations.:  87%|████████▋ | 4332/5000 [00:30<00:04, 144.15it/s]Running 5000 simulations.:  87%|████████▋ | 4347/5000 [00:30<00:04, 143.89it/s]Running 5000 simulations.:  87%|████████▋ | 4362/5000 [00:30<00:04, 143.45it/s]Running 5000 simulations.:  88%|████████▊ | 4377/5000 [00:30<00:04, 143.02it/s]Running 5000 simulations.:  88%|████████▊ | 4392/5000 [00:30<00:04, 142.94it/s]Running 5000 simulations.:  88%|████████▊ | 4407/5000 [00:30<00:04, 142.89it/s]Running 5000 simulations.:  88%|████████▊ | 4422/5000 [00:31<00:04, 143.16it/s]Running 5000 simulations.:  89%|████████▊ | 4437/5000 [00:31<00:03, 143.54it/s]Running 5000 simulations.:  89%|████████▉ | 4452/5000 [00:31<00:03, 143.75it/s]Running 5000 simulations.:  89%|████████▉ | 4467/5000 [00:31<00:03, 143.63it/s]Running 5000 simulations.:  90%|████████▉ | 4482/5000 [00:31<00:03, 143.82it/s]Running 5000 simulations.:  90%|████████▉ | 4497/5000 [00:31<00:03, 143.76it/s]Running 5000 simulations.:  90%|█████████ | 4512/5000 [00:31<00:03, 143.40it/s]Running 5000 simulations.:  91%|█████████ | 4527/5000 [00:31<00:03, 143.47it/s]Running 5000 simulations.:  91%|█████████ | 4542/5000 [00:31<00:03, 143.89it/s]Running 5000 simulations.:  91%|█████████ | 4557/5000 [00:32<00:03, 144.53it/s]Running 5000 simulations.:  91%|█████████▏| 4572/5000 [00:32<00:02, 144.85it/s]Running 5000 simulations.:  92%|█████████▏| 4587/5000 [00:32<00:02, 145.22it/s]Running 5000 simulations.:  92%|█████████▏| 4602/5000 [00:32<00:02, 145.14it/s]Running 5000 simulations.:  92%|█████████▏| 4617/5000 [00:32<00:02, 144.46it/s]Running 5000 simulations.:  93%|█████████▎| 4632/5000 [00:32<00:02, 144.19it/s]Running 5000 simulations.:  93%|█████████▎| 4647/5000 [00:32<00:02, 144.24it/s]Running 5000 simulations.:  93%|█████████▎| 4662/5000 [00:32<00:02, 144.56it/s]Running 5000 simulations.:  94%|█████████▎| 4677/5000 [00:32<00:02, 144.01it/s]Running 5000 simulations.:  94%|█████████▍| 4692/5000 [00:32<00:02, 142.98it/s]Running 5000 simulations.:  94%|█████████▍| 4707/5000 [00:33<00:02, 142.08it/s]Running 5000 simulations.:  94%|█████████▍| 4722/5000 [00:33<00:01, 142.20it/s]Running 5000 simulations.:  95%|█████████▍| 4737/5000 [00:33<00:01, 141.62it/s]Running 5000 simulations.:  95%|█████████▌| 4752/5000 [00:33<00:01, 140.59it/s]Running 5000 simulations.:  95%|█████████▌| 4767/5000 [00:33<00:01, 140.05it/s]Running 5000 simulations.:  96%|█████████▌| 4782/5000 [00:33<00:01, 139.59it/s]Running 5000 simulations.:  96%|█████████▌| 4796/5000 [00:33<00:01, 139.56it/s]Running 5000 simulations.:  96%|█████████▌| 4810/5000 [00:33<00:01, 139.14it/s]Running 5000 simulations.:  96%|█████████▋| 4824/5000 [00:33<00:01, 139.15it/s]Running 5000 simulations.:  97%|█████████▋| 4838/5000 [00:34<00:01, 138.80it/s]Running 5000 simulations.:  97%|█████████▋| 4852/5000 [00:34<00:01, 138.24it/s]Running 5000 simulations.:  97%|█████████▋| 4867/5000 [00:34<00:00, 138.82it/s]Running 5000 simulations.:  98%|█████████▊| 4881/5000 [00:34<00:00, 138.24it/s]Running 5000 simulations.:  98%|█████████▊| 4895/5000 [00:34<00:00, 138.53it/s]Running 5000 simulations.:  98%|█████████▊| 4909/5000 [00:34<00:00, 138.32it/s]Running 5000 simulations.:  98%|█████████▊| 4923/5000 [00:34<00:00, 138.59it/s]Running 5000 simulations.:  99%|█████████▊| 4937/5000 [00:34<00:00, 138.30it/s]Running 5000 simulations.:  99%|█████████▉| 4951/5000 [00:34<00:00, 137.85it/s]Running 5000 simulations.:  99%|█████████▉| 4965/5000 [00:34<00:00, 138.04it/s]Running 5000 simulations.: 100%|█████████▉| 4979/5000 [00:35<00:00, 138.02it/s]Running 5000 simulations.: 100%|█████████▉| 4993/5000 [00:35<00:00, 138.48it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:35<00:00, 142.06it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 15/5000 [00:00<00:35, 140.72it/s]Running 5000 simulations.:   1%|          | 30/5000 [00:00<00:35, 140.92it/s]Running 5000 simulations.:   1%|          | 45/5000 [00:00<00:35, 141.38it/s]Running 5000 simulations.:   1%|          | 60/5000 [00:00<00:34, 141.94it/s]Running 5000 simulations.:   2%|▏         | 75/5000 [00:00<00:34, 141.45it/s]Running 5000 simulations.:   2%|▏         | 90/5000 [00:00<00:34, 141.98it/s]Running 5000 simulations.:   2%|▏         | 105/5000 [00:00<00:34, 141.66it/s]Running 5000 simulations.:   2%|▏         | 119/5000 [00:00<00:34, 140.72it/s]Running 5000 simulations.:   3%|▎         | 133/5000 [00:00<00:34, 140.29it/s]Running 5000 simulations.:   3%|▎         | 148/5000 [00:01<00:34, 140.36it/s]Running 5000 simulations.:   3%|▎         | 163/5000 [00:01<00:34, 140.33it/s]Running 5000 simulations.:   4%|▎         | 177/5000 [00:01<00:34, 140.16it/s]Running 5000 simulations.:   4%|▍         | 191/5000 [00:01<00:34, 138.66it/s]Running 5000 simulations.:   4%|▍         | 205/5000 [00:01<00:34, 137.76it/s]Running 5000 simulations.:   4%|▍         | 219/5000 [00:01<00:34, 137.74it/s]Running 5000 simulations.:   5%|▍         | 234/5000 [00:01<00:34, 139.87it/s]Running 5000 simulations.:   5%|▍         | 249/5000 [00:01<00:33, 142.23it/s]Running 5000 simulations.:   5%|▌         | 264/5000 [00:01<00:32, 143.79it/s]Running 5000 simulations.:   6%|▌         | 279/5000 [00:01<00:32, 144.74it/s]Running 5000 simulations.:   6%|▌         | 294/5000 [00:02<00:32, 144.92it/s]Running 5000 simulations.:   6%|▌         | 309/5000 [00:02<00:32, 144.63it/s]Running 5000 simulations.:   6%|▋         | 324/5000 [00:02<00:32, 145.47it/s]Running 5000 simulations.:   7%|▋         | 339/5000 [00:02<00:32, 145.45it/s]Running 5000 simulations.:   7%|▋         | 354/5000 [00:02<00:32, 145.05it/s]Running 5000 simulations.:   7%|▋         | 369/5000 [00:02<00:31, 145.32it/s]Running 5000 simulations.:   8%|▊         | 384/5000 [00:02<00:31, 145.41it/s]Running 5000 simulations.:   8%|▊         | 399/5000 [00:02<00:31, 145.44it/s]Running 5000 simulations.:   8%|▊         | 414/5000 [00:02<00:31, 145.10it/s]Running 5000 simulations.:   9%|▊         | 429/5000 [00:03<00:31, 144.75it/s]Running 5000 simulations.:   9%|▉         | 444/5000 [00:03<00:31, 143.70it/s]Running 5000 simulations.:   9%|▉         | 459/5000 [00:03<00:31, 144.09it/s]Running 5000 simulations.:   9%|▉         | 474/5000 [00:03<00:31, 144.47it/s]Running 5000 simulations.:  10%|▉         | 489/5000 [00:03<00:31, 145.11it/s]Running 5000 simulations.:  10%|█         | 504/5000 [00:03<00:30, 145.41it/s]Running 5000 simulations.:  10%|█         | 519/5000 [00:03<00:30, 144.95it/s]Running 5000 simulations.:  11%|█         | 534/5000 [00:03<00:31, 144.05it/s]Running 5000 simulations.:  11%|█         | 549/5000 [00:03<00:30, 144.19it/s]Running 5000 simulations.:  11%|█▏        | 564/5000 [00:03<00:30, 143.99it/s]Running 5000 simulations.:  12%|█▏        | 579/5000 [00:04<00:30, 143.36it/s]Running 5000 simulations.:  12%|█▏        | 594/5000 [00:04<00:30, 143.61it/s]Running 5000 simulations.:  12%|█▏        | 609/5000 [00:04<00:30, 143.70it/s]Running 5000 simulations.:  12%|█▏        | 624/5000 [00:04<00:30, 143.50it/s]Running 5000 simulations.:  13%|█▎        | 639/5000 [00:04<00:30, 143.30it/s]Running 5000 simulations.:  13%|█▎        | 654/5000 [00:04<00:30, 143.81it/s]Running 5000 simulations.:  13%|█▎        | 669/5000 [00:04<00:30, 143.72it/s]Running 5000 simulations.:  14%|█▎        | 684/5000 [00:04<00:30, 142.79it/s]Running 5000 simulations.:  14%|█▍        | 699/5000 [00:04<00:30, 142.89it/s]Running 5000 simulations.:  14%|█▍        | 714/5000 [00:04<00:29, 143.19it/s]Running 5000 simulations.:  15%|█▍        | 729/5000 [00:05<00:30, 141.86it/s]Running 5000 simulations.:  15%|█▍        | 744/5000 [00:05<00:30, 141.81it/s]Running 5000 simulations.:  15%|█▌        | 759/5000 [00:05<00:29, 142.48it/s]Running 5000 simulations.:  15%|█▌        | 774/5000 [00:05<00:29, 142.64it/s]Running 5000 simulations.:  16%|█▌        | 789/5000 [00:05<00:29, 143.23it/s]Running 5000 simulations.:  16%|█▌        | 804/5000 [00:05<00:29, 143.22it/s]Running 5000 simulations.:  16%|█▋        | 819/5000 [00:05<00:29, 142.82it/s]Running 5000 simulations.:  17%|█▋        | 834/5000 [00:05<00:29, 142.99it/s]Running 5000 simulations.:  17%|█▋        | 849/5000 [00:05<00:28, 143.23it/s]Running 5000 simulations.:  17%|█▋        | 864/5000 [00:06<00:28, 143.35it/s]Running 5000 simulations.:  18%|█▊        | 879/5000 [00:06<00:28, 143.48it/s]Running 5000 simulations.:  18%|█▊        | 894/5000 [00:06<00:28, 142.95it/s]Running 5000 simulations.:  18%|█▊        | 909/5000 [00:06<00:28, 141.68it/s]Running 5000 simulations.:  18%|█▊        | 924/5000 [00:06<00:28, 140.81it/s]Running 5000 simulations.:  19%|█▉        | 939/5000 [00:06<00:28, 141.12it/s]Running 5000 simulations.:  19%|█▉        | 954/5000 [00:06<00:28, 141.41it/s]Running 5000 simulations.:  19%|█▉        | 969/5000 [00:06<00:28, 142.43it/s]Running 5000 simulations.:  20%|█▉        | 984/5000 [00:06<00:28, 143.03it/s]Running 5000 simulations.:  20%|█▉        | 999/5000 [00:06<00:27, 142.92it/s]Running 5000 simulations.:  20%|██        | 1014/5000 [00:07<00:27, 143.14it/s]Running 5000 simulations.:  21%|██        | 1029/5000 [00:07<00:27, 142.65it/s]Running 5000 simulations.:  21%|██        | 1044/5000 [00:07<00:27, 142.59it/s]Running 5000 simulations.:  21%|██        | 1059/5000 [00:07<00:28, 139.94it/s]Running 5000 simulations.:  21%|██▏       | 1074/5000 [00:07<00:28, 137.15it/s]Running 5000 simulations.:  22%|██▏       | 1088/5000 [00:07<00:28, 135.32it/s]Running 5000 simulations.:  22%|██▏       | 1102/5000 [00:07<00:29, 134.11it/s]Running 5000 simulations.:  22%|██▏       | 1117/5000 [00:07<00:28, 136.27it/s]Running 5000 simulations.:  23%|██▎       | 1131/5000 [00:07<00:28, 136.78it/s]Running 5000 simulations.:  23%|██▎       | 1145/5000 [00:08<00:28, 137.02it/s]Running 5000 simulations.:  23%|██▎       | 1159/5000 [00:08<00:28, 137.04it/s]Running 5000 simulations.:  23%|██▎       | 1173/5000 [00:08<00:27, 137.30it/s]Running 5000 simulations.:  24%|██▎       | 1187/5000 [00:08<00:27, 137.03it/s]Running 5000 simulations.:  24%|██▍       | 1201/5000 [00:08<00:27, 137.27it/s]Running 5000 simulations.:  24%|██▍       | 1215/5000 [00:08<00:27, 136.97it/s]Running 5000 simulations.:  25%|██▍       | 1229/5000 [00:08<00:27, 137.24it/s]Running 5000 simulations.:  25%|██▍       | 1243/5000 [00:08<00:27, 138.02it/s]Running 5000 simulations.:  25%|██▌       | 1257/5000 [00:08<00:27, 138.26it/s]Running 5000 simulations.:  25%|██▌       | 1271/5000 [00:08<00:26, 138.39it/s]Running 5000 simulations.:  26%|██▌       | 1285/5000 [00:09<00:26, 138.42it/s]Running 5000 simulations.:  26%|██▌       | 1299/5000 [00:09<00:26, 138.23it/s]Running 5000 simulations.:  26%|██▋       | 1313/5000 [00:09<00:26, 137.79it/s]Running 5000 simulations.:  27%|██▋       | 1327/5000 [00:09<00:26, 137.60it/s]Running 5000 simulations.:  27%|██▋       | 1341/5000 [00:09<00:26, 137.61it/s]Running 5000 simulations.:  27%|██▋       | 1355/5000 [00:09<00:26, 137.72it/s]Running 5000 simulations.:  27%|██▋       | 1369/5000 [00:09<00:26, 137.94it/s]Running 5000 simulations.:  28%|██▊       | 1383/5000 [00:09<00:26, 137.82it/s]Running 5000 simulations.:  28%|██▊       | 1397/5000 [00:09<00:26, 137.30it/s]Running 5000 simulations.:  28%|██▊       | 1411/5000 [00:09<00:26, 137.09it/s]Running 5000 simulations.:  28%|██▊       | 1425/5000 [00:10<00:26, 136.99it/s]Running 5000 simulations.:  29%|██▉       | 1439/5000 [00:10<00:25, 137.02it/s]Running 5000 simulations.:  29%|██▉       | 1453/5000 [00:10<00:25, 137.73it/s]Running 5000 simulations.:  29%|██▉       | 1468/5000 [00:10<00:25, 139.02it/s]Running 5000 simulations.:  30%|██▉       | 1482/5000 [00:10<00:25, 138.80it/s]Running 5000 simulations.:  30%|██▉       | 1496/5000 [00:10<00:25, 137.89it/s]Running 5000 simulations.:  30%|███       | 1510/5000 [00:10<00:25, 136.78it/s]Running 5000 simulations.:  30%|███       | 1524/5000 [00:10<00:25, 135.39it/s]Running 5000 simulations.:  31%|███       | 1538/5000 [00:10<00:25, 133.65it/s]Running 5000 simulations.:  31%|███       | 1552/5000 [00:11<00:25, 132.77it/s]Running 5000 simulations.:  31%|███▏      | 1566/5000 [00:11<00:25, 132.42it/s]Running 5000 simulations.:  32%|███▏      | 1580/5000 [00:11<00:25, 132.02it/s]Running 5000 simulations.:  32%|███▏      | 1594/5000 [00:11<00:25, 131.80it/s]Running 5000 simulations.:  32%|███▏      | 1608/5000 [00:11<00:25, 131.60it/s]Running 5000 simulations.:  32%|███▏      | 1622/5000 [00:11<00:25, 131.36it/s]Running 5000 simulations.:  33%|███▎      | 1636/5000 [00:11<00:25, 130.83it/s]Running 5000 simulations.:  33%|███▎      | 1650/5000 [00:11<00:25, 129.99it/s]Running 5000 simulations.:  33%|███▎      | 1664/5000 [00:11<00:25, 129.08it/s]Running 5000 simulations.:  34%|███▎      | 1677/5000 [00:11<00:25, 129.10it/s]Running 5000 simulations.:  34%|███▍      | 1690/5000 [00:12<00:25, 128.39it/s]Running 5000 simulations.:  34%|███▍      | 1703/5000 [00:12<00:25, 128.48it/s]Running 5000 simulations.:  34%|███▍      | 1716/5000 [00:12<00:25, 128.76it/s]Running 5000 simulations.:  35%|███▍      | 1729/5000 [00:12<00:25, 128.89it/s]Running 5000 simulations.:  35%|███▍      | 1742/5000 [00:12<00:25, 128.99it/s]Running 5000 simulations.:  35%|███▌      | 1755/5000 [00:12<00:25, 128.52it/s]Running 5000 simulations.:  35%|███▌      | 1768/5000 [00:12<00:25, 128.13it/s]Running 5000 simulations.:  36%|███▌      | 1781/5000 [00:12<00:25, 127.73it/s]Running 5000 simulations.:  36%|███▌      | 1794/5000 [00:12<00:25, 128.01it/s]Running 5000 simulations.:  36%|███▌      | 1807/5000 [00:13<00:24, 128.29it/s]Running 5000 simulations.:  36%|███▋      | 1820/5000 [00:13<00:24, 128.20it/s]Running 5000 simulations.:  37%|███▋      | 1833/5000 [00:13<00:24, 128.18it/s]Running 5000 simulations.:  37%|███▋      | 1847/5000 [00:13<00:24, 131.14it/s]Running 5000 simulations.:  37%|███▋      | 1862/5000 [00:13<00:23, 133.89it/s]Running 5000 simulations.:  38%|███▊      | 1877/5000 [00:13<00:22, 136.43it/s]Running 5000 simulations.:  38%|███▊      | 1892/5000 [00:13<00:22, 138.64it/s]Running 5000 simulations.:  38%|███▊      | 1907/5000 [00:13<00:22, 140.53it/s]Running 5000 simulations.:  38%|███▊      | 1922/5000 [00:13<00:21, 141.11it/s]Running 5000 simulations.:  39%|███▊      | 1937/5000 [00:13<00:21, 141.48it/s]Running 5000 simulations.:  39%|███▉      | 1952/5000 [00:14<00:21, 142.17it/s]Running 5000 simulations.:  39%|███▉      | 1967/5000 [00:14<00:21, 142.61it/s]Running 5000 simulations.:  40%|███▉      | 1982/5000 [00:14<00:21, 143.13it/s]Running 5000 simulations.:  40%|███▉      | 1997/5000 [00:14<00:20, 143.19it/s]Running 5000 simulations.:  40%|████      | 2012/5000 [00:14<00:20, 143.58it/s]Running 5000 simulations.:  41%|████      | 2027/5000 [00:14<00:20, 143.75it/s]Running 5000 simulations.:  41%|████      | 2042/5000 [00:14<00:20, 143.48it/s]Running 5000 simulations.:  41%|████      | 2057/5000 [00:14<00:20, 143.46it/s]Running 5000 simulations.:  41%|████▏     | 2072/5000 [00:14<00:20, 143.88it/s]Running 5000 simulations.:  42%|████▏     | 2087/5000 [00:14<00:20, 143.58it/s]Running 5000 simulations.:  42%|████▏     | 2102/5000 [00:15<00:20, 143.50it/s]Running 5000 simulations.:  42%|████▏     | 2117/5000 [00:15<00:20, 143.84it/s]Running 5000 simulations.:  43%|████▎     | 2132/5000 [00:15<00:19, 144.01it/s]Running 5000 simulations.:  43%|████▎     | 2147/5000 [00:15<00:19, 144.12it/s]Running 5000 simulations.:  43%|████▎     | 2162/5000 [00:15<00:19, 143.93it/s]Running 5000 simulations.:  44%|████▎     | 2177/5000 [00:15<00:19, 144.08it/s]Running 5000 simulations.:  44%|████▍     | 2192/5000 [00:15<00:19, 143.39it/s]Running 5000 simulations.:  44%|████▍     | 2207/5000 [00:15<00:19, 143.63it/s]Running 5000 simulations.:  44%|████▍     | 2222/5000 [00:15<00:19, 143.43it/s]Running 5000 simulations.:  45%|████▍     | 2237/5000 [00:16<00:19, 143.83it/s]Running 5000 simulations.:  45%|████▌     | 2252/5000 [00:16<00:19, 144.13it/s]Running 5000 simulations.:  45%|████▌     | 2267/5000 [00:16<00:18, 144.59it/s]Running 5000 simulations.:  46%|████▌     | 2282/5000 [00:16<00:18, 144.30it/s]Running 5000 simulations.:  46%|████▌     | 2297/5000 [00:16<00:18, 144.38it/s]Running 5000 simulations.:  46%|████▌     | 2312/5000 [00:16<00:18, 143.10it/s]Running 5000 simulations.:  47%|████▋     | 2327/5000 [00:16<00:18, 143.23it/s]Running 5000 simulations.:  47%|████▋     | 2342/5000 [00:16<00:18, 143.65it/s]Running 5000 simulations.:  47%|████▋     | 2357/5000 [00:16<00:18, 144.38it/s]Running 5000 simulations.:  47%|████▋     | 2372/5000 [00:16<00:18, 144.75it/s]Running 5000 simulations.:  48%|████▊     | 2387/5000 [00:17<00:18, 144.42it/s]Running 5000 simulations.:  48%|████▊     | 2402/5000 [00:17<00:18, 143.73it/s]Running 5000 simulations.:  48%|████▊     | 2417/5000 [00:17<00:18, 143.36it/s]Running 5000 simulations.:  49%|████▊     | 2432/5000 [00:17<00:17, 143.62it/s]Running 5000 simulations.:  49%|████▉     | 2447/5000 [00:17<00:17, 143.55it/s]Running 5000 simulations.:  49%|████▉     | 2462/5000 [00:17<00:17, 143.58it/s]Running 5000 simulations.:  50%|████▉     | 2477/5000 [00:17<00:17, 144.57it/s]Running 5000 simulations.:  50%|████▉     | 2492/5000 [00:17<00:17, 144.80it/s]Running 5000 simulations.:  50%|█████     | 2507/5000 [00:17<00:17, 144.67it/s]Running 5000 simulations.:  50%|█████     | 2522/5000 [00:18<00:17, 144.39it/s]Running 5000 simulations.:  51%|█████     | 2537/5000 [00:18<00:17, 143.35it/s]Running 5000 simulations.:  51%|█████     | 2552/5000 [00:18<00:17, 142.37it/s]Running 5000 simulations.:  51%|█████▏    | 2567/5000 [00:18<00:17, 142.84it/s]Running 5000 simulations.:  52%|█████▏    | 2582/5000 [00:18<00:16, 142.84it/s]Running 5000 simulations.:  52%|█████▏    | 2597/5000 [00:18<00:16, 143.14it/s]Running 5000 simulations.:  52%|█████▏    | 2612/5000 [00:18<00:16, 143.01it/s]Running 5000 simulations.:  53%|█████▎    | 2627/5000 [00:18<00:16, 142.52it/s]Running 5000 simulations.:  53%|█████▎    | 2642/5000 [00:18<00:16, 142.32it/s]Running 5000 simulations.:  53%|█████▎    | 2657/5000 [00:18<00:16, 142.56it/s]Running 5000 simulations.:  53%|█████▎    | 2672/5000 [00:19<00:16, 142.59it/s]Running 5000 simulations.:  54%|█████▎    | 2687/5000 [00:19<00:16, 142.79it/s]Running 5000 simulations.:  54%|█████▍    | 2702/5000 [00:19<00:16, 142.70it/s]Running 5000 simulations.:  54%|█████▍    | 2717/5000 [00:19<00:15, 142.78it/s]Running 5000 simulations.:  55%|█████▍    | 2732/5000 [00:19<00:15, 143.07it/s]Running 5000 simulations.:  55%|█████▍    | 2747/5000 [00:19<00:15, 142.62it/s]Running 5000 simulations.:  55%|█████▌    | 2762/5000 [00:19<00:15, 143.03it/s]Running 5000 simulations.:  56%|█████▌    | 2777/5000 [00:19<00:15, 143.09it/s]Running 5000 simulations.:  56%|█████▌    | 2792/5000 [00:19<00:15, 142.51it/s]Running 5000 simulations.:  56%|█████▌    | 2807/5000 [00:20<00:15, 141.72it/s]Running 5000 simulations.:  56%|█████▋    | 2822/5000 [00:20<00:15, 141.74it/s]Running 5000 simulations.:  57%|█████▋    | 2837/5000 [00:20<00:15, 142.50it/s]Running 5000 simulations.:  57%|█████▋    | 2854/5000 [00:20<00:14, 147.99it/s]Running 5000 simulations.:  57%|█████▋    | 2870/5000 [00:20<00:14, 150.29it/s]Running 5000 simulations.:  58%|█████▊    | 2886/5000 [00:20<00:14, 147.96it/s]Running 5000 simulations.:  58%|█████▊    | 2901/5000 [00:20<00:14, 145.76it/s]Running 5000 simulations.:  58%|█████▊    | 2916/5000 [00:20<00:14, 144.53it/s]Running 5000 simulations.:  59%|█████▊    | 2931/5000 [00:20<00:14, 144.49it/s]Running 5000 simulations.:  59%|█████▉    | 2946/5000 [00:20<00:14, 144.31it/s]Running 5000 simulations.:  59%|█████▉    | 2961/5000 [00:21<00:14, 143.57it/s]Running 5000 simulations.:  60%|█████▉    | 2976/5000 [00:21<00:14, 142.87it/s]Running 5000 simulations.:  60%|█████▉    | 2991/5000 [00:21<00:14, 142.10it/s]Running 5000 simulations.:  60%|██████    | 3006/5000 [00:21<00:14, 142.06it/s]Running 5000 simulations.:  60%|██████    | 3021/5000 [00:21<00:13, 142.89it/s]Running 5000 simulations.:  61%|██████    | 3036/5000 [00:21<00:13, 143.53it/s]Running 5000 simulations.:  61%|██████    | 3051/5000 [00:21<00:13, 143.74it/s]Running 5000 simulations.:  61%|██████▏   | 3066/5000 [00:21<00:13, 143.14it/s]Running 5000 simulations.:  62%|██████▏   | 3081/5000 [00:21<00:13, 141.43it/s]Running 5000 simulations.:  62%|██████▏   | 3096/5000 [00:22<00:13, 141.21it/s]Running 5000 simulations.:  62%|██████▏   | 3111/5000 [00:22<00:13, 141.16it/s]Running 5000 simulations.:  63%|██████▎   | 3126/5000 [00:22<00:13, 140.63it/s]Running 5000 simulations.:  63%|██████▎   | 3141/5000 [00:22<00:13, 139.73it/s]Running 5000 simulations.:  63%|██████▎   | 3155/5000 [00:22<00:13, 139.57it/s]Running 5000 simulations.:  63%|██████▎   | 3169/5000 [00:22<00:13, 138.40it/s]Running 5000 simulations.:  64%|██████▎   | 3183/5000 [00:22<00:13, 138.55it/s]Running 5000 simulations.:  64%|██████▍   | 3197/5000 [00:22<00:13, 138.63it/s]Running 5000 simulations.:  64%|██████▍   | 3212/5000 [00:22<00:12, 139.67it/s]Running 5000 simulations.:  65%|██████▍   | 3226/5000 [00:22<00:12, 139.45it/s]Running 5000 simulations.:  65%|██████▍   | 3240/5000 [00:23<00:12, 139.32it/s]Running 5000 simulations.:  65%|██████▌   | 3255/5000 [00:23<00:12, 140.06it/s]Running 5000 simulations.:  65%|██████▌   | 3270/5000 [00:23<00:12, 140.88it/s]Running 5000 simulations.:  66%|██████▌   | 3285/5000 [00:23<00:12, 140.95it/s]Running 5000 simulations.:  66%|██████▌   | 3300/5000 [00:23<00:12, 141.57it/s]Running 5000 simulations.:  66%|██████▋   | 3315/5000 [00:23<00:11, 140.52it/s]Running 5000 simulations.:  67%|██████▋   | 3330/5000 [00:23<00:12, 132.90it/s]Running 5000 simulations.:  67%|██████▋   | 3345/5000 [00:23<00:12, 135.05it/s]Running 5000 simulations.:  67%|██████▋   | 3360/5000 [00:23<00:11, 136.76it/s]Running 5000 simulations.:  68%|██████▊   | 3375/5000 [00:24<00:11, 138.77it/s]Running 5000 simulations.:  68%|██████▊   | 3390/5000 [00:24<00:11, 139.57it/s]Running 5000 simulations.:  68%|██████▊   | 3404/5000 [00:24<00:11, 139.65it/s]Running 5000 simulations.:  68%|██████▊   | 3419/5000 [00:24<00:11, 140.01it/s]Running 5000 simulations.:  69%|██████▊   | 3434/5000 [00:24<00:11, 140.63it/s]Running 5000 simulations.:  69%|██████▉   | 3449/5000 [00:24<00:10, 141.07it/s]Running 5000 simulations.:  69%|██████▉   | 3464/5000 [00:24<00:10, 141.37it/s]Running 5000 simulations.:  70%|██████▉   | 3479/5000 [00:24<00:10, 140.99it/s]Running 5000 simulations.:  70%|██████▉   | 3494/5000 [00:24<00:10, 139.79it/s]Running 5000 simulations.:  70%|███████   | 3508/5000 [00:24<00:10, 139.74it/s]Running 5000 simulations.:  70%|███████   | 3523/5000 [00:25<00:10, 140.23it/s]Running 5000 simulations.:  71%|███████   | 3538/5000 [00:25<00:10, 140.21it/s]Running 5000 simulations.:  71%|███████   | 3553/5000 [00:25<00:10, 140.21it/s]Running 5000 simulations.:  71%|███████▏  | 3568/5000 [00:25<00:10, 140.38it/s]Running 5000 simulations.:  72%|███████▏  | 3583/5000 [00:25<00:10, 140.44it/s]Running 5000 simulations.:  72%|███████▏  | 3598/5000 [00:25<00:09, 140.94it/s]Running 5000 simulations.:  72%|███████▏  | 3613/5000 [00:25<00:09, 141.23it/s]Running 5000 simulations.:  73%|███████▎  | 3628/5000 [00:25<00:09, 141.11it/s]Running 5000 simulations.:  73%|███████▎  | 3643/5000 [00:25<00:09, 141.03it/s]Running 5000 simulations.:  73%|███████▎  | 3658/5000 [00:26<00:09, 140.77it/s]Running 5000 simulations.:  73%|███████▎  | 3673/5000 [00:26<00:09, 139.83it/s]Running 5000 simulations.:  74%|███████▎  | 3687/5000 [00:26<00:09, 139.39it/s]Running 5000 simulations.:  74%|███████▍  | 3702/5000 [00:26<00:09, 140.39it/s]Running 5000 simulations.:  74%|███████▍  | 3717/5000 [00:26<00:09, 140.27it/s]Running 5000 simulations.:  75%|███████▍  | 3732/5000 [00:26<00:09, 140.10it/s]Running 5000 simulations.:  75%|███████▍  | 3747/5000 [00:26<00:08, 140.06it/s]Running 5000 simulations.:  75%|███████▌  | 3762/5000 [00:26<00:08, 139.77it/s]Running 5000 simulations.:  76%|███████▌  | 3776/5000 [00:26<00:08, 139.33it/s]Running 5000 simulations.:  76%|███████▌  | 3790/5000 [00:26<00:08, 139.21it/s]Running 5000 simulations.:  76%|███████▌  | 3804/5000 [00:27<00:08, 139.44it/s]Running 5000 simulations.:  76%|███████▋  | 3818/5000 [00:27<00:08, 139.55it/s]Running 5000 simulations.:  77%|███████▋  | 3833/5000 [00:27<00:08, 139.88it/s]Running 5000 simulations.:  77%|███████▋  | 3847/5000 [00:27<00:08, 138.97it/s]Running 5000 simulations.:  77%|███████▋  | 3862/5000 [00:27<00:08, 139.66it/s]Running 5000 simulations.:  78%|███████▊  | 3877/5000 [00:27<00:08, 140.19it/s]Running 5000 simulations.:  78%|███████▊  | 3892/5000 [00:27<00:07, 140.37it/s]Running 5000 simulations.:  78%|███████▊  | 3907/5000 [00:27<00:07, 140.46it/s]Running 5000 simulations.:  78%|███████▊  | 3922/5000 [00:27<00:07, 140.28it/s]Running 5000 simulations.:  79%|███████▊  | 3937/5000 [00:28<00:07, 140.54it/s]Running 5000 simulations.:  79%|███████▉  | 3952/5000 [00:28<00:07, 140.70it/s]Running 5000 simulations.:  79%|███████▉  | 3967/5000 [00:28<00:07, 140.98it/s]Running 5000 simulations.:  80%|███████▉  | 3982/5000 [00:28<00:07, 141.10it/s]Running 5000 simulations.:  80%|███████▉  | 3997/5000 [00:28<00:07, 141.03it/s]Running 5000 simulations.:  80%|████████  | 4012/5000 [00:28<00:06, 141.42it/s]Running 5000 simulations.:  81%|████████  | 4027/5000 [00:28<00:06, 141.99it/s]Running 5000 simulations.:  81%|████████  | 4042/5000 [00:28<00:06, 142.62it/s]Running 5000 simulations.:  81%|████████  | 4057/5000 [00:28<00:06, 142.63it/s]Running 5000 simulations.:  81%|████████▏ | 4072/5000 [00:28<00:06, 142.36it/s]Running 5000 simulations.:  82%|████████▏ | 4087/5000 [00:29<00:06, 142.47it/s]Running 5000 simulations.:  82%|████████▏ | 4102/5000 [00:29<00:06, 143.01it/s]Running 5000 simulations.:  82%|████████▏ | 4117/5000 [00:29<00:06, 143.05it/s]Running 5000 simulations.:  83%|████████▎ | 4132/5000 [00:29<00:06, 142.85it/s]Running 5000 simulations.:  83%|████████▎ | 4147/5000 [00:29<00:05, 142.90it/s]Running 5000 simulations.:  83%|████████▎ | 4162/5000 [00:29<00:05, 142.13it/s]Running 5000 simulations.:  84%|████████▎ | 4177/5000 [00:29<00:05, 142.61it/s]Running 5000 simulations.:  84%|████████▍ | 4192/5000 [00:29<00:05, 142.90it/s]Running 5000 simulations.:  84%|████████▍ | 4207/5000 [00:29<00:05, 143.30it/s]Running 5000 simulations.:  84%|████████▍ | 4222/5000 [00:30<00:05, 143.27it/s]Running 5000 simulations.:  85%|████████▍ | 4237/5000 [00:30<00:05, 143.17it/s]Running 5000 simulations.:  85%|████████▌ | 4252/5000 [00:30<00:05, 142.34it/s]Running 5000 simulations.:  85%|████████▌ | 4267/5000 [00:30<00:05, 142.28it/s]Running 5000 simulations.:  86%|████████▌ | 4282/5000 [00:30<00:05, 142.00it/s]Running 5000 simulations.:  86%|████████▌ | 4297/5000 [00:30<00:04, 141.92it/s]Running 5000 simulations.:  86%|████████▌ | 4312/5000 [00:30<00:04, 142.01it/s]Running 5000 simulations.:  87%|████████▋ | 4327/5000 [00:30<00:04, 141.93it/s]Running 5000 simulations.:  87%|████████▋ | 4342/5000 [00:30<00:04, 141.58it/s]Running 5000 simulations.:  87%|████████▋ | 4357/5000 [00:30<00:04, 141.95it/s]Running 5000 simulations.:  87%|████████▋ | 4372/5000 [00:31<00:04, 142.19it/s]Running 5000 simulations.:  88%|████████▊ | 4387/5000 [00:31<00:04, 142.13it/s]Running 5000 simulations.:  88%|████████▊ | 4402/5000 [00:31<00:04, 142.26it/s]Running 5000 simulations.:  88%|████████▊ | 4417/5000 [00:31<00:04, 142.46it/s]Running 5000 simulations.:  89%|████████▊ | 4432/5000 [00:31<00:03, 142.28it/s]Running 5000 simulations.:  89%|████████▉ | 4447/5000 [00:31<00:03, 142.70it/s]Running 5000 simulations.:  89%|████████▉ | 4462/5000 [00:31<00:03, 142.60it/s]Running 5000 simulations.:  90%|████████▉ | 4477/5000 [00:31<00:03, 142.38it/s]Running 5000 simulations.:  90%|████████▉ | 4492/5000 [00:31<00:03, 143.81it/s]Running 5000 simulations.:  90%|█████████ | 4508/5000 [00:32<00:03, 147.94it/s]Running 5000 simulations.:  90%|█████████ | 4523/5000 [00:32<00:03, 148.16it/s]Running 5000 simulations.:  91%|█████████ | 4538/5000 [00:32<00:03, 147.75it/s]Running 5000 simulations.:  91%|█████████ | 4553/5000 [00:32<00:03, 146.85it/s]Running 5000 simulations.:  91%|█████████▏| 4568/5000 [00:32<00:02, 146.61it/s]Running 5000 simulations.:  92%|█████████▏| 4583/5000 [00:32<00:02, 146.26it/s]Running 5000 simulations.:  92%|█████████▏| 4598/5000 [00:32<00:02, 145.76it/s]Running 5000 simulations.:  92%|█████████▏| 4613/5000 [00:32<00:02, 142.09it/s]Running 5000 simulations.:  93%|█████████▎| 4628/5000 [00:32<00:02, 143.14it/s]Running 5000 simulations.:  93%|█████████▎| 4643/5000 [00:32<00:02, 144.52it/s]Running 5000 simulations.:  93%|█████████▎| 4658/5000 [00:33<00:02, 145.07it/s]Running 5000 simulations.:  93%|█████████▎| 4673/5000 [00:33<00:02, 145.50it/s]Running 5000 simulations.:  94%|█████████▍| 4688/5000 [00:33<00:02, 145.15it/s]Running 5000 simulations.:  94%|█████████▍| 4703/5000 [00:33<00:02, 145.34it/s]Running 5000 simulations.:  94%|█████████▍| 4718/5000 [00:33<00:01, 145.76it/s]Running 5000 simulations.:  95%|█████████▍| 4733/5000 [00:33<00:01, 146.11it/s]Running 5000 simulations.:  95%|█████████▍| 4748/5000 [00:33<00:01, 145.93it/s]Running 5000 simulations.:  95%|█████████▌| 4763/5000 [00:33<00:01, 145.79it/s]Running 5000 simulations.:  96%|█████████▌| 4778/5000 [00:33<00:01, 145.12it/s]Running 5000 simulations.:  96%|█████████▌| 4793/5000 [00:33<00:01, 145.00it/s]Running 5000 simulations.:  96%|█████████▌| 4808/5000 [00:34<00:01, 145.58it/s]Running 5000 simulations.:  96%|█████████▋| 4823/5000 [00:34<00:01, 145.37it/s]Running 5000 simulations.:  97%|█████████▋| 4838/5000 [00:34<00:01, 143.18it/s]Running 5000 simulations.:  97%|█████████▋| 4853/5000 [00:34<00:01, 143.01it/s]Running 5000 simulations.:  97%|█████████▋| 4868/5000 [00:34<00:00, 144.71it/s]Running 5000 simulations.:  98%|█████████▊| 4883/5000 [00:34<00:00, 145.86it/s]Running 5000 simulations.:  98%|█████████▊| 4898/5000 [00:34<00:00, 146.14it/s]Running 5000 simulations.:  98%|█████████▊| 4913/5000 [00:34<00:00, 145.63it/s]Running 5000 simulations.:  99%|█████████▊| 4928/5000 [00:34<00:00, 145.43it/s]Running 5000 simulations.:  99%|█████████▉| 4943/5000 [00:35<00:00, 145.79it/s]Running 5000 simulations.:  99%|█████████▉| 4958/5000 [00:35<00:00, 146.14it/s]Running 5000 simulations.:  99%|█████████▉| 4973/5000 [00:35<00:00, 146.45it/s]Running 5000 simulations.: 100%|█████████▉| 4988/5000 [00:35<00:00, 147.10it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:35<00:00, 141.24it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:33, 150.03it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:33, 150.05it/s]Running 5000 simulations.:   1%|          | 47/5000 [00:00<00:33, 149.52it/s]Running 5000 simulations.:   1%|          | 62/5000 [00:00<00:33, 148.83it/s]Running 5000 simulations.:   2%|▏         | 77/5000 [00:00<00:33, 149.03it/s]Running 5000 simulations.:   2%|▏         | 92/5000 [00:00<00:32, 149.30it/s]Running 5000 simulations.:   2%|▏         | 108/5000 [00:00<00:32, 149.84it/s]Running 5000 simulations.:   2%|▏         | 123/5000 [00:00<00:32, 149.44it/s]Running 5000 simulations.:   3%|▎         | 138/5000 [00:00<00:32, 149.39it/s]Running 5000 simulations.:   3%|▎         | 154/5000 [00:01<00:32, 149.84it/s]Running 5000 simulations.:   3%|▎         | 169/5000 [00:01<00:32, 149.13it/s]Running 5000 simulations.:   4%|▎         | 184/5000 [00:01<00:32, 147.96it/s]Running 5000 simulations.:   4%|▍         | 199/5000 [00:01<00:32, 147.63it/s]Running 5000 simulations.:   4%|▍         | 214/5000 [00:01<00:32, 146.88it/s]Running 5000 simulations.:   5%|▍         | 229/5000 [00:01<00:32, 147.00it/s]Running 5000 simulations.:   5%|▍         | 244/5000 [00:01<00:32, 146.68it/s]Running 5000 simulations.:   5%|▌         | 259/5000 [00:01<00:32, 146.59it/s]Running 5000 simulations.:   5%|▌         | 274/5000 [00:01<00:32, 146.36it/s]Running 5000 simulations.:   6%|▌         | 289/5000 [00:01<00:32, 145.98it/s]Running 5000 simulations.:   6%|▌         | 304/5000 [00:02<00:32, 146.32it/s]Running 5000 simulations.:   6%|▋         | 319/5000 [00:02<00:31, 146.83it/s]Running 5000 simulations.:   7%|▋         | 334/5000 [00:02<00:31, 146.40it/s]Running 5000 simulations.:   7%|▋         | 349/5000 [00:02<00:31, 145.63it/s]Running 5000 simulations.:   7%|▋         | 364/5000 [00:02<00:32, 144.86it/s]Running 5000 simulations.:   8%|▊         | 379/5000 [00:02<00:31, 144.84it/s]Running 5000 simulations.:   8%|▊         | 394/5000 [00:02<00:31, 144.68it/s]Running 5000 simulations.:   8%|▊         | 409/5000 [00:02<00:31, 144.80it/s]Running 5000 simulations.:   8%|▊         | 424/5000 [00:02<00:31, 144.58it/s]Running 5000 simulations.:   9%|▉         | 439/5000 [00:02<00:31, 144.65it/s]Running 5000 simulations.:   9%|▉         | 454/5000 [00:03<00:31, 144.88it/s]Running 5000 simulations.:   9%|▉         | 469/5000 [00:03<00:31, 145.17it/s]Running 5000 simulations.:  10%|▉         | 484/5000 [00:03<00:31, 144.55it/s]Running 5000 simulations.:  10%|▉         | 499/5000 [00:03<00:31, 144.13it/s]Running 5000 simulations.:  10%|█         | 514/5000 [00:03<00:31, 144.13it/s]Running 5000 simulations.:  11%|█         | 529/5000 [00:03<00:30, 144.24it/s]Running 5000 simulations.:  11%|█         | 544/5000 [00:03<00:30, 144.40it/s]Running 5000 simulations.:  11%|█         | 559/5000 [00:03<00:30, 144.68it/s]Running 5000 simulations.:  11%|█▏        | 574/5000 [00:03<00:30, 144.67it/s]Running 5000 simulations.:  12%|█▏        | 589/5000 [00:04<00:30, 144.96it/s]Running 5000 simulations.:  12%|█▏        | 604/5000 [00:04<00:30, 144.77it/s]Running 5000 simulations.:  12%|█▏        | 619/5000 [00:04<00:30, 144.64it/s]Running 5000 simulations.:  13%|█▎        | 634/5000 [00:04<00:30, 144.54it/s]Running 5000 simulations.:  13%|█▎        | 649/5000 [00:04<00:30, 144.90it/s]Running 5000 simulations.:  13%|█▎        | 664/5000 [00:04<00:29, 145.15it/s]Running 5000 simulations.:  14%|█▎        | 679/5000 [00:04<00:29, 145.16it/s]Running 5000 simulations.:  14%|█▍        | 694/5000 [00:04<00:29, 145.15it/s]Running 5000 simulations.:  14%|█▍        | 709/5000 [00:04<00:29, 144.88it/s]Running 5000 simulations.:  14%|█▍        | 724/5000 [00:04<00:29, 145.05it/s]Running 5000 simulations.:  15%|█▍        | 739/5000 [00:05<00:29, 145.56it/s]Running 5000 simulations.:  15%|█▌        | 754/5000 [00:05<00:29, 144.39it/s]Running 5000 simulations.:  15%|█▌        | 769/5000 [00:05<00:29, 144.13it/s]Running 5000 simulations.:  16%|█▌        | 784/5000 [00:05<00:29, 144.27it/s]Running 5000 simulations.:  16%|█▌        | 799/5000 [00:05<00:29, 144.08it/s]Running 5000 simulations.:  16%|█▋        | 814/5000 [00:05<00:29, 144.14it/s]Running 5000 simulations.:  17%|█▋        | 829/5000 [00:05<00:28, 144.21it/s]Running 5000 simulations.:  17%|█▋        | 844/5000 [00:05<00:28, 144.27it/s]Running 5000 simulations.:  17%|█▋        | 859/5000 [00:05<00:28, 144.31it/s]Running 5000 simulations.:  17%|█▋        | 874/5000 [00:05<00:28, 144.19it/s]Running 5000 simulations.:  18%|█▊        | 889/5000 [00:06<00:28, 143.29it/s]Running 5000 simulations.:  18%|█▊        | 904/5000 [00:06<00:28, 142.92it/s]Running 5000 simulations.:  18%|█▊        | 919/5000 [00:06<00:28, 143.00it/s]Running 5000 simulations.:  19%|█▊        | 934/5000 [00:06<00:28, 143.18it/s]Running 5000 simulations.:  19%|█▉        | 949/5000 [00:06<00:28, 143.57it/s]Running 5000 simulations.:  19%|█▉        | 964/5000 [00:06<00:28, 144.00it/s]Running 5000 simulations.:  20%|█▉        | 979/5000 [00:06<00:27, 144.30it/s]Running 5000 simulations.:  20%|█▉        | 994/5000 [00:06<00:27, 144.06it/s]Running 5000 simulations.:  20%|██        | 1009/5000 [00:06<00:27, 143.81it/s]Running 5000 simulations.:  20%|██        | 1024/5000 [00:07<00:27, 143.28it/s]Running 5000 simulations.:  21%|██        | 1039/5000 [00:07<00:27, 143.63it/s]Running 5000 simulations.:  21%|██        | 1054/5000 [00:07<00:27, 143.72it/s]Running 5000 simulations.:  21%|██▏       | 1069/5000 [00:07<00:27, 143.69it/s]Running 5000 simulations.:  22%|██▏       | 1084/5000 [00:07<00:27, 144.09it/s]Running 5000 simulations.:  22%|██▏       | 1099/5000 [00:07<00:27, 143.53it/s]Running 5000 simulations.:  22%|██▏       | 1114/5000 [00:07<00:27, 143.41it/s]Running 5000 simulations.:  23%|██▎       | 1129/5000 [00:07<00:26, 143.41it/s]Running 5000 simulations.:  23%|██▎       | 1144/5000 [00:07<00:26, 143.65it/s]Running 5000 simulations.:  23%|██▎       | 1159/5000 [00:07<00:26, 143.38it/s]Running 5000 simulations.:  23%|██▎       | 1174/5000 [00:08<00:26, 143.00it/s]Running 5000 simulations.:  24%|██▍       | 1189/5000 [00:08<00:26, 143.42it/s]Running 5000 simulations.:  24%|██▍       | 1204/5000 [00:08<00:26, 143.80it/s]Running 5000 simulations.:  24%|██▍       | 1219/5000 [00:08<00:26, 143.48it/s]Running 5000 simulations.:  25%|██▍       | 1234/5000 [00:08<00:26, 143.58it/s]Running 5000 simulations.:  25%|██▍       | 1249/5000 [00:08<00:26, 143.55it/s]Running 5000 simulations.:  25%|██▌       | 1264/5000 [00:08<00:26, 143.54it/s]Running 5000 simulations.:  26%|██▌       | 1279/5000 [00:08<00:25, 143.43it/s]Running 5000 simulations.:  26%|██▌       | 1294/5000 [00:08<00:25, 143.41it/s]Running 5000 simulations.:  26%|██▌       | 1309/5000 [00:09<00:25, 143.54it/s]Running 5000 simulations.:  26%|██▋       | 1324/5000 [00:09<00:25, 143.71it/s]Running 5000 simulations.:  27%|██▋       | 1339/5000 [00:09<00:25, 143.67it/s]Running 5000 simulations.:  27%|██▋       | 1354/5000 [00:09<00:25, 143.45it/s]Running 5000 simulations.:  27%|██▋       | 1369/5000 [00:09<00:25, 142.95it/s]Running 5000 simulations.:  28%|██▊       | 1384/5000 [00:09<00:25, 142.82it/s]Running 5000 simulations.:  28%|██▊       | 1399/5000 [00:09<00:25, 143.34it/s]Running 5000 simulations.:  28%|██▊       | 1414/5000 [00:09<00:25, 143.28it/s]Running 5000 simulations.:  29%|██▊       | 1429/5000 [00:09<00:24, 143.57it/s]Running 5000 simulations.:  29%|██▉       | 1444/5000 [00:09<00:24, 143.56it/s]Running 5000 simulations.:  29%|██▉       | 1459/5000 [00:10<00:24, 143.66it/s]Running 5000 simulations.:  29%|██▉       | 1474/5000 [00:10<00:24, 143.71it/s]Running 5000 simulations.:  30%|██▉       | 1489/5000 [00:10<00:24, 143.18it/s]Running 5000 simulations.:  30%|███       | 1504/5000 [00:10<00:24, 142.91it/s]Running 5000 simulations.:  30%|███       | 1519/5000 [00:10<00:24, 142.92it/s]Running 5000 simulations.:  31%|███       | 1534/5000 [00:10<00:24, 142.87it/s]Running 5000 simulations.:  31%|███       | 1549/5000 [00:10<00:24, 143.41it/s]Running 5000 simulations.:  31%|███▏      | 1564/5000 [00:10<00:23, 143.34it/s]Running 5000 simulations.:  32%|███▏      | 1579/5000 [00:10<00:23, 143.36it/s]Running 5000 simulations.:  32%|███▏      | 1594/5000 [00:11<00:23, 143.56it/s]Running 5000 simulations.:  32%|███▏      | 1609/5000 [00:11<00:23, 143.62it/s]Running 5000 simulations.:  32%|███▏      | 1624/5000 [00:11<00:23, 142.93it/s]Running 5000 simulations.:  33%|███▎      | 1639/5000 [00:11<00:23, 142.86it/s]Running 5000 simulations.:  33%|███▎      | 1654/5000 [00:11<00:23, 142.85it/s]Running 5000 simulations.:  33%|███▎      | 1669/5000 [00:11<00:23, 142.73it/s]Running 5000 simulations.:  34%|███▎      | 1684/5000 [00:11<00:23, 142.89it/s]Running 5000 simulations.:  34%|███▍      | 1699/5000 [00:11<00:23, 143.04it/s]Running 5000 simulations.:  34%|███▍      | 1714/5000 [00:11<00:22, 143.21it/s]Running 5000 simulations.:  35%|███▍      | 1729/5000 [00:11<00:22, 143.31it/s]Running 5000 simulations.:  35%|███▍      | 1744/5000 [00:12<00:22, 143.32it/s]Running 5000 simulations.:  35%|███▌      | 1759/5000 [00:12<00:22, 143.22it/s]Running 5000 simulations.:  35%|███▌      | 1774/5000 [00:12<00:22, 143.23it/s]Running 5000 simulations.:  36%|███▌      | 1789/5000 [00:12<00:22, 142.89it/s]Running 5000 simulations.:  36%|███▌      | 1804/5000 [00:12<00:22, 143.19it/s]Running 5000 simulations.:  36%|███▋      | 1819/5000 [00:12<00:22, 143.46it/s]Running 5000 simulations.:  37%|███▋      | 1834/5000 [00:12<00:22, 143.31it/s]Running 5000 simulations.:  37%|███▋      | 1849/5000 [00:12<00:22, 143.11it/s]Running 5000 simulations.:  37%|███▋      | 1864/5000 [00:12<00:21, 143.68it/s]Running 5000 simulations.:  38%|███▊      | 1879/5000 [00:13<00:21, 143.22it/s]Running 5000 simulations.:  38%|███▊      | 1894/5000 [00:13<00:21, 144.37it/s]Running 5000 simulations.:  38%|███▊      | 1910/5000 [00:13<00:20, 147.75it/s]Running 5000 simulations.:  38%|███▊      | 1925/5000 [00:13<00:20, 148.07it/s]Running 5000 simulations.:  39%|███▉      | 1940/5000 [00:13<00:21, 145.18it/s]Running 5000 simulations.:  39%|███▉      | 1955/5000 [00:13<00:21, 143.42it/s]Running 5000 simulations.:  39%|███▉      | 1970/5000 [00:13<00:21, 142.48it/s]Running 5000 simulations.:  40%|███▉      | 1985/5000 [00:13<00:21, 142.28it/s]Running 5000 simulations.:  40%|████      | 2000/5000 [00:13<00:21, 142.26it/s]Running 5000 simulations.:  40%|████      | 2015/5000 [00:13<00:21, 142.05it/s]Running 5000 simulations.:  41%|████      | 2030/5000 [00:14<00:21, 141.42it/s]Running 5000 simulations.:  41%|████      | 2045/5000 [00:14<00:21, 140.65it/s]Running 5000 simulations.:  41%|████      | 2060/5000 [00:14<00:21, 139.56it/s]Running 5000 simulations.:  41%|████▏     | 2074/5000 [00:14<00:21, 139.15it/s]Running 5000 simulations.:  42%|████▏     | 2089/5000 [00:14<00:20, 139.84it/s]Running 5000 simulations.:  42%|████▏     | 2103/5000 [00:14<00:20, 139.82it/s]Running 5000 simulations.:  42%|████▏     | 2117/5000 [00:14<00:20, 139.87it/s]Running 5000 simulations.:  43%|████▎     | 2132/5000 [00:14<00:20, 140.88it/s]Running 5000 simulations.:  43%|████▎     | 2147/5000 [00:14<00:20, 141.00it/s]Running 5000 simulations.:  43%|████▎     | 2162/5000 [00:15<00:20, 139.30it/s]Running 5000 simulations.:  44%|████▎     | 2176/5000 [00:15<00:20, 138.53it/s]Running 5000 simulations.:  44%|████▍     | 2191/5000 [00:15<00:20, 139.42it/s]Running 5000 simulations.:  44%|████▍     | 2206/5000 [00:15<00:19, 140.04it/s]Running 5000 simulations.:  44%|████▍     | 2221/5000 [00:15<00:19, 141.04it/s]Running 5000 simulations.:  45%|████▍     | 2236/5000 [00:15<00:19, 141.05it/s]Running 5000 simulations.:  45%|████▌     | 2251/5000 [00:15<00:19, 141.40it/s]Running 5000 simulations.:  45%|████▌     | 2266/5000 [00:15<00:19, 141.20it/s]Running 5000 simulations.:  46%|████▌     | 2281/5000 [00:15<00:19, 140.73it/s]Running 5000 simulations.:  46%|████▌     | 2296/5000 [00:15<00:19, 139.92it/s]Running 5000 simulations.:  46%|████▌     | 2311/5000 [00:16<00:19, 140.02it/s]Running 5000 simulations.:  47%|████▋     | 2326/5000 [00:16<00:18, 140.74it/s]Running 5000 simulations.:  47%|████▋     | 2341/5000 [00:16<00:18, 141.02it/s]Running 5000 simulations.:  47%|████▋     | 2356/5000 [00:16<00:18, 141.66it/s]Running 5000 simulations.:  47%|████▋     | 2371/5000 [00:16<00:18, 142.27it/s]Running 5000 simulations.:  48%|████▊     | 2386/5000 [00:16<00:18, 142.03it/s]Running 5000 simulations.:  48%|████▊     | 2401/5000 [00:16<00:18, 141.54it/s]Running 5000 simulations.:  48%|████▊     | 2416/5000 [00:16<00:18, 141.24it/s]Running 5000 simulations.:  49%|████▊     | 2431/5000 [00:16<00:18, 141.08it/s]Running 5000 simulations.:  49%|████▉     | 2446/5000 [00:17<00:18, 140.92it/s]Running 5000 simulations.:  49%|████▉     | 2461/5000 [00:17<00:17, 141.37it/s]Running 5000 simulations.:  50%|████▉     | 2476/5000 [00:17<00:17, 141.30it/s]Running 5000 simulations.:  50%|████▉     | 2491/5000 [00:17<00:17, 140.69it/s]Running 5000 simulations.:  50%|█████     | 2506/5000 [00:17<00:17, 141.43it/s]Running 5000 simulations.:  50%|█████     | 2521/5000 [00:17<00:17, 140.78it/s]Running 5000 simulations.:  51%|█████     | 2536/5000 [00:17<00:17, 139.67it/s]Running 5000 simulations.:  51%|█████     | 2550/5000 [00:17<00:17, 139.71it/s]Running 5000 simulations.:  51%|█████▏    | 2565/5000 [00:17<00:17, 140.31it/s]Running 5000 simulations.:  52%|█████▏    | 2580/5000 [00:17<00:17, 140.50it/s]Running 5000 simulations.:  52%|█████▏    | 2595/5000 [00:18<00:17, 140.87it/s]Running 5000 simulations.:  52%|█████▏    | 2610/5000 [00:18<00:16, 141.24it/s]Running 5000 simulations.:  52%|█████▎    | 2625/5000 [00:18<00:16, 141.59it/s]Running 5000 simulations.:  53%|█████▎    | 2640/5000 [00:18<00:16, 141.54it/s]Running 5000 simulations.:  53%|█████▎    | 2655/5000 [00:18<00:16, 141.59it/s]Running 5000 simulations.:  53%|█████▎    | 2670/5000 [00:18<00:16, 140.18it/s]Running 5000 simulations.:  54%|█████▎    | 2685/5000 [00:18<00:16, 139.26it/s]Running 5000 simulations.:  54%|█████▍    | 2700/5000 [00:18<00:16, 140.08it/s]Running 5000 simulations.:  54%|█████▍    | 2715/5000 [00:18<00:16, 140.47it/s]Running 5000 simulations.:  55%|█████▍    | 2730/5000 [00:19<00:16, 141.19it/s]Running 5000 simulations.:  55%|█████▍    | 2745/5000 [00:19<00:15, 141.80it/s]Running 5000 simulations.:  55%|█████▌    | 2760/5000 [00:19<00:15, 141.83it/s]Running 5000 simulations.:  56%|█████▌    | 2775/5000 [00:19<00:15, 141.25it/s]Running 5000 simulations.:  56%|█████▌    | 2790/5000 [00:19<00:15, 141.04it/s]Running 5000 simulations.:  56%|█████▌    | 2805/5000 [00:19<00:15, 138.93it/s]Running 5000 simulations.:  56%|█████▋    | 2819/5000 [00:19<00:15, 137.76it/s]Running 5000 simulations.:  57%|█████▋    | 2834/5000 [00:19<00:15, 138.65it/s]Running 5000 simulations.:  57%|█████▋    | 2849/5000 [00:19<00:15, 139.29it/s]Running 5000 simulations.:  57%|█████▋    | 2864/5000 [00:19<00:15, 140.01it/s]Running 5000 simulations.:  58%|█████▊    | 2879/5000 [00:20<00:15, 140.48it/s]Running 5000 simulations.:  58%|█████▊    | 2894/5000 [00:20<00:14, 141.01it/s]Running 5000 simulations.:  58%|█████▊    | 2909/5000 [00:20<00:14, 140.79it/s]Running 5000 simulations.:  58%|█████▊    | 2924/5000 [00:20<00:14, 141.17it/s]Running 5000 simulations.:  59%|█████▉    | 2939/5000 [00:20<00:14, 140.04it/s]Running 5000 simulations.:  59%|█████▉    | 2954/5000 [00:20<00:14, 138.49it/s]Running 5000 simulations.:  59%|█████▉    | 2969/5000 [00:20<00:14, 139.39it/s]Running 5000 simulations.:  60%|█████▉    | 2984/5000 [00:20<00:14, 140.23it/s]Running 5000 simulations.:  60%|█████▉    | 2999/5000 [00:20<00:14, 141.11it/s]Running 5000 simulations.:  60%|██████    | 3014/5000 [00:21<00:13, 142.01it/s]Running 5000 simulations.:  61%|██████    | 3029/5000 [00:21<00:13, 141.93it/s]Running 5000 simulations.:  61%|██████    | 3044/5000 [00:21<00:13, 141.18it/s]Running 5000 simulations.:  61%|██████    | 3059/5000 [00:21<00:13, 141.51it/s]Running 5000 simulations.:  61%|██████▏   | 3074/5000 [00:21<00:13, 141.33it/s]Running 5000 simulations.:  62%|██████▏   | 3089/5000 [00:21<00:13, 140.93it/s]Running 5000 simulations.:  62%|██████▏   | 3104/5000 [00:21<00:13, 141.09it/s]Running 5000 simulations.:  62%|██████▏   | 3119/5000 [00:21<00:13, 141.54it/s]Running 5000 simulations.:  63%|██████▎   | 3134/5000 [00:21<00:13, 141.83it/s]Running 5000 simulations.:  63%|██████▎   | 3149/5000 [00:22<00:13, 142.17it/s]Running 5000 simulations.:  63%|██████▎   | 3164/5000 [00:22<00:12, 141.72it/s]Running 5000 simulations.:  64%|██████▎   | 3179/5000 [00:22<00:12, 141.54it/s]Running 5000 simulations.:  64%|██████▍   | 3194/5000 [00:22<00:12, 142.14it/s]Running 5000 simulations.:  64%|██████▍   | 3209/5000 [00:22<00:12, 141.14it/s]Running 5000 simulations.:  64%|██████▍   | 3224/5000 [00:22<00:12, 138.64it/s]Running 5000 simulations.:  65%|██████▍   | 3238/5000 [00:22<00:12, 138.31it/s]Running 5000 simulations.:  65%|██████▌   | 3252/5000 [00:22<00:13, 130.93it/s]Running 5000 simulations.:  65%|██████▌   | 3267/5000 [00:22<00:12, 133.74it/s]Running 5000 simulations.:  66%|██████▌   | 3282/5000 [00:22<00:12, 135.91it/s]Running 5000 simulations.:  66%|██████▌   | 3297/5000 [00:23<00:12, 137.34it/s]Running 5000 simulations.:  66%|██████▌   | 3311/5000 [00:23<00:12, 136.83it/s]Running 5000 simulations.:  66%|██████▋   | 3325/5000 [00:23<00:12, 136.40it/s]Running 5000 simulations.:  67%|██████▋   | 3339/5000 [00:23<00:12, 136.70it/s]Running 5000 simulations.:  67%|██████▋   | 3353/5000 [00:23<00:12, 137.20it/s]Running 5000 simulations.:  67%|██████▋   | 3368/5000 [00:23<00:11, 138.16it/s]Running 5000 simulations.:  68%|██████▊   | 3383/5000 [00:23<00:11, 139.30it/s]Running 5000 simulations.:  68%|██████▊   | 3397/5000 [00:23<00:11, 138.95it/s]Running 5000 simulations.:  68%|██████▊   | 3411/5000 [00:23<00:11, 137.60it/s]Running 5000 simulations.:  68%|██████▊   | 3425/5000 [00:24<00:11, 137.27it/s]Running 5000 simulations.:  69%|██████▉   | 3439/5000 [00:24<00:11, 136.80it/s]Running 5000 simulations.:  69%|██████▉   | 3454/5000 [00:24<00:11, 137.69it/s]Running 5000 simulations.:  69%|██████▉   | 3469/5000 [00:24<00:11, 138.82it/s]Running 5000 simulations.:  70%|██████▉   | 3484/5000 [00:24<00:10, 139.41it/s]Running 5000 simulations.:  70%|██████▉   | 3498/5000 [00:24<00:10, 139.26it/s]Running 5000 simulations.:  70%|███████   | 3512/5000 [00:24<00:10, 139.29it/s]Running 5000 simulations.:  71%|███████   | 3527/5000 [00:24<00:10, 139.84it/s]Running 5000 simulations.:  71%|███████   | 3541/5000 [00:24<00:10, 138.88it/s]Running 5000 simulations.:  71%|███████   | 3555/5000 [00:24<00:10, 137.80it/s]Running 5000 simulations.:  71%|███████▏  | 3570/5000 [00:25<00:10, 138.61it/s]Running 5000 simulations.:  72%|███████▏  | 3585/5000 [00:25<00:10, 139.51it/s]Running 5000 simulations.:  72%|███████▏  | 3600/5000 [00:25<00:09, 140.13it/s]Running 5000 simulations.:  72%|███████▏  | 3615/5000 [00:25<00:09, 140.68it/s]Running 5000 simulations.:  73%|███████▎  | 3630/5000 [00:25<00:09, 140.38it/s]Running 5000 simulations.:  73%|███████▎  | 3645/5000 [00:25<00:09, 140.83it/s]Running 5000 simulations.:  73%|███████▎  | 3660/5000 [00:25<00:09, 141.06it/s]Running 5000 simulations.:  74%|███████▎  | 3675/5000 [00:25<00:09, 141.11it/s]Running 5000 simulations.:  74%|███████▍  | 3690/5000 [00:25<00:09, 140.98it/s]Running 5000 simulations.:  74%|███████▍  | 3705/5000 [00:26<00:09, 141.11it/s]Running 5000 simulations.:  74%|███████▍  | 3720/5000 [00:26<00:09, 141.23it/s]Running 5000 simulations.:  75%|███████▍  | 3735/5000 [00:26<00:08, 141.20it/s]Running 5000 simulations.:  75%|███████▌  | 3750/5000 [00:26<00:09, 136.76it/s]Running 5000 simulations.:  75%|███████▌  | 3764/5000 [00:26<00:09, 132.76it/s]Running 5000 simulations.:  76%|███████▌  | 3778/5000 [00:26<00:09, 129.83it/s]Running 5000 simulations.:  76%|███████▌  | 3792/5000 [00:26<00:09, 127.88it/s]Running 5000 simulations.:  76%|███████▌  | 3805/5000 [00:26<00:09, 126.82it/s]Running 5000 simulations.:  76%|███████▋  | 3818/5000 [00:26<00:09, 125.29it/s]Running 5000 simulations.:  77%|███████▋  | 3831/5000 [00:27<00:09, 126.18it/s]Running 5000 simulations.:  77%|███████▋  | 3844/5000 [00:27<00:09, 126.16it/s]Running 5000 simulations.:  77%|███████▋  | 3857/5000 [00:27<00:09, 126.71it/s]Running 5000 simulations.:  77%|███████▋  | 3870/5000 [00:27<00:08, 127.19it/s]Running 5000 simulations.:  78%|███████▊  | 3883/5000 [00:27<00:08, 127.85it/s]Running 5000 simulations.:  78%|███████▊  | 3896/5000 [00:27<00:08, 127.67it/s]Running 5000 simulations.:  78%|███████▊  | 3909/5000 [00:27<00:08, 127.89it/s]Running 5000 simulations.:  78%|███████▊  | 3922/5000 [00:27<00:08, 127.72it/s]Running 5000 simulations.:  79%|███████▊  | 3935/5000 [00:27<00:08, 126.66it/s]Running 5000 simulations.:  79%|███████▉  | 3948/5000 [00:27<00:08, 125.62it/s]Running 5000 simulations.:  79%|███████▉  | 3961/5000 [00:28<00:08, 126.30it/s]Running 5000 simulations.:  79%|███████▉  | 3974/5000 [00:28<00:08, 127.25it/s]Running 5000 simulations.:  80%|███████▉  | 3987/5000 [00:28<00:07, 127.53it/s]Running 5000 simulations.:  80%|████████  | 4000/5000 [00:28<00:07, 127.86it/s]Running 5000 simulations.:  80%|████████  | 4013/5000 [00:28<00:07, 127.59it/s]Running 5000 simulations.:  81%|████████  | 4026/5000 [00:28<00:07, 127.39it/s]Running 5000 simulations.:  81%|████████  | 4039/5000 [00:28<00:07, 126.87it/s]Running 5000 simulations.:  81%|████████  | 4052/5000 [00:28<00:07, 127.40it/s]Running 5000 simulations.:  81%|████████▏ | 4065/5000 [00:28<00:07, 126.53it/s]Running 5000 simulations.:  82%|████████▏ | 4078/5000 [00:28<00:07, 126.92it/s]Running 5000 simulations.:  82%|████████▏ | 4091/5000 [00:29<00:07, 126.66it/s]Running 5000 simulations.:  82%|████████▏ | 4104/5000 [00:29<00:07, 127.00it/s]Running 5000 simulations.:  82%|████████▏ | 4117/5000 [00:29<00:06, 127.06it/s]Running 5000 simulations.:  83%|████████▎ | 4130/5000 [00:29<00:06, 127.23it/s]Running 5000 simulations.:  83%|████████▎ | 4143/5000 [00:29<00:06, 127.18it/s]Running 5000 simulations.:  83%|████████▎ | 4156/5000 [00:29<00:06, 127.31it/s]Running 5000 simulations.:  83%|████████▎ | 4170/5000 [00:29<00:06, 128.61it/s]Running 5000 simulations.:  84%|████████▎ | 4184/5000 [00:29<00:06, 130.81it/s]Running 5000 simulations.:  84%|████████▍ | 4198/5000 [00:29<00:06, 132.39it/s]Running 5000 simulations.:  84%|████████▍ | 4212/5000 [00:29<00:05, 134.14it/s]Running 5000 simulations.:  85%|████████▍ | 4226/5000 [00:30<00:05, 134.89it/s]Running 5000 simulations.:  85%|████████▍ | 4241/5000 [00:30<00:05, 136.41it/s]Running 5000 simulations.:  85%|████████▌ | 4255/5000 [00:30<00:05, 136.66it/s]Running 5000 simulations.:  85%|████████▌ | 4269/5000 [00:30<00:05, 136.81it/s]Running 5000 simulations.:  86%|████████▌ | 4283/5000 [00:30<00:05, 136.80it/s]Running 5000 simulations.:  86%|████████▌ | 4297/5000 [00:30<00:05, 136.47it/s]Running 5000 simulations.:  86%|████████▌ | 4311/5000 [00:30<00:05, 137.16it/s]Running 5000 simulations.:  86%|████████▋ | 4325/5000 [00:30<00:04, 137.89it/s]Running 5000 simulations.:  87%|████████▋ | 4339/5000 [00:30<00:04, 137.16it/s]Running 5000 simulations.:  87%|████████▋ | 4353/5000 [00:31<00:04, 135.87it/s]Running 5000 simulations.:  87%|████████▋ | 4368/5000 [00:31<00:04, 137.59it/s]Running 5000 simulations.:  88%|████████▊ | 4382/5000 [00:31<00:04, 138.17it/s]Running 5000 simulations.:  88%|████████▊ | 4396/5000 [00:31<00:04, 138.23it/s]Running 5000 simulations.:  88%|████████▊ | 4411/5000 [00:31<00:04, 138.98it/s]Running 5000 simulations.:  88%|████████▊ | 4425/5000 [00:31<00:04, 138.59it/s]Running 5000 simulations.:  89%|████████▉ | 4439/5000 [00:31<00:04, 138.26it/s]Running 5000 simulations.:  89%|████████▉ | 4453/5000 [00:31<00:03, 137.26it/s]Running 5000 simulations.:  89%|████████▉ | 4467/5000 [00:31<00:03, 136.92it/s]Running 5000 simulations.:  90%|████████▉ | 4481/5000 [00:31<00:03, 136.97it/s]Running 5000 simulations.:  90%|████████▉ | 4495/5000 [00:32<00:03, 137.63it/s]Running 5000 simulations.:  90%|█████████ | 4509/5000 [00:32<00:03, 137.60it/s]Running 5000 simulations.:  90%|█████████ | 4524/5000 [00:32<00:03, 139.41it/s]Running 5000 simulations.:  91%|█████████ | 4538/5000 [00:32<00:03, 139.30it/s]Running 5000 simulations.:  91%|█████████ | 4552/5000 [00:32<00:03, 139.18it/s]Running 5000 simulations.:  91%|█████████▏| 4567/5000 [00:32<00:03, 139.99it/s]Running 5000 simulations.:  92%|█████████▏| 4582/5000 [00:32<00:02, 140.33it/s]Running 5000 simulations.:  92%|█████████▏| 4597/5000 [00:32<00:02, 139.16it/s]Running 5000 simulations.:  92%|█████████▏| 4611/5000 [00:32<00:02, 136.50it/s]Running 5000 simulations.:  92%|█████████▎| 4625/5000 [00:32<00:02, 137.27it/s]Running 5000 simulations.:  93%|█████████▎| 4640/5000 [00:33<00:02, 138.74it/s]Running 5000 simulations.:  93%|█████████▎| 4655/5000 [00:33<00:02, 139.54it/s]Running 5000 simulations.:  93%|█████████▎| 4669/5000 [00:33<00:02, 138.49it/s]Running 5000 simulations.:  94%|█████████▎| 4683/5000 [00:33<00:02, 138.56it/s]Running 5000 simulations.:  94%|█████████▍| 4697/5000 [00:33<00:02, 138.89it/s]Running 5000 simulations.:  94%|█████████▍| 4711/5000 [00:33<00:02, 138.99it/s]Running 5000 simulations.:  94%|█████████▍| 4725/5000 [00:33<00:01, 139.26it/s]Running 5000 simulations.:  95%|█████████▍| 4739/5000 [00:33<00:01, 138.68it/s]Running 5000 simulations.:  95%|█████████▌| 4753/5000 [00:33<00:01, 137.96it/s]Running 5000 simulations.:  95%|█████████▌| 4767/5000 [00:33<00:01, 138.53it/s]Running 5000 simulations.:  96%|█████████▌| 4781/5000 [00:34<00:01, 138.77it/s]Running 5000 simulations.:  96%|█████████▌| 4795/5000 [00:34<00:01, 138.98it/s]Running 5000 simulations.:  96%|█████████▌| 4809/5000 [00:34<00:01, 138.55it/s]Running 5000 simulations.:  96%|█████████▋| 4823/5000 [00:34<00:01, 138.93it/s]Running 5000 simulations.:  97%|█████████▋| 4837/5000 [00:34<00:01, 138.88it/s]Running 5000 simulations.:  97%|█████████▋| 4852/5000 [00:34<00:01, 139.90it/s]Running 5000 simulations.:  97%|█████████▋| 4867/5000 [00:34<00:00, 139.99it/s]Running 5000 simulations.:  98%|█████████▊| 4882/5000 [00:34<00:00, 138.60it/s]Running 5000 simulations.:  98%|█████████▊| 4897/5000 [00:34<00:00, 139.48it/s]Running 5000 simulations.:  98%|█████████▊| 4911/5000 [00:35<00:00, 139.62it/s]Running 5000 simulations.:  99%|█████████▊| 4926/5000 [00:35<00:00, 139.82it/s]Running 5000 simulations.:  99%|█████████▉| 4941/5000 [00:35<00:00, 140.42it/s]Running 5000 simulations.:  99%|█████████▉| 4956/5000 [00:35<00:00, 140.15it/s]Running 5000 simulations.:  99%|█████████▉| 4971/5000 [00:35<00:00, 139.38it/s]Running 5000 simulations.: 100%|█████████▉| 4986/5000 [00:35<00:00, 139.93it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:35<00:00, 138.82it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:35<00:00, 140.22it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 15/5000 [00:00<00:35, 141.15it/s]Running 5000 simulations.:   1%|          | 30/5000 [00:00<00:35, 141.27it/s]Running 5000 simulations.:   1%|          | 44/5000 [00:00<00:35, 140.37it/s]Running 5000 simulations.:   1%|          | 59/5000 [00:00<00:34, 142.88it/s]Running 5000 simulations.:   1%|▏         | 74/5000 [00:00<00:34, 144.68it/s]Running 5000 simulations.:   2%|▏         | 89/5000 [00:00<00:33, 145.88it/s]Running 5000 simulations.:   2%|▏         | 104/5000 [00:00<00:33, 146.95it/s]Running 5000 simulations.:   2%|▏         | 120/5000 [00:00<00:32, 148.34it/s]Running 5000 simulations.:   3%|▎         | 135/5000 [00:00<00:32, 147.61it/s]Running 5000 simulations.:   3%|▎         | 150/5000 [00:01<00:33, 146.97it/s]Running 5000 simulations.:   3%|▎         | 165/5000 [00:01<00:32, 147.55it/s]Running 5000 simulations.:   4%|▎         | 180/5000 [00:01<00:32, 147.74it/s]Running 5000 simulations.:   4%|▍         | 195/5000 [00:01<00:32, 147.85it/s]Running 5000 simulations.:   4%|▍         | 210/5000 [00:01<00:32, 147.88it/s]Running 5000 simulations.:   4%|▍         | 225/5000 [00:01<00:32, 148.01it/s]Running 5000 simulations.:   5%|▍         | 240/5000 [00:01<00:32, 147.80it/s]Running 5000 simulations.:   5%|▌         | 255/5000 [00:01<00:32, 147.74it/s]Running 5000 simulations.:   5%|▌         | 270/5000 [00:01<00:32, 146.59it/s]Running 5000 simulations.:   6%|▌         | 285/5000 [00:01<00:32, 145.63it/s]Running 5000 simulations.:   6%|▌         | 300/5000 [00:02<00:33, 141.53it/s]Running 5000 simulations.:   6%|▋         | 315/5000 [00:02<00:33, 138.45it/s]Running 5000 simulations.:   7%|▋         | 329/5000 [00:02<00:34, 136.85it/s]Running 5000 simulations.:   7%|▋         | 343/5000 [00:02<00:34, 136.04it/s]Running 5000 simulations.:   7%|▋         | 357/5000 [00:02<00:34, 135.70it/s]Running 5000 simulations.:   7%|▋         | 371/5000 [00:02<00:34, 135.65it/s]Running 5000 simulations.:   8%|▊         | 385/5000 [00:02<00:33, 136.00it/s]Running 5000 simulations.:   8%|▊         | 399/5000 [00:02<00:34, 135.23it/s]Running 5000 simulations.:   8%|▊         | 413/5000 [00:02<00:34, 133.74it/s]Running 5000 simulations.:   9%|▊         | 427/5000 [00:03<00:34, 133.91it/s]Running 5000 simulations.:   9%|▉         | 441/5000 [00:03<00:34, 133.34it/s]Running 5000 simulations.:   9%|▉         | 456/5000 [00:03<00:33, 136.45it/s]Running 5000 simulations.:   9%|▉         | 471/5000 [00:03<00:32, 139.30it/s]Running 5000 simulations.:  10%|▉         | 486/5000 [00:03<00:31, 141.77it/s]Running 5000 simulations.:  10%|█         | 501/5000 [00:03<00:31, 143.48it/s]Running 5000 simulations.:  10%|█         | 516/5000 [00:03<00:31, 144.32it/s]Running 5000 simulations.:  11%|█         | 531/5000 [00:03<00:30, 145.00it/s]Running 5000 simulations.:  11%|█         | 546/5000 [00:03<00:30, 144.57it/s]Running 5000 simulations.:  11%|█         | 561/5000 [00:03<00:30, 144.52it/s]Running 5000 simulations.:  12%|█▏        | 576/5000 [00:04<00:30, 144.11it/s]Running 5000 simulations.:  12%|█▏        | 591/5000 [00:04<00:30, 144.21it/s]Running 5000 simulations.:  12%|█▏        | 606/5000 [00:04<00:30, 144.78it/s]Running 5000 simulations.:  12%|█▏        | 621/5000 [00:04<00:30, 145.43it/s]Running 5000 simulations.:  13%|█▎        | 636/5000 [00:04<00:29, 145.76it/s]Running 5000 simulations.:  13%|█▎        | 651/5000 [00:04<00:29, 145.87it/s]Running 5000 simulations.:  13%|█▎        | 666/5000 [00:04<00:29, 145.93it/s]Running 5000 simulations.:  14%|█▎        | 681/5000 [00:04<00:29, 145.53it/s]Running 5000 simulations.:  14%|█▍        | 696/5000 [00:04<00:29, 145.02it/s]Running 5000 simulations.:  14%|█▍        | 711/5000 [00:04<00:29, 144.64it/s]Running 5000 simulations.:  15%|█▍        | 726/5000 [00:05<00:29, 145.03it/s]Running 5000 simulations.:  15%|█▍        | 741/5000 [00:05<00:29, 145.06it/s]Running 5000 simulations.:  15%|█▌        | 756/5000 [00:05<00:29, 145.00it/s]Running 5000 simulations.:  15%|█▌        | 771/5000 [00:05<00:29, 145.36it/s]Running 5000 simulations.:  16%|█▌        | 786/5000 [00:05<00:28, 145.86it/s]Running 5000 simulations.:  16%|█▌        | 801/5000 [00:05<00:28, 146.25it/s]Running 5000 simulations.:  16%|█▋        | 816/5000 [00:05<00:28, 145.76it/s]Running 5000 simulations.:  17%|█▋        | 831/5000 [00:05<00:28, 144.72it/s]Running 5000 simulations.:  17%|█▋        | 846/5000 [00:05<00:28, 144.64it/s]Running 5000 simulations.:  17%|█▋        | 861/5000 [00:06<00:28, 144.38it/s]Running 5000 simulations.:  18%|█▊        | 876/5000 [00:06<00:28, 144.56it/s]Running 5000 simulations.:  18%|█▊        | 891/5000 [00:06<00:28, 145.01it/s]Running 5000 simulations.:  18%|█▊        | 906/5000 [00:06<00:28, 145.06it/s]Running 5000 simulations.:  18%|█▊        | 921/5000 [00:06<00:28, 144.81it/s]Running 5000 simulations.:  19%|█▊        | 936/5000 [00:06<00:28, 144.74it/s]Running 5000 simulations.:  19%|█▉        | 951/5000 [00:06<00:28, 143.90it/s]Running 5000 simulations.:  19%|█▉        | 966/5000 [00:06<00:28, 143.65it/s]Running 5000 simulations.:  20%|█▉        | 981/5000 [00:06<00:27, 143.83it/s]Running 5000 simulations.:  20%|█▉        | 996/5000 [00:06<00:27, 143.92it/s]Running 5000 simulations.:  20%|██        | 1011/5000 [00:07<00:27, 144.31it/s]Running 5000 simulations.:  21%|██        | 1026/5000 [00:07<00:27, 144.51it/s]Running 5000 simulations.:  21%|██        | 1041/5000 [00:07<00:27, 144.84it/s]Running 5000 simulations.:  21%|██        | 1056/5000 [00:07<00:27, 145.03it/s]Running 5000 simulations.:  21%|██▏       | 1071/5000 [00:07<00:27, 144.95it/s]Running 5000 simulations.:  22%|██▏       | 1086/5000 [00:07<00:26, 145.58it/s]Running 5000 simulations.:  22%|██▏       | 1101/5000 [00:07<00:26, 144.72it/s]Running 5000 simulations.:  22%|██▏       | 1116/5000 [00:07<00:26, 143.92it/s]Running 5000 simulations.:  23%|██▎       | 1131/5000 [00:07<00:26, 143.54it/s]Running 5000 simulations.:  23%|██▎       | 1146/5000 [00:07<00:26, 143.64it/s]Running 5000 simulations.:  23%|██▎       | 1161/5000 [00:08<00:26, 143.95it/s]Running 5000 simulations.:  24%|██▎       | 1176/5000 [00:08<00:26, 143.92it/s]Running 5000 simulations.:  24%|██▍       | 1191/5000 [00:08<00:26, 143.85it/s]Running 5000 simulations.:  24%|██▍       | 1206/5000 [00:08<00:26, 143.08it/s]Running 5000 simulations.:  24%|██▍       | 1221/5000 [00:08<00:26, 143.31it/s]Running 5000 simulations.:  25%|██▍       | 1236/5000 [00:08<00:26, 143.01it/s]Running 5000 simulations.:  25%|██▌       | 1251/5000 [00:08<00:26, 143.31it/s]Running 5000 simulations.:  25%|██▌       | 1266/5000 [00:08<00:25, 143.92it/s]Running 5000 simulations.:  26%|██▌       | 1281/5000 [00:08<00:25, 143.13it/s]Running 5000 simulations.:  26%|██▌       | 1296/5000 [00:09<00:25, 143.34it/s]Running 5000 simulations.:  26%|██▌       | 1311/5000 [00:09<00:25, 143.92it/s]Running 5000 simulations.:  27%|██▋       | 1326/5000 [00:09<00:25, 143.96it/s]Running 5000 simulations.:  27%|██▋       | 1341/5000 [00:09<00:25, 144.17it/s]Running 5000 simulations.:  27%|██▋       | 1356/5000 [00:09<00:25, 143.70it/s]Running 5000 simulations.:  27%|██▋       | 1371/5000 [00:09<00:25, 143.05it/s]Running 5000 simulations.:  28%|██▊       | 1386/5000 [00:09<00:25, 143.14it/s]Running 5000 simulations.:  28%|██▊       | 1401/5000 [00:09<00:25, 142.75it/s]Running 5000 simulations.:  28%|██▊       | 1416/5000 [00:09<00:25, 140.82it/s]Running 5000 simulations.:  29%|██▊       | 1431/5000 [00:09<00:25, 140.95it/s]Running 5000 simulations.:  29%|██▉       | 1446/5000 [00:10<00:25, 140.53it/s]Running 5000 simulations.:  29%|██▉       | 1461/5000 [00:10<00:25, 140.55it/s]Running 5000 simulations.:  30%|██▉       | 1476/5000 [00:10<00:24, 141.16it/s]Running 5000 simulations.:  30%|██▉       | 1491/5000 [00:10<00:24, 141.71it/s]Running 5000 simulations.:  30%|███       | 1506/5000 [00:10<00:24, 141.69it/s]Running 5000 simulations.:  30%|███       | 1521/5000 [00:10<00:24, 142.08it/s]Running 5000 simulations.:  31%|███       | 1536/5000 [00:10<00:24, 141.17it/s]Running 5000 simulations.:  31%|███       | 1551/5000 [00:10<00:24, 138.76it/s]Running 5000 simulations.:  31%|███▏      | 1565/5000 [00:10<00:24, 138.82it/s]Running 5000 simulations.:  32%|███▏      | 1580/5000 [00:11<00:24, 139.24it/s]Running 5000 simulations.:  32%|███▏      | 1595/5000 [00:11<00:24, 139.75it/s]Running 5000 simulations.:  32%|███▏      | 1610/5000 [00:11<00:24, 140.73it/s]Running 5000 simulations.:  32%|███▎      | 1625/5000 [00:11<00:23, 141.23it/s]Running 5000 simulations.:  33%|███▎      | 1640/5000 [00:11<00:23, 142.05it/s]Running 5000 simulations.:  33%|███▎      | 1655/5000 [00:11<00:23, 142.77it/s]Running 5000 simulations.:  33%|███▎      | 1670/5000 [00:11<00:23, 142.25it/s]Running 5000 simulations.:  34%|███▎      | 1685/5000 [00:11<00:23, 141.84it/s]Running 5000 simulations.:  34%|███▍      | 1700/5000 [00:11<00:23, 141.36it/s]Running 5000 simulations.:  34%|███▍      | 1715/5000 [00:11<00:23, 140.56it/s]Running 5000 simulations.:  35%|███▍      | 1730/5000 [00:12<00:23, 140.88it/s]Running 5000 simulations.:  35%|███▍      | 1745/5000 [00:12<00:22, 141.87it/s]Running 5000 simulations.:  35%|███▌      | 1760/5000 [00:12<00:22, 142.53it/s]Running 5000 simulations.:  36%|███▌      | 1775/5000 [00:12<00:22, 143.35it/s]Running 5000 simulations.:  36%|███▌      | 1790/5000 [00:12<00:22, 141.16it/s]Running 5000 simulations.:  36%|███▌      | 1805/5000 [00:12<00:23, 136.47it/s]Running 5000 simulations.:  36%|███▋      | 1819/5000 [00:12<00:23, 132.71it/s]Running 5000 simulations.:  37%|███▋      | 1833/5000 [00:12<00:24, 130.67it/s]Running 5000 simulations.:  37%|███▋      | 1847/5000 [00:12<00:24, 129.50it/s]Running 5000 simulations.:  37%|███▋      | 1860/5000 [00:13<00:24, 127.78it/s]Running 5000 simulations.:  37%|███▋      | 1873/5000 [00:13<00:24, 127.53it/s]Running 5000 simulations.:  38%|███▊      | 1886/5000 [00:13<00:24, 128.10it/s]Running 5000 simulations.:  38%|███▊      | 1900/5000 [00:13<00:23, 130.45it/s]Running 5000 simulations.:  38%|███▊      | 1915/5000 [00:13<00:22, 134.50it/s]Running 5000 simulations.:  39%|███▊      | 1930/5000 [00:13<00:22, 137.96it/s]Running 5000 simulations.:  39%|███▉      | 1945/5000 [00:13<00:22, 138.55it/s]Running 5000 simulations.:  39%|███▉      | 1960/5000 [00:13<00:21, 139.66it/s]Running 5000 simulations.:  40%|███▉      | 1975/5000 [00:13<00:21, 140.56it/s]Running 5000 simulations.:  40%|███▉      | 1990/5000 [00:14<00:21, 141.20it/s]Running 5000 simulations.:  40%|████      | 2005/5000 [00:14<00:21, 141.64it/s]Running 5000 simulations.:  40%|████      | 2020/5000 [00:14<00:20, 142.35it/s]Running 5000 simulations.:  41%|████      | 2035/5000 [00:14<00:20, 143.12it/s]Running 5000 simulations.:  41%|████      | 2050/5000 [00:14<00:20, 143.11it/s]Running 5000 simulations.:  41%|████▏     | 2065/5000 [00:14<00:20, 142.65it/s]Running 5000 simulations.:  42%|████▏     | 2080/5000 [00:14<00:20, 141.51it/s]Running 5000 simulations.:  42%|████▏     | 2095/5000 [00:14<00:20, 141.12it/s]Running 5000 simulations.:  42%|████▏     | 2110/5000 [00:14<00:20, 141.53it/s]Running 5000 simulations.:  42%|████▎     | 2125/5000 [00:14<00:20, 141.56it/s]Running 5000 simulations.:  43%|████▎     | 2140/5000 [00:15<00:20, 142.16it/s]Running 5000 simulations.:  43%|████▎     | 2155/5000 [00:15<00:19, 142.47it/s]Running 5000 simulations.:  43%|████▎     | 2170/5000 [00:15<00:19, 143.17it/s]Running 5000 simulations.:  44%|████▎     | 2185/5000 [00:15<00:19, 143.65it/s]Running 5000 simulations.:  44%|████▍     | 2200/5000 [00:15<00:19, 142.34it/s]Running 5000 simulations.:  44%|████▍     | 2215/5000 [00:15<00:19, 141.45it/s]Running 5000 simulations.:  45%|████▍     | 2230/5000 [00:15<00:19, 139.45it/s]Running 5000 simulations.:  45%|████▍     | 2244/5000 [00:15<00:19, 139.48it/s]Running 5000 simulations.:  45%|████▌     | 2259/5000 [00:15<00:19, 140.20it/s]Running 5000 simulations.:  45%|████▌     | 2274/5000 [00:16<00:19, 141.29it/s]Running 5000 simulations.:  46%|████▌     | 2289/5000 [00:16<00:19, 142.35it/s]Running 5000 simulations.:  46%|████▌     | 2304/5000 [00:16<00:18, 142.59it/s]Running 5000 simulations.:  46%|████▋     | 2319/5000 [00:16<00:18, 142.03it/s]Running 5000 simulations.:  47%|████▋     | 2334/5000 [00:16<00:18, 142.72it/s]Running 5000 simulations.:  47%|████▋     | 2349/5000 [00:16<00:18, 143.37it/s]Running 5000 simulations.:  47%|████▋     | 2364/5000 [00:16<00:18, 143.77it/s]Running 5000 simulations.:  48%|████▊     | 2379/5000 [00:16<00:18, 143.55it/s]Running 5000 simulations.:  48%|████▊     | 2394/5000 [00:16<00:18, 142.24it/s]Running 5000 simulations.:  48%|████▊     | 2409/5000 [00:16<00:18, 141.26it/s]Running 5000 simulations.:  48%|████▊     | 2424/5000 [00:17<00:18, 141.40it/s]Running 5000 simulations.:  49%|████▉     | 2439/5000 [00:17<00:18, 141.55it/s]Running 5000 simulations.:  49%|████▉     | 2454/5000 [00:17<00:18, 141.33it/s]Running 5000 simulations.:  49%|████▉     | 2469/5000 [00:17<00:17, 141.66it/s]Running 5000 simulations.:  50%|████▉     | 2484/5000 [00:17<00:17, 142.22it/s]Running 5000 simulations.:  50%|████▉     | 2499/5000 [00:17<00:17, 142.65it/s]Running 5000 simulations.:  50%|█████     | 2514/5000 [00:17<00:17, 142.95it/s]Running 5000 simulations.:  51%|█████     | 2529/5000 [00:17<00:17, 143.23it/s]Running 5000 simulations.:  51%|█████     | 2544/5000 [00:17<00:17, 141.87it/s]Running 5000 simulations.:  51%|█████     | 2559/5000 [00:18<00:17, 141.16it/s]Running 5000 simulations.:  51%|█████▏    | 2574/5000 [00:18<00:17, 141.40it/s]Running 5000 simulations.:  52%|█████▏    | 2589/5000 [00:18<00:17, 141.64it/s]Running 5000 simulations.:  52%|█████▏    | 2604/5000 [00:18<00:16, 142.10it/s]Running 5000 simulations.:  52%|█████▏    | 2619/5000 [00:18<00:16, 142.70it/s]Running 5000 simulations.:  53%|█████▎    | 2634/5000 [00:18<00:16, 143.25it/s]Running 5000 simulations.:  53%|█████▎    | 2649/5000 [00:18<00:16, 143.21it/s]Running 5000 simulations.:  53%|█████▎    | 2664/5000 [00:18<00:16, 143.89it/s]Running 5000 simulations.:  54%|█████▎    | 2679/5000 [00:18<00:16, 143.16it/s]Running 5000 simulations.:  54%|█████▍    | 2694/5000 [00:18<00:16, 142.42it/s]Running 5000 simulations.:  54%|█████▍    | 2709/5000 [00:19<00:16, 143.07it/s]Running 5000 simulations.:  54%|█████▍    | 2724/5000 [00:19<00:15, 142.72it/s]Running 5000 simulations.:  55%|█████▍    | 2739/5000 [00:19<00:15, 142.73it/s]Running 5000 simulations.:  55%|█████▌    | 2754/5000 [00:19<00:15, 142.81it/s]Running 5000 simulations.:  55%|█████▌    | 2769/5000 [00:19<00:15, 143.22it/s]Running 5000 simulations.:  56%|█████▌    | 2784/5000 [00:19<00:15, 143.48it/s]Running 5000 simulations.:  56%|█████▌    | 2799/5000 [00:19<00:15, 143.22it/s]Running 5000 simulations.:  56%|█████▋    | 2814/5000 [00:19<00:15, 142.43it/s]Running 5000 simulations.:  57%|█████▋    | 2829/5000 [00:19<00:15, 142.03it/s]Running 5000 simulations.:  57%|█████▋    | 2844/5000 [00:20<00:15, 142.38it/s]Running 5000 simulations.:  57%|█████▋    | 2859/5000 [00:20<00:15, 142.36it/s]Running 5000 simulations.:  57%|█████▋    | 2874/5000 [00:20<00:14, 142.35it/s]Running 5000 simulations.:  58%|█████▊    | 2889/5000 [00:20<00:14, 142.14it/s]Running 5000 simulations.:  58%|█████▊    | 2904/5000 [00:20<00:14, 142.67it/s]Running 5000 simulations.:  58%|█████▊    | 2919/5000 [00:20<00:14, 142.97it/s]Running 5000 simulations.:  59%|█████▊    | 2934/5000 [00:20<00:14, 142.85it/s]Running 5000 simulations.:  59%|█████▉    | 2949/5000 [00:20<00:14, 143.13it/s]Running 5000 simulations.:  59%|█████▉    | 2964/5000 [00:20<00:14, 141.59it/s]Running 5000 simulations.:  60%|█████▉    | 2979/5000 [00:20<00:14, 141.42it/s]Running 5000 simulations.:  60%|█████▉    | 2994/5000 [00:21<00:14, 141.92it/s]Running 5000 simulations.:  60%|██████    | 3009/5000 [00:21<00:14, 142.20it/s]Running 5000 simulations.:  60%|██████    | 3024/5000 [00:21<00:13, 142.68it/s]Running 5000 simulations.:  61%|██████    | 3039/5000 [00:21<00:13, 143.61it/s]Running 5000 simulations.:  61%|██████    | 3054/5000 [00:21<00:13, 143.86it/s]Running 5000 simulations.:  61%|██████▏   | 3069/5000 [00:21<00:13, 144.02it/s]Running 5000 simulations.:  62%|██████▏   | 3084/5000 [00:21<00:13, 143.21it/s]Running 5000 simulations.:  62%|██████▏   | 3099/5000 [00:21<00:13, 142.03it/s]Running 5000 simulations.:  62%|██████▏   | 3114/5000 [00:21<00:13, 142.13it/s]Running 5000 simulations.:  63%|██████▎   | 3129/5000 [00:22<00:13, 141.76it/s]Running 5000 simulations.:  63%|██████▎   | 3144/5000 [00:22<00:13, 141.84it/s]Running 5000 simulations.:  63%|██████▎   | 3159/5000 [00:22<00:12, 142.39it/s]Running 5000 simulations.:  63%|██████▎   | 3174/5000 [00:22<00:12, 142.71it/s]Running 5000 simulations.:  64%|██████▍   | 3189/5000 [00:22<00:12, 142.83it/s]Running 5000 simulations.:  64%|██████▍   | 3204/5000 [00:22<00:12, 142.75it/s]Running 5000 simulations.:  64%|██████▍   | 3219/5000 [00:22<00:12, 142.02it/s]Running 5000 simulations.:  65%|██████▍   | 3234/5000 [00:22<00:12, 141.06it/s]Running 5000 simulations.:  65%|██████▍   | 3249/5000 [00:22<00:12, 141.18it/s]Running 5000 simulations.:  65%|██████▌   | 3264/5000 [00:22<00:12, 141.21it/s]Running 5000 simulations.:  66%|██████▌   | 3279/5000 [00:23<00:12, 141.43it/s]Running 5000 simulations.:  66%|██████▌   | 3294/5000 [00:23<00:12, 141.87it/s]Running 5000 simulations.:  66%|██████▌   | 3309/5000 [00:23<00:11, 142.24it/s]Running 5000 simulations.:  66%|██████▋   | 3324/5000 [00:23<00:11, 142.38it/s]Running 5000 simulations.:  67%|██████▋   | 3339/5000 [00:23<00:11, 142.17it/s]Running 5000 simulations.:  67%|██████▋   | 3354/5000 [00:23<00:11, 141.59it/s]Running 5000 simulations.:  67%|██████▋   | 3369/5000 [00:23<00:11, 140.23it/s]Running 5000 simulations.:  68%|██████▊   | 3384/5000 [00:23<00:11, 139.25it/s]Running 5000 simulations.:  68%|██████▊   | 3398/5000 [00:23<00:11, 138.07it/s]Running 5000 simulations.:  68%|██████▊   | 3412/5000 [00:24<00:11, 137.31it/s]Running 5000 simulations.:  69%|██████▊   | 3427/5000 [00:24<00:11, 138.22it/s]Running 5000 simulations.:  69%|██████▉   | 3442/5000 [00:24<00:11, 139.41it/s]Running 5000 simulations.:  69%|██████▉   | 3456/5000 [00:24<00:11, 139.20it/s]Running 5000 simulations.:  69%|██████▉   | 3470/5000 [00:24<00:11, 138.82it/s]Running 5000 simulations.:  70%|██████▉   | 3484/5000 [00:24<00:10, 139.09it/s]Running 5000 simulations.:  70%|██████▉   | 3498/5000 [00:24<00:10, 139.32it/s]Running 5000 simulations.:  70%|███████   | 3513/5000 [00:24<00:10, 139.84it/s]Running 5000 simulations.:  71%|███████   | 3528/5000 [00:24<00:10, 140.55it/s]Running 5000 simulations.:  71%|███████   | 3543/5000 [00:24<00:10, 139.53it/s]Running 5000 simulations.:  71%|███████   | 3557/5000 [00:25<00:10, 138.02it/s]Running 5000 simulations.:  71%|███████▏  | 3571/5000 [00:25<00:10, 137.47it/s]Running 5000 simulations.:  72%|███████▏  | 3585/5000 [00:25<00:10, 136.56it/s]Running 5000 simulations.:  72%|███████▏  | 3600/5000 [00:25<00:10, 139.07it/s]Running 5000 simulations.:  72%|███████▏  | 3616/5000 [00:25<00:09, 143.83it/s]Running 5000 simulations.:  73%|███████▎  | 3632/5000 [00:25<00:09, 146.96it/s]Running 5000 simulations.:  73%|███████▎  | 3647/5000 [00:25<00:09, 145.97it/s]Running 5000 simulations.:  73%|███████▎  | 3662/5000 [00:25<00:09, 144.64it/s]Running 5000 simulations.:  74%|███████▎  | 3677/5000 [00:25<00:09, 142.77it/s]Running 5000 simulations.:  74%|███████▍  | 3692/5000 [00:26<00:09, 140.53it/s]Running 5000 simulations.:  74%|███████▍  | 3707/5000 [00:26<00:09, 139.58it/s]Running 5000 simulations.:  74%|███████▍  | 3721/5000 [00:26<00:09, 138.83it/s]Running 5000 simulations.:  75%|███████▍  | 3735/5000 [00:26<00:09, 138.55it/s]Running 5000 simulations.:  75%|███████▍  | 3749/5000 [00:26<00:09, 138.83it/s]Running 5000 simulations.:  75%|███████▌  | 3763/5000 [00:26<00:08, 139.04it/s]Running 5000 simulations.:  76%|███████▌  | 3777/5000 [00:26<00:08, 139.24it/s]Running 5000 simulations.:  76%|███████▌  | 3791/5000 [00:26<00:08, 139.32it/s]Running 5000 simulations.:  76%|███████▌  | 3806/5000 [00:26<00:08, 139.94it/s]Running 5000 simulations.:  76%|███████▋  | 3820/5000 [00:26<00:08, 138.90it/s]Running 5000 simulations.:  77%|███████▋  | 3834/5000 [00:27<00:08, 137.75it/s]Running 5000 simulations.:  77%|███████▋  | 3848/5000 [00:27<00:08, 137.17it/s]Running 5000 simulations.:  77%|███████▋  | 3862/5000 [00:27<00:08, 137.18it/s]Running 5000 simulations.:  78%|███████▊  | 3876/5000 [00:27<00:08, 136.44it/s]Running 5000 simulations.:  78%|███████▊  | 3891/5000 [00:27<00:08, 137.91it/s]Running 5000 simulations.:  78%|███████▊  | 3906/5000 [00:27<00:07, 138.57it/s]Running 5000 simulations.:  78%|███████▊  | 3920/5000 [00:27<00:07, 138.78it/s]Running 5000 simulations.:  79%|███████▊  | 3935/5000 [00:27<00:07, 139.61it/s]Running 5000 simulations.:  79%|███████▉  | 3949/5000 [00:27<00:07, 138.28it/s]Running 5000 simulations.:  79%|███████▉  | 3963/5000 [00:27<00:07, 138.35it/s]Running 5000 simulations.:  80%|███████▉  | 3978/5000 [00:28<00:07, 139.16it/s]Running 5000 simulations.:  80%|███████▉  | 3993/5000 [00:28<00:07, 139.61it/s]Running 5000 simulations.:  80%|████████  | 4007/5000 [00:28<00:07, 139.46it/s]Running 5000 simulations.:  80%|████████  | 4021/5000 [00:28<00:07, 139.44it/s]Running 5000 simulations.:  81%|████████  | 4036/5000 [00:28<00:06, 140.42it/s]Running 5000 simulations.:  81%|████████  | 4051/5000 [00:28<00:06, 141.07it/s]Running 5000 simulations.:  81%|████████▏ | 4066/5000 [00:28<00:06, 141.16it/s]Running 5000 simulations.:  82%|████████▏ | 4081/5000 [00:28<00:06, 139.60it/s]Running 5000 simulations.:  82%|████████▏ | 4095/5000 [00:28<00:06, 137.87it/s]Running 5000 simulations.:  82%|████████▏ | 4109/5000 [00:29<00:06, 137.71it/s]Running 5000 simulations.:  82%|████████▏ | 4123/5000 [00:29<00:06, 137.55it/s]Running 5000 simulations.:  83%|████████▎ | 4137/5000 [00:29<00:06, 137.21it/s]Running 5000 simulations.:  83%|████████▎ | 4151/5000 [00:29<00:06, 137.38it/s]Running 5000 simulations.:  83%|████████▎ | 4166/5000 [00:29<00:06, 138.51it/s]Running 5000 simulations.:  84%|████████▎ | 4181/5000 [00:29<00:05, 139.00it/s]Running 5000 simulations.:  84%|████████▍ | 4195/5000 [00:29<00:06, 128.86it/s]Running 5000 simulations.:  84%|████████▍ | 4209/5000 [00:29<00:06, 130.01it/s]Running 5000 simulations.:  84%|████████▍ | 4223/5000 [00:29<00:06, 128.46it/s]Running 5000 simulations.:  85%|████████▍ | 4236/5000 [00:29<00:05, 128.58it/s]Running 5000 simulations.:  85%|████████▍ | 4249/5000 [00:30<00:05, 128.35it/s]Running 5000 simulations.:  85%|████████▌ | 4262/5000 [00:30<00:05, 128.45it/s]Running 5000 simulations.:  86%|████████▌ | 4275/5000 [00:30<00:05, 128.37it/s]Running 5000 simulations.:  86%|████████▌ | 4289/5000 [00:30<00:05, 129.56it/s]Running 5000 simulations.:  86%|████████▌ | 4303/5000 [00:30<00:05, 130.59it/s]Running 5000 simulations.:  86%|████████▋ | 4317/5000 [00:30<00:05, 131.38it/s]Running 5000 simulations.:  87%|████████▋ | 4331/5000 [00:30<00:05, 132.00it/s]Running 5000 simulations.:  87%|████████▋ | 4345/5000 [00:30<00:05, 130.68it/s]Running 5000 simulations.:  87%|████████▋ | 4359/5000 [00:30<00:04, 129.54it/s]Running 5000 simulations.:  87%|████████▋ | 4373/5000 [00:31<00:04, 131.79it/s]Running 5000 simulations.:  88%|████████▊ | 4388/5000 [00:31<00:04, 134.84it/s]Running 5000 simulations.:  88%|████████▊ | 4403/5000 [00:31<00:04, 136.52it/s]Running 5000 simulations.:  88%|████████▊ | 4418/5000 [00:31<00:04, 138.19it/s]Running 5000 simulations.:  89%|████████▊ | 4433/5000 [00:31<00:04, 140.04it/s]Running 5000 simulations.:  89%|████████▉ | 4448/5000 [00:31<00:03, 140.87it/s]Running 5000 simulations.:  89%|████████▉ | 4463/5000 [00:31<00:03, 141.72it/s]Running 5000 simulations.:  90%|████████▉ | 4478/5000 [00:31<00:03, 141.50it/s]Running 5000 simulations.:  90%|████████▉ | 4493/5000 [00:31<00:03, 140.69it/s]Running 5000 simulations.:  90%|█████████ | 4508/5000 [00:31<00:03, 141.10it/s]Running 5000 simulations.:  90%|█████████ | 4523/5000 [00:32<00:03, 141.66it/s]Running 5000 simulations.:  91%|█████████ | 4538/5000 [00:32<00:03, 141.79it/s]Running 5000 simulations.:  91%|█████████ | 4553/5000 [00:32<00:03, 141.74it/s]Running 5000 simulations.:  91%|█████████▏| 4568/5000 [00:32<00:03, 141.93it/s]Running 5000 simulations.:  92%|█████████▏| 4583/5000 [00:32<00:02, 142.02it/s]Running 5000 simulations.:  92%|█████████▏| 4598/5000 [00:32<00:02, 142.45it/s]Running 5000 simulations.:  92%|█████████▏| 4613/5000 [00:32<00:02, 142.85it/s]Running 5000 simulations.:  93%|█████████▎| 4628/5000 [00:32<00:02, 142.91it/s]Running 5000 simulations.:  93%|█████████▎| 4643/5000 [00:32<00:02, 143.39it/s]Running 5000 simulations.:  93%|█████████▎| 4658/5000 [00:33<00:02, 143.84it/s]Running 5000 simulations.:  93%|█████████▎| 4673/5000 [00:33<00:02, 143.38it/s]Running 5000 simulations.:  94%|█████████▍| 4688/5000 [00:33<00:02, 142.20it/s]Running 5000 simulations.:  94%|█████████▍| 4703/5000 [00:33<00:02, 142.13it/s]Running 5000 simulations.:  94%|█████████▍| 4718/5000 [00:33<00:01, 142.05it/s]Running 5000 simulations.:  95%|█████████▍| 4733/5000 [00:33<00:01, 141.91it/s]Running 5000 simulations.:  95%|█████████▍| 4748/5000 [00:33<00:01, 141.83it/s]Running 5000 simulations.:  95%|█████████▌| 4763/5000 [00:33<00:01, 142.78it/s]Running 5000 simulations.:  96%|█████████▌| 4778/5000 [00:33<00:01, 143.98it/s]Running 5000 simulations.:  96%|█████████▌| 4793/5000 [00:33<00:01, 144.72it/s]Running 5000 simulations.:  96%|█████████▌| 4808/5000 [00:34<00:01, 143.77it/s]Running 5000 simulations.:  96%|█████████▋| 4823/5000 [00:34<00:01, 142.53it/s]Running 5000 simulations.:  97%|█████████▋| 4838/5000 [00:34<00:01, 142.56it/s]Running 5000 simulations.:  97%|█████████▋| 4853/5000 [00:34<00:01, 142.73it/s]Running 5000 simulations.:  97%|█████████▋| 4868/5000 [00:34<00:00, 142.69it/s]Running 5000 simulations.:  98%|█████████▊| 4883/5000 [00:34<00:00, 143.37it/s]Running 5000 simulations.:  98%|█████████▊| 4898/5000 [00:34<00:00, 143.76it/s]Running 5000 simulations.:  98%|█████████▊| 4913/5000 [00:34<00:00, 143.77it/s]Running 5000 simulations.:  99%|█████████▊| 4928/5000 [00:34<00:00, 143.80it/s]Running 5000 simulations.:  99%|█████████▉| 4943/5000 [00:35<00:00, 144.18it/s]Running 5000 simulations.:  99%|█████████▉| 4958/5000 [00:35<00:00, 142.67it/s]Running 5000 simulations.:  99%|█████████▉| 4973/5000 [00:35<00:00, 141.61it/s]Running 5000 simulations.: 100%|█████████▉| 4988/5000 [00:35<00:00, 142.23it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:35<00:00, 141.18it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 15/5000 [00:00<00:34, 146.49it/s]Running 5000 simulations.:   1%|          | 30/5000 [00:00<00:33, 146.45it/s]Running 5000 simulations.:   1%|          | 45/5000 [00:00<00:33, 146.09it/s]Running 5000 simulations.:   1%|          | 60/5000 [00:00<00:33, 146.03it/s]Running 5000 simulations.:   1%|▏         | 74/5000 [00:00<00:34, 143.31it/s]Running 5000 simulations.:   2%|▏         | 88/5000 [00:00<00:34, 141.07it/s]Running 5000 simulations.:   2%|▏         | 102/5000 [00:00<00:35, 139.92it/s]Running 5000 simulations.:   2%|▏         | 116/5000 [00:00<00:35, 138.92it/s]Running 5000 simulations.:   3%|▎         | 130/5000 [00:00<00:35, 138.43it/s]Running 5000 simulations.:   3%|▎         | 144/5000 [00:01<00:35, 138.63it/s]Running 5000 simulations.:   3%|▎         | 158/5000 [00:01<00:34, 138.48it/s]Running 5000 simulations.:   3%|▎         | 172/5000 [00:01<00:34, 138.53it/s]Running 5000 simulations.:   4%|▎         | 187/5000 [00:01<00:34, 140.05it/s]Running 5000 simulations.:   4%|▍         | 201/5000 [00:01<00:35, 133.78it/s]Running 5000 simulations.:   4%|▍         | 216/5000 [00:01<00:34, 137.59it/s]Running 5000 simulations.:   5%|▍         | 231/5000 [00:01<00:33, 140.49it/s]Running 5000 simulations.:   5%|▍         | 246/5000 [00:01<00:33, 142.52it/s]Running 5000 simulations.:   5%|▌         | 261/5000 [00:01<00:32, 143.93it/s]Running 5000 simulations.:   6%|▌         | 276/5000 [00:01<00:32, 144.49it/s]Running 5000 simulations.:   6%|▌         | 291/5000 [00:02<00:32, 145.29it/s]Running 5000 simulations.:   6%|▌         | 306/5000 [00:02<00:32, 146.25it/s]Running 5000 simulations.:   6%|▋         | 321/5000 [00:02<00:31, 147.01it/s]Running 5000 simulations.:   7%|▋         | 336/5000 [00:02<00:31, 147.40it/s]Running 5000 simulations.:   7%|▋         | 351/5000 [00:02<00:31, 147.62it/s]Running 5000 simulations.:   7%|▋         | 366/5000 [00:02<00:31, 147.89it/s]Running 5000 simulations.:   8%|▊         | 381/5000 [00:02<00:31, 148.28it/s]Running 5000 simulations.:   8%|▊         | 396/5000 [00:02<00:31, 148.06it/s]Running 5000 simulations.:   8%|▊         | 411/5000 [00:02<00:31, 148.02it/s]Running 5000 simulations.:   9%|▊         | 426/5000 [00:02<00:31, 146.58it/s]Running 5000 simulations.:   9%|▉         | 441/5000 [00:03<00:31, 143.64it/s]Running 5000 simulations.:   9%|▉         | 456/5000 [00:03<00:32, 140.91it/s]Running 5000 simulations.:   9%|▉         | 471/5000 [00:03<00:32, 139.37it/s]Running 5000 simulations.:  10%|▉         | 485/5000 [00:03<00:32, 137.74it/s]Running 5000 simulations.:  10%|▉         | 499/5000 [00:03<00:33, 135.82it/s]Running 5000 simulations.:  10%|█         | 513/5000 [00:03<00:33, 135.62it/s]Running 5000 simulations.:  11%|█         | 527/5000 [00:03<00:32, 135.86it/s]Running 5000 simulations.:  11%|█         | 541/5000 [00:03<00:32, 135.97it/s]Running 5000 simulations.:  11%|█         | 556/5000 [00:03<00:32, 138.70it/s]Running 5000 simulations.:  11%|█▏        | 572/5000 [00:04<00:30, 144.29it/s]Running 5000 simulations.:  12%|█▏        | 588/5000 [00:04<00:29, 148.36it/s]Running 5000 simulations.:  12%|█▏        | 604/5000 [00:04<00:29, 148.95it/s]Running 5000 simulations.:  12%|█▏        | 619/5000 [00:04<00:29, 148.87it/s]Running 5000 simulations.:  13%|█▎        | 634/5000 [00:04<00:29, 148.91it/s]Running 5000 simulations.:  13%|█▎        | 649/5000 [00:04<00:29, 149.07it/s]Running 5000 simulations.:  13%|█▎        | 664/5000 [00:04<00:29, 148.34it/s]Running 5000 simulations.:  14%|█▎        | 679/5000 [00:04<00:29, 147.82it/s]Running 5000 simulations.:  14%|█▍        | 694/5000 [00:04<00:29, 147.38it/s]Running 5000 simulations.:  14%|█▍        | 709/5000 [00:04<00:29, 147.06it/s]Running 5000 simulations.:  14%|█▍        | 724/5000 [00:05<00:29, 146.90it/s]Running 5000 simulations.:  15%|█▍        | 739/5000 [00:05<00:28, 147.06it/s]Running 5000 simulations.:  15%|█▌        | 754/5000 [00:05<00:28, 147.27it/s]Running 5000 simulations.:  15%|█▌        | 769/5000 [00:05<00:28, 147.15it/s]Running 5000 simulations.:  16%|█▌        | 784/5000 [00:05<00:28, 146.77it/s]Running 5000 simulations.:  16%|█▌        | 799/5000 [00:05<00:28, 146.67it/s]Running 5000 simulations.:  16%|█▋        | 814/5000 [00:05<00:28, 147.41it/s]Running 5000 simulations.:  17%|█▋        | 829/5000 [00:05<00:28, 147.59it/s]Running 5000 simulations.:  17%|█▋        | 844/5000 [00:05<00:28, 147.98it/s]Running 5000 simulations.:  17%|█▋        | 859/5000 [00:05<00:27, 148.09it/s]Running 5000 simulations.:  17%|█▋        | 874/5000 [00:06<00:27, 147.55it/s]Running 5000 simulations.:  18%|█▊        | 889/5000 [00:06<00:27, 147.06it/s]Running 5000 simulations.:  18%|█▊        | 904/5000 [00:06<00:27, 146.83it/s]Running 5000 simulations.:  18%|█▊        | 919/5000 [00:06<00:27, 147.27it/s]Running 5000 simulations.:  19%|█▊        | 934/5000 [00:06<00:27, 147.69it/s]Running 5000 simulations.:  19%|█▉        | 949/5000 [00:06<00:27, 148.05it/s]Running 5000 simulations.:  19%|█▉        | 964/5000 [00:06<00:27, 148.19it/s]Running 5000 simulations.:  20%|█▉        | 979/5000 [00:06<00:27, 147.56it/s]Running 5000 simulations.:  20%|█▉        | 994/5000 [00:06<00:27, 147.12it/s]Running 5000 simulations.:  20%|██        | 1009/5000 [00:06<00:27, 146.92it/s]Running 5000 simulations.:  20%|██        | 1024/5000 [00:07<00:27, 146.55it/s]Running 5000 simulations.:  21%|██        | 1039/5000 [00:07<00:27, 146.49it/s]Running 5000 simulations.:  21%|██        | 1054/5000 [00:07<00:26, 146.55it/s]Running 5000 simulations.:  21%|██▏       | 1069/5000 [00:07<00:26, 146.68it/s]Running 5000 simulations.:  22%|██▏       | 1084/5000 [00:07<00:26, 146.68it/s]Running 5000 simulations.:  22%|██▏       | 1099/5000 [00:07<00:26, 146.48it/s]Running 5000 simulations.:  22%|██▏       | 1114/5000 [00:07<00:26, 146.37it/s]Running 5000 simulations.:  23%|██▎       | 1129/5000 [00:07<00:26, 146.25it/s]Running 5000 simulations.:  23%|██▎       | 1144/5000 [00:07<00:26, 146.13it/s]Running 5000 simulations.:  23%|██▎       | 1159/5000 [00:08<00:26, 146.09it/s]Running 5000 simulations.:  23%|██▎       | 1174/5000 [00:08<00:26, 145.83it/s]Running 5000 simulations.:  24%|██▍       | 1189/5000 [00:08<00:26, 145.80it/s]Running 5000 simulations.:  24%|██▍       | 1204/5000 [00:08<00:26, 145.79it/s]Running 5000 simulations.:  24%|██▍       | 1219/5000 [00:08<00:25, 146.00it/s]Running 5000 simulations.:  25%|██▍       | 1234/5000 [00:08<00:25, 146.09it/s]Running 5000 simulations.:  25%|██▍       | 1249/5000 [00:08<00:25, 146.18it/s]Running 5000 simulations.:  25%|██▌       | 1264/5000 [00:08<00:25, 146.19it/s]Running 5000 simulations.:  26%|██▌       | 1279/5000 [00:08<00:25, 146.57it/s]Running 5000 simulations.:  26%|██▌       | 1294/5000 [00:08<00:25, 146.49it/s]Running 5000 simulations.:  26%|██▌       | 1309/5000 [00:09<00:25, 146.56it/s]Running 5000 simulations.:  26%|██▋       | 1324/5000 [00:09<00:25, 146.87it/s]Running 5000 simulations.:  27%|██▋       | 1339/5000 [00:09<00:24, 146.67it/s]Running 5000 simulations.:  27%|██▋       | 1354/5000 [00:09<00:24, 146.84it/s]Running 5000 simulations.:  27%|██▋       | 1369/5000 [00:09<00:24, 146.36it/s]Running 5000 simulations.:  28%|██▊       | 1384/5000 [00:09<00:24, 146.29it/s]Running 5000 simulations.:  28%|██▊       | 1399/5000 [00:09<00:24, 146.39it/s]Running 5000 simulations.:  28%|██▊       | 1414/5000 [00:09<00:24, 146.16it/s]Running 5000 simulations.:  29%|██▊       | 1429/5000 [00:09<00:24, 146.02it/s]Running 5000 simulations.:  29%|██▉       | 1444/5000 [00:09<00:24, 146.00it/s]Running 5000 simulations.:  29%|██▉       | 1459/5000 [00:10<00:24, 146.06it/s]Running 5000 simulations.:  29%|██▉       | 1474/5000 [00:10<00:24, 146.05it/s]Running 5000 simulations.:  30%|██▉       | 1489/5000 [00:10<00:24, 146.07it/s]Running 5000 simulations.:  30%|███       | 1504/5000 [00:10<00:24, 145.47it/s]Running 5000 simulations.:  30%|███       | 1519/5000 [00:10<00:24, 145.00it/s]Running 5000 simulations.:  31%|███       | 1534/5000 [00:10<00:23, 144.76it/s]Running 5000 simulations.:  31%|███       | 1549/5000 [00:10<00:23, 144.71it/s]Running 5000 simulations.:  31%|███▏      | 1564/5000 [00:10<00:23, 144.94it/s]Running 5000 simulations.:  32%|███▏      | 1579/5000 [00:10<00:23, 144.86it/s]Running 5000 simulations.:  32%|███▏      | 1594/5000 [00:10<00:23, 145.08it/s]Running 5000 simulations.:  32%|███▏      | 1609/5000 [00:11<00:23, 144.94it/s]Running 5000 simulations.:  32%|███▏      | 1624/5000 [00:11<00:23, 144.47it/s]Running 5000 simulations.:  33%|███▎      | 1639/5000 [00:11<00:23, 143.57it/s]Running 5000 simulations.:  33%|███▎      | 1654/5000 [00:11<00:23, 143.58it/s]Running 5000 simulations.:  33%|███▎      | 1669/5000 [00:11<00:23, 143.81it/s]Running 5000 simulations.:  34%|███▎      | 1684/5000 [00:11<00:23, 144.00it/s]Running 5000 simulations.:  34%|███▍      | 1699/5000 [00:11<00:22, 144.23it/s]Running 5000 simulations.:  34%|███▍      | 1714/5000 [00:11<00:22, 144.52it/s]Running 5000 simulations.:  35%|███▍      | 1729/5000 [00:11<00:22, 145.44it/s]Running 5000 simulations.:  35%|███▍      | 1744/5000 [00:12<00:22, 145.79it/s]Running 5000 simulations.:  35%|███▌      | 1759/5000 [00:12<00:22, 145.98it/s]Running 5000 simulations.:  35%|███▌      | 1774/5000 [00:12<00:22, 145.79it/s]Running 5000 simulations.:  36%|███▌      | 1789/5000 [00:12<00:22, 145.61it/s]Running 5000 simulations.:  36%|███▌      | 1804/5000 [00:12<00:21, 145.31it/s]Running 5000 simulations.:  36%|███▋      | 1819/5000 [00:12<00:21, 144.87it/s]Running 5000 simulations.:  37%|███▋      | 1834/5000 [00:12<00:21, 144.77it/s]Running 5000 simulations.:  37%|███▋      | 1849/5000 [00:12<00:21, 144.67it/s]Running 5000 simulations.:  37%|███▋      | 1864/5000 [00:12<00:21, 144.47it/s]Running 5000 simulations.:  38%|███▊      | 1879/5000 [00:12<00:21, 144.58it/s]Running 5000 simulations.:  38%|███▊      | 1894/5000 [00:13<00:21, 145.18it/s]Running 5000 simulations.:  38%|███▊      | 1909/5000 [00:13<00:21, 145.78it/s]Running 5000 simulations.:  38%|███▊      | 1924/5000 [00:13<00:21, 146.08it/s]Running 5000 simulations.:  39%|███▉      | 1939/5000 [00:13<00:20, 146.24it/s]Running 5000 simulations.:  39%|███▉      | 1954/5000 [00:13<00:21, 142.82it/s]Running 5000 simulations.:  39%|███▉      | 1969/5000 [00:13<00:21, 140.04it/s]Running 5000 simulations.:  40%|███▉      | 1984/5000 [00:13<00:21, 137.51it/s]Running 5000 simulations.:  40%|███▉      | 1998/5000 [00:13<00:22, 135.76it/s]Running 5000 simulations.:  40%|████      | 2012/5000 [00:13<00:22, 134.51it/s]Running 5000 simulations.:  41%|████      | 2026/5000 [00:14<00:22, 134.58it/s]Running 5000 simulations.:  41%|████      | 2040/5000 [00:14<00:22, 134.07it/s]Running 5000 simulations.:  41%|████      | 2054/5000 [00:14<00:21, 135.18it/s]Running 5000 simulations.:  41%|████▏     | 2068/5000 [00:14<00:21, 136.40it/s]Running 5000 simulations.:  42%|████▏     | 2082/5000 [00:14<00:21, 137.10it/s]Running 5000 simulations.:  42%|████▏     | 2097/5000 [00:14<00:20, 138.40it/s]Running 5000 simulations.:  42%|████▏     | 2111/5000 [00:14<00:20, 138.45it/s]Running 5000 simulations.:  42%|████▎     | 2125/5000 [00:14<00:20, 137.12it/s]Running 5000 simulations.:  43%|████▎     | 2139/5000 [00:14<00:20, 136.46it/s]Running 5000 simulations.:  43%|████▎     | 2153/5000 [00:14<00:20, 135.85it/s]Running 5000 simulations.:  43%|████▎     | 2167/5000 [00:15<00:20, 135.54it/s]Running 5000 simulations.:  44%|████▎     | 2181/5000 [00:15<00:20, 134.67it/s]Running 5000 simulations.:  44%|████▍     | 2195/5000 [00:15<00:20, 134.16it/s]Running 5000 simulations.:  44%|████▍     | 2209/5000 [00:15<00:20, 134.15it/s]Running 5000 simulations.:  44%|████▍     | 2223/5000 [00:15<00:20, 134.17it/s]Running 5000 simulations.:  45%|████▍     | 2237/5000 [00:15<00:20, 134.51it/s]Running 5000 simulations.:  45%|████▌     | 2251/5000 [00:15<00:20, 135.70it/s]Running 5000 simulations.:  45%|████▌     | 2266/5000 [00:15<00:19, 138.67it/s]Running 5000 simulations.:  46%|████▌     | 2281/5000 [00:15<00:19, 140.86it/s]Running 5000 simulations.:  46%|████▌     | 2296/5000 [00:15<00:18, 142.35it/s]Running 5000 simulations.:  46%|████▌     | 2311/5000 [00:16<00:18, 143.16it/s]Running 5000 simulations.:  47%|████▋     | 2326/5000 [00:16<00:18, 143.66it/s]Running 5000 simulations.:  47%|████▋     | 2341/5000 [00:16<00:18, 144.13it/s]Running 5000 simulations.:  47%|████▋     | 2356/5000 [00:16<00:18, 144.64it/s]Running 5000 simulations.:  47%|████▋     | 2371/5000 [00:16<00:18, 144.89it/s]Running 5000 simulations.:  48%|████▊     | 2386/5000 [00:16<00:18, 144.83it/s]Running 5000 simulations.:  48%|████▊     | 2401/5000 [00:16<00:17, 145.07it/s]Running 5000 simulations.:  48%|████▊     | 2416/5000 [00:16<00:17, 145.40it/s]Running 5000 simulations.:  49%|████▊     | 2431/5000 [00:16<00:17, 145.40it/s]Running 5000 simulations.:  49%|████▉     | 2446/5000 [00:17<00:17, 145.65it/s]Running 5000 simulations.:  49%|████▉     | 2461/5000 [00:17<00:17, 145.75it/s]Running 5000 simulations.:  50%|████▉     | 2476/5000 [00:17<00:17, 145.96it/s]Running 5000 simulations.:  50%|████▉     | 2491/5000 [00:17<00:17, 145.76it/s]Running 5000 simulations.:  50%|█████     | 2506/5000 [00:17<00:17, 145.73it/s]Running 5000 simulations.:  50%|█████     | 2521/5000 [00:17<00:17, 145.32it/s]Running 5000 simulations.:  51%|█████     | 2536/5000 [00:17<00:16, 145.21it/s]Running 5000 simulations.:  51%|█████     | 2551/5000 [00:17<00:16, 145.27it/s]Running 5000 simulations.:  51%|█████▏    | 2566/5000 [00:17<00:16, 145.23it/s]Running 5000 simulations.:  52%|█████▏    | 2581/5000 [00:17<00:16, 145.32it/s]Running 5000 simulations.:  52%|█████▏    | 2596/5000 [00:18<00:16, 145.28it/s]Running 5000 simulations.:  52%|█████▏    | 2611/5000 [00:18<00:16, 145.51it/s]Running 5000 simulations.:  53%|█████▎    | 2626/5000 [00:18<00:16, 145.63it/s]Running 5000 simulations.:  53%|█████▎    | 2641/5000 [00:18<00:16, 145.52it/s]Running 5000 simulations.:  53%|█████▎    | 2656/5000 [00:18<00:16, 145.64it/s]Running 5000 simulations.:  53%|█████▎    | 2671/5000 [00:18<00:15, 145.66it/s]Running 5000 simulations.:  54%|█████▎    | 2686/5000 [00:18<00:15, 145.95it/s]Running 5000 simulations.:  54%|█████▍    | 2701/5000 [00:18<00:15, 146.23it/s]Running 5000 simulations.:  54%|█████▍    | 2716/5000 [00:18<00:15, 146.55it/s]Running 5000 simulations.:  55%|█████▍    | 2731/5000 [00:18<00:15, 146.48it/s]Running 5000 simulations.:  55%|█████▍    | 2746/5000 [00:19<00:15, 146.26it/s]Running 5000 simulations.:  55%|█████▌    | 2761/5000 [00:19<00:15, 146.00it/s]Running 5000 simulations.:  56%|█████▌    | 2776/5000 [00:19<00:15, 145.93it/s]Running 5000 simulations.:  56%|█████▌    | 2791/5000 [00:19<00:15, 145.59it/s]Running 5000 simulations.:  56%|█████▌    | 2806/5000 [00:19<00:15, 145.61it/s]Running 5000 simulations.:  56%|█████▋    | 2821/5000 [00:19<00:14, 145.57it/s]Running 5000 simulations.:  57%|█████▋    | 2836/5000 [00:19<00:14, 145.87it/s]Running 5000 simulations.:  57%|█████▋    | 2851/5000 [00:19<00:14, 145.91it/s]Running 5000 simulations.:  57%|█████▋    | 2866/5000 [00:19<00:14, 145.64it/s]Running 5000 simulations.:  58%|█████▊    | 2881/5000 [00:19<00:14, 145.47it/s]Running 5000 simulations.:  58%|█████▊    | 2896/5000 [00:20<00:14, 145.17it/s]Running 5000 simulations.:  58%|█████▊    | 2911/5000 [00:20<00:14, 145.57it/s]Running 5000 simulations.:  59%|█████▊    | 2926/5000 [00:20<00:14, 146.39it/s]Running 5000 simulations.:  59%|█████▉    | 2941/5000 [00:20<00:14, 146.96it/s]Running 5000 simulations.:  59%|█████▉    | 2956/5000 [00:20<00:13, 147.62it/s]Running 5000 simulations.:  59%|█████▉    | 2971/5000 [00:20<00:13, 147.33it/s]Running 5000 simulations.:  60%|█████▉    | 2986/5000 [00:20<00:13, 146.84it/s]Running 5000 simulations.:  60%|██████    | 3001/5000 [00:20<00:13, 146.65it/s]Running 5000 simulations.:  60%|██████    | 3016/5000 [00:20<00:13, 146.47it/s]Running 5000 simulations.:  61%|██████    | 3031/5000 [00:21<00:13, 146.41it/s]Running 5000 simulations.:  61%|██████    | 3046/5000 [00:21<00:13, 146.75it/s]Running 5000 simulations.:  61%|██████    | 3061/5000 [00:21<00:13, 147.08it/s]Running 5000 simulations.:  62%|██████▏   | 3076/5000 [00:21<00:13, 147.50it/s]Running 5000 simulations.:  62%|██████▏   | 3091/5000 [00:21<00:12, 147.65it/s]Running 5000 simulations.:  62%|██████▏   | 3106/5000 [00:21<00:12, 147.07it/s]Running 5000 simulations.:  62%|██████▏   | 3121/5000 [00:21<00:12, 146.86it/s]Running 5000 simulations.:  63%|██████▎   | 3136/5000 [00:21<00:12, 146.37it/s]Running 5000 simulations.:  63%|██████▎   | 3151/5000 [00:21<00:12, 146.42it/s]Running 5000 simulations.:  63%|██████▎   | 3166/5000 [00:21<00:12, 146.69it/s]Running 5000 simulations.:  64%|██████▎   | 3181/5000 [00:22<00:12, 146.49it/s]Running 5000 simulations.:  64%|██████▍   | 3196/5000 [00:22<00:12, 146.39it/s]Running 5000 simulations.:  64%|██████▍   | 3211/5000 [00:22<00:12, 145.98it/s]Running 5000 simulations.:  65%|██████▍   | 3226/5000 [00:22<00:12, 145.46it/s]Running 5000 simulations.:  65%|██████▍   | 3241/5000 [00:22<00:12, 144.66it/s]Running 5000 simulations.:  65%|██████▌   | 3256/5000 [00:22<00:12, 144.46it/s]Running 5000 simulations.:  65%|██████▌   | 3271/5000 [00:22<00:11, 144.42it/s]Running 5000 simulations.:  66%|██████▌   | 3286/5000 [00:22<00:11, 144.74it/s]Running 5000 simulations.:  66%|██████▌   | 3301/5000 [00:22<00:11, 145.21it/s]Running 5000 simulations.:  66%|██████▋   | 3316/5000 [00:22<00:11, 145.09it/s]Running 5000 simulations.:  67%|██████▋   | 3331/5000 [00:23<00:11, 145.04it/s]Running 5000 simulations.:  67%|██████▋   | 3346/5000 [00:23<00:11, 144.88it/s]Running 5000 simulations.:  67%|██████▋   | 3361/5000 [00:23<00:11, 144.76it/s]Running 5000 simulations.:  68%|██████▊   | 3376/5000 [00:23<00:11, 144.47it/s]Running 5000 simulations.:  68%|██████▊   | 3391/5000 [00:23<00:11, 144.33it/s]Running 5000 simulations.:  68%|██████▊   | 3406/5000 [00:23<00:11, 144.46it/s]Running 5000 simulations.:  68%|██████▊   | 3421/5000 [00:23<00:10, 144.75it/s]Running 5000 simulations.:  69%|██████▊   | 3436/5000 [00:23<00:10, 144.89it/s]Running 5000 simulations.:  69%|██████▉   | 3451/5000 [00:23<00:10, 145.13it/s]Running 5000 simulations.:  69%|██████▉   | 3466/5000 [00:24<00:10, 145.32it/s]Running 5000 simulations.:  70%|██████▉   | 3481/5000 [00:24<00:10, 143.66it/s]Running 5000 simulations.:  70%|██████▉   | 3496/5000 [00:24<00:10, 142.64it/s]Running 5000 simulations.:  70%|███████   | 3511/5000 [00:24<00:10, 139.74it/s]Running 5000 simulations.:  71%|███████   | 3526/5000 [00:24<00:10, 141.93it/s]Running 5000 simulations.:  71%|███████   | 3541/5000 [00:24<00:10, 143.23it/s]Running 5000 simulations.:  71%|███████   | 3556/5000 [00:24<00:10, 144.01it/s]Running 5000 simulations.:  71%|███████▏  | 3571/5000 [00:24<00:09, 144.67it/s]Running 5000 simulations.:  72%|███████▏  | 3586/5000 [00:24<00:09, 145.49it/s]Running 5000 simulations.:  72%|███████▏  | 3601/5000 [00:24<00:09, 146.08it/s]Running 5000 simulations.:  72%|███████▏  | 3616/5000 [00:25<00:09, 146.43it/s]Running 5000 simulations.:  73%|███████▎  | 3631/5000 [00:25<00:09, 146.65it/s]Running 5000 simulations.:  73%|███████▎  | 3646/5000 [00:25<00:09, 147.09it/s]Running 5000 simulations.:  73%|███████▎  | 3661/5000 [00:25<00:09, 147.47it/s]Running 5000 simulations.:  74%|███████▎  | 3676/5000 [00:25<00:09, 146.41it/s]Running 5000 simulations.:  74%|███████▍  | 3691/5000 [00:25<00:08, 146.37it/s]Running 5000 simulations.:  74%|███████▍  | 3706/5000 [00:25<00:08, 146.25it/s]Running 5000 simulations.:  74%|███████▍  | 3721/5000 [00:25<00:08, 146.80it/s]Running 5000 simulations.:  75%|███████▍  | 3736/5000 [00:25<00:08, 146.94it/s]Running 5000 simulations.:  75%|███████▌  | 3751/5000 [00:25<00:08, 146.61it/s]Running 5000 simulations.:  75%|███████▌  | 3766/5000 [00:26<00:08, 146.58it/s]Running 5000 simulations.:  76%|███████▌  | 3781/5000 [00:26<00:08, 146.59it/s]Running 5000 simulations.:  76%|███████▌  | 3796/5000 [00:26<00:08, 146.46it/s]Running 5000 simulations.:  76%|███████▌  | 3811/5000 [00:26<00:08, 146.33it/s]Running 5000 simulations.:  77%|███████▋  | 3826/5000 [00:26<00:08, 146.37it/s]Running 5000 simulations.:  77%|███████▋  | 3841/5000 [00:26<00:07, 146.30it/s]Running 5000 simulations.:  77%|███████▋  | 3856/5000 [00:26<00:07, 146.26it/s]Running 5000 simulations.:  77%|███████▋  | 3871/5000 [00:26<00:07, 145.34it/s]Running 5000 simulations.:  78%|███████▊  | 3886/5000 [00:26<00:07, 144.98it/s]Running 5000 simulations.:  78%|███████▊  | 3901/5000 [00:27<00:07, 145.33it/s]Running 5000 simulations.:  78%|███████▊  | 3916/5000 [00:27<00:07, 145.62it/s]Running 5000 simulations.:  79%|███████▊  | 3931/5000 [00:27<00:07, 145.89it/s]Running 5000 simulations.:  79%|███████▉  | 3946/5000 [00:27<00:07, 145.88it/s]Running 5000 simulations.:  79%|███████▉  | 3961/5000 [00:27<00:07, 146.26it/s]Running 5000 simulations.:  80%|███████▉  | 3976/5000 [00:27<00:06, 146.62it/s]Running 5000 simulations.:  80%|███████▉  | 3991/5000 [00:27<00:06, 145.86it/s]Running 5000 simulations.:  80%|████████  | 4006/5000 [00:27<00:06, 145.25it/s]Running 5000 simulations.:  80%|████████  | 4021/5000 [00:27<00:06, 145.59it/s]Running 5000 simulations.:  81%|████████  | 4036/5000 [00:27<00:06, 145.80it/s]Running 5000 simulations.:  81%|████████  | 4051/5000 [00:28<00:06, 145.83it/s]Running 5000 simulations.:  81%|████████▏ | 4066/5000 [00:28<00:06, 145.90it/s]Running 5000 simulations.:  82%|████████▏ | 4081/5000 [00:28<00:06, 146.11it/s]Running 5000 simulations.:  82%|████████▏ | 4096/5000 [00:28<00:06, 146.59it/s]Running 5000 simulations.:  82%|████████▏ | 4111/5000 [00:28<00:06, 146.37it/s]Running 5000 simulations.:  83%|████████▎ | 4126/5000 [00:28<00:05, 146.37it/s]Running 5000 simulations.:  83%|████████▎ | 4141/5000 [00:28<00:05, 146.59it/s]Running 5000 simulations.:  83%|████████▎ | 4156/5000 [00:28<00:05, 146.52it/s]Running 5000 simulations.:  83%|████████▎ | 4171/5000 [00:28<00:05, 146.40it/s]Running 5000 simulations.:  84%|████████▎ | 4186/5000 [00:28<00:05, 146.91it/s]Running 5000 simulations.:  84%|████████▍ | 4201/5000 [00:29<00:05, 146.90it/s]Running 5000 simulations.:  84%|████████▍ | 4216/5000 [00:29<00:05, 147.15it/s]Running 5000 simulations.:  85%|████████▍ | 4231/5000 [00:29<00:05, 147.17it/s]Running 5000 simulations.:  85%|████████▍ | 4246/5000 [00:29<00:05, 147.42it/s]Running 5000 simulations.:  85%|████████▌ | 4261/5000 [00:29<00:05, 147.66it/s]Running 5000 simulations.:  86%|████████▌ | 4276/5000 [00:29<00:04, 147.70it/s]Running 5000 simulations.:  86%|████████▌ | 4291/5000 [00:29<00:04, 147.25it/s]Running 5000 simulations.:  86%|████████▌ | 4306/5000 [00:29<00:04, 146.88it/s]Running 5000 simulations.:  86%|████████▋ | 4321/5000 [00:29<00:04, 146.88it/s]Running 5000 simulations.:  87%|████████▋ | 4336/5000 [00:29<00:04, 146.33it/s]Running 5000 simulations.:  87%|████████▋ | 4351/5000 [00:30<00:04, 146.07it/s]Running 5000 simulations.:  87%|████████▋ | 4366/5000 [00:30<00:04, 146.85it/s]Running 5000 simulations.:  88%|████████▊ | 4381/5000 [00:30<00:04, 145.56it/s]Running 5000 simulations.:  88%|████████▊ | 4396/5000 [00:30<00:04, 144.17it/s]Running 5000 simulations.:  88%|████████▊ | 4411/5000 [00:30<00:04, 144.04it/s]Running 5000 simulations.:  89%|████████▊ | 4426/5000 [00:30<00:03, 144.47it/s]Running 5000 simulations.:  89%|████████▉ | 4441/5000 [00:30<00:03, 144.83it/s]Running 5000 simulations.:  89%|████████▉ | 4456/5000 [00:30<00:03, 145.50it/s]Running 5000 simulations.:  89%|████████▉ | 4471/5000 [00:30<00:03, 145.58it/s]Running 5000 simulations.:  90%|████████▉ | 4486/5000 [00:31<00:03, 145.74it/s]Running 5000 simulations.:  90%|█████████ | 4501/5000 [00:31<00:03, 146.02it/s]Running 5000 simulations.:  90%|█████████ | 4516/5000 [00:31<00:03, 146.06it/s]Running 5000 simulations.:  91%|█████████ | 4531/5000 [00:31<00:03, 145.81it/s]Running 5000 simulations.:  91%|█████████ | 4546/5000 [00:31<00:03, 138.10it/s]Running 5000 simulations.:  91%|█████████ | 4561/5000 [00:31<00:03, 140.45it/s]Running 5000 simulations.:  92%|█████████▏| 4576/5000 [00:31<00:02, 142.44it/s]Running 5000 simulations.:  92%|█████████▏| 4591/5000 [00:31<00:02, 144.20it/s]Running 5000 simulations.:  92%|█████████▏| 4606/5000 [00:31<00:02, 144.79it/s]Running 5000 simulations.:  92%|█████████▏| 4621/5000 [00:31<00:02, 145.10it/s]Running 5000 simulations.:  93%|█████████▎| 4636/5000 [00:32<00:02, 145.39it/s]Running 5000 simulations.:  93%|█████████▎| 4651/5000 [00:32<00:02, 145.51it/s]Running 5000 simulations.:  93%|█████████▎| 4666/5000 [00:32<00:02, 145.62it/s]Running 5000 simulations.:  94%|█████████▎| 4681/5000 [00:32<00:02, 145.68it/s]Running 5000 simulations.:  94%|█████████▍| 4696/5000 [00:32<00:02, 145.87it/s]Running 5000 simulations.:  94%|█████████▍| 4711/5000 [00:32<00:01, 146.07it/s]Running 5000 simulations.:  95%|█████████▍| 4726/5000 [00:32<00:01, 146.09it/s]Running 5000 simulations.:  95%|█████████▍| 4741/5000 [00:32<00:01, 145.92it/s]Running 5000 simulations.:  95%|█████████▌| 4756/5000 [00:32<00:01, 145.98it/s]Running 5000 simulations.:  95%|█████████▌| 4771/5000 [00:32<00:01, 146.34it/s]Running 5000 simulations.:  96%|█████████▌| 4786/5000 [00:33<00:01, 146.56it/s]Running 5000 simulations.:  96%|█████████▌| 4801/5000 [00:33<00:01, 146.58it/s]Running 5000 simulations.:  96%|█████████▋| 4816/5000 [00:33<00:01, 146.74it/s]Running 5000 simulations.:  97%|█████████▋| 4831/5000 [00:33<00:01, 146.92it/s]Running 5000 simulations.:  97%|█████████▋| 4846/5000 [00:33<00:01, 147.01it/s]Running 5000 simulations.:  97%|█████████▋| 4861/5000 [00:33<00:00, 147.43it/s]Running 5000 simulations.:  98%|█████████▊| 4876/5000 [00:33<00:00, 147.53it/s]Running 5000 simulations.:  98%|█████████▊| 4891/5000 [00:33<00:00, 147.38it/s]Running 5000 simulations.:  98%|█████████▊| 4906/5000 [00:33<00:00, 146.81it/s]Running 5000 simulations.:  98%|█████████▊| 4921/5000 [00:33<00:00, 146.45it/s]Running 5000 simulations.:  99%|█████████▊| 4936/5000 [00:34<00:00, 146.18it/s]Running 5000 simulations.:  99%|█████████▉| 4951/5000 [00:34<00:00, 146.17it/s]Running 5000 simulations.:  99%|█████████▉| 4966/5000 [00:34<00:00, 146.46it/s]Running 5000 simulations.: 100%|█████████▉| 4981/5000 [00:34<00:00, 146.57it/s]Running 5000 simulations.: 100%|█████████▉| 4996/5000 [00:34<00:00, 146.76it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:34<00:00, 144.80it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:32, 151.76it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:32, 151.98it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:32, 151.85it/s]Running 5000 simulations.:   1%|▏         | 64/5000 [00:00<00:32, 151.41it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:32, 151.40it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:32, 151.55it/s]Running 5000 simulations.:   2%|▏         | 112/5000 [00:00<00:32, 151.69it/s]Running 5000 simulations.:   3%|▎         | 128/5000 [00:00<00:32, 151.75it/s]Running 5000 simulations.:   3%|▎         | 143/5000 [00:00<00:32, 151.13it/s]Running 5000 simulations.:   3%|▎         | 159/5000 [00:01<00:32, 150.87it/s]Running 5000 simulations.:   3%|▎         | 174/5000 [00:01<00:32, 150.35it/s]Running 5000 simulations.:   4%|▍         | 189/5000 [00:01<00:32, 149.89it/s]Running 5000 simulations.:   4%|▍         | 205/5000 [00:01<00:31, 150.35it/s]Running 5000 simulations.:   4%|▍         | 220/5000 [00:01<00:31, 150.20it/s]Running 5000 simulations.:   5%|▍         | 236/5000 [00:01<00:31, 150.48it/s]Running 5000 simulations.:   5%|▌         | 252/5000 [00:01<00:31, 150.52it/s]Running 5000 simulations.:   5%|▌         | 268/5000 [00:01<00:31, 150.43it/s]Running 5000 simulations.:   6%|▌         | 284/5000 [00:01<00:31, 150.29it/s]Running 5000 simulations.:   6%|▌         | 300/5000 [00:01<00:31, 149.95it/s]Running 5000 simulations.:   6%|▋         | 316/5000 [00:02<00:31, 150.21it/s]Running 5000 simulations.:   7%|▋         | 332/5000 [00:02<00:31, 149.96it/s]Running 5000 simulations.:   7%|▋         | 347/5000 [00:02<00:31, 149.42it/s]Running 5000 simulations.:   7%|▋         | 362/5000 [00:02<00:31, 148.76it/s]Running 5000 simulations.:   8%|▊         | 377/5000 [00:02<00:31, 148.98it/s]Running 5000 simulations.:   8%|▊         | 392/5000 [00:02<00:30, 148.65it/s]Running 5000 simulations.:   8%|▊         | 407/5000 [00:02<00:30, 148.44it/s]Running 5000 simulations.:   8%|▊         | 422/5000 [00:02<00:30, 148.74it/s]Running 5000 simulations.:   9%|▊         | 437/5000 [00:02<00:30, 148.16it/s]Running 5000 simulations.:   9%|▉         | 452/5000 [00:03<00:30, 147.75it/s]Running 5000 simulations.:   9%|▉         | 467/5000 [00:03<00:30, 148.23it/s]Running 5000 simulations.:  10%|▉         | 482/5000 [00:03<00:30, 148.12it/s]Running 5000 simulations.:  10%|▉         | 497/5000 [00:03<00:30, 148.36it/s]Running 5000 simulations.:  10%|█         | 512/5000 [00:03<00:30, 148.22it/s]Running 5000 simulations.:  11%|█         | 527/5000 [00:03<00:30, 148.06it/s]Running 5000 simulations.:  11%|█         | 542/5000 [00:03<00:30, 148.14it/s]Running 5000 simulations.:  11%|█         | 557/5000 [00:03<00:30, 147.29it/s]Running 5000 simulations.:  11%|█▏        | 572/5000 [00:03<00:30, 147.33it/s]Running 5000 simulations.:  12%|█▏        | 587/5000 [00:03<00:29, 147.80it/s]Running 5000 simulations.:  12%|█▏        | 602/5000 [00:04<00:29, 148.33it/s]Running 5000 simulations.:  12%|█▏        | 617/5000 [00:04<00:29, 147.55it/s]Running 5000 simulations.:  13%|█▎        | 632/5000 [00:04<00:29, 146.98it/s]Running 5000 simulations.:  13%|█▎        | 647/5000 [00:04<00:29, 147.18it/s]Running 5000 simulations.:  13%|█▎        | 662/5000 [00:04<00:29, 147.13it/s]Running 5000 simulations.:  14%|█▎        | 677/5000 [00:04<00:29, 146.99it/s]Running 5000 simulations.:  14%|█▍        | 692/5000 [00:04<00:29, 147.11it/s]Running 5000 simulations.:  14%|█▍        | 707/5000 [00:04<00:29, 147.01it/s]Running 5000 simulations.:  14%|█▍        | 722/5000 [00:04<00:29, 147.11it/s]Running 5000 simulations.:  15%|█▍        | 737/5000 [00:04<00:28, 147.28it/s]Running 5000 simulations.:  15%|█▌        | 752/5000 [00:05<00:28, 147.22it/s]Running 5000 simulations.:  15%|█▌        | 767/5000 [00:05<00:28, 147.89it/s]Running 5000 simulations.:  16%|█▌        | 782/5000 [00:05<00:28, 148.23it/s]Running 5000 simulations.:  16%|█▌        | 797/5000 [00:05<00:28, 148.49it/s]Running 5000 simulations.:  16%|█▌        | 812/5000 [00:05<00:28, 147.89it/s]Running 5000 simulations.:  17%|█▋        | 827/5000 [00:05<00:28, 147.70it/s]Running 5000 simulations.:  17%|█▋        | 842/5000 [00:05<00:28, 147.46it/s]Running 5000 simulations.:  17%|█▋        | 857/5000 [00:05<00:28, 147.92it/s]Running 5000 simulations.:  17%|█▋        | 872/5000 [00:05<00:27, 147.70it/s]Running 5000 simulations.:  18%|█▊        | 887/5000 [00:05<00:27, 147.62it/s]Running 5000 simulations.:  18%|█▊        | 902/5000 [00:06<00:27, 147.83it/s]Running 5000 simulations.:  18%|█▊        | 917/5000 [00:06<00:27, 147.73it/s]Running 5000 simulations.:  19%|█▊        | 932/5000 [00:06<00:27, 148.05it/s]Running 5000 simulations.:  19%|█▉        | 947/5000 [00:06<00:27, 147.73it/s]Running 5000 simulations.:  19%|█▉        | 962/5000 [00:06<00:27, 147.90it/s]Running 5000 simulations.:  20%|█▉        | 977/5000 [00:06<00:27, 147.70it/s]Running 5000 simulations.:  20%|█▉        | 992/5000 [00:06<00:27, 147.39it/s]Running 5000 simulations.:  20%|██        | 1007/5000 [00:06<00:27, 147.22it/s]Running 5000 simulations.:  20%|██        | 1022/5000 [00:06<00:27, 147.12it/s]Running 5000 simulations.:  21%|██        | 1037/5000 [00:06<00:26, 147.41it/s]Running 5000 simulations.:  21%|██        | 1052/5000 [00:07<00:26, 147.70it/s]Running 5000 simulations.:  21%|██▏       | 1067/5000 [00:07<00:26, 147.83it/s]Running 5000 simulations.:  22%|██▏       | 1082/5000 [00:07<00:26, 147.49it/s]Running 5000 simulations.:  22%|██▏       | 1097/5000 [00:07<00:26, 147.31it/s]Running 5000 simulations.:  22%|██▏       | 1112/5000 [00:07<00:26, 147.69it/s]Running 5000 simulations.:  23%|██▎       | 1127/5000 [00:07<00:26, 147.89it/s]Running 5000 simulations.:  23%|██▎       | 1142/5000 [00:07<00:26, 147.97it/s]Running 5000 simulations.:  23%|██▎       | 1157/5000 [00:07<00:25, 148.14it/s]Running 5000 simulations.:  23%|██▎       | 1172/5000 [00:07<00:25, 148.47it/s]Running 5000 simulations.:  24%|██▎       | 1187/5000 [00:07<00:25, 148.27it/s]Running 5000 simulations.:  24%|██▍       | 1202/5000 [00:08<00:25, 148.57it/s]Running 5000 simulations.:  24%|██▍       | 1217/5000 [00:08<00:25, 148.82it/s]Running 5000 simulations.:  25%|██▍       | 1233/5000 [00:08<00:25, 149.20it/s]Running 5000 simulations.:  25%|██▍       | 1249/5000 [00:08<00:25, 149.53it/s]Running 5000 simulations.:  25%|██▌       | 1264/5000 [00:08<00:25, 149.40it/s]Running 5000 simulations.:  26%|██▌       | 1279/5000 [00:08<00:24, 149.44it/s]Running 5000 simulations.:  26%|██▌       | 1294/5000 [00:08<00:24, 149.35it/s]Running 5000 simulations.:  26%|██▌       | 1309/5000 [00:08<00:24, 149.47it/s]Running 5000 simulations.:  26%|██▋       | 1324/5000 [00:08<00:24, 149.57it/s]Running 5000 simulations.:  27%|██▋       | 1339/5000 [00:09<00:24, 149.62it/s]Running 5000 simulations.:  27%|██▋       | 1354/5000 [00:09<00:24, 149.65it/s]Running 5000 simulations.:  27%|██▋       | 1369/5000 [00:09<00:24, 148.15it/s]Running 5000 simulations.:  28%|██▊       | 1384/5000 [00:09<00:24, 146.76it/s]Running 5000 simulations.:  28%|██▊       | 1399/5000 [00:09<00:24, 145.56it/s]Running 5000 simulations.:  28%|██▊       | 1414/5000 [00:09<00:24, 144.21it/s]Running 5000 simulations.:  29%|██▊       | 1429/5000 [00:09<00:24, 143.38it/s]Running 5000 simulations.:  29%|██▉       | 1444/5000 [00:09<00:24, 144.27it/s]Running 5000 simulations.:  29%|██▉       | 1459/5000 [00:09<00:24, 143.28it/s]Running 5000 simulations.:  29%|██▉       | 1474/5000 [00:09<00:24, 142.45it/s]Running 5000 simulations.:  30%|██▉       | 1489/5000 [00:10<00:24, 143.08it/s]Running 5000 simulations.:  30%|███       | 1504/5000 [00:10<00:24, 143.74it/s]Running 5000 simulations.:  30%|███       | 1519/5000 [00:10<00:24, 141.31it/s]Running 5000 simulations.:  31%|███       | 1534/5000 [00:10<00:24, 142.62it/s]Running 5000 simulations.:  31%|███       | 1549/5000 [00:10<00:24, 143.33it/s]Running 5000 simulations.:  31%|███▏      | 1564/5000 [00:10<00:24, 141.91it/s]Running 5000 simulations.:  32%|███▏      | 1579/5000 [00:10<00:24, 141.18it/s]Running 5000 simulations.:  32%|███▏      | 1594/5000 [00:10<00:23, 142.70it/s]Running 5000 simulations.:  32%|███▏      | 1609/5000 [00:10<00:23, 143.52it/s]Running 5000 simulations.:  32%|███▏      | 1624/5000 [00:10<00:23, 144.08it/s]Running 5000 simulations.:  33%|███▎      | 1639/5000 [00:11<00:23, 143.75it/s]Running 5000 simulations.:  33%|███▎      | 1654/5000 [00:11<00:23, 143.91it/s]Running 5000 simulations.:  33%|███▎      | 1669/5000 [00:11<00:23, 143.41it/s]Running 5000 simulations.:  34%|███▎      | 1684/5000 [00:11<00:23, 143.05it/s]Running 5000 simulations.:  34%|███▍      | 1699/5000 [00:11<00:23, 143.38it/s]Running 5000 simulations.:  34%|███▍      | 1714/5000 [00:11<00:22, 143.98it/s]Running 5000 simulations.:  35%|███▍      | 1729/5000 [00:11<00:22, 143.39it/s]Running 5000 simulations.:  35%|███▍      | 1744/5000 [00:11<00:22, 142.42it/s]Running 5000 simulations.:  35%|███▌      | 1759/5000 [00:11<00:22, 142.16it/s]Running 5000 simulations.:  35%|███▌      | 1774/5000 [00:12<00:22, 143.39it/s]Running 5000 simulations.:  36%|███▌      | 1789/5000 [00:12<00:22, 144.48it/s]Running 5000 simulations.:  36%|███▌      | 1804/5000 [00:12<00:22, 145.06it/s]Running 5000 simulations.:  36%|███▋      | 1819/5000 [00:12<00:21, 145.86it/s]Running 5000 simulations.:  37%|███▋      | 1834/5000 [00:12<00:21, 146.35it/s]Running 5000 simulations.:  37%|███▋      | 1849/5000 [00:12<00:21, 146.54it/s]Running 5000 simulations.:  37%|███▋      | 1864/5000 [00:12<00:21, 146.34it/s]Running 5000 simulations.:  38%|███▊      | 1879/5000 [00:12<00:21, 146.20it/s]Running 5000 simulations.:  38%|███▊      | 1894/5000 [00:12<00:21, 146.56it/s]Running 5000 simulations.:  38%|███▊      | 1909/5000 [00:12<00:21, 146.96it/s]Running 5000 simulations.:  38%|███▊      | 1924/5000 [00:13<00:20, 147.10it/s]Running 5000 simulations.:  39%|███▉      | 1939/5000 [00:13<00:20, 146.95it/s]Running 5000 simulations.:  39%|███▉      | 1954/5000 [00:13<00:20, 147.13it/s]Running 5000 simulations.:  39%|███▉      | 1969/5000 [00:13<00:20, 147.51it/s]Running 5000 simulations.:  40%|███▉      | 1984/5000 [00:13<00:20, 147.67it/s]Running 5000 simulations.:  40%|███▉      | 1999/5000 [00:13<00:20, 147.65it/s]Running 5000 simulations.:  40%|████      | 2014/5000 [00:13<00:20, 147.77it/s]Running 5000 simulations.:  41%|████      | 2029/5000 [00:13<00:20, 147.58it/s]Running 5000 simulations.:  41%|████      | 2044/5000 [00:13<00:19, 148.18it/s]Running 5000 simulations.:  41%|████      | 2059/5000 [00:13<00:19, 148.10it/s]Running 5000 simulations.:  41%|████▏     | 2074/5000 [00:14<00:19, 147.79it/s]Running 5000 simulations.:  42%|████▏     | 2089/5000 [00:14<00:19, 147.55it/s]Running 5000 simulations.:  42%|████▏     | 2104/5000 [00:14<00:19, 147.32it/s]Running 5000 simulations.:  42%|████▏     | 2119/5000 [00:14<00:19, 146.96it/s]Running 5000 simulations.:  43%|████▎     | 2134/5000 [00:14<00:19, 146.05it/s]Running 5000 simulations.:  43%|████▎     | 2149/5000 [00:14<00:19, 145.95it/s]Running 5000 simulations.:  43%|████▎     | 2164/5000 [00:14<00:19, 146.27it/s]Running 5000 simulations.:  44%|████▎     | 2179/5000 [00:14<00:19, 146.08it/s]Running 5000 simulations.:  44%|████▍     | 2194/5000 [00:14<00:19, 146.51it/s]Running 5000 simulations.:  44%|████▍     | 2209/5000 [00:15<00:19, 146.12it/s]Running 5000 simulations.:  44%|████▍     | 2224/5000 [00:15<00:19, 145.78it/s]Running 5000 simulations.:  45%|████▍     | 2239/5000 [00:15<00:18, 146.09it/s]Running 5000 simulations.:  45%|████▌     | 2254/5000 [00:15<00:18, 146.48it/s]Running 5000 simulations.:  45%|████▌     | 2269/5000 [00:15<00:18, 146.90it/s]Running 5000 simulations.:  46%|████▌     | 2284/5000 [00:15<00:18, 147.01it/s]Running 5000 simulations.:  46%|████▌     | 2299/5000 [00:15<00:18, 146.87it/s]Running 5000 simulations.:  46%|████▋     | 2314/5000 [00:15<00:18, 146.96it/s]Running 5000 simulations.:  47%|████▋     | 2329/5000 [00:15<00:18, 146.74it/s]Running 5000 simulations.:  47%|████▋     | 2344/5000 [00:15<00:18, 146.38it/s]Running 5000 simulations.:  47%|████▋     | 2359/5000 [00:16<00:18, 146.13it/s]Running 5000 simulations.:  47%|████▋     | 2374/5000 [00:16<00:17, 146.40it/s]Running 5000 simulations.:  48%|████▊     | 2389/5000 [00:16<00:17, 146.15it/s]Running 5000 simulations.:  48%|████▊     | 2404/5000 [00:16<00:17, 146.65it/s]Running 5000 simulations.:  48%|████▊     | 2419/5000 [00:16<00:17, 147.01it/s]Running 5000 simulations.:  49%|████▊     | 2434/5000 [00:16<00:17, 146.95it/s]Running 5000 simulations.:  49%|████▉     | 2449/5000 [00:16<00:17, 146.38it/s]Running 5000 simulations.:  49%|████▉     | 2464/5000 [00:16<00:17, 145.93it/s]Running 5000 simulations.:  50%|████▉     | 2479/5000 [00:16<00:17, 145.96it/s]Running 5000 simulations.:  50%|████▉     | 2494/5000 [00:16<00:17, 146.15it/s]Running 5000 simulations.:  50%|█████     | 2509/5000 [00:17<00:17, 146.31it/s]Running 5000 simulations.:  50%|█████     | 2524/5000 [00:17<00:16, 146.36it/s]Running 5000 simulations.:  51%|█████     | 2539/5000 [00:17<00:16, 146.00it/s]Running 5000 simulations.:  51%|█████     | 2554/5000 [00:17<00:16, 146.16it/s]Running 5000 simulations.:  51%|█████▏    | 2569/5000 [00:17<00:16, 146.14it/s]Running 5000 simulations.:  52%|█████▏    | 2584/5000 [00:17<00:16, 146.14it/s]Running 5000 simulations.:  52%|█████▏    | 2599/5000 [00:17<00:16, 146.04it/s]Running 5000 simulations.:  52%|█████▏    | 2614/5000 [00:17<00:16, 145.70it/s]Running 5000 simulations.:  53%|█████▎    | 2629/5000 [00:17<00:16, 145.94it/s]Running 5000 simulations.:  53%|█████▎    | 2644/5000 [00:17<00:16, 146.27it/s]Running 5000 simulations.:  53%|█████▎    | 2659/5000 [00:18<00:15, 146.50it/s]Running 5000 simulations.:  53%|█████▎    | 2674/5000 [00:18<00:15, 146.70it/s]Running 5000 simulations.:  54%|█████▍    | 2689/5000 [00:18<00:15, 146.95it/s]Running 5000 simulations.:  54%|█████▍    | 2704/5000 [00:18<00:15, 147.37it/s]Running 5000 simulations.:  54%|█████▍    | 2719/5000 [00:18<00:15, 147.64it/s]Running 5000 simulations.:  55%|█████▍    | 2734/5000 [00:18<00:15, 147.45it/s]Running 5000 simulations.:  55%|█████▍    | 2749/5000 [00:18<00:15, 146.97it/s]Running 5000 simulations.:  55%|█████▌    | 2764/5000 [00:18<00:15, 146.94it/s]Running 5000 simulations.:  56%|█████▌    | 2779/5000 [00:18<00:15, 147.23it/s]Running 5000 simulations.:  56%|█████▌    | 2794/5000 [00:18<00:14, 147.50it/s]Running 5000 simulations.:  56%|█████▌    | 2809/5000 [00:19<00:14, 147.58it/s]Running 5000 simulations.:  56%|█████▋    | 2824/5000 [00:19<00:14, 146.95it/s]Running 5000 simulations.:  57%|█████▋    | 2839/5000 [00:19<00:14, 147.05it/s]Running 5000 simulations.:  57%|█████▋    | 2854/5000 [00:19<00:14, 147.37it/s]Running 5000 simulations.:  57%|█████▋    | 2869/5000 [00:19<00:14, 147.09it/s]Running 5000 simulations.:  58%|█████▊    | 2884/5000 [00:19<00:14, 146.55it/s]Running 5000 simulations.:  58%|█████▊    | 2899/5000 [00:19<00:14, 146.13it/s]Running 5000 simulations.:  58%|█████▊    | 2914/5000 [00:19<00:14, 146.01it/s]Running 5000 simulations.:  59%|█████▊    | 2929/5000 [00:19<00:14, 146.27it/s]Running 5000 simulations.:  59%|█████▉    | 2944/5000 [00:20<00:14, 145.86it/s]Running 5000 simulations.:  59%|█████▉    | 2959/5000 [00:20<00:13, 145.98it/s]Running 5000 simulations.:  59%|█████▉    | 2974/5000 [00:20<00:13, 146.57it/s]Running 5000 simulations.:  60%|█████▉    | 2989/5000 [00:20<00:13, 146.83it/s]Running 5000 simulations.:  60%|██████    | 3004/5000 [00:20<00:13, 147.10it/s]Running 5000 simulations.:  60%|██████    | 3019/5000 [00:20<00:13, 147.35it/s]Running 5000 simulations.:  61%|██████    | 3034/5000 [00:20<00:13, 147.41it/s]Running 5000 simulations.:  61%|██████    | 3049/5000 [00:20<00:13, 146.93it/s]Running 5000 simulations.:  61%|██████▏   | 3064/5000 [00:20<00:13, 147.14it/s]Running 5000 simulations.:  62%|██████▏   | 3079/5000 [00:20<00:13, 147.08it/s]Running 5000 simulations.:  62%|██████▏   | 3094/5000 [00:21<00:13, 146.48it/s]Running 5000 simulations.:  62%|██████▏   | 3109/5000 [00:21<00:12, 145.79it/s]Running 5000 simulations.:  62%|██████▏   | 3124/5000 [00:21<00:12, 145.92it/s]Running 5000 simulations.:  63%|██████▎   | 3139/5000 [00:21<00:12, 146.27it/s]Running 5000 simulations.:  63%|██████▎   | 3154/5000 [00:21<00:12, 146.00it/s]Running 5000 simulations.:  63%|██████▎   | 3169/5000 [00:21<00:12, 145.77it/s]Running 5000 simulations.:  64%|██████▎   | 3184/5000 [00:21<00:12, 145.50it/s]Running 5000 simulations.:  64%|██████▍   | 3199/5000 [00:21<00:12, 145.37it/s]Running 5000 simulations.:  64%|██████▍   | 3214/5000 [00:21<00:12, 145.43it/s]Running 5000 simulations.:  65%|██████▍   | 3229/5000 [00:21<00:12, 145.43it/s]Running 5000 simulations.:  65%|██████▍   | 3244/5000 [00:22<00:12, 145.27it/s]Running 5000 simulations.:  65%|██████▌   | 3259/5000 [00:22<00:11, 145.78it/s]Running 5000 simulations.:  65%|██████▌   | 3274/5000 [00:22<00:11, 145.85it/s]Running 5000 simulations.:  66%|██████▌   | 3289/5000 [00:22<00:11, 145.82it/s]Running 5000 simulations.:  66%|██████▌   | 3304/5000 [00:22<00:12, 137.88it/s]Running 5000 simulations.:  66%|██████▋   | 3321/5000 [00:22<00:11, 144.58it/s]Running 5000 simulations.:  67%|██████▋   | 3338/5000 [00:22<00:11, 150.82it/s]Running 5000 simulations.:  67%|██████▋   | 3354/5000 [00:22<00:10, 149.90it/s]Running 5000 simulations.:  67%|██████▋   | 3370/5000 [00:22<00:10, 148.96it/s]Running 5000 simulations.:  68%|██████▊   | 3385/5000 [00:23<00:10, 148.37it/s]Running 5000 simulations.:  68%|██████▊   | 3400/5000 [00:23<00:10, 148.04it/s]Running 5000 simulations.:  68%|██████▊   | 3415/5000 [00:23<00:10, 147.70it/s]Running 5000 simulations.:  69%|██████▊   | 3430/5000 [00:23<00:10, 147.07it/s]Running 5000 simulations.:  69%|██████▉   | 3445/5000 [00:23<00:10, 146.69it/s]Running 5000 simulations.:  69%|██████▉   | 3460/5000 [00:23<00:10, 146.61it/s]Running 5000 simulations.:  70%|██████▉   | 3475/5000 [00:23<00:10, 146.68it/s]Running 5000 simulations.:  70%|██████▉   | 3490/5000 [00:23<00:10, 147.00it/s]Running 5000 simulations.:  70%|███████   | 3505/5000 [00:23<00:10, 147.24it/s]Running 5000 simulations.:  70%|███████   | 3520/5000 [00:23<00:10, 146.78it/s]Running 5000 simulations.:  71%|███████   | 3535/5000 [00:24<00:10, 146.40it/s]Running 5000 simulations.:  71%|███████   | 3550/5000 [00:24<00:09, 146.21it/s]Running 5000 simulations.:  71%|███████▏  | 3565/5000 [00:24<00:09, 145.79it/s]Running 5000 simulations.:  72%|███████▏  | 3580/5000 [00:24<00:09, 146.22it/s]Running 5000 simulations.:  72%|███████▏  | 3595/5000 [00:24<00:09, 145.60it/s]Running 5000 simulations.:  72%|███████▏  | 3610/5000 [00:24<00:09, 145.58it/s]Running 5000 simulations.:  72%|███████▎  | 3625/5000 [00:24<00:09, 145.73it/s]Running 5000 simulations.:  73%|███████▎  | 3640/5000 [00:24<00:09, 146.13it/s]Running 5000 simulations.:  73%|███████▎  | 3655/5000 [00:24<00:09, 146.19it/s]Running 5000 simulations.:  73%|███████▎  | 3670/5000 [00:24<00:09, 145.78it/s]Running 5000 simulations.:  74%|███████▎  | 3685/5000 [00:25<00:09, 145.82it/s]Running 5000 simulations.:  74%|███████▍  | 3700/5000 [00:25<00:08, 146.20it/s]Running 5000 simulations.:  74%|███████▍  | 3715/5000 [00:25<00:08, 146.12it/s]Running 5000 simulations.:  75%|███████▍  | 3730/5000 [00:25<00:08, 145.95it/s]Running 5000 simulations.:  75%|███████▍  | 3745/5000 [00:25<00:08, 146.01it/s]Running 5000 simulations.:  75%|███████▌  | 3760/5000 [00:25<00:08, 146.63it/s]Running 5000 simulations.:  76%|███████▌  | 3775/5000 [00:25<00:08, 146.97it/s]Running 5000 simulations.:  76%|███████▌  | 3790/5000 [00:25<00:08, 147.26it/s]Running 5000 simulations.:  76%|███████▌  | 3805/5000 [00:25<00:08, 147.51it/s]Running 5000 simulations.:  76%|███████▋  | 3820/5000 [00:25<00:08, 147.17it/s]Running 5000 simulations.:  77%|███████▋  | 3835/5000 [00:26<00:07, 146.37it/s]Running 5000 simulations.:  77%|███████▋  | 3850/5000 [00:26<00:07, 146.19it/s]Running 5000 simulations.:  77%|███████▋  | 3865/5000 [00:26<00:07, 145.82it/s]Running 5000 simulations.:  78%|███████▊  | 3880/5000 [00:26<00:07, 145.67it/s]Running 5000 simulations.:  78%|███████▊  | 3895/5000 [00:26<00:07, 145.24it/s]Running 5000 simulations.:  78%|███████▊  | 3910/5000 [00:26<00:07, 145.11it/s]Running 5000 simulations.:  78%|███████▊  | 3925/5000 [00:26<00:07, 144.67it/s]Running 5000 simulations.:  79%|███████▉  | 3940/5000 [00:26<00:07, 144.63it/s]Running 5000 simulations.:  79%|███████▉  | 3955/5000 [00:26<00:07, 144.55it/s]Running 5000 simulations.:  79%|███████▉  | 3970/5000 [00:27<00:07, 144.50it/s]Running 5000 simulations.:  80%|███████▉  | 3985/5000 [00:27<00:07, 144.65it/s]Running 5000 simulations.:  80%|████████  | 4000/5000 [00:27<00:06, 144.49it/s]Running 5000 simulations.:  80%|████████  | 4015/5000 [00:27<00:06, 144.94it/s]Running 5000 simulations.:  81%|████████  | 4030/5000 [00:27<00:06, 145.56it/s]Running 5000 simulations.:  81%|████████  | 4045/5000 [00:27<00:06, 145.62it/s]Running 5000 simulations.:  81%|████████  | 4060/5000 [00:27<00:06, 144.74it/s]Running 5000 simulations.:  82%|████████▏ | 4075/5000 [00:27<00:06, 144.63it/s]Running 5000 simulations.:  82%|████████▏ | 4090/5000 [00:27<00:06, 144.88it/s]Running 5000 simulations.:  82%|████████▏ | 4105/5000 [00:27<00:06, 145.19it/s]Running 5000 simulations.:  82%|████████▏ | 4120/5000 [00:28<00:06, 145.61it/s]Running 5000 simulations.:  83%|████████▎ | 4135/5000 [00:28<00:05, 145.56it/s]Running 5000 simulations.:  83%|████████▎ | 4150/5000 [00:28<00:05, 145.59it/s]Running 5000 simulations.:  83%|████████▎ | 4165/5000 [00:28<00:05, 145.50it/s]Running 5000 simulations.:  84%|████████▎ | 4180/5000 [00:28<00:05, 145.80it/s]Running 5000 simulations.:  84%|████████▍ | 4195/5000 [00:28<00:05, 146.31it/s]Running 5000 simulations.:  84%|████████▍ | 4210/5000 [00:28<00:05, 146.18it/s]Running 5000 simulations.:  84%|████████▍ | 4225/5000 [00:28<00:05, 146.14it/s]Running 5000 simulations.:  85%|████████▍ | 4240/5000 [00:28<00:05, 146.31it/s]Running 5000 simulations.:  85%|████████▌ | 4255/5000 [00:28<00:05, 146.50it/s]Running 5000 simulations.:  85%|████████▌ | 4270/5000 [00:29<00:04, 146.67it/s]Running 5000 simulations.:  86%|████████▌ | 4285/5000 [00:29<00:04, 146.32it/s]Running 5000 simulations.:  86%|████████▌ | 4300/5000 [00:29<00:04, 146.14it/s]Running 5000 simulations.:  86%|████████▋ | 4315/5000 [00:29<00:04, 145.70it/s]Running 5000 simulations.:  87%|████████▋ | 4330/5000 [00:29<00:04, 145.93it/s]Running 5000 simulations.:  87%|████████▋ | 4345/5000 [00:29<00:04, 146.00it/s]Running 5000 simulations.:  87%|████████▋ | 4360/5000 [00:29<00:04, 146.29it/s]Running 5000 simulations.:  88%|████████▊ | 4375/5000 [00:29<00:04, 145.99it/s]Running 5000 simulations.:  88%|████████▊ | 4390/5000 [00:29<00:04, 146.34it/s]Running 5000 simulations.:  88%|████████▊ | 4405/5000 [00:30<00:04, 146.45it/s]Running 5000 simulations.:  88%|████████▊ | 4420/5000 [00:30<00:03, 146.58it/s]Running 5000 simulations.:  89%|████████▊ | 4435/5000 [00:30<00:03, 146.62it/s]Running 5000 simulations.:  89%|████████▉ | 4450/5000 [00:30<00:03, 147.01it/s]Running 5000 simulations.:  89%|████████▉ | 4465/5000 [00:30<00:03, 147.22it/s]Running 5000 simulations.:  90%|████████▉ | 4480/5000 [00:30<00:03, 146.78it/s]Running 5000 simulations.:  90%|████████▉ | 4495/5000 [00:30<00:03, 146.73it/s]Running 5000 simulations.:  90%|█████████ | 4510/5000 [00:30<00:03, 146.34it/s]Running 5000 simulations.:  90%|█████████ | 4525/5000 [00:30<00:03, 145.97it/s]Running 5000 simulations.:  91%|█████████ | 4540/5000 [00:30<00:03, 146.02it/s]Running 5000 simulations.:  91%|█████████ | 4555/5000 [00:31<00:03, 146.34it/s]Running 5000 simulations.:  91%|█████████▏| 4570/5000 [00:31<00:02, 146.57it/s]Running 5000 simulations.:  92%|█████████▏| 4585/5000 [00:31<00:02, 146.33it/s]Running 5000 simulations.:  92%|█████████▏| 4600/5000 [00:31<00:02, 146.25it/s]Running 5000 simulations.:  92%|█████████▏| 4615/5000 [00:31<00:02, 146.64it/s]Running 5000 simulations.:  93%|█████████▎| 4630/5000 [00:31<00:02, 147.06it/s]Running 5000 simulations.:  93%|█████████▎| 4645/5000 [00:31<00:02, 147.30it/s]Running 5000 simulations.:  93%|█████████▎| 4660/5000 [00:31<00:02, 147.10it/s]Running 5000 simulations.:  94%|█████████▎| 4675/5000 [00:31<00:02, 146.90it/s]Running 5000 simulations.:  94%|█████████▍| 4690/5000 [00:31<00:02, 146.84it/s]Running 5000 simulations.:  94%|█████████▍| 4705/5000 [00:32<00:02, 146.48it/s]Running 5000 simulations.:  94%|█████████▍| 4720/5000 [00:32<00:01, 146.72it/s]Running 5000 simulations.:  95%|█████████▍| 4735/5000 [00:32<00:01, 147.27it/s]Running 5000 simulations.:  95%|█████████▌| 4750/5000 [00:32<00:01, 147.02it/s]Running 5000 simulations.:  95%|█████████▌| 4765/5000 [00:32<00:01, 147.00it/s]Running 5000 simulations.:  96%|█████████▌| 4780/5000 [00:32<00:01, 147.35it/s]Running 5000 simulations.:  96%|█████████▌| 4795/5000 [00:32<00:01, 146.85it/s]Running 5000 simulations.:  96%|█████████▌| 4810/5000 [00:32<00:01, 146.99it/s]Running 5000 simulations.:  96%|█████████▋| 4825/5000 [00:32<00:01, 147.25it/s]Running 5000 simulations.:  97%|█████████▋| 4840/5000 [00:32<00:01, 147.13it/s]Running 5000 simulations.:  97%|█████████▋| 4855/5000 [00:33<00:00, 146.81it/s]Running 5000 simulations.:  97%|█████████▋| 4870/5000 [00:33<00:00, 147.19it/s]Running 5000 simulations.:  98%|█████████▊| 4885/5000 [00:33<00:00, 147.62it/s]Running 5000 simulations.:  98%|█████████▊| 4900/5000 [00:33<00:00, 147.61it/s]Running 5000 simulations.:  98%|█████████▊| 4915/5000 [00:33<00:00, 147.62it/s]Running 5000 simulations.:  99%|█████████▊| 4930/5000 [00:33<00:00, 147.10it/s]Running 5000 simulations.:  99%|█████████▉| 4945/5000 [00:33<00:00, 146.81it/s]Running 5000 simulations.:  99%|█████████▉| 4960/5000 [00:33<00:00, 146.64it/s]Running 5000 simulations.: 100%|█████████▉| 4975/5000 [00:33<00:00, 146.64it/s]Running 5000 simulations.: 100%|█████████▉| 4990/5000 [00:33<00:00, 146.76it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:34<00:00, 146.77it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:32, 153.13it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:32, 153.41it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:32, 153.14it/s]Running 5000 simulations.:   1%|▏         | 64/5000 [00:00<00:32, 152.90it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:32, 152.99it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:31, 153.52it/s]Running 5000 simulations.:   2%|▏         | 112/5000 [00:00<00:31, 153.48it/s]Running 5000 simulations.:   3%|▎         | 128/5000 [00:00<00:31, 153.45it/s]Running 5000 simulations.:   3%|▎         | 144/5000 [00:00<00:31, 153.46it/s]Running 5000 simulations.:   3%|▎         | 160/5000 [00:01<00:31, 153.61it/s]Running 5000 simulations.:   4%|▎         | 176/5000 [00:01<00:31, 152.86it/s]Running 5000 simulations.:   4%|▍         | 192/5000 [00:01<00:31, 152.90it/s]Running 5000 simulations.:   4%|▍         | 208/5000 [00:01<00:31, 152.60it/s]Running 5000 simulations.:   4%|▍         | 224/5000 [00:01<00:31, 152.02it/s]Running 5000 simulations.:   5%|▍         | 240/5000 [00:01<00:31, 151.83it/s]Running 5000 simulations.:   5%|▌         | 256/5000 [00:01<00:31, 152.02it/s]Running 5000 simulations.:   5%|▌         | 272/5000 [00:01<00:31, 151.90it/s]Running 5000 simulations.:   6%|▌         | 288/5000 [00:01<00:31, 151.62it/s]Running 5000 simulations.:   6%|▌         | 304/5000 [00:01<00:31, 151.38it/s]Running 5000 simulations.:   6%|▋         | 320/5000 [00:02<00:30, 151.29it/s]Running 5000 simulations.:   7%|▋         | 336/5000 [00:02<00:30, 150.69it/s]Running 5000 simulations.:   7%|▋         | 352/5000 [00:02<00:30, 150.55it/s]Running 5000 simulations.:   7%|▋         | 368/5000 [00:02<00:30, 150.94it/s]Running 5000 simulations.:   8%|▊         | 384/5000 [00:02<00:30, 150.94it/s]Running 5000 simulations.:   8%|▊         | 400/5000 [00:02<00:30, 150.52it/s]Running 5000 simulations.:   8%|▊         | 416/5000 [00:02<00:30, 150.39it/s]Running 5000 simulations.:   9%|▊         | 432/5000 [00:02<00:30, 150.48it/s]Running 5000 simulations.:   9%|▉         | 448/5000 [00:02<00:30, 150.24it/s]Running 5000 simulations.:   9%|▉         | 464/5000 [00:03<00:30, 150.74it/s]Running 5000 simulations.:  10%|▉         | 480/5000 [00:03<00:29, 150.89it/s]Running 5000 simulations.:  10%|▉         | 496/5000 [00:03<00:29, 151.20it/s]Running 5000 simulations.:  10%|█         | 512/5000 [00:03<00:29, 150.77it/s]Running 5000 simulations.:  11%|█         | 528/5000 [00:03<00:29, 150.56it/s]Running 5000 simulations.:  11%|█         | 544/5000 [00:03<00:29, 150.53it/s]Running 5000 simulations.:  11%|█         | 560/5000 [00:03<00:29, 150.88it/s]Running 5000 simulations.:  12%|█▏        | 576/5000 [00:03<00:29, 150.62it/s]Running 5000 simulations.:  12%|█▏        | 592/5000 [00:03<00:29, 150.78it/s]Running 5000 simulations.:  12%|█▏        | 608/5000 [00:04<00:29, 150.34it/s]Running 5000 simulations.:  12%|█▏        | 624/5000 [00:04<00:29, 150.01it/s]Running 5000 simulations.:  13%|█▎        | 640/5000 [00:04<00:29, 149.52it/s]Running 5000 simulations.:  13%|█▎        | 655/5000 [00:04<00:29, 149.17it/s]Running 5000 simulations.:  13%|█▎        | 670/5000 [00:04<00:29, 149.07it/s]Running 5000 simulations.:  14%|█▎        | 685/5000 [00:04<00:29, 148.69it/s]Running 5000 simulations.:  14%|█▍        | 700/5000 [00:04<00:28, 148.46it/s]Running 5000 simulations.:  14%|█▍        | 715/5000 [00:04<00:28, 148.43it/s]Running 5000 simulations.:  15%|█▍        | 730/5000 [00:04<00:28, 148.83it/s]Running 5000 simulations.:  15%|█▍        | 745/5000 [00:04<00:28, 148.49it/s]Running 5000 simulations.:  15%|█▌        | 760/5000 [00:05<00:28, 148.34it/s]Running 5000 simulations.:  16%|█▌        | 775/5000 [00:05<00:28, 148.08it/s]Running 5000 simulations.:  16%|█▌        | 790/5000 [00:05<00:28, 148.25it/s]Running 5000 simulations.:  16%|█▌        | 805/5000 [00:05<00:28, 148.58it/s]Running 5000 simulations.:  16%|█▋        | 820/5000 [00:05<00:28, 148.77it/s]Running 5000 simulations.:  17%|█▋        | 836/5000 [00:05<00:27, 149.33it/s]Running 5000 simulations.:  17%|█▋        | 851/5000 [00:05<00:27, 148.99it/s]Running 5000 simulations.:  17%|█▋        | 866/5000 [00:05<00:27, 148.92it/s]Running 5000 simulations.:  18%|█▊        | 882/5000 [00:05<00:27, 149.29it/s]Running 5000 simulations.:  18%|█▊        | 898/5000 [00:05<00:27, 149.73it/s]Running 5000 simulations.:  18%|█▊        | 914/5000 [00:06<00:27, 149.96it/s]Running 5000 simulations.:  19%|█▊        | 929/5000 [00:06<00:27, 149.93it/s]Running 5000 simulations.:  19%|█▉        | 944/5000 [00:06<00:27, 149.17it/s]Running 5000 simulations.:  19%|█▉        | 959/5000 [00:06<00:27, 149.25it/s]Running 5000 simulations.:  19%|█▉        | 974/5000 [00:06<00:27, 149.08it/s]Running 5000 simulations.:  20%|█▉        | 989/5000 [00:06<00:26, 148.91it/s]Running 5000 simulations.:  20%|██        | 1004/5000 [00:06<00:26, 148.37it/s]Running 5000 simulations.:  20%|██        | 1019/5000 [00:06<00:26, 147.94it/s]Running 5000 simulations.:  21%|██        | 1034/5000 [00:06<00:26, 148.46it/s]Running 5000 simulations.:  21%|██        | 1050/5000 [00:06<00:26, 148.98it/s]Running 5000 simulations.:  21%|██▏       | 1065/5000 [00:07<00:26, 148.83it/s]Running 5000 simulations.:  22%|██▏       | 1080/5000 [00:07<00:26, 148.52it/s]Running 5000 simulations.:  22%|██▏       | 1095/5000 [00:07<00:26, 148.27it/s]Running 5000 simulations.:  22%|██▏       | 1110/5000 [00:07<00:26, 148.37it/s]Running 5000 simulations.:  23%|██▎       | 1126/5000 [00:07<00:26, 148.97it/s]Running 5000 simulations.:  23%|██▎       | 1142/5000 [00:07<00:25, 149.49it/s]Running 5000 simulations.:  23%|██▎       | 1157/5000 [00:07<00:25, 149.44it/s]Running 5000 simulations.:  23%|██▎       | 1172/5000 [00:07<00:25, 148.72it/s]Running 5000 simulations.:  24%|██▎       | 1187/5000 [00:07<00:25, 148.41it/s]Running 5000 simulations.:  24%|██▍       | 1202/5000 [00:08<00:25, 148.17it/s]Running 5000 simulations.:  24%|██▍       | 1217/5000 [00:08<00:25, 148.01it/s]Running 5000 simulations.:  25%|██▍       | 1232/5000 [00:08<00:25, 147.68it/s]Running 5000 simulations.:  25%|██▍       | 1247/5000 [00:08<00:25, 147.55it/s]Running 5000 simulations.:  25%|██▌       | 1262/5000 [00:08<00:25, 147.81it/s]Running 5000 simulations.:  26%|██▌       | 1277/5000 [00:08<00:25, 148.39it/s]Running 5000 simulations.:  26%|██▌       | 1292/5000 [00:08<00:25, 148.26it/s]Running 5000 simulations.:  26%|██▌       | 1307/5000 [00:08<00:24, 148.09it/s]Running 5000 simulations.:  26%|██▋       | 1322/5000 [00:08<00:24, 147.81it/s]Running 5000 simulations.:  27%|██▋       | 1337/5000 [00:08<00:24, 147.70it/s]Running 5000 simulations.:  27%|██▋       | 1352/5000 [00:09<00:24, 147.89it/s]Running 5000 simulations.:  27%|██▋       | 1367/5000 [00:09<00:24, 148.40it/s]Running 5000 simulations.:  28%|██▊       | 1382/5000 [00:09<00:24, 148.56it/s]Running 5000 simulations.:  28%|██▊       | 1397/5000 [00:09<00:24, 148.60it/s]Running 5000 simulations.:  28%|██▊       | 1412/5000 [00:09<00:24, 148.73it/s]Running 5000 simulations.:  29%|██▊       | 1427/5000 [00:09<00:24, 148.43it/s]Running 5000 simulations.:  29%|██▉       | 1442/5000 [00:09<00:23, 148.34it/s]Running 5000 simulations.:  29%|██▉       | 1457/5000 [00:09<00:23, 148.44it/s]Running 5000 simulations.:  29%|██▉       | 1473/5000 [00:09<00:23, 148.91it/s]Running 5000 simulations.:  30%|██▉       | 1488/5000 [00:09<00:23, 148.67it/s]Running 5000 simulations.:  30%|███       | 1503/5000 [00:10<00:23, 147.91it/s]Running 5000 simulations.:  30%|███       | 1519/5000 [00:10<00:23, 148.97it/s]Running 5000 simulations.:  31%|███       | 1535/5000 [00:10<00:23, 149.30it/s]Running 5000 simulations.:  31%|███       | 1550/5000 [00:10<00:23, 149.42it/s]Running 5000 simulations.:  31%|███▏      | 1565/5000 [00:10<00:23, 149.16it/s]Running 5000 simulations.:  32%|███▏      | 1580/5000 [00:10<00:22, 148.96it/s]Running 5000 simulations.:  32%|███▏      | 1596/5000 [00:10<00:22, 149.41it/s]Running 5000 simulations.:  32%|███▏      | 1611/5000 [00:10<00:22, 149.45it/s]Running 5000 simulations.:  33%|███▎      | 1626/5000 [00:10<00:22, 149.03it/s]Running 5000 simulations.:  33%|███▎      | 1641/5000 [00:10<00:22, 148.40it/s]Running 5000 simulations.:  33%|███▎      | 1656/5000 [00:11<00:22, 148.40it/s]Running 5000 simulations.:  33%|███▎      | 1671/5000 [00:11<00:22, 148.33it/s]Running 5000 simulations.:  34%|███▎      | 1686/5000 [00:11<00:22, 148.79it/s]Running 5000 simulations.:  34%|███▍      | 1701/5000 [00:11<00:22, 148.81it/s]Running 5000 simulations.:  34%|███▍      | 1716/5000 [00:11<00:22, 148.69it/s]Running 5000 simulations.:  35%|███▍      | 1731/5000 [00:11<00:21, 149.03it/s]Running 5000 simulations.:  35%|███▍      | 1746/5000 [00:11<00:21, 149.12it/s]Running 5000 simulations.:  35%|███▌      | 1761/5000 [00:11<00:21, 148.86it/s]Running 5000 simulations.:  36%|███▌      | 1776/5000 [00:11<00:21, 148.42it/s]Running 5000 simulations.:  36%|███▌      | 1791/5000 [00:11<00:21, 148.29it/s]Running 5000 simulations.:  36%|███▌      | 1806/5000 [00:12<00:21, 148.28it/s]Running 5000 simulations.:  36%|███▋      | 1821/5000 [00:12<00:21, 148.48it/s]Running 5000 simulations.:  37%|███▋      | 1836/5000 [00:12<00:21, 148.66it/s]Running 5000 simulations.:  37%|███▋      | 1851/5000 [00:12<00:21, 148.98it/s]Running 5000 simulations.:  37%|███▋      | 1866/5000 [00:12<00:21, 148.83it/s]Running 5000 simulations.:  38%|███▊      | 1881/5000 [00:12<00:21, 148.48it/s]Running 5000 simulations.:  38%|███▊      | 1896/5000 [00:12<00:20, 148.32it/s]Running 5000 simulations.:  38%|███▊      | 1911/5000 [00:12<00:20, 148.44it/s]Running 5000 simulations.:  39%|███▊      | 1926/5000 [00:12<00:20, 148.56it/s]Running 5000 simulations.:  39%|███▉      | 1941/5000 [00:12<00:20, 148.74it/s]Running 5000 simulations.:  39%|███▉      | 1956/5000 [00:13<00:20, 148.83it/s]Running 5000 simulations.:  39%|███▉      | 1971/5000 [00:13<00:20, 148.61it/s]Running 5000 simulations.:  40%|███▉      | 1986/5000 [00:13<00:20, 148.89it/s]Running 5000 simulations.:  40%|████      | 2001/5000 [00:13<00:20, 148.99it/s]Running 5000 simulations.:  40%|████      | 2016/5000 [00:13<00:20, 148.78it/s]Running 5000 simulations.:  41%|████      | 2031/5000 [00:13<00:19, 148.62it/s]Running 5000 simulations.:  41%|████      | 2046/5000 [00:13<00:19, 148.53it/s]Running 5000 simulations.:  41%|████      | 2061/5000 [00:13<00:19, 148.27it/s]Running 5000 simulations.:  42%|████▏     | 2076/5000 [00:13<00:19, 148.44it/s]Running 5000 simulations.:  42%|████▏     | 2091/5000 [00:13<00:19, 148.44it/s]Running 5000 simulations.:  42%|████▏     | 2106/5000 [00:14<00:19, 147.99it/s]Running 5000 simulations.:  42%|████▏     | 2121/5000 [00:14<00:19, 148.11it/s]Running 5000 simulations.:  43%|████▎     | 2136/5000 [00:14<00:19, 148.39it/s]Running 5000 simulations.:  43%|████▎     | 2151/5000 [00:14<00:19, 148.22it/s]Running 5000 simulations.:  43%|████▎     | 2166/5000 [00:14<00:19, 148.38it/s]Running 5000 simulations.:  44%|████▎     | 2181/5000 [00:14<00:19, 148.29it/s]Running 5000 simulations.:  44%|████▍     | 2196/5000 [00:14<00:18, 148.34it/s]Running 5000 simulations.:  44%|████▍     | 2211/5000 [00:14<00:18, 147.81it/s]Running 5000 simulations.:  45%|████▍     | 2226/5000 [00:14<00:18, 147.90it/s]Running 5000 simulations.:  45%|████▍     | 2241/5000 [00:15<00:18, 148.25it/s]Running 5000 simulations.:  45%|████▌     | 2257/5000 [00:15<00:18, 148.88it/s]Running 5000 simulations.:  45%|████▌     | 2272/5000 [00:15<00:18, 148.64it/s]Running 5000 simulations.:  46%|████▌     | 2287/5000 [00:15<00:18, 148.43it/s]Running 5000 simulations.:  46%|████▌     | 2302/5000 [00:15<00:18, 147.89it/s]Running 5000 simulations.:  46%|████▋     | 2317/5000 [00:15<00:18, 147.98it/s]Running 5000 simulations.:  47%|████▋     | 2332/5000 [00:15<00:18, 148.01it/s]Running 5000 simulations.:  47%|████▋     | 2347/5000 [00:15<00:17, 148.24it/s]Running 5000 simulations.:  47%|████▋     | 2362/5000 [00:15<00:17, 148.58it/s]Running 5000 simulations.:  48%|████▊     | 2377/5000 [00:15<00:17, 148.75it/s]Running 5000 simulations.:  48%|████▊     | 2392/5000 [00:16<00:17, 148.38it/s]Running 5000 simulations.:  48%|████▊     | 2407/5000 [00:16<00:18, 141.73it/s]Running 5000 simulations.:  48%|████▊     | 2422/5000 [00:16<00:18, 141.36it/s]Running 5000 simulations.:  49%|████▊     | 2437/5000 [00:16<00:17, 143.00it/s]Running 5000 simulations.:  49%|████▉     | 2452/5000 [00:16<00:17, 144.29it/s]Running 5000 simulations.:  49%|████▉     | 2467/5000 [00:16<00:17, 145.61it/s]Running 5000 simulations.:  50%|████▉     | 2482/5000 [00:16<00:17, 146.20it/s]Running 5000 simulations.:  50%|████▉     | 2497/5000 [00:16<00:17, 146.63it/s]Running 5000 simulations.:  50%|█████     | 2512/5000 [00:16<00:16, 146.91it/s]Running 5000 simulations.:  51%|█████     | 2527/5000 [00:16<00:16, 147.46it/s]Running 5000 simulations.:  51%|█████     | 2542/5000 [00:17<00:16, 147.15it/s]Running 5000 simulations.:  51%|█████     | 2557/5000 [00:17<00:16, 147.10it/s]Running 5000 simulations.:  51%|█████▏    | 2572/5000 [00:17<00:16, 146.88it/s]Running 5000 simulations.:  52%|█████▏    | 2587/5000 [00:17<00:16, 146.99it/s]Running 5000 simulations.:  52%|█████▏    | 2602/5000 [00:17<00:16, 147.24it/s]Running 5000 simulations.:  52%|█████▏    | 2617/5000 [00:17<00:16, 147.75it/s]Running 5000 simulations.:  53%|█████▎    | 2632/5000 [00:17<00:15, 148.12it/s]Running 5000 simulations.:  53%|█████▎    | 2647/5000 [00:17<00:15, 147.40it/s]Running 5000 simulations.:  53%|█████▎    | 2662/5000 [00:17<00:15, 146.92it/s]Running 5000 simulations.:  54%|█████▎    | 2677/5000 [00:17<00:15, 147.23it/s]Running 5000 simulations.:  54%|█████▍    | 2692/5000 [00:18<00:15, 147.84it/s]Running 5000 simulations.:  54%|█████▍    | 2707/5000 [00:18<00:15, 147.52it/s]Running 5000 simulations.:  54%|█████▍    | 2722/5000 [00:18<00:15, 146.58it/s]Running 5000 simulations.:  55%|█████▍    | 2737/5000 [00:18<00:15, 146.69it/s]Running 5000 simulations.:  55%|█████▌    | 2752/5000 [00:18<00:15, 147.26it/s]Running 5000 simulations.:  55%|█████▌    | 2767/5000 [00:18<00:15, 147.28it/s]Running 5000 simulations.:  56%|█████▌    | 2782/5000 [00:18<00:14, 147.95it/s]Running 5000 simulations.:  56%|█████▌    | 2797/5000 [00:18<00:14, 148.31it/s]Running 5000 simulations.:  56%|█████▌    | 2812/5000 [00:18<00:14, 148.32it/s]Running 5000 simulations.:  57%|█████▋    | 2827/5000 [00:18<00:14, 147.73it/s]Running 5000 simulations.:  57%|█████▋    | 2842/5000 [00:19<00:14, 147.35it/s]Running 5000 simulations.:  57%|█████▋    | 2857/5000 [00:19<00:14, 147.27it/s]Running 5000 simulations.:  57%|█████▋    | 2872/5000 [00:19<00:14, 146.84it/s]Running 5000 simulations.:  58%|█████▊    | 2887/5000 [00:19<00:14, 147.31it/s]Running 5000 simulations.:  58%|█████▊    | 2902/5000 [00:19<00:14, 147.88it/s]Running 5000 simulations.:  58%|█████▊    | 2917/5000 [00:19<00:14, 147.94it/s]Running 5000 simulations.:  59%|█████▊    | 2932/5000 [00:19<00:14, 147.40it/s]Running 5000 simulations.:  59%|█████▉    | 2947/5000 [00:19<00:13, 147.50it/s]Running 5000 simulations.:  59%|█████▉    | 2962/5000 [00:19<00:13, 147.29it/s]Running 5000 simulations.:  60%|█████▉    | 2977/5000 [00:20<00:13, 147.36it/s]Running 5000 simulations.:  60%|█████▉    | 2992/5000 [00:20<00:13, 147.32it/s]Running 5000 simulations.:  60%|██████    | 3007/5000 [00:20<00:13, 147.52it/s]Running 5000 simulations.:  60%|██████    | 3022/5000 [00:20<00:13, 147.85it/s]Running 5000 simulations.:  61%|██████    | 3037/5000 [00:20<00:13, 148.18it/s]Running 5000 simulations.:  61%|██████    | 3052/5000 [00:20<00:13, 147.74it/s]Running 5000 simulations.:  61%|██████▏   | 3067/5000 [00:20<00:13, 147.60it/s]Running 5000 simulations.:  62%|██████▏   | 3082/5000 [00:20<00:13, 147.46it/s]Running 5000 simulations.:  62%|██████▏   | 3097/5000 [00:20<00:12, 147.45it/s]Running 5000 simulations.:  62%|██████▏   | 3112/5000 [00:20<00:12, 146.18it/s]Running 5000 simulations.:  63%|██████▎   | 3127/5000 [00:21<00:12, 145.56it/s]Running 5000 simulations.:  63%|██████▎   | 3142/5000 [00:21<00:12, 145.10it/s]Running 5000 simulations.:  63%|██████▎   | 3157/5000 [00:21<00:12, 144.82it/s]Running 5000 simulations.:  63%|██████▎   | 3172/5000 [00:21<00:12, 144.42it/s]Running 5000 simulations.:  64%|██████▎   | 3187/5000 [00:21<00:12, 144.59it/s]Running 5000 simulations.:  64%|██████▍   | 3202/5000 [00:21<00:12, 144.15it/s]Running 5000 simulations.:  64%|██████▍   | 3217/5000 [00:21<00:12, 142.79it/s]Running 5000 simulations.:  65%|██████▍   | 3232/5000 [00:21<00:12, 143.83it/s]Running 5000 simulations.:  65%|██████▍   | 3247/5000 [00:21<00:12, 143.61it/s]Running 5000 simulations.:  65%|██████▌   | 3262/5000 [00:21<00:12, 143.43it/s]Running 5000 simulations.:  66%|██████▌   | 3277/5000 [00:22<00:11, 144.48it/s]Running 5000 simulations.:  66%|██████▌   | 3292/5000 [00:22<00:11, 144.40it/s]Running 5000 simulations.:  66%|██████▌   | 3307/5000 [00:22<00:11, 144.55it/s]Running 5000 simulations.:  66%|██████▋   | 3322/5000 [00:22<00:11, 144.93it/s]Running 5000 simulations.:  67%|██████▋   | 3337/5000 [00:22<00:11, 145.15it/s]Running 5000 simulations.:  67%|██████▋   | 3353/5000 [00:22<00:11, 148.93it/s]Running 5000 simulations.:  67%|██████▋   | 3370/5000 [00:22<00:10, 152.76it/s]Running 5000 simulations.:  68%|██████▊   | 3386/5000 [00:22<00:10, 152.78it/s]Running 5000 simulations.:  68%|██████▊   | 3402/5000 [00:22<00:10, 150.87it/s]Running 5000 simulations.:  68%|██████▊   | 3418/5000 [00:23<00:10, 149.85it/s]Running 5000 simulations.:  69%|██████▊   | 3434/5000 [00:23<00:10, 149.32it/s]Running 5000 simulations.:  69%|██████▉   | 3449/5000 [00:23<00:10, 148.85it/s]Running 5000 simulations.:  69%|██████▉   | 3464/5000 [00:23<00:10, 148.87it/s]Running 5000 simulations.:  70%|██████▉   | 3479/5000 [00:23<00:10, 148.86it/s]Running 5000 simulations.:  70%|██████▉   | 3494/5000 [00:23<00:10, 148.39it/s]Running 5000 simulations.:  70%|███████   | 3509/5000 [00:23<00:10, 147.81it/s]Running 5000 simulations.:  70%|███████   | 3524/5000 [00:23<00:09, 147.64it/s]Running 5000 simulations.:  71%|███████   | 3539/5000 [00:23<00:09, 147.90it/s]Running 5000 simulations.:  71%|███████   | 3554/5000 [00:23<00:09, 147.75it/s]Running 5000 simulations.:  71%|███████▏  | 3569/5000 [00:24<00:09, 147.56it/s]Running 5000 simulations.:  72%|███████▏  | 3584/5000 [00:24<00:09, 147.75it/s]Running 5000 simulations.:  72%|███████▏  | 3599/5000 [00:24<00:09, 148.10it/s]Running 5000 simulations.:  72%|███████▏  | 3614/5000 [00:24<00:09, 147.72it/s]Running 5000 simulations.:  73%|███████▎  | 3629/5000 [00:24<00:09, 147.93it/s]Running 5000 simulations.:  73%|███████▎  | 3644/5000 [00:24<00:09, 148.20it/s]Running 5000 simulations.:  73%|███████▎  | 3659/5000 [00:24<00:09, 147.73it/s]Running 5000 simulations.:  73%|███████▎  | 3674/5000 [00:24<00:08, 147.79it/s]Running 5000 simulations.:  74%|███████▍  | 3689/5000 [00:24<00:08, 147.76it/s]Running 5000 simulations.:  74%|███████▍  | 3704/5000 [00:24<00:08, 147.50it/s]Running 5000 simulations.:  74%|███████▍  | 3719/5000 [00:25<00:08, 147.73it/s]Running 5000 simulations.:  75%|███████▍  | 3734/5000 [00:25<00:08, 148.00it/s]Running 5000 simulations.:  75%|███████▍  | 3749/5000 [00:25<00:08, 148.29it/s]Running 5000 simulations.:  75%|███████▌  | 3764/5000 [00:25<00:08, 148.31it/s]Running 5000 simulations.:  76%|███████▌  | 3779/5000 [00:25<00:08, 147.85it/s]Running 5000 simulations.:  76%|███████▌  | 3794/5000 [00:25<00:08, 147.73it/s]Running 5000 simulations.:  76%|███████▌  | 3809/5000 [00:25<00:08, 147.82it/s]Running 5000 simulations.:  76%|███████▋  | 3824/5000 [00:25<00:07, 147.81it/s]Running 5000 simulations.:  77%|███████▋  | 3839/5000 [00:25<00:07, 147.96it/s]Running 5000 simulations.:  77%|███████▋  | 3854/5000 [00:25<00:07, 148.19it/s]Running 5000 simulations.:  77%|███████▋  | 3869/5000 [00:26<00:07, 147.64it/s]Running 5000 simulations.:  78%|███████▊  | 3884/5000 [00:26<00:07, 147.55it/s]Running 5000 simulations.:  78%|███████▊  | 3899/5000 [00:26<00:07, 147.38it/s]Running 5000 simulations.:  78%|███████▊  | 3914/5000 [00:26<00:07, 147.41it/s]Running 5000 simulations.:  79%|███████▊  | 3929/5000 [00:26<00:07, 147.44it/s]Running 5000 simulations.:  79%|███████▉  | 3944/5000 [00:26<00:07, 147.80it/s]Running 5000 simulations.:  79%|███████▉  | 3959/5000 [00:26<00:07, 147.65it/s]Running 5000 simulations.:  79%|███████▉  | 3974/5000 [00:26<00:06, 147.89it/s]Running 5000 simulations.:  80%|███████▉  | 3989/5000 [00:26<00:06, 148.06it/s]Running 5000 simulations.:  80%|████████  | 4004/5000 [00:26<00:06, 148.18it/s]Running 5000 simulations.:  80%|████████  | 4019/5000 [00:27<00:06, 148.21it/s]Running 5000 simulations.:  81%|████████  | 4034/5000 [00:27<00:06, 148.33it/s]Running 5000 simulations.:  81%|████████  | 4049/5000 [00:27<00:06, 147.96it/s]Running 5000 simulations.:  81%|████████▏ | 4064/5000 [00:27<00:06, 147.96it/s]Running 5000 simulations.:  82%|████████▏ | 4079/5000 [00:27<00:06, 148.13it/s]Running 5000 simulations.:  82%|████████▏ | 4094/5000 [00:27<00:06, 148.10it/s]Running 5000 simulations.:  82%|████████▏ | 4109/5000 [00:27<00:06, 148.33it/s]Running 5000 simulations.:  82%|████████▏ | 4124/5000 [00:27<00:05, 147.39it/s]Running 5000 simulations.:  83%|████████▎ | 4139/5000 [00:27<00:05, 146.95it/s]Running 5000 simulations.:  83%|████████▎ | 4154/5000 [00:27<00:05, 147.33it/s]Running 5000 simulations.:  83%|████████▎ | 4169/5000 [00:28<00:05, 147.51it/s]Running 5000 simulations.:  84%|████████▎ | 4184/5000 [00:28<00:05, 147.78it/s]Running 5000 simulations.:  84%|████████▍ | 4199/5000 [00:28<00:05, 147.62it/s]Running 5000 simulations.:  84%|████████▍ | 4214/5000 [00:28<00:05, 147.91it/s]Running 5000 simulations.:  85%|████████▍ | 4229/5000 [00:28<00:05, 147.97it/s]Running 5000 simulations.:  85%|████████▍ | 4244/5000 [00:28<00:05, 147.55it/s]Running 5000 simulations.:  85%|████████▌ | 4259/5000 [00:28<00:05, 147.40it/s]Running 5000 simulations.:  85%|████████▌ | 4274/5000 [00:28<00:04, 147.10it/s]Running 5000 simulations.:  86%|████████▌ | 4289/5000 [00:28<00:04, 147.41it/s]Running 5000 simulations.:  86%|████████▌ | 4304/5000 [00:29<00:04, 147.82it/s]Running 5000 simulations.:  86%|████████▋ | 4319/5000 [00:29<00:04, 147.31it/s]Running 5000 simulations.:  87%|████████▋ | 4334/5000 [00:29<00:04, 147.10it/s]Running 5000 simulations.:  87%|████████▋ | 4349/5000 [00:29<00:04, 147.27it/s]Running 5000 simulations.:  87%|████████▋ | 4364/5000 [00:29<00:04, 147.81it/s]Running 5000 simulations.:  88%|████████▊ | 4379/5000 [00:29<00:04, 148.06it/s]Running 5000 simulations.:  88%|████████▊ | 4394/5000 [00:29<00:04, 147.92it/s]Running 5000 simulations.:  88%|████████▊ | 4409/5000 [00:29<00:04, 147.71it/s]Running 5000 simulations.:  88%|████████▊ | 4424/5000 [00:29<00:03, 148.10it/s]Running 5000 simulations.:  89%|████████▉ | 4439/5000 [00:29<00:03, 148.44it/s]Running 5000 simulations.:  89%|████████▉ | 4454/5000 [00:30<00:03, 148.14it/s]Running 5000 simulations.:  89%|████████▉ | 4469/5000 [00:30<00:03, 148.37it/s]Running 5000 simulations.:  90%|████████▉ | 4484/5000 [00:30<00:03, 148.43it/s]Running 5000 simulations.:  90%|████████▉ | 4499/5000 [00:30<00:03, 148.11it/s]Running 5000 simulations.:  90%|█████████ | 4514/5000 [00:30<00:03, 148.43it/s]Running 5000 simulations.:  91%|█████████ | 4529/5000 [00:30<00:03, 148.41it/s]Running 5000 simulations.:  91%|█████████ | 4544/5000 [00:30<00:03, 148.68it/s]Running 5000 simulations.:  91%|█████████ | 4559/5000 [00:30<00:02, 148.78it/s]Running 5000 simulations.:  91%|█████████▏| 4574/5000 [00:30<00:02, 148.98it/s]Running 5000 simulations.:  92%|█████████▏| 4589/5000 [00:30<00:02, 149.27it/s]Running 5000 simulations.:  92%|█████████▏| 4604/5000 [00:31<00:02, 149.02it/s]Running 5000 simulations.:  92%|█████████▏| 4619/5000 [00:31<00:02, 148.72it/s]Running 5000 simulations.:  93%|█████████▎| 4634/5000 [00:31<00:02, 148.76it/s]Running 5000 simulations.:  93%|█████████▎| 4649/5000 [00:31<00:02, 149.00it/s]Running 5000 simulations.:  93%|█████████▎| 4664/5000 [00:31<00:02, 148.88it/s]Running 5000 simulations.:  94%|█████████▎| 4679/5000 [00:31<00:02, 148.53it/s]Running 5000 simulations.:  94%|█████████▍| 4695/5000 [00:31<00:02, 148.97it/s]Running 5000 simulations.:  94%|█████████▍| 4710/5000 [00:31<00:01, 148.91it/s]Running 5000 simulations.:  94%|█████████▍| 4725/5000 [00:31<00:01, 148.36it/s]Running 5000 simulations.:  95%|█████████▍| 4740/5000 [00:31<00:01, 148.21it/s]Running 5000 simulations.:  95%|█████████▌| 4755/5000 [00:32<00:01, 148.01it/s]Running 5000 simulations.:  95%|█████████▌| 4770/5000 [00:32<00:01, 147.48it/s]Running 5000 simulations.:  96%|█████████▌| 4785/5000 [00:32<00:01, 146.70it/s]Running 5000 simulations.:  96%|█████████▌| 4800/5000 [00:32<00:01, 145.69it/s]Running 5000 simulations.:  96%|█████████▋| 4815/5000 [00:32<00:01, 145.01it/s]Running 5000 simulations.:  97%|█████████▋| 4830/5000 [00:32<00:01, 144.24it/s]Running 5000 simulations.:  97%|█████████▋| 4845/5000 [00:32<00:01, 144.03it/s]Running 5000 simulations.:  97%|█████████▋| 4860/5000 [00:32<00:00, 144.10it/s]Running 5000 simulations.:  98%|█████████▊| 4875/5000 [00:32<00:00, 144.36it/s]Running 5000 simulations.:  98%|█████████▊| 4890/5000 [00:32<00:00, 145.40it/s]Running 5000 simulations.:  98%|█████████▊| 4905/5000 [00:33<00:00, 145.88it/s]Running 5000 simulations.:  98%|█████████▊| 4920/5000 [00:33<00:00, 144.93it/s]Running 5000 simulations.:  99%|█████████▊| 4935/5000 [00:33<00:00, 143.00it/s]Running 5000 simulations.:  99%|█████████▉| 4950/5000 [00:33<00:00, 141.93it/s]Running 5000 simulations.:  99%|█████████▉| 4965/5000 [00:33<00:00, 140.76it/s]Running 5000 simulations.: 100%|█████████▉| 4980/5000 [00:33<00:00, 139.85it/s]Running 5000 simulations.: 100%|█████████▉| 4994/5000 [00:33<00:00, 139.42it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:33<00:00, 148.09it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:31, 156.02it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:32, 155.11it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:31, 154.90it/s]Running 5000 simulations.:   1%|▏         | 64/5000 [00:00<00:31, 155.16it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:31, 155.46it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:31, 155.25it/s]Running 5000 simulations.:   2%|▏         | 112/5000 [00:00<00:31, 154.35it/s]Running 5000 simulations.:   3%|▎         | 128/5000 [00:00<00:31, 153.82it/s]Running 5000 simulations.:   3%|▎         | 144/5000 [00:00<00:31, 154.29it/s]Running 5000 simulations.:   3%|▎         | 160/5000 [00:01<00:31, 154.81it/s]Running 5000 simulations.:   4%|▎         | 177/5000 [00:01<00:30, 157.47it/s]Running 5000 simulations.:   4%|▍         | 195/5000 [00:01<00:29, 161.97it/s]Running 5000 simulations.:   4%|▍         | 212/5000 [00:01<00:29, 159.62it/s]Running 5000 simulations.:   5%|▍         | 228/5000 [00:01<00:30, 158.33it/s]Running 5000 simulations.:   5%|▍         | 244/5000 [00:01<00:30, 156.78it/s]Running 5000 simulations.:   5%|▌         | 260/5000 [00:01<00:30, 155.44it/s]Running 5000 simulations.:   6%|▌         | 276/5000 [00:01<00:30, 155.22it/s]Running 5000 simulations.:   6%|▌         | 292/5000 [00:01<00:30, 155.25it/s]Running 5000 simulations.:   6%|▌         | 308/5000 [00:01<00:30, 155.41it/s]Running 5000 simulations.:   6%|▋         | 324/5000 [00:02<00:30, 155.40it/s]Running 5000 simulations.:   7%|▋         | 340/5000 [00:02<00:30, 154.95it/s]Running 5000 simulations.:   7%|▋         | 356/5000 [00:02<00:30, 154.56it/s]Running 5000 simulations.:   7%|▋         | 372/5000 [00:02<00:29, 154.28it/s]Running 5000 simulations.:   8%|▊         | 388/5000 [00:02<00:29, 154.27it/s]Running 5000 simulations.:   8%|▊         | 404/5000 [00:02<00:29, 154.45it/s]Running 5000 simulations.:   8%|▊         | 420/5000 [00:02<00:29, 154.69it/s]Running 5000 simulations.:   9%|▊         | 436/5000 [00:02<00:29, 154.63it/s]Running 5000 simulations.:   9%|▉         | 452/5000 [00:02<00:29, 154.32it/s]Running 5000 simulations.:   9%|▉         | 468/5000 [00:03<00:29, 154.13it/s]Running 5000 simulations.:  10%|▉         | 484/5000 [00:03<00:29, 154.04it/s]Running 5000 simulations.:  10%|█         | 500/5000 [00:03<00:29, 154.06it/s]Running 5000 simulations.:  10%|█         | 516/5000 [00:03<00:29, 153.42it/s]Running 5000 simulations.:  11%|█         | 532/5000 [00:03<00:29, 152.77it/s]Running 5000 simulations.:  11%|█         | 548/5000 [00:03<00:29, 152.69it/s]Running 5000 simulations.:  11%|█▏        | 564/5000 [00:03<00:29, 152.96it/s]Running 5000 simulations.:  12%|█▏        | 580/5000 [00:03<00:28, 153.17it/s]Running 5000 simulations.:  12%|█▏        | 596/5000 [00:03<00:28, 153.05it/s]Running 5000 simulations.:  12%|█▏        | 612/5000 [00:03<00:28, 152.99it/s]Running 5000 simulations.:  13%|█▎        | 628/5000 [00:04<00:28, 152.35it/s]Running 5000 simulations.:  13%|█▎        | 644/5000 [00:04<00:28, 152.51it/s]Running 5000 simulations.:  13%|█▎        | 660/5000 [00:04<00:28, 152.42it/s]Running 5000 simulations.:  14%|█▎        | 676/5000 [00:04<00:28, 152.68it/s]Running 5000 simulations.:  14%|█▍        | 692/5000 [00:04<00:28, 151.65it/s]Running 5000 simulations.:  14%|█▍        | 708/5000 [00:04<00:28, 151.87it/s]Running 5000 simulations.:  14%|█▍        | 724/5000 [00:04<00:28, 152.21it/s]Running 5000 simulations.:  15%|█▍        | 740/5000 [00:04<00:27, 152.21it/s]Running 5000 simulations.:  15%|█▌        | 756/5000 [00:04<00:27, 151.73it/s]Running 5000 simulations.:  15%|█▌        | 772/5000 [00:05<00:27, 151.35it/s]Running 5000 simulations.:  16%|█▌        | 788/5000 [00:05<00:27, 151.67it/s]Running 5000 simulations.:  16%|█▌        | 804/5000 [00:05<00:27, 151.94it/s]Running 5000 simulations.:  16%|█▋        | 820/5000 [00:05<00:27, 152.36it/s]Running 5000 simulations.:  17%|█▋        | 836/5000 [00:05<00:27, 152.22it/s]Running 5000 simulations.:  17%|█▋        | 852/5000 [00:05<00:27, 152.68it/s]Running 5000 simulations.:  17%|█▋        | 868/5000 [00:05<00:27, 152.29it/s]Running 5000 simulations.:  18%|█▊        | 884/5000 [00:05<00:27, 151.90it/s]Running 5000 simulations.:  18%|█▊        | 900/5000 [00:05<00:27, 151.36it/s]Running 5000 simulations.:  18%|█▊        | 916/5000 [00:05<00:26, 151.60it/s]Running 5000 simulations.:  19%|█▊        | 932/5000 [00:06<00:26, 151.63it/s]Running 5000 simulations.:  19%|█▉        | 948/5000 [00:06<00:26, 151.94it/s]Running 5000 simulations.:  19%|█▉        | 964/5000 [00:06<00:26, 152.23it/s]Running 5000 simulations.:  20%|█▉        | 980/5000 [00:06<00:26, 152.41it/s]Running 5000 simulations.:  20%|█▉        | 996/5000 [00:06<00:26, 152.53it/s]Running 5000 simulations.:  20%|██        | 1012/5000 [00:06<00:26, 152.49it/s]Running 5000 simulations.:  21%|██        | 1028/5000 [00:06<00:26, 152.63it/s]Running 5000 simulations.:  21%|██        | 1044/5000 [00:06<00:25, 152.78it/s]Running 5000 simulations.:  21%|██        | 1060/5000 [00:06<00:25, 152.82it/s]Running 5000 simulations.:  22%|██▏       | 1076/5000 [00:07<00:25, 152.72it/s]Running 5000 simulations.:  22%|██▏       | 1092/5000 [00:07<00:25, 152.68it/s]Running 5000 simulations.:  22%|██▏       | 1108/5000 [00:07<00:25, 152.89it/s]Running 5000 simulations.:  22%|██▏       | 1124/5000 [00:07<00:25, 152.71it/s]Running 5000 simulations.:  23%|██▎       | 1140/5000 [00:07<00:25, 152.67it/s]Running 5000 simulations.:  23%|██▎       | 1156/5000 [00:07<00:25, 152.73it/s]Running 5000 simulations.:  23%|██▎       | 1172/5000 [00:07<00:25, 152.22it/s]Running 5000 simulations.:  24%|██▍       | 1188/5000 [00:07<00:25, 152.16it/s]Running 5000 simulations.:  24%|██▍       | 1204/5000 [00:07<00:24, 152.26it/s]Running 5000 simulations.:  24%|██▍       | 1220/5000 [00:07<00:24, 152.39it/s]Running 5000 simulations.:  25%|██▍       | 1236/5000 [00:08<00:24, 152.10it/s]Running 5000 simulations.:  25%|██▌       | 1252/5000 [00:08<00:24, 152.32it/s]Running 5000 simulations.:  25%|██▌       | 1268/5000 [00:08<00:24, 152.39it/s]Running 5000 simulations.:  26%|██▌       | 1284/5000 [00:08<00:24, 152.41it/s]Running 5000 simulations.:  26%|██▌       | 1300/5000 [00:08<00:24, 152.30it/s]Running 5000 simulations.:  26%|██▋       | 1316/5000 [00:08<00:24, 152.20it/s]Running 5000 simulations.:  27%|██▋       | 1332/5000 [00:08<00:24, 152.62it/s]Running 5000 simulations.:  27%|██▋       | 1348/5000 [00:08<00:23, 152.43it/s]Running 5000 simulations.:  27%|██▋       | 1364/5000 [00:08<00:23, 151.84it/s]Running 5000 simulations.:  28%|██▊       | 1380/5000 [00:08<00:23, 151.85it/s]Running 5000 simulations.:  28%|██▊       | 1396/5000 [00:09<00:23, 151.60it/s]Running 5000 simulations.:  28%|██▊       | 1412/5000 [00:09<00:23, 151.60it/s]Running 5000 simulations.:  29%|██▊       | 1428/5000 [00:09<00:23, 152.06it/s]Running 5000 simulations.:  29%|██▉       | 1444/5000 [00:09<00:23, 152.31it/s]Running 5000 simulations.:  29%|██▉       | 1460/5000 [00:09<00:23, 152.54it/s]Running 5000 simulations.:  30%|██▉       | 1476/5000 [00:09<00:23, 152.72it/s]Running 5000 simulations.:  30%|██▉       | 1492/5000 [00:09<00:23, 152.49it/s]Running 5000 simulations.:  30%|███       | 1508/5000 [00:09<00:22, 152.26it/s]Running 5000 simulations.:  30%|███       | 1524/5000 [00:09<00:22, 152.44it/s]Running 5000 simulations.:  31%|███       | 1540/5000 [00:10<00:22, 152.66it/s]Running 5000 simulations.:  31%|███       | 1556/5000 [00:10<00:22, 152.50it/s]Running 5000 simulations.:  31%|███▏      | 1572/5000 [00:10<00:22, 152.18it/s]Running 5000 simulations.:  32%|███▏      | 1588/5000 [00:10<00:22, 151.94it/s]Running 5000 simulations.:  32%|███▏      | 1604/5000 [00:10<00:22, 151.48it/s]Running 5000 simulations.:  32%|███▏      | 1620/5000 [00:10<00:22, 151.31it/s]Running 5000 simulations.:  33%|███▎      | 1636/5000 [00:10<00:22, 150.93it/s]Running 5000 simulations.:  33%|███▎      | 1652/5000 [00:10<00:22, 150.71it/s]Running 5000 simulations.:  33%|███▎      | 1668/5000 [00:10<00:22, 150.73it/s]Running 5000 simulations.:  34%|███▎      | 1684/5000 [00:11<00:21, 150.94it/s]Running 5000 simulations.:  34%|███▍      | 1700/5000 [00:11<00:21, 151.37it/s]Running 5000 simulations.:  34%|███▍      | 1716/5000 [00:11<00:21, 151.43it/s]Running 5000 simulations.:  35%|███▍      | 1732/5000 [00:11<00:21, 151.19it/s]Running 5000 simulations.:  35%|███▍      | 1748/5000 [00:11<00:21, 150.99it/s]Running 5000 simulations.:  35%|███▌      | 1764/5000 [00:11<00:21, 150.92it/s]Running 5000 simulations.:  36%|███▌      | 1780/5000 [00:11<00:21, 151.17it/s]Running 5000 simulations.:  36%|███▌      | 1796/5000 [00:11<00:21, 151.11it/s]Running 5000 simulations.:  36%|███▌      | 1812/5000 [00:11<00:21, 150.89it/s]Running 5000 simulations.:  37%|███▋      | 1828/5000 [00:11<00:20, 151.09it/s]Running 5000 simulations.:  37%|███▋      | 1844/5000 [00:12<00:20, 150.77it/s]Running 5000 simulations.:  37%|███▋      | 1860/5000 [00:12<00:20, 150.74it/s]Running 5000 simulations.:  38%|███▊      | 1876/5000 [00:12<00:20, 150.69it/s]Running 5000 simulations.:  38%|███▊      | 1892/5000 [00:12<00:20, 151.22it/s]Running 5000 simulations.:  38%|███▊      | 1908/5000 [00:12<00:20, 150.95it/s]Running 5000 simulations.:  38%|███▊      | 1924/5000 [00:12<00:20, 150.46it/s]Running 5000 simulations.:  39%|███▉      | 1940/5000 [00:12<00:20, 150.75it/s]Running 5000 simulations.:  39%|███▉      | 1956/5000 [00:12<00:20, 150.69it/s]Running 5000 simulations.:  39%|███▉      | 1972/5000 [00:12<00:20, 150.55it/s]Running 5000 simulations.:  40%|███▉      | 1988/5000 [00:13<00:19, 150.67it/s]Running 5000 simulations.:  40%|████      | 2004/5000 [00:13<00:19, 150.76it/s]Running 5000 simulations.:  40%|████      | 2020/5000 [00:13<00:19, 150.84it/s]Running 5000 simulations.:  41%|████      | 2036/5000 [00:13<00:19, 151.08it/s]Running 5000 simulations.:  41%|████      | 2052/5000 [00:13<00:19, 151.12it/s]Running 5000 simulations.:  41%|████▏     | 2068/5000 [00:13<00:19, 151.18it/s]Running 5000 simulations.:  42%|████▏     | 2084/5000 [00:13<00:19, 151.19it/s]Running 5000 simulations.:  42%|████▏     | 2100/5000 [00:13<00:19, 151.00it/s]Running 5000 simulations.:  42%|████▏     | 2116/5000 [00:13<00:19, 151.32it/s]Running 5000 simulations.:  43%|████▎     | 2132/5000 [00:13<00:18, 151.30it/s]Running 5000 simulations.:  43%|████▎     | 2148/5000 [00:14<00:18, 150.88it/s]Running 5000 simulations.:  43%|████▎     | 2164/5000 [00:14<00:18, 150.67it/s]Running 5000 simulations.:  44%|████▎     | 2180/5000 [00:14<00:18, 150.50it/s]Running 5000 simulations.:  44%|████▍     | 2196/5000 [00:14<00:18, 150.49it/s]Running 5000 simulations.:  44%|████▍     | 2212/5000 [00:14<00:18, 150.64it/s]Running 5000 simulations.:  45%|████▍     | 2228/5000 [00:14<00:18, 150.84it/s]Running 5000 simulations.:  45%|████▍     | 2244/5000 [00:14<00:18, 151.04it/s]Running 5000 simulations.:  45%|████▌     | 2260/5000 [00:14<00:18, 150.95it/s]Running 5000 simulations.:  46%|████▌     | 2276/5000 [00:14<00:18, 150.82it/s]Running 5000 simulations.:  46%|████▌     | 2292/5000 [00:15<00:17, 150.76it/s]Running 5000 simulations.:  46%|████▌     | 2308/5000 [00:15<00:17, 150.73it/s]Running 5000 simulations.:  46%|████▋     | 2324/5000 [00:15<00:17, 150.45it/s]Running 5000 simulations.:  47%|████▋     | 2340/5000 [00:15<00:17, 150.90it/s]Running 5000 simulations.:  47%|████▋     | 2356/5000 [00:15<00:17, 150.92it/s]Running 5000 simulations.:  47%|████▋     | 2372/5000 [00:15<00:17, 151.06it/s]Running 5000 simulations.:  48%|████▊     | 2388/5000 [00:15<00:17, 151.22it/s]Running 5000 simulations.:  48%|████▊     | 2404/5000 [00:15<00:17, 150.85it/s]Running 5000 simulations.:  48%|████▊     | 2420/5000 [00:15<00:17, 151.16it/s]Running 5000 simulations.:  49%|████▊     | 2436/5000 [00:15<00:17, 150.81it/s]Running 5000 simulations.:  49%|████▉     | 2452/5000 [00:16<00:16, 150.92it/s]Running 5000 simulations.:  49%|████▉     | 2468/5000 [00:16<00:16, 150.47it/s]Running 5000 simulations.:  50%|████▉     | 2484/5000 [00:16<00:16, 150.60it/s]Running 5000 simulations.:  50%|█████     | 2500/5000 [00:16<00:16, 150.49it/s]Running 5000 simulations.:  50%|█████     | 2516/5000 [00:16<00:16, 150.61it/s]Running 5000 simulations.:  51%|█████     | 2532/5000 [00:16<00:16, 150.48it/s]Running 5000 simulations.:  51%|█████     | 2548/5000 [00:16<00:16, 150.64it/s]Running 5000 simulations.:  51%|█████▏    | 2564/5000 [00:16<00:16, 150.95it/s]Running 5000 simulations.:  52%|█████▏    | 2580/5000 [00:16<00:16, 150.80it/s]Running 5000 simulations.:  52%|█████▏    | 2596/5000 [00:17<00:15, 150.72it/s]Running 5000 simulations.:  52%|█████▏    | 2612/5000 [00:17<00:15, 150.83it/s]Running 5000 simulations.:  53%|█████▎    | 2628/5000 [00:17<00:15, 150.61it/s]Running 5000 simulations.:  53%|█████▎    | 2644/5000 [00:17<00:15, 150.75it/s]Running 5000 simulations.:  53%|█████▎    | 2660/5000 [00:17<00:15, 150.54it/s]Running 5000 simulations.:  54%|█████▎    | 2676/5000 [00:17<00:15, 150.73it/s]Running 5000 simulations.:  54%|█████▍    | 2692/5000 [00:17<00:15, 150.59it/s]Running 5000 simulations.:  54%|█████▍    | 2708/5000 [00:17<00:15, 150.40it/s]Running 5000 simulations.:  54%|█████▍    | 2724/5000 [00:17<00:15, 150.38it/s]Running 5000 simulations.:  55%|█████▍    | 2740/5000 [00:18<00:15, 150.46it/s]Running 5000 simulations.:  55%|█████▌    | 2756/5000 [00:18<00:14, 150.25it/s]Running 5000 simulations.:  55%|█████▌    | 2772/5000 [00:18<00:14, 150.50it/s]Running 5000 simulations.:  56%|█████▌    | 2788/5000 [00:18<00:14, 150.88it/s]Running 5000 simulations.:  56%|█████▌    | 2804/5000 [00:18<00:14, 150.70it/s]Running 5000 simulations.:  56%|█████▋    | 2820/5000 [00:18<00:14, 150.57it/s]Running 5000 simulations.:  57%|█████▋    | 2836/5000 [00:18<00:14, 150.45it/s]Running 5000 simulations.:  57%|█████▋    | 2852/5000 [00:18<00:14, 150.57it/s]Running 5000 simulations.:  57%|█████▋    | 2868/5000 [00:18<00:14, 150.71it/s]Running 5000 simulations.:  58%|█████▊    | 2884/5000 [00:18<00:14, 150.83it/s]Running 5000 simulations.:  58%|█████▊    | 2900/5000 [00:19<00:13, 151.17it/s]Running 5000 simulations.:  58%|█████▊    | 2916/5000 [00:19<00:13, 150.90it/s]Running 5000 simulations.:  59%|█████▊    | 2932/5000 [00:19<00:13, 150.89it/s]Running 5000 simulations.:  59%|█████▉    | 2948/5000 [00:19<00:13, 150.67it/s]Running 5000 simulations.:  59%|█████▉    | 2964/5000 [00:19<00:13, 150.74it/s]Running 5000 simulations.:  60%|█████▉    | 2980/5000 [00:19<00:13, 150.94it/s]Running 5000 simulations.:  60%|█████▉    | 2996/5000 [00:19<00:13, 150.96it/s]Running 5000 simulations.:  60%|██████    | 3012/5000 [00:19<00:13, 150.93it/s]Running 5000 simulations.:  61%|██████    | 3028/5000 [00:19<00:13, 151.08it/s]Running 5000 simulations.:  61%|██████    | 3044/5000 [00:20<00:12, 150.71it/s]Running 5000 simulations.:  61%|██████    | 3060/5000 [00:20<00:12, 150.31it/s]Running 5000 simulations.:  62%|██████▏   | 3076/5000 [00:20<00:12, 150.29it/s]Running 5000 simulations.:  62%|██████▏   | 3092/5000 [00:20<00:12, 150.50it/s]Running 5000 simulations.:  62%|██████▏   | 3108/5000 [00:20<00:12, 149.76it/s]Running 5000 simulations.:  62%|██████▏   | 3123/5000 [00:20<00:12, 148.58it/s]Running 5000 simulations.:  63%|██████▎   | 3138/5000 [00:20<00:12, 147.96it/s]Running 5000 simulations.:  63%|██████▎   | 3153/5000 [00:20<00:12, 146.93it/s]Running 5000 simulations.:  63%|██████▎   | 3168/5000 [00:20<00:12, 146.38it/s]Running 5000 simulations.:  64%|██████▎   | 3183/5000 [00:20<00:12, 146.09it/s]Running 5000 simulations.:  64%|██████▍   | 3198/5000 [00:21<00:12, 145.81it/s]Running 5000 simulations.:  64%|██████▍   | 3213/5000 [00:21<00:12, 145.46it/s]Running 5000 simulations.:  65%|██████▍   | 3228/5000 [00:21<00:12, 145.56it/s]Running 5000 simulations.:  65%|██████▍   | 3243/5000 [00:21<00:12, 145.94it/s]Running 5000 simulations.:  65%|██████▌   | 3258/5000 [00:21<00:11, 146.27it/s]Running 5000 simulations.:  65%|██████▌   | 3273/5000 [00:21<00:11, 146.03it/s]Running 5000 simulations.:  66%|██████▌   | 3288/5000 [00:21<00:11, 145.35it/s]Running 5000 simulations.:  66%|██████▌   | 3303/5000 [00:21<00:11, 145.20it/s]Running 5000 simulations.:  66%|██████▋   | 3318/5000 [00:21<00:11, 145.70it/s]Running 5000 simulations.:  67%|██████▋   | 3333/5000 [00:21<00:11, 146.06it/s]Running 5000 simulations.:  67%|██████▋   | 3348/5000 [00:22<00:11, 146.18it/s]Running 5000 simulations.:  67%|██████▋   | 3363/5000 [00:22<00:11, 146.52it/s]Running 5000 simulations.:  68%|██████▊   | 3378/5000 [00:22<00:11, 145.20it/s]Running 5000 simulations.:  68%|██████▊   | 3393/5000 [00:22<00:11, 143.32it/s]Running 5000 simulations.:  68%|██████▊   | 3408/5000 [00:22<00:11, 144.46it/s]Running 5000 simulations.:  68%|██████▊   | 3423/5000 [00:22<00:10, 144.72it/s]Running 5000 simulations.:  69%|██████▉   | 3438/5000 [00:22<00:10, 144.84it/s]Running 5000 simulations.:  69%|██████▉   | 3453/5000 [00:22<00:10, 144.52it/s]Running 5000 simulations.:  69%|██████▉   | 3468/5000 [00:22<00:10, 144.53it/s]Running 5000 simulations.:  70%|██████▉   | 3483/5000 [00:23<00:10, 144.84it/s]Running 5000 simulations.:  70%|██████▉   | 3498/5000 [00:23<00:10, 145.42it/s]Running 5000 simulations.:  70%|███████   | 3513/5000 [00:23<00:10, 144.08it/s]Running 5000 simulations.:  71%|███████   | 3528/5000 [00:23<00:10, 144.66it/s]Running 5000 simulations.:  71%|███████   | 3543/5000 [00:23<00:09, 145.99it/s]Running 5000 simulations.:  71%|███████   | 3558/5000 [00:23<00:09, 146.88it/s]Running 5000 simulations.:  71%|███████▏  | 3574/5000 [00:23<00:09, 147.92it/s]Running 5000 simulations.:  72%|███████▏  | 3590/5000 [00:23<00:09, 148.82it/s]Running 5000 simulations.:  72%|███████▏  | 3606/5000 [00:23<00:09, 149.40it/s]Running 5000 simulations.:  72%|███████▏  | 3622/5000 [00:23<00:09, 149.65it/s]Running 5000 simulations.:  73%|███████▎  | 3638/5000 [00:24<00:09, 150.08it/s]Running 5000 simulations.:  73%|███████▎  | 3654/5000 [00:24<00:08, 150.12it/s]Running 5000 simulations.:  73%|███████▎  | 3670/5000 [00:24<00:08, 149.84it/s]Running 5000 simulations.:  74%|███████▎  | 3685/5000 [00:24<00:08, 149.88it/s]Running 5000 simulations.:  74%|███████▍  | 3700/5000 [00:24<00:08, 149.91it/s]Running 5000 simulations.:  74%|███████▍  | 3715/5000 [00:24<00:08, 149.86it/s]Running 5000 simulations.:  75%|███████▍  | 3730/5000 [00:24<00:08, 149.84it/s]Running 5000 simulations.:  75%|███████▍  | 3746/5000 [00:24<00:08, 150.20it/s]Running 5000 simulations.:  75%|███████▌  | 3762/5000 [00:24<00:08, 150.38it/s]Running 5000 simulations.:  76%|███████▌  | 3778/5000 [00:25<00:08, 150.27it/s]Running 5000 simulations.:  76%|███████▌  | 3794/5000 [00:25<00:08, 150.29it/s]Running 5000 simulations.:  76%|███████▌  | 3810/5000 [00:25<00:07, 150.26it/s]Running 5000 simulations.:  77%|███████▋  | 3826/5000 [00:25<00:07, 150.18it/s]Running 5000 simulations.:  77%|███████▋  | 3842/5000 [00:25<00:07, 150.27it/s]Running 5000 simulations.:  77%|███████▋  | 3858/5000 [00:25<00:07, 150.30it/s]Running 5000 simulations.:  77%|███████▋  | 3874/5000 [00:25<00:07, 150.42it/s]Running 5000 simulations.:  78%|███████▊  | 3890/5000 [00:25<00:07, 150.32it/s]Running 5000 simulations.:  78%|███████▊  | 3906/5000 [00:25<00:07, 150.24it/s]Running 5000 simulations.:  78%|███████▊  | 3922/5000 [00:25<00:07, 147.70it/s]Running 5000 simulations.:  79%|███████▊  | 3937/5000 [00:26<00:07, 148.21it/s]Running 5000 simulations.:  79%|███████▉  | 3952/5000 [00:26<00:07, 148.66it/s]Running 5000 simulations.:  79%|███████▉  | 3967/5000 [00:26<00:06, 148.96it/s]Running 5000 simulations.:  80%|███████▉  | 3983/5000 [00:26<00:06, 149.42it/s]Running 5000 simulations.:  80%|███████▉  | 3999/5000 [00:26<00:06, 149.66it/s]Running 5000 simulations.:  80%|████████  | 4014/5000 [00:26<00:06, 149.66it/s]Running 5000 simulations.:  81%|████████  | 4029/5000 [00:26<00:06, 149.45it/s]Running 5000 simulations.:  81%|████████  | 4044/5000 [00:26<00:06, 149.58it/s]Running 5000 simulations.:  81%|████████  | 4060/5000 [00:26<00:06, 149.89it/s]Running 5000 simulations.:  82%|████████▏ | 4076/5000 [00:26<00:06, 150.05it/s]Running 5000 simulations.:  82%|████████▏ | 4092/5000 [00:27<00:06, 150.04it/s]Running 5000 simulations.:  82%|████████▏ | 4108/5000 [00:27<00:05, 150.06it/s]Running 5000 simulations.:  82%|████████▏ | 4124/5000 [00:27<00:05, 150.13it/s]Running 5000 simulations.:  83%|████████▎ | 4140/5000 [00:27<00:05, 150.00it/s]Running 5000 simulations.:  83%|████████▎ | 4156/5000 [00:27<00:05, 149.98it/s]Running 5000 simulations.:  83%|████████▎ | 4172/5000 [00:27<00:05, 150.19it/s]Running 5000 simulations.:  84%|████████▍ | 4188/5000 [00:27<00:05, 149.83it/s]Running 5000 simulations.:  84%|████████▍ | 4204/5000 [00:27<00:05, 150.08it/s]Running 5000 simulations.:  84%|████████▍ | 4220/5000 [00:27<00:05, 150.07it/s]Running 5000 simulations.:  85%|████████▍ | 4236/5000 [00:28<00:05, 150.19it/s]Running 5000 simulations.:  85%|████████▌ | 4252/5000 [00:28<00:04, 150.05it/s]Running 5000 simulations.:  85%|████████▌ | 4268/5000 [00:28<00:04, 149.98it/s]Running 5000 simulations.:  86%|████████▌ | 4284/5000 [00:28<00:04, 150.03it/s]Running 5000 simulations.:  86%|████████▌ | 4300/5000 [00:28<00:04, 150.25it/s]Running 5000 simulations.:  86%|████████▋ | 4316/5000 [00:28<00:04, 150.37it/s]Running 5000 simulations.:  87%|████████▋ | 4332/5000 [00:28<00:04, 150.42it/s]Running 5000 simulations.:  87%|████████▋ | 4348/5000 [00:28<00:04, 150.67it/s]Running 5000 simulations.:  87%|████████▋ | 4364/5000 [00:28<00:04, 150.70it/s]Running 5000 simulations.:  88%|████████▊ | 4380/5000 [00:29<00:04, 150.77it/s]Running 5000 simulations.:  88%|████████▊ | 4396/5000 [00:29<00:04, 150.63it/s]Running 5000 simulations.:  88%|████████▊ | 4412/5000 [00:29<00:04, 144.61it/s]Running 5000 simulations.:  89%|████████▊ | 4428/5000 [00:29<00:03, 146.42it/s]Running 5000 simulations.:  89%|████████▉ | 4444/5000 [00:29<00:03, 147.87it/s]Running 5000 simulations.:  89%|████████▉ | 4460/5000 [00:29<00:03, 148.96it/s]Running 5000 simulations.:  90%|████████▉ | 4476/5000 [00:29<00:03, 149.47it/s]Running 5000 simulations.:  90%|████████▉ | 4492/5000 [00:29<00:03, 149.65it/s]Running 5000 simulations.:  90%|█████████ | 4508/5000 [00:29<00:03, 149.99it/s]Running 5000 simulations.:  90%|█████████ | 4524/5000 [00:29<00:03, 150.13it/s]Running 5000 simulations.:  91%|█████████ | 4540/5000 [00:30<00:03, 150.31it/s]Running 5000 simulations.:  91%|█████████ | 4556/5000 [00:30<00:02, 150.49it/s]Running 5000 simulations.:  91%|█████████▏| 4572/5000 [00:30<00:02, 150.91it/s]Running 5000 simulations.:  92%|█████████▏| 4588/5000 [00:30<00:02, 150.96it/s]Running 5000 simulations.:  92%|█████████▏| 4604/5000 [00:30<00:02, 151.25it/s]Running 5000 simulations.:  92%|█████████▏| 4620/5000 [00:30<00:02, 151.22it/s]Running 5000 simulations.:  93%|█████████▎| 4636/5000 [00:30<00:02, 151.43it/s]Running 5000 simulations.:  93%|█████████▎| 4652/5000 [00:30<00:02, 151.16it/s]Running 5000 simulations.:  93%|█████████▎| 4668/5000 [00:30<00:02, 151.48it/s]Running 5000 simulations.:  94%|█████████▎| 4684/5000 [00:31<00:02, 151.64it/s]Running 5000 simulations.:  94%|█████████▍| 4700/5000 [00:31<00:01, 151.56it/s]Running 5000 simulations.:  94%|█████████▍| 4716/5000 [00:31<00:01, 151.62it/s]Running 5000 simulations.:  95%|█████████▍| 4732/5000 [00:31<00:01, 151.61it/s]Running 5000 simulations.:  95%|█████████▍| 4748/5000 [00:31<00:01, 151.52it/s]Running 5000 simulations.:  95%|█████████▌| 4764/5000 [00:31<00:01, 151.37it/s]Running 5000 simulations.:  96%|█████████▌| 4780/5000 [00:31<00:01, 151.58it/s]Running 5000 simulations.:  96%|█████████▌| 4796/5000 [00:31<00:01, 151.66it/s]Running 5000 simulations.:  96%|█████████▌| 4812/5000 [00:31<00:01, 151.57it/s]Running 5000 simulations.:  97%|█████████▋| 4828/5000 [00:31<00:01, 151.82it/s]Running 5000 simulations.:  97%|█████████▋| 4844/5000 [00:32<00:01, 151.88it/s]Running 5000 simulations.:  97%|█████████▋| 4860/5000 [00:32<00:00, 151.75it/s]Running 5000 simulations.:  98%|█████████▊| 4876/5000 [00:32<00:00, 151.74it/s]Running 5000 simulations.:  98%|█████████▊| 4892/5000 [00:32<00:00, 151.74it/s]Running 5000 simulations.:  98%|█████████▊| 4908/5000 [00:32<00:00, 152.38it/s]Running 5000 simulations.:  99%|█████████▊| 4926/5000 [00:32<00:00, 158.04it/s]Running 5000 simulations.:  99%|█████████▉| 4943/5000 [00:32<00:00, 160.06it/s]Running 5000 simulations.:  99%|█████████▉| 4960/5000 [00:32<00:00, 157.79it/s]Running 5000 simulations.: 100%|█████████▉| 4976/5000 [00:32<00:00, 155.14it/s]Running 5000 simulations.: 100%|█████████▉| 4992/5000 [00:33<00:00, 153.80it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:33<00:00, 151.03it/s]
Running 5000 simulations.:   0%|          | 0/5000 [00:00<?, ?it/s]Running 5000 simulations.:   0%|          | 16/5000 [00:00<00:31, 156.64it/s]Running 5000 simulations.:   1%|          | 32/5000 [00:00<00:31, 155.46it/s]Running 5000 simulations.:   1%|          | 48/5000 [00:00<00:31, 155.08it/s]Running 5000 simulations.:   1%|▏         | 64/5000 [00:00<00:31, 154.89it/s]Running 5000 simulations.:   2%|▏         | 80/5000 [00:00<00:31, 154.58it/s]Running 5000 simulations.:   2%|▏         | 96/5000 [00:00<00:31, 154.17it/s]Running 5000 simulations.:   2%|▏         | 112/5000 [00:00<00:31, 154.11it/s]Running 5000 simulations.:   3%|▎         | 128/5000 [00:00<00:31, 153.16it/s]Running 5000 simulations.:   3%|▎         | 144/5000 [00:00<00:31, 152.87it/s]Running 5000 simulations.:   3%|▎         | 160/5000 [00:01<00:31, 152.66it/s]Running 5000 simulations.:   4%|▎         | 176/5000 [00:01<00:31, 152.96it/s]Running 5000 simulations.:   4%|▍         | 192/5000 [00:01<00:31, 153.20it/s]Running 5000 simulations.:   4%|▍         | 208/5000 [00:01<00:31, 153.08it/s]Running 5000 simulations.:   4%|▍         | 224/5000 [00:01<00:31, 152.49it/s]Running 5000 simulations.:   5%|▍         | 240/5000 [00:01<00:31, 152.34it/s]Running 5000 simulations.:   5%|▌         | 256/5000 [00:01<00:31, 152.20it/s]Running 5000 simulations.:   5%|▌         | 272/5000 [00:01<00:31, 151.92it/s]Running 5000 simulations.:   6%|▌         | 288/5000 [00:01<00:31, 151.81it/s]Running 5000 simulations.:   6%|▌         | 304/5000 [00:01<00:30, 151.88it/s]Running 5000 simulations.:   6%|▋         | 320/5000 [00:02<00:31, 150.69it/s]Running 5000 simulations.:   7%|▋         | 336/5000 [00:02<00:30, 150.80it/s]Running 5000 simulations.:   7%|▋         | 352/5000 [00:02<00:30, 151.35it/s]Running 5000 simulations.:   7%|▋         | 368/5000 [00:02<00:30, 151.65it/s]Running 5000 simulations.:   8%|▊         | 384/5000 [00:02<00:30, 151.66it/s]Running 5000 simulations.:   8%|▊         | 400/5000 [00:02<00:30, 151.88it/s]Running 5000 simulations.:   8%|▊         | 416/5000 [00:02<00:30, 151.13it/s]Running 5000 simulations.:   9%|▊         | 432/5000 [00:02<00:30, 150.51it/s]Running 5000 simulations.:   9%|▉         | 448/5000 [00:02<00:30, 150.86it/s]Running 5000 simulations.:   9%|▉         | 464/5000 [00:03<00:30, 150.16it/s]Running 5000 simulations.:  10%|▉         | 480/5000 [00:03<00:30, 149.82it/s]Running 5000 simulations.:  10%|▉         | 496/5000 [00:03<00:29, 150.15it/s]Running 5000 simulations.:  10%|█         | 512/5000 [00:03<00:29, 150.04it/s]Running 5000 simulations.:  11%|█         | 528/5000 [00:03<00:29, 150.38it/s]Running 5000 simulations.:  11%|█         | 544/5000 [00:03<00:29, 149.76it/s]Running 5000 simulations.:  11%|█         | 559/5000 [00:03<00:29, 149.77it/s]Running 5000 simulations.:  12%|█▏        | 575/5000 [00:03<00:29, 150.12it/s]Running 5000 simulations.:  12%|█▏        | 591/5000 [00:03<00:29, 150.62it/s]Running 5000 simulations.:  12%|█▏        | 607/5000 [00:04<00:29, 150.87it/s]Running 5000 simulations.:  12%|█▏        | 623/5000 [00:04<00:28, 151.04it/s]Running 5000 simulations.:  13%|█▎        | 639/5000 [00:04<00:28, 150.61it/s]Running 5000 simulations.:  13%|█▎        | 655/5000 [00:04<00:28, 150.62it/s]Running 5000 simulations.:  13%|█▎        | 671/5000 [00:04<00:28, 150.41it/s]Running 5000 simulations.:  14%|█▎        | 687/5000 [00:04<00:28, 150.52it/s]Running 5000 simulations.:  14%|█▍        | 703/5000 [00:04<00:28, 150.66it/s]Running 5000 simulations.:  14%|█▍        | 719/5000 [00:04<00:28, 151.02it/s]Running 5000 simulations.:  15%|█▍        | 735/5000 [00:04<00:28, 150.17it/s]Running 5000 simulations.:  15%|█▌        | 751/5000 [00:04<00:28, 149.92it/s]Running 5000 simulations.:  15%|█▌        | 767/5000 [00:05<00:28, 150.40it/s]Running 5000 simulations.:  16%|█▌        | 783/5000 [00:05<00:28, 150.18it/s]Running 5000 simulations.:  16%|█▌        | 799/5000 [00:05<00:27, 150.39it/s]Running 5000 simulations.:  16%|█▋        | 815/5000 [00:05<00:27, 150.58it/s]Running 5000 simulations.:  17%|█▋        | 831/5000 [00:05<00:27, 149.48it/s]Running 5000 simulations.:  17%|█▋        | 846/5000 [00:05<00:27, 148.93it/s]Running 5000 simulations.:  17%|█▋        | 861/5000 [00:05<00:27, 148.55it/s]Running 5000 simulations.:  18%|█▊        | 876/5000 [00:05<00:27, 148.56it/s]Running 5000 simulations.:  18%|█▊        | 891/5000 [00:05<00:27, 148.43it/s]Running 5000 simulations.:  18%|█▊        | 906/5000 [00:05<00:27, 148.43it/s]Running 5000 simulations.:  18%|█▊        | 922/5000 [00:06<00:27, 149.63it/s]Running 5000 simulations.:  19%|█▉        | 939/5000 [00:06<00:26, 153.69it/s]Running 5000 simulations.:  19%|█▉        | 956/5000 [00:06<00:26, 155.44it/s]Running 5000 simulations.:  19%|█▉        | 972/5000 [00:06<00:26, 153.68it/s]Running 5000 simulations.:  20%|█▉        | 988/5000 [00:06<00:26, 152.91it/s]Running 5000 simulations.:  20%|██        | 1004/5000 [00:06<00:26, 151.96it/s]Running 5000 simulations.:  20%|██        | 1020/5000 [00:06<00:26, 151.22it/s]Running 5000 simulations.:  21%|██        | 1036/5000 [00:06<00:26, 149.21it/s]Running 5000 simulations.:  21%|██        | 1051/5000 [00:06<00:26, 148.83it/s]Running 5000 simulations.:  21%|██▏       | 1066/5000 [00:07<00:26, 148.93it/s]Running 5000 simulations.:  22%|██▏       | 1081/5000 [00:07<00:26, 149.06it/s]Running 5000 simulations.:  22%|██▏       | 1096/5000 [00:07<00:26, 149.18it/s]Running 5000 simulations.:  22%|██▏       | 1112/5000 [00:07<00:25, 149.63it/s]Running 5000 simulations.:  23%|██▎       | 1127/5000 [00:07<00:26, 148.10it/s]Running 5000 simulations.:  23%|██▎       | 1142/5000 [00:07<00:25, 148.39it/s]Running 5000 simulations.:  23%|██▎       | 1157/5000 [00:07<00:25, 148.41it/s]Running 5000 simulations.:  23%|██▎       | 1172/5000 [00:07<00:25, 148.37it/s]Running 5000 simulations.:  24%|██▍       | 1188/5000 [00:07<00:25, 148.95it/s]Running 5000 simulations.:  24%|██▍       | 1204/5000 [00:07<00:25, 149.40it/s]Running 5000 simulations.:  24%|██▍       | 1219/5000 [00:08<00:25, 147.82it/s]Running 5000 simulations.:  25%|██▍       | 1235/5000 [00:08<00:25, 148.48it/s]Running 5000 simulations.:  25%|██▌       | 1251/5000 [00:08<00:25, 148.99it/s]Running 5000 simulations.:  25%|██▌       | 1266/5000 [00:08<00:25, 149.11it/s]Running 5000 simulations.:  26%|██▌       | 1281/5000 [00:08<00:24, 148.97it/s]Running 5000 simulations.:  26%|██▌       | 1296/5000 [00:08<00:25, 148.04it/s]Running 5000 simulations.:  26%|██▌       | 1311/5000 [00:08<00:25, 146.48it/s]Running 5000 simulations.:  27%|██▋       | 1326/5000 [00:08<00:25, 145.95it/s]Running 5000 simulations.:  27%|██▋       | 1341/5000 [00:08<00:25, 146.36it/s]Running 5000 simulations.:  27%|██▋       | 1356/5000 [00:09<00:25, 142.56it/s]Running 5000 simulations.:  27%|██▋       | 1371/5000 [00:09<00:25, 142.89it/s]Running 5000 simulations.:  28%|██▊       | 1386/5000 [00:09<00:25, 143.60it/s]Running 5000 simulations.:  28%|██▊       | 1401/5000 [00:09<00:25, 143.70it/s]Running 5000 simulations.:  28%|██▊       | 1416/5000 [00:09<00:24, 145.27it/s]Running 5000 simulations.:  29%|██▊       | 1431/5000 [00:09<00:24, 146.64it/s]Running 5000 simulations.:  29%|██▉       | 1446/5000 [00:09<00:24, 144.68it/s]Running 5000 simulations.:  29%|██▉       | 1461/5000 [00:09<00:24, 145.18it/s]Running 5000 simulations.:  30%|██▉       | 1476/5000 [00:09<00:24, 146.54it/s]Running 5000 simulations.:  30%|██▉       | 1491/5000 [00:09<00:23, 146.21it/s]Running 5000 simulations.:  30%|███       | 1506/5000 [00:10<00:23, 145.80it/s]Running 5000 simulations.:  30%|███       | 1521/5000 [00:10<00:23, 146.12it/s]Running 5000 simulations.:  31%|███       | 1536/5000 [00:10<00:24, 141.86it/s]Running 5000 simulations.:  31%|███       | 1551/5000 [00:10<00:24, 140.77it/s]Running 5000 simulations.:  31%|███▏      | 1566/5000 [00:10<00:24, 142.31it/s]Running 5000 simulations.:  32%|███▏      | 1581/5000 [00:10<00:23, 143.31it/s]Running 5000 simulations.:  32%|███▏      | 1596/5000 [00:10<00:23, 143.79it/s]Running 5000 simulations.:  32%|███▏      | 1611/5000 [00:10<00:23, 144.37it/s]Running 5000 simulations.:  33%|███▎      | 1626/5000 [00:10<00:23, 145.68it/s]Running 5000 simulations.:  33%|███▎      | 1641/5000 [00:10<00:22, 146.88it/s]Running 5000 simulations.:  33%|███▎      | 1656/5000 [00:11<00:22, 147.56it/s]Running 5000 simulations.:  33%|███▎      | 1671/5000 [00:11<00:22, 148.08it/s]Running 5000 simulations.:  34%|███▎      | 1687/5000 [00:11<00:22, 148.69it/s]Running 5000 simulations.:  34%|███▍      | 1702/5000 [00:11<00:22, 148.94it/s]Running 5000 simulations.:  34%|███▍      | 1717/5000 [00:11<00:22, 147.89it/s]Running 5000 simulations.:  35%|███▍      | 1732/5000 [00:11<00:22, 148.35it/s]Running 5000 simulations.:  35%|███▍      | 1748/5000 [00:11<00:21, 148.93it/s]Running 5000 simulations.:  35%|███▌      | 1764/5000 [00:11<00:21, 149.49it/s]Running 5000 simulations.:  36%|███▌      | 1779/5000 [00:11<00:21, 149.60it/s]Running 5000 simulations.:  36%|███▌      | 1795/5000 [00:12<00:21, 150.07it/s]Running 5000 simulations.:  36%|███▌      | 1811/5000 [00:12<00:21, 148.90it/s]Running 5000 simulations.:  37%|███▋      | 1826/5000 [00:12<00:21, 148.77it/s]Running 5000 simulations.:  37%|███▋      | 1842/5000 [00:12<00:21, 149.25it/s]Running 5000 simulations.:  37%|███▋      | 1857/5000 [00:12<00:21, 148.97it/s]Running 5000 simulations.:  37%|███▋      | 1872/5000 [00:12<00:21, 148.95it/s]Running 5000 simulations.:  38%|███▊      | 1887/5000 [00:12<00:20, 149.17it/s]Running 5000 simulations.:  38%|███▊      | 1902/5000 [00:12<00:20, 148.05it/s]Running 5000 simulations.:  38%|███▊      | 1917/5000 [00:12<00:20, 148.27it/s]Running 5000 simulations.:  39%|███▊      | 1933/5000 [00:12<00:20, 149.00it/s]Running 5000 simulations.:  39%|███▉      | 1948/5000 [00:13<00:20, 149.02it/s]Running 5000 simulations.:  39%|███▉      | 1964/5000 [00:13<00:20, 149.35it/s]Running 5000 simulations.:  40%|███▉      | 1980/5000 [00:13<00:20, 149.76it/s]Running 5000 simulations.:  40%|███▉      | 1995/5000 [00:13<00:20, 148.42it/s]Running 5000 simulations.:  40%|████      | 2010/5000 [00:13<00:20, 148.29it/s]Running 5000 simulations.:  40%|████      | 2025/5000 [00:13<00:20, 148.42it/s]Running 5000 simulations.:  41%|████      | 2040/5000 [00:13<00:19, 148.07it/s]Running 5000 simulations.:  41%|████      | 2055/5000 [00:13<00:19, 147.95it/s]Running 5000 simulations.:  41%|████▏     | 2070/5000 [00:13<00:19, 147.91it/s]Running 5000 simulations.:  42%|████▏     | 2085/5000 [00:13<00:19, 148.18it/s]Running 5000 simulations.:  42%|████▏     | 2100/5000 [00:14<00:19, 148.49it/s]Running 5000 simulations.:  42%|████▏     | 2116/5000 [00:14<00:19, 149.92it/s]Running 5000 simulations.:  43%|████▎     | 2132/5000 [00:14<00:19, 150.20it/s]Running 5000 simulations.:  43%|████▎     | 2148/5000 [00:14<00:19, 149.51it/s]Running 5000 simulations.:  43%|████▎     | 2164/5000 [00:14<00:18, 149.71it/s]Running 5000 simulations.:  44%|████▎     | 2179/5000 [00:14<00:18, 149.51it/s]Running 5000 simulations.:  44%|████▍     | 2194/5000 [00:14<00:18, 149.58it/s]Running 5000 simulations.:  44%|████▍     | 2209/5000 [00:14<00:18, 149.12it/s]Running 5000 simulations.:  44%|████▍     | 2224/5000 [00:14<00:18, 149.31it/s]Running 5000 simulations.:  45%|████▍     | 2239/5000 [00:15<00:18, 147.91it/s]Running 5000 simulations.:  45%|████▌     | 2254/5000 [00:15<00:18, 148.10it/s]Running 5000 simulations.:  45%|████▌     | 2269/5000 [00:15<00:18, 148.48it/s]Running 5000 simulations.:  46%|████▌     | 2284/5000 [00:15<00:18, 148.89it/s]Running 5000 simulations.:  46%|████▌     | 2299/5000 [00:15<00:18, 149.07it/s]Running 5000 simulations.:  46%|████▋     | 2314/5000 [00:15<00:17, 149.34it/s]Running 5000 simulations.:  47%|████▋     | 2329/5000 [00:15<00:18, 148.05it/s]Running 5000 simulations.:  47%|████▋     | 2344/5000 [00:15<00:17, 148.40it/s]Running 5000 simulations.:  47%|████▋     | 2360/5000 [00:15<00:17, 149.24it/s]Running 5000 simulations.:  48%|████▊     | 2375/5000 [00:15<00:17, 149.07it/s]Running 5000 simulations.:  48%|████▊     | 2390/5000 [00:16<00:17, 149.02it/s]Running 5000 simulations.:  48%|████▊     | 2406/5000 [00:16<00:17, 149.41it/s]Running 5000 simulations.:  48%|████▊     | 2421/5000 [00:16<00:17, 148.40it/s]Running 5000 simulations.:  49%|████▊     | 2436/5000 [00:16<00:17, 148.29it/s]Running 5000 simulations.:  49%|████▉     | 2451/5000 [00:16<00:17, 148.46it/s]Running 5000 simulations.:  49%|████▉     | 2466/5000 [00:16<00:17, 148.68it/s]Running 5000 simulations.:  50%|████▉     | 2481/5000 [00:16<00:16, 149.05it/s]Running 5000 simulations.:  50%|████▉     | 2496/5000 [00:16<00:16, 149.22it/s]Running 5000 simulations.:  50%|█████     | 2512/5000 [00:16<00:16, 149.53it/s]Running 5000 simulations.:  51%|█████     | 2527/5000 [00:16<00:16, 149.16it/s]Running 5000 simulations.:  51%|█████     | 2542/5000 [00:17<00:16, 149.37it/s]Running 5000 simulations.:  51%|█████     | 2557/5000 [00:17<00:16, 149.39it/s]Running 5000 simulations.:  51%|█████▏    | 2572/5000 [00:17<00:16, 149.06it/s]Running 5000 simulations.:  52%|█████▏    | 2587/5000 [00:17<00:16, 149.18it/s]Running 5000 simulations.:  52%|█████▏    | 2602/5000 [00:17<00:16, 148.95it/s]Running 5000 simulations.:  52%|█████▏    | 2617/5000 [00:17<00:16, 146.90it/s]Running 5000 simulations.:  53%|█████▎    | 2632/5000 [00:17<00:16, 147.66it/s]Running 5000 simulations.:  53%|█████▎    | 2648/5000 [00:17<00:15, 148.40it/s]Running 5000 simulations.:  53%|█████▎    | 2664/5000 [00:17<00:15, 148.95it/s]Running 5000 simulations.:  54%|█████▎    | 2680/5000 [00:17<00:15, 149.57it/s]Running 5000 simulations.:  54%|█████▍    | 2695/5000 [00:18<00:15, 148.00it/s]Running 5000 simulations.:  54%|█████▍    | 2710/5000 [00:18<00:15, 147.78it/s]Running 5000 simulations.:  55%|█████▍    | 2726/5000 [00:18<00:15, 148.55it/s]Running 5000 simulations.:  55%|█████▍    | 2742/5000 [00:18<00:15, 149.29it/s]Running 5000 simulations.:  55%|█████▌    | 2757/5000 [00:18<00:15, 149.46it/s]Running 5000 simulations.:  55%|█████▌    | 2773/5000 [00:18<00:14, 150.37it/s]Running 5000 simulations.:  56%|█████▌    | 2789/5000 [00:18<00:14, 148.26it/s]Running 5000 simulations.:  56%|█████▌    | 2804/5000 [00:18<00:14, 147.85it/s]Running 5000 simulations.:  56%|█████▋    | 2819/5000 [00:18<00:14, 148.47it/s]Running 5000 simulations.:  57%|█████▋    | 2834/5000 [00:19<00:14, 148.53it/s]Running 5000 simulations.:  57%|█████▋    | 2849/5000 [00:19<00:14, 148.46it/s]Running 5000 simulations.:  57%|█████▋    | 2865/5000 [00:19<00:14, 149.15it/s]Running 5000 simulations.:  58%|█████▊    | 2881/5000 [00:19<00:14, 149.60it/s]Running 5000 simulations.:  58%|█████▊    | 2897/5000 [00:19<00:14, 150.02it/s]Running 5000 simulations.:  58%|█████▊    | 2913/5000 [00:19<00:13, 150.79it/s]Running 5000 simulations.:  59%|█████▊    | 2929/5000 [00:19<00:13, 149.28it/s]Running 5000 simulations.:  59%|█████▉    | 2944/5000 [00:19<00:13, 149.14it/s]Running 5000 simulations.:  59%|█████▉    | 2960/5000 [00:19<00:13, 149.76it/s]Running 5000 simulations.:  60%|█████▉    | 2975/5000 [00:19<00:13, 149.81it/s]Running 5000 simulations.:  60%|█████▉    | 2991/5000 [00:20<00:13, 149.94it/s]Running 5000 simulations.:  60%|██████    | 3007/5000 [00:20<00:13, 150.01it/s]Running 5000 simulations.:  60%|██████    | 3023/5000 [00:20<00:13, 147.74it/s]Running 5000 simulations.:  61%|██████    | 3038/5000 [00:20<00:13, 147.84it/s]Running 5000 simulations.:  61%|██████    | 3054/5000 [00:20<00:13, 148.79it/s]Running 5000 simulations.:  61%|██████▏   | 3069/5000 [00:20<00:12, 148.95it/s]Running 5000 simulations.:  62%|██████▏   | 3085/5000 [00:20<00:12, 149.80it/s]Running 5000 simulations.:  62%|██████▏   | 3101/5000 [00:20<00:12, 150.24it/s]Running 5000 simulations.:  62%|██████▏   | 3117/5000 [00:20<00:12, 148.59it/s]Running 5000 simulations.:  63%|██████▎   | 3132/5000 [00:20<00:12, 148.64it/s]Running 5000 simulations.:  63%|██████▎   | 3147/5000 [00:21<00:12, 147.92it/s]Running 5000 simulations.:  63%|██████▎   | 3163/5000 [00:21<00:12, 148.78it/s]Running 5000 simulations.:  64%|██████▎   | 3179/5000 [00:21<00:12, 149.52it/s]Running 5000 simulations.:  64%|██████▍   | 3195/5000 [00:21<00:12, 149.64it/s]Running 5000 simulations.:  64%|██████▍   | 3210/5000 [00:21<00:12, 147.50it/s]Running 5000 simulations.:  64%|██████▍   | 3225/5000 [00:21<00:12, 147.72it/s]Running 5000 simulations.:  65%|██████▍   | 3241/5000 [00:21<00:11, 148.59it/s]Running 5000 simulations.:  65%|██████▌   | 3257/5000 [00:21<00:11, 149.21it/s]Running 5000 simulations.:  65%|██████▌   | 3273/5000 [00:21<00:11, 149.61it/s]Running 5000 simulations.:  66%|██████▌   | 3289/5000 [00:22<00:11, 149.95it/s]Running 5000 simulations.:  66%|██████▌   | 3304/5000 [00:22<00:11, 148.10it/s]Running 5000 simulations.:  66%|██████▋   | 3320/5000 [00:22<00:11, 149.20it/s]Running 5000 simulations.:  67%|██████▋   | 3336/5000 [00:22<00:11, 149.66it/s]Running 5000 simulations.:  67%|██████▋   | 3351/5000 [00:22<00:11, 149.50it/s]Running 5000 simulations.:  67%|██████▋   | 3367/5000 [00:22<00:10, 149.71it/s]Running 5000 simulations.:  68%|██████▊   | 3382/5000 [00:22<00:10, 149.56it/s]Running 5000 simulations.:  68%|██████▊   | 3397/5000 [00:22<00:10, 147.37it/s]Running 5000 simulations.:  68%|██████▊   | 3412/5000 [00:22<00:10, 148.08it/s]Running 5000 simulations.:  69%|██████▊   | 3427/5000 [00:22<00:10, 148.37it/s]Running 5000 simulations.:  69%|██████▉   | 3442/5000 [00:23<00:10, 148.74it/s]Running 5000 simulations.:  69%|██████▉   | 3458/5000 [00:23<00:10, 149.27it/s]Running 5000 simulations.:  69%|██████▉   | 3473/5000 [00:23<00:10, 149.30it/s]Running 5000 simulations.:  70%|██████▉   | 3488/5000 [00:23<00:10, 147.81it/s]Running 5000 simulations.:  70%|███████   | 3503/5000 [00:23<00:10, 148.14it/s]Running 5000 simulations.:  70%|███████   | 3518/5000 [00:23<00:09, 148.68it/s]Running 5000 simulations.:  71%|███████   | 3533/5000 [00:23<00:09, 149.04it/s]Running 5000 simulations.:  71%|███████   | 3548/5000 [00:23<00:09, 149.20it/s]Running 5000 simulations.:  71%|███████▏  | 3564/5000 [00:23<00:09, 149.63it/s]Running 5000 simulations.:  72%|███████▏  | 3579/5000 [00:23<00:09, 148.86it/s]Running 5000 simulations.:  72%|███████▏  | 3594/5000 [00:24<00:09, 148.44it/s]Running 5000 simulations.:  72%|███████▏  | 3610/5000 [00:24<00:09, 148.96it/s]Running 5000 simulations.:  72%|███████▎  | 3625/5000 [00:24<00:09, 148.72it/s]Running 5000 simulations.:  73%|███████▎  | 3640/5000 [00:24<00:09, 148.10it/s]Running 5000 simulations.:  73%|███████▎  | 3655/5000 [00:24<00:09, 147.42it/s]Running 5000 simulations.:  73%|███████▎  | 3670/5000 [00:24<00:09, 141.75it/s]Running 5000 simulations.:  74%|███████▎  | 3685/5000 [00:24<00:09, 143.55it/s]Running 5000 simulations.:  74%|███████▍  | 3700/5000 [00:24<00:08, 144.93it/s]Running 5000 simulations.:  74%|███████▍  | 3715/5000 [00:24<00:08, 145.63it/s]Running 5000 simulations.:  75%|███████▍  | 3730/5000 [00:25<00:08, 146.35it/s]Running 5000 simulations.:  75%|███████▍  | 3745/5000 [00:25<00:08, 145.21it/s]Running 5000 simulations.:  75%|███████▌  | 3760/5000 [00:25<00:08, 145.34it/s]Running 5000 simulations.:  76%|███████▌  | 3776/5000 [00:25<00:08, 146.97it/s]Running 5000 simulations.:  76%|███████▌  | 3792/5000 [00:25<00:08, 148.43it/s]Running 5000 simulations.:  76%|███████▌  | 3807/5000 [00:25<00:08, 148.34it/s]Running 5000 simulations.:  76%|███████▋  | 3822/5000 [00:25<00:07, 148.58it/s]Running 5000 simulations.:  77%|███████▋  | 3838/5000 [00:25<00:07, 149.47it/s]Running 5000 simulations.:  77%|███████▋  | 3854/5000 [00:25<00:07, 149.93it/s]Running 5000 simulations.:  77%|███████▋  | 3870/5000 [00:25<00:07, 150.44it/s]Running 5000 simulations.:  78%|███████▊  | 3886/5000 [00:26<00:07, 150.71it/s]Running 5000 simulations.:  78%|███████▊  | 3902/5000 [00:26<00:07, 150.94it/s]Running 5000 simulations.:  78%|███████▊  | 3918/5000 [00:26<00:07, 150.80it/s]Running 5000 simulations.:  79%|███████▊  | 3934/5000 [00:26<00:07, 150.98it/s]Running 5000 simulations.:  79%|███████▉  | 3950/5000 [00:26<00:06, 151.08it/s]Running 5000 simulations.:  79%|███████▉  | 3966/5000 [00:26<00:06, 151.63it/s]Running 5000 simulations.:  80%|███████▉  | 3982/5000 [00:26<00:06, 151.98it/s]Running 5000 simulations.:  80%|███████▉  | 3998/5000 [00:26<00:06, 152.28it/s]Running 5000 simulations.:  80%|████████  | 4014/5000 [00:26<00:06, 152.59it/s]Running 5000 simulations.:  81%|████████  | 4030/5000 [00:27<00:06, 152.21it/s]Running 5000 simulations.:  81%|████████  | 4046/5000 [00:27<00:06, 151.77it/s]Running 5000 simulations.:  81%|████████  | 4062/5000 [00:27<00:06, 151.51it/s]Running 5000 simulations.:  82%|████████▏ | 4078/5000 [00:27<00:06, 151.42it/s]Running 5000 simulations.:  82%|████████▏ | 4094/5000 [00:27<00:05, 151.09it/s]Running 5000 simulations.:  82%|████████▏ | 4110/5000 [00:27<00:05, 151.16it/s]Running 5000 simulations.:  83%|████████▎ | 4126/5000 [00:27<00:05, 151.15it/s]Running 5000 simulations.:  83%|████████▎ | 4142/5000 [00:27<00:05, 151.78it/s]Running 5000 simulations.:  83%|████████▎ | 4158/5000 [00:27<00:05, 151.50it/s]Running 5000 simulations.:  83%|████████▎ | 4174/5000 [00:27<00:05, 151.68it/s]Running 5000 simulations.:  84%|████████▍ | 4190/5000 [00:28<00:05, 151.82it/s]Running 5000 simulations.:  84%|████████▍ | 4206/5000 [00:28<00:05, 151.71it/s]Running 5000 simulations.:  84%|████████▍ | 4222/5000 [00:28<00:05, 151.70it/s]Running 5000 simulations.:  85%|████████▍ | 4238/5000 [00:28<00:05, 151.89it/s]Running 5000 simulations.:  85%|████████▌ | 4254/5000 [00:28<00:04, 152.53it/s]Running 5000 simulations.:  85%|████████▌ | 4270/5000 [00:28<00:04, 152.32it/s]Running 5000 simulations.:  86%|████████▌ | 4286/5000 [00:28<00:04, 151.76it/s]Running 5000 simulations.:  86%|████████▌ | 4302/5000 [00:28<00:04, 151.31it/s]Running 5000 simulations.:  86%|████████▋ | 4318/5000 [00:28<00:04, 151.35it/s]Running 5000 simulations.:  87%|████████▋ | 4334/5000 [00:29<00:04, 150.85it/s]Running 5000 simulations.:  87%|████████▋ | 4350/5000 [00:29<00:04, 150.06it/s]Running 5000 simulations.:  87%|████████▋ | 4366/5000 [00:29<00:04, 150.41it/s]Running 5000 simulations.:  88%|████████▊ | 4382/5000 [00:29<00:04, 150.66it/s]Running 5000 simulations.:  88%|████████▊ | 4398/5000 [00:29<00:03, 150.88it/s]Running 5000 simulations.:  88%|████████▊ | 4414/5000 [00:29<00:03, 151.08it/s]Running 5000 simulations.:  89%|████████▊ | 4430/5000 [00:29<00:03, 151.61it/s]Running 5000 simulations.:  89%|████████▉ | 4446/5000 [00:29<00:03, 151.85it/s]Running 5000 simulations.:  89%|████████▉ | 4462/5000 [00:29<00:03, 151.92it/s]Running 5000 simulations.:  90%|████████▉ | 4478/5000 [00:29<00:03, 151.40it/s]Running 5000 simulations.:  90%|████████▉ | 4494/5000 [00:30<00:03, 151.16it/s]Running 5000 simulations.:  90%|█████████ | 4510/5000 [00:30<00:03, 151.29it/s]Running 5000 simulations.:  91%|█████████ | 4526/5000 [00:30<00:03, 151.47it/s]Running 5000 simulations.:  91%|█████████ | 4542/5000 [00:30<00:03, 151.51it/s]Running 5000 simulations.:  91%|█████████ | 4558/5000 [00:30<00:02, 151.64it/s]Running 5000 simulations.:  91%|█████████▏| 4574/5000 [00:30<00:02, 151.93it/s]Running 5000 simulations.:  92%|█████████▏| 4590/5000 [00:30<00:02, 152.53it/s]Running 5000 simulations.:  92%|█████████▏| 4606/5000 [00:30<00:02, 152.56it/s]Running 5000 simulations.:  92%|█████████▏| 4622/5000 [00:30<00:02, 152.87it/s]Running 5000 simulations.:  93%|█████████▎| 4638/5000 [00:31<00:02, 152.00it/s]Running 5000 simulations.:  93%|█████████▎| 4654/5000 [00:31<00:02, 151.01it/s]Running 5000 simulations.:  93%|█████████▎| 4670/5000 [00:31<00:02, 149.44it/s]Running 5000 simulations.:  94%|█████████▎| 4686/5000 [00:31<00:02, 150.06it/s]Running 5000 simulations.:  94%|█████████▍| 4702/5000 [00:31<00:01, 151.64it/s]Running 5000 simulations.:  94%|█████████▍| 4718/5000 [00:31<00:01, 152.70it/s]Running 5000 simulations.:  95%|█████████▍| 4734/5000 [00:31<00:01, 153.21it/s]Running 5000 simulations.:  95%|█████████▌| 4750/5000 [00:31<00:01, 153.99it/s]Running 5000 simulations.:  95%|█████████▌| 4766/5000 [00:31<00:01, 154.29it/s]Running 5000 simulations.:  96%|█████████▌| 4782/5000 [00:31<00:01, 154.12it/s]Running 5000 simulations.:  96%|█████████▌| 4798/5000 [00:32<00:01, 154.45it/s]Running 5000 simulations.:  96%|█████████▋| 4814/5000 [00:32<00:01, 153.95it/s]Running 5000 simulations.:  97%|█████████▋| 4830/5000 [00:32<00:01, 153.42it/s]Running 5000 simulations.:  97%|█████████▋| 4846/5000 [00:32<00:01, 153.28it/s]Running 5000 simulations.:  97%|█████████▋| 4862/5000 [00:32<00:00, 153.14it/s]Running 5000 simulations.:  98%|█████████▊| 4878/5000 [00:32<00:00, 151.76it/s]Running 5000 simulations.:  98%|█████████▊| 4894/5000 [00:32<00:00, 151.94it/s]Running 5000 simulations.:  98%|█████████▊| 4910/5000 [00:32<00:00, 152.23it/s]Running 5000 simulations.:  99%|█████████▊| 4926/5000 [00:32<00:00, 152.18it/s]Running 5000 simulations.:  99%|█████████▉| 4942/5000 [00:33<00:00, 152.78it/s]Running 5000 simulations.:  99%|█████████▉| 4958/5000 [00:33<00:00, 152.04it/s]Running 5000 simulations.:  99%|█████████▉| 4974/5000 [00:33<00:00, 151.68it/s]Running 5000 simulations.: 100%|█████████▉| 4990/5000 [00:33<00:00, 151.26it/s]Running 5000 simulations.: 100%|██████████| 5000/5000 [00:33<00:00, 149.63it/s]
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 359061.41it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18066it [00:00, 322701.97it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 360323.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 359942.67it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 15014it [00:00, 269882.40it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17069it [00:00, 308865.60it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 360066.27it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 359439.88it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 356509.96it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 363565.00it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19852it [00:00, 357480.04it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 364779.18it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 358662.25it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 356576.64it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 355244.77it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18785it [00:00, 333594.43it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19752it [00:00, 353638.77it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19998it [00:00, 362861.84it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19060it [00:00, 342006.92it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 357570.67it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 354824.04it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 358432.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 357814.71it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 360227.08it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 356710.07it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 358423.19it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19685it [00:00, 352791.59it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 362343.22it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 360837.59it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18104it [00:00, 349096.29it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 353878.03it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17331it [00:00, 295894.37it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 356413.01it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19332it [00:00, 346810.46it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18345it [00:00, 329295.51it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16466it [00:00, 293648.18it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 357220.46it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 353055.89it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 355750.98it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 358527.36it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19091it [00:00, 344433.08it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19995it [00:00, 361946.21it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 356767.72it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 353866.09it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19183it [00:00, 346365.56it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18576it [00:00, 331281.32it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19107it [00:00, 339447.78it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19992it [00:00, 363465.26it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16704it [00:00, 303615.28it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 351120.00it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 349781.84it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19999it [00:00, 353554.58it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 348161.70it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 353594.62it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 360927.64it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 356831.46it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19723it [00:00, 359306.69it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19997it [00:00, 359888.68it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 357454.87it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 15183it [00:00, 272119.06it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18385it [00:00, 337629.78it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18053it [00:00, 323421.52it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18439it [00:00, 336599.44it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18417it [00:00, 327017.75it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 15767it [00:00, 282821.52it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18458it [00:00, 335780.08it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18363it [00:00, 324717.23it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18392it [00:00, 334163.48it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18481it [00:00, 330795.30it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18348it [00:00, 337127.43it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18403it [00:00, 336478.26it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18410it [00:00, 330448.13it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18431it [00:00, 331133.99it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18433it [00:00, 334619.95it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18399it [00:00, 327993.64it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18320it [00:00, 333628.51it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18412it [00:00, 329736.15it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18429it [00:00, 330174.53it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18355it [00:00, 328727.98it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18348it [00:00, 333456.49it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18428it [00:00, 337010.28it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18385it [00:00, 329866.70it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18382it [00:00, 325708.22it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18374it [00:00, 331992.75it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18441it [00:00, 333270.83it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18364it [00:00, 329208.26it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18306it [00:00, 331717.23it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18391it [00:00, 336669.79it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18397it [00:00, 330666.20it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18382it [00:00, 328333.91it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18321it [00:00, 330762.96it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17630it [00:00, 314875.08it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18260it [00:00, 323139.72it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18313it [00:00, 333311.44it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17419it [00:00, 310994.02it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18265it [00:00, 325952.59it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18252it [00:00, 329667.67it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18241it [00:00, 330216.97it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18204it [00:00, 328533.30it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18235it [00:00, 324949.90it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18248it [00:00, 325057.27it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18322it [00:00, 326149.67it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18311it [00:00, 327220.40it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18224it [00:00, 329852.27it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18289it [00:00, 326587.93it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18250it [00:00, 326076.14it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 15828it [00:00, 281910.75it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18282it [00:00, 325389.30it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18288it [00:00, 326444.99it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18361it [00:00, 326544.56it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18291it [00:00, 329648.10it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18239it [00:00, 328770.27it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18289it [00:00, 325960.66it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18228it [00:00, 323050.48it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18259it [00:00, 327791.08it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18305it [00:00, 325427.19it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18338it [00:00, 333069.82it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18325it [00:00, 328593.38it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18250it [00:00, 325128.80it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18330it [00:00, 326443.09it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 354332.44it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16937it [00:00, 295020.73it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 361827.47it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19946it [00:00, 358113.40it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16549it [00:00, 296297.48it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18110it [00:00, 327507.63it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 366071.78it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 351484.86it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 359704.98it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 356631.21it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19680it [00:00, 352851.24it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19998it [00:00, 357163.44it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 354797.03it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 358190.56it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19974it [00:00, 359040.30it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19348it [00:00, 347240.07it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19033it [00:00, 343858.49it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 361874.29it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19823it [00:00, 355859.34it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 295583.76it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 354812.03it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 361730.73it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 360955.59it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 359384.45it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 359526.15it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 363184.08it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 19341it [00:00, 348051.20it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 357336.11it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 100%|██████████| 10000/10000 [00:00<00:00, 355088.38it/s]
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17428it [00:00, 316715.83it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18340it [00:00, 333361.66it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17688it [00:00, 317960.84it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18292it [00:00, 329670.38it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18307it [00:00, 334159.28it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17435it [00:00, 318363.12it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18307it [00:00, 331755.42it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18313it [00:00, 329690.44it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18319it [00:00, 323218.30it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18328it [00:00, 330364.62it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18397it [00:00, 331930.74it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18282it [00:00, 327827.94it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18282it [00:00, 333562.14it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18251it [00:00, 324192.35it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18294it [00:00, 330870.82it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18249it [00:00, 325282.30it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18301it [00:00, 326935.84it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18351it [00:00, 328357.70it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18285it [00:00, 330276.51it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18390it [00:00, 328919.43it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18290it [00:00, 323805.87it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18254it [00:00, 326815.86it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18356it [00:00, 327279.95it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18348it [00:00, 327613.60it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18311it [00:00, 327548.36it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18292it [00:00, 330881.73it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18287it [00:00, 329263.34it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18341it [00:00, 329397.11it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18302it [00:00, 332032.06it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18392it [00:00, 333895.90it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18319it [00:00, 328999.18it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18260it [00:00, 330378.40it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18262it [00:00, 332858.41it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18315it [00:00, 325685.03it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18415it [00:00, 330495.45it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 14721it [00:00, 268972.93it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18357it [00:00, 333771.91it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18263it [00:00, 328451.93it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18366it [00:00, 327763.38it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18274it [00:00, 326118.75it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18347it [00:00, 330391.88it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18299it [00:00, 331808.28it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18370it [00:00, 334215.46it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18362it [00:00, 329877.37it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18283it [00:00, 326660.19it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18472it [00:00, 334559.04it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18317it [00:00, 330669.40it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18263it [00:00, 329137.82it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18335it [00:00, 327673.02it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18297it [00:00, 329326.06it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18333it [00:00, 330292.93it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18315it [00:00, 326982.61it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18326it [00:00, 332108.34it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18305it [00:00, 327663.22it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18334it [00:00, 333521.43it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18312it [00:00, 327781.53it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18406it [00:00, 333392.47it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18325it [00:00, 334734.02it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18359it [00:00, 334459.27it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18301it [00:00, 327397.40it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18306it [00:00, 327801.43it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18422it [00:00, 331950.56it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18395it [00:00, 334113.78it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18379it [00:00, 330824.68it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18372it [00:00, 330728.49it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 17915it [00:00, 321640.27it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18420it [00:00, 336248.21it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18456it [00:00, 331878.84it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18443it [00:00, 336990.73it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18349it [00:00, 329834.55it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18410it [00:00, 333368.46it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18405it [00:00, 328400.69it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18389it [00:00, 333850.11it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18387it [00:00, 330981.47it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18417it [00:00, 330838.53it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18359it [00:00, 332452.13it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18428it [00:00, 327600.00it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18380it [00:00, 332184.16it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18394it [00:00, 329177.67it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18376it [00:00, 330065.27it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18365it [00:00, 332395.75it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18419it [00:00, 336253.37it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18368it [00:00, 328348.12it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18350it [00:00, 330362.22it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18445it [00:00, 328263.96it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18387it [00:00, 327935.82it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18388it [00:00, 327907.65it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18363it [00:00, 329229.74it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18417it [00:00, 332890.16it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18330it [00:00, 325306.31it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18446it [00:00, 327565.96it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18278it [00:00, 328247.38it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18300it [00:00, 330503.33it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18374it [00:00, 330302.34it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18323it [00:00, 333884.65it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16627it [00:00, 303430.70it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18368it [00:00, 332629.47it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18386it [00:00, 337565.38it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18324it [00:00, 327748.75it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18318it [00:00, 328632.24it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18370it [00:00, 327939.41it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18366it [00:00, 336373.62it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18397it [00:00, 329253.83it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18362it [00:00, 332539.48it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18331it [00:00, 332216.18it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18333it [00:00, 279247.59it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18333it [00:00, 328482.65it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18048it [00:00, 325114.99it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18337it [00:00, 328816.99it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18357it [00:00, 332024.57it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18313it [00:00, 323857.64it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18342it [00:00, 325149.19it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18364it [00:00, 329319.46it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18324it [00:00, 326368.11it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18305it [00:00, 327825.51it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18384it [00:00, 327060.39it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18351it [00:00, 328605.83it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18318it [00:00, 326728.81it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18305it [00:00, 326933.49it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18345it [00:00, 329001.24it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18361it [00:00, 325397.99it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18296it [00:00, 338821.42it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18208it [00:00, 330750.19it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18240it [00:00, 328928.25it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18254it [00:00, 330117.47it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 16609it [00:00, 300961.23it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18239it [00:00, 326488.77it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18310it [00:00, 341124.53it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18231it [00:00, 321814.55it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18305it [00:00, 331118.31it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18261it [00:00, 327919.62it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18295it [00:00, 332388.13it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18286it [00:00, 332266.36it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18321it [00:00, 329967.60it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18258it [00:00, 331517.74it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18262it [00:00, 328880.98it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18355it [00:00, 331748.06it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18238it [00:00, 329338.21it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18255it [00:00, 326357.33it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18296it [00:00, 331274.11it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18255it [00:00, 331859.78it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18315it [00:00, 325830.08it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18248it [00:00, 328256.76it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18303it [00:00, 331787.58it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18152it [00:00, 322072.36it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18247it [00:00, 330646.68it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18322it [00:00, 342165.77it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18259it [00:00, 328620.94it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18269it [00:00, 324895.97it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18289it [00:00, 329053.57it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]Drawing 10000 posterior samples: 18294it [00:00, 325539.33it/s]           
lstm_sbi.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  points=torch.tensor(true_theta),
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr_value = np.array(value)
/home/SHARED/software/anaconda3/2020.07e/lib/python3.8/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr_value = np.array(value)
30
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Training neural network. Epochs trained:  340Training neural network. Epochs trained:  341Training neural network. Epochs trained:  342Training neural network. Epochs trained:  343Training neural network. Epochs trained:  344Training neural network. Epochs trained:  345Training neural network. Epochs trained:  346Training neural network. Epochs trained:  347Training neural network. Epochs trained:  348Training neural network. Epochs trained:  349Training neural network. Epochs trained:  350Training neural network. Epochs trained:  351Training neural network. Epochs trained:  352Training neural network. Epochs trained:  353Training neural network. Epochs trained:  354Training neural network. Epochs trained:  355Training neural network. Epochs trained:  356Training neural network. Epochs trained:  357Training neural network. Epochs trained:  358Training neural network. Epochs trained:  359Training neural network. Epochs trained:  360Training neural network. Epochs trained:  361Training neural network. Epochs trained:  362Training neural network. Epochs trained:  363Training neural network. Epochs trained:  364Training neural network. Epochs trained:  365Training neural network. Epochs trained:  366Training neural network. Epochs trained:  367Training neural network. Epochs trained:  368Training neural network. Epochs trained:  369Training neural network. Epochs trained:  370Training neural network. Epochs trained:  371Training neural network. Epochs trained:  372Training neural network. Epochs trained:  373Training neural network. Epochs trained:  374Training neural network. Epochs trained:  375Training neural network. Epochs trained:  376Neural network successfully converged after 376 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Neural network successfully converged after 170 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Neural network successfully converged after 337 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Neural network successfully converged after 226 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Neural network successfully converged after 313 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Neural network successfully converged after 221 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Neural network successfully converged after 325 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Neural network successfully converged after 271 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Training neural network. Epochs trained:  340Training neural network. Epochs trained:  341Training neural network. Epochs trained:  342Training neural network. Epochs trained:  343Training neural network. Epochs trained:  344Training neural network. Epochs trained:  345Training neural network. Epochs trained:  346Training neural network. Epochs trained:  347Training neural network. Epochs trained:  348Training neural network. Epochs trained:  349Training neural network. Epochs trained:  350Training neural network. Epochs trained:  351Training neural network. Epochs trained:  352Training neural network. Epochs trained:  353Training neural network. Epochs trained:  354Training neural network. Epochs trained:  355Training neural network. Epochs trained:  356Training neural network. Epochs trained:  357Training neural network. Epochs trained:  358Training neural network. Epochs trained:  359Training neural network. Epochs trained:  360Training neural network. Epochs trained:  361Training neural network. Epochs trained:  362Training neural network. Epochs trained:  363Training neural network. Epochs trained:  364Training neural network. Epochs trained:  365Training neural network. Epochs trained:  366Training neural network. Epochs trained:  367Training neural network. Epochs trained:  368Training neural network. Epochs trained:  369Training neural network. Epochs trained:  370Training neural network. Epochs trained:  371Training neural network. Epochs trained:  372Training neural network. Epochs trained:  373Training neural network. Epochs trained:  374Training neural network. Epochs trained:  375Training neural network. Epochs trained:  376Training neural network. Epochs trained:  377Training neural network. Epochs trained:  378Training neural network. Epochs trained:  379Training neural network. Epochs trained:  380Training neural network. Epochs trained:  381Training neural network. Epochs trained:  382Training neural network. Epochs trained:  383Training neural network. Epochs trained:  384Training neural network. Epochs trained:  385Training neural network. Epochs trained:  386Training neural network. Epochs trained:  387Training neural network. Epochs trained:  388Training neural network. Epochs trained:  389Training neural network. Epochs trained:  390Training neural network. Epochs trained:  391Training neural network. Epochs trained:  392Training neural network. Epochs trained:  393Training neural network. Epochs trained:  394Training neural network. Epochs trained:  395Training neural network. Epochs trained:  396Training neural network. Epochs trained:  397Training neural network. Epochs trained:  398Training neural network. Epochs trained:  399Training neural network. Epochs trained:  400Training neural network. Epochs trained:  401Neural network successfully converged after 401 epochs.
Training neural network. Epochs trained:  1Training neural network. Epochs trained:  2Training neural network. Epochs trained:  3Training neural network. Epochs trained:  4Training neural network. Epochs trained:  5Training neural network. Epochs trained:  6Training neural network. Epochs trained:  7Training neural network. Epochs trained:  8Training neural network. Epochs trained:  9Training neural network. Epochs trained:  10Training neural network. Epochs trained:  11Training neural network. Epochs trained:  12Training neural network. Epochs trained:  13Training neural network. Epochs trained:  14Training neural network. Epochs trained:  15Training neural network. Epochs trained:  16Training neural network. Epochs trained:  17Training neural network. Epochs trained:  18Training neural network. Epochs trained:  19Training neural network. Epochs trained:  20Training neural network. Epochs trained:  21Training neural network. Epochs trained:  22Training neural network. Epochs trained:  23Training neural network. Epochs trained:  24Training neural network. Epochs trained:  25Training neural network. Epochs trained:  26Training neural network. Epochs trained:  27Training neural network. Epochs trained:  28Training neural network. Epochs trained:  29Training neural network. Epochs trained:  30Training neural network. Epochs trained:  31Training neural network. Epochs trained:  32Training neural network. Epochs trained:  33Training neural network. Epochs trained:  34Training neural network. Epochs trained:  35Training neural network. Epochs trained:  36Training neural network. Epochs trained:  37Training neural network. Epochs trained:  38Training neural network. Epochs trained:  39Training neural network. Epochs trained:  40Training neural network. Epochs trained:  41Training neural network. Epochs trained:  42Training neural network. Epochs trained:  43Training neural network. Epochs trained:  44Training neural network. Epochs trained:  45Training neural network. Epochs trained:  46Training neural network. Epochs trained:  47Training neural network. Epochs trained:  48Training neural network. Epochs trained:  49Training neural network. Epochs trained:  50Training neural network. Epochs trained:  51Training neural network. Epochs trained:  52Training neural network. Epochs trained:  53Training neural network. Epochs trained:  54Training neural network. Epochs trained:  55Training neural network. Epochs trained:  56Training neural network. Epochs trained:  57Training neural network. Epochs trained:  58Training neural network. Epochs trained:  59Training neural network. Epochs trained:  60Training neural network. Epochs trained:  61Training neural network. Epochs trained:  62Training neural network. Epochs trained:  63Training neural network. Epochs trained:  64Training neural network. Epochs trained:  65Training neural network. Epochs trained:  66Training neural network. Epochs trained:  67Training neural network. Epochs trained:  68Training neural network. Epochs trained:  69Training neural network. Epochs trained:  70Training neural network. Epochs trained:  71Training neural network. Epochs trained:  72Training neural network. Epochs trained:  73Training neural network. Epochs trained:  74Training neural network. Epochs trained:  75Training neural network. Epochs trained:  76Training neural network. Epochs trained:  77Training neural network. Epochs trained:  78Training neural network. Epochs trained:  79Training neural network. Epochs trained:  80Training neural network. Epochs trained:  81Training neural network. Epochs trained:  82Training neural network. Epochs trained:  83Training neural network. Epochs trained:  84Training neural network. Epochs trained:  85Training neural network. Epochs trained:  86Training neural network. Epochs trained:  87Training neural network. Epochs trained:  88Training neural network. Epochs trained:  89Training neural network. Epochs trained:  90Training neural network. Epochs trained:  91Training neural network. Epochs trained:  92Training neural network. Epochs trained:  93Training neural network. Epochs trained:  94Training neural network. Epochs trained:  95Training neural network. Epochs trained:  96Training neural network. Epochs trained:  97Training neural network. Epochs trained:  98Training neural network. Epochs trained:  99Training neural network. Epochs trained:  100Training neural network. Epochs trained:  101Training neural network. Epochs trained:  102Training neural network. Epochs trained:  103Training neural network. Epochs trained:  104Training neural network. Epochs trained:  105Training neural network. Epochs trained:  106Training neural network. Epochs trained:  107Training neural network. Epochs trained:  108Training neural network. Epochs trained:  109Training neural network. Epochs trained:  110Training neural network. Epochs trained:  111Training neural network. Epochs trained:  112Training neural network. Epochs trained:  113Training neural network. Epochs trained:  114Training neural network. Epochs trained:  115Training neural network. Epochs trained:  116Training neural network. Epochs trained:  117Training neural network. Epochs trained:  118Training neural network. Epochs trained:  119Training neural network. Epochs trained:  120Training neural network. Epochs trained:  121Training neural network. Epochs trained:  122Training neural network. Epochs trained:  123Training neural network. Epochs trained:  124Training neural network. Epochs trained:  125Training neural network. Epochs trained:  126Training neural network. Epochs trained:  127Training neural network. Epochs trained:  128Training neural network. Epochs trained:  129Training neural network. Epochs trained:  130Training neural network. Epochs trained:  131Training neural network. Epochs trained:  132Training neural network. Epochs trained:  133Training neural network. Epochs trained:  134Training neural network. Epochs trained:  135Training neural network. Epochs trained:  136Training neural network. Epochs trained:  137Training neural network. Epochs trained:  138Training neural network. Epochs trained:  139Training neural network. Epochs trained:  140Training neural network. Epochs trained:  141Training neural network. Epochs trained:  142Training neural network. Epochs trained:  143Training neural network. Epochs trained:  144Training neural network. Epochs trained:  145Training neural network. Epochs trained:  146Training neural network. Epochs trained:  147Training neural network. Epochs trained:  148Training neural network. Epochs trained:  149Training neural network. Epochs trained:  150Training neural network. Epochs trained:  151Training neural network. Epochs trained:  152Training neural network. Epochs trained:  153Training neural network. Epochs trained:  154Training neural network. Epochs trained:  155Training neural network. Epochs trained:  156Training neural network. Epochs trained:  157Training neural network. Epochs trained:  158Training neural network. Epochs trained:  159Training neural network. Epochs trained:  160Training neural network. Epochs trained:  161Training neural network. Epochs trained:  162Training neural network. Epochs trained:  163Training neural network. Epochs trained:  164Training neural network. Epochs trained:  165Training neural network. Epochs trained:  166Training neural network. Epochs trained:  167Training neural network. Epochs trained:  168Training neural network. Epochs trained:  169Training neural network. Epochs trained:  170Training neural network. Epochs trained:  171Training neural network. Epochs trained:  172Training neural network. Epochs trained:  173Training neural network. Epochs trained:  174Training neural network. Epochs trained:  175Training neural network. Epochs trained:  176Training neural network. Epochs trained:  177Training neural network. Epochs trained:  178Training neural network. Epochs trained:  179Training neural network. Epochs trained:  180Training neural network. Epochs trained:  181Training neural network. Epochs trained:  182Training neural network. Epochs trained:  183Training neural network. Epochs trained:  184Training neural network. Epochs trained:  185Training neural network. Epochs trained:  186Training neural network. Epochs trained:  187Training neural network. Epochs trained:  188Training neural network. Epochs trained:  189Training neural network. Epochs trained:  190Training neural network. Epochs trained:  191Training neural network. Epochs trained:  192Training neural network. Epochs trained:  193Training neural network. Epochs trained:  194Training neural network. Epochs trained:  195Training neural network. Epochs trained:  196Training neural network. Epochs trained:  197Training neural network. Epochs trained:  198Training neural network. Epochs trained:  199Training neural network. Epochs trained:  200Training neural network. Epochs trained:  201Training neural network. Epochs trained:  202Training neural network. Epochs trained:  203Training neural network. Epochs trained:  204Training neural network. Epochs trained:  205Training neural network. Epochs trained:  206Training neural network. Epochs trained:  207Training neural network. Epochs trained:  208Training neural network. Epochs trained:  209Training neural network. Epochs trained:  210Training neural network. Epochs trained:  211Training neural network. Epochs trained:  212Training neural network. Epochs trained:  213Training neural network. Epochs trained:  214Training neural network. Epochs trained:  215Training neural network. Epochs trained:  216Training neural network. Epochs trained:  217Training neural network. Epochs trained:  218Training neural network. Epochs trained:  219Training neural network. Epochs trained:  220Training neural network. Epochs trained:  221Training neural network. Epochs trained:  222Training neural network. Epochs trained:  223Training neural network. Epochs trained:  224Training neural network. Epochs trained:  225Training neural network. Epochs trained:  226Training neural network. Epochs trained:  227Training neural network. Epochs trained:  228Training neural network. Epochs trained:  229Training neural network. Epochs trained:  230Training neural network. Epochs trained:  231Training neural network. Epochs trained:  232Training neural network. Epochs trained:  233Training neural network. Epochs trained:  234Training neural network. Epochs trained:  235Training neural network. Epochs trained:  236Training neural network. Epochs trained:  237Training neural network. Epochs trained:  238Training neural network. Epochs trained:  239Training neural network. Epochs trained:  240Training neural network. Epochs trained:  241Training neural network. Epochs trained:  242Training neural network. Epochs trained:  243Training neural network. Epochs trained:  244Training neural network. Epochs trained:  245Training neural network. Epochs trained:  246Training neural network. Epochs trained:  247Training neural network. Epochs trained:  248Training neural network. Epochs trained:  249Training neural network. Epochs trained:  250Training neural network. Epochs trained:  251Training neural network. Epochs trained:  252Training neural network. Epochs trained:  253Training neural network. Epochs trained:  254Training neural network. Epochs trained:  255Training neural network. Epochs trained:  256Training neural network. Epochs trained:  257Training neural network. Epochs trained:  258Training neural network. Epochs trained:  259Training neural network. Epochs trained:  260Training neural network. Epochs trained:  261Training neural network. Epochs trained:  262Training neural network. Epochs trained:  263Training neural network. Epochs trained:  264Training neural network. Epochs trained:  265Training neural network. Epochs trained:  266Training neural network. Epochs trained:  267Training neural network. Epochs trained:  268Training neural network. Epochs trained:  269Training neural network. Epochs trained:  270Training neural network. Epochs trained:  271Training neural network. Epochs trained:  272Training neural network. Epochs trained:  273Training neural network. Epochs trained:  274Training neural network. Epochs trained:  275Training neural network. Epochs trained:  276Training neural network. Epochs trained:  277Training neural network. Epochs trained:  278Training neural network. Epochs trained:  279Training neural network. Epochs trained:  280Training neural network. Epochs trained:  281Training neural network. Epochs trained:  282Training neural network. Epochs trained:  283Training neural network. Epochs trained:  284Training neural network. Epochs trained:  285Training neural network. Epochs trained:  286Training neural network. Epochs trained:  287Training neural network. Epochs trained:  288Training neural network. Epochs trained:  289Training neural network. Epochs trained:  290Training neural network. Epochs trained:  291Training neural network. Epochs trained:  292Training neural network. Epochs trained:  293Training neural network. Epochs trained:  294Training neural network. Epochs trained:  295Training neural network. Epochs trained:  296Training neural network. Epochs trained:  297Training neural network. Epochs trained:  298Training neural network. Epochs trained:  299Training neural network. Epochs trained:  300Training neural network. Epochs trained:  301Training neural network. Epochs trained:  302Training neural network. Epochs trained:  303Training neural network. Epochs trained:  304Training neural network. Epochs trained:  305Training neural network. Epochs trained:  306Training neural network. Epochs trained:  307Training neural network. Epochs trained:  308Training neural network. Epochs trained:  309Training neural network. Epochs trained:  310Training neural network. Epochs trained:  311Training neural network. Epochs trained:  312Training neural network. Epochs trained:  313Training neural network. Epochs trained:  314Training neural network. Epochs trained:  315Training neural network. Epochs trained:  316Training neural network. Epochs trained:  317Training neural network. Epochs trained:  318Training neural network. Epochs trained:  319Training neural network. Epochs trained:  320Training neural network. Epochs trained:  321Training neural network. Epochs trained:  322Training neural network. Epochs trained:  323Training neural network. Epochs trained:  324Training neural network. Epochs trained:  325Training neural network. Epochs trained:  326Training neural network. Epochs trained:  327Training neural network. Epochs trained:  328Training neural network. Epochs trained:  329Training neural network. Epochs trained:  330Training neural network. Epochs trained:  331Training neural network. Epochs trained:  332Training neural network. Epochs trained:  333Training neural network. Epochs trained:  334Training neural network. Epochs trained:  335Training neural network. Epochs trained:  336Training neural network. Epochs trained:  337Training neural network. Epochs trained:  338Training neural network. Epochs trained:  339Training neural network. Epochs trained:  340Training neural network. Epochs trained:  341Neural network successfully converged after 341 epochs.
log prob true 6.5981073
log prob true 6.08541
log prob true 6.1878085
log prob true 6.0540767
log prob true 5.2968755
log prob true 5.9423285
log prob true 5.5720654
log prob true 6.34495
log prob true 5.722201
log prob true 6.189149
log prob true 6.0352488
log prob true 6.1181645
log prob true 6.625503
log prob true 6.283266
log prob true 5.6583385
log prob true 5.4362383
log prob true 5.708617
log prob true 5.926439
log prob true 5.871702
log prob true 6.18396
log prob true 6.3889403
log prob true 6.1283474
log prob true 6.408256
log prob true 5.8907537
log prob true 6.2901397
log prob true 6.004779
log prob true 5.3063755
log prob true 5.86758
log prob true 6.232621
log prob true 5.432955
log prob true 5.9020977
log prob true 5.427278
log prob true 5.7053237
log prob true 5.3133807
log prob true 4.3109436
log prob true 5.254841
log prob true 5.0814137
log prob true 5.598247
log prob true 5.1826878
log prob true 5.5100837
log prob true 5.324061
log prob true 5.5421553
log prob true 6.0404744
log prob true 5.660272
log prob true 5.092
log prob true 4.885417
log prob true 5.117604
log prob true 5.1911354
log prob true 5.218996
log prob true 5.7299204
log prob true 5.7522964
log prob true 5.5094905
log prob true 5.8520317
log prob true 5.186629
log prob true 5.6552744
log prob true 5.39983
log prob true 4.703344
log prob true 5.1856446
log prob true 5.5866423
log prob true 5.2582664
log prob true 4.0788827
log prob true 3.789348
log prob true 3.8886409
log prob true 3.1597443
log prob true 3.0421002
log prob true 2.8664372
log prob true 3.4722157
log prob true 3.971316
log prob true 3.6509323
log prob true 3.9338276
log prob true 3.257029
log prob true 3.9592478
log prob true 4.146884
log prob true 3.7884166
log prob true 3.0863523
log prob true 2.8033276
log prob true 3.3235962
log prob true 3.8709254
log prob true 2.9033513
log prob true 4.0722466
log prob true 3.79695
log prob true 3.6179917
log prob true 3.5030499
log prob true 3.6559408
log prob true 4.086372
log prob true 3.8478556
log prob true 2.9662385
log prob true 3.3716533
log prob true 3.983359
log prob true 2.6674366
log prob true 4.192972
log prob true 3.6546528
log prob true 4.078531
log prob true 3.5276644
log prob true 2.7187994
log prob true 2.8598163
log prob true 3.4114637
log prob true 3.980779
log prob true 3.51673
log prob true 3.860463
log prob true 3.28242
log prob true 3.916725
log prob true 3.613714
log prob true 3.9297616
log prob true 3.2839668
log prob true 2.5556414
log prob true 3.2646103
log prob true 3.6709461
log prob true 3.208052
log prob true 4.0326023
log prob true 3.5848303
log prob true 3.5459962
log prob true 4.257192
log prob true 3.5910668
log prob true 4.0573225
log prob true 3.810941
log prob true 2.9485524
log prob true 3.3528173
log prob true 4.0299277
log prob true 2.5335414
log prob true 6.2216372
log prob true 5.865399
log prob true 6.0972047
log prob true 5.648434
log prob true 5.462347
log prob true 5.5292225
log prob true 5.5100636
log prob true 5.940789
log prob true 5.6751094
log prob true 5.905801
log prob true 5.5629773
log prob true 5.8774953
log prob true 6.3568997
log prob true 6.0750036
log prob true 5.573335
log prob true 5.114958
log prob true 5.3980594
log prob true 5.86046
log prob true 5.1236672
log prob true 6.1836953
log prob true 6.0495853
log prob true 5.831536
log prob true 6.1819015
log prob true 5.6562777
log prob true 6.0048122
log prob true 5.927693
log prob true 4.963371
log prob true 5.490474
log prob true 6.1604505
log prob true 5.3283596
log prob true 4.277496
log prob true 3.4974568
log prob true 3.4760685
log prob true 3.3682096
log prob true 2.966475
log prob true 2.5475736
log prob true 3.1647406
log prob true 3.9512093
log prob true 3.52968
log prob true 3.7619383
log prob true 3.1564796
log prob true 3.7830503
log prob true 4.002336
log prob true 3.9253223
log prob true 2.7831547
log prob true 2.7399375
log prob true 3.4012222
log prob true 3.817517
log prob true 2.5489209
log prob true 3.9401321
log prob true 3.7801178
log prob true 3.3744671
log prob true 3.6012146
log prob true 3.4680576
log prob true 4.0904913
log prob true 3.5875244
log prob true 2.8475106
log prob true 3.3128116
log prob true 3.614771
log prob true 2.8863935
log prob true 4.3005853
log prob true 3.6686249
log prob true 3.8866148
log prob true 3.068522
log prob true 2.5108418
log prob true 2.7804298
log prob true 3.514085
log prob true 4.095932
log prob true 3.3555484
log prob true 3.7340138
log prob true 3.2854679
log prob true 3.8803794
log prob true 4.164293
log prob true 3.935321
log prob true 3.0507638
log prob true 2.581287
log prob true 3.058288
log prob true 3.3372862
log prob true 2.7610817
log prob true 4.12928
log prob true 3.8119016
log prob true 3.6289747
log prob true 4.0559645
log prob true 3.6133926
log prob true 3.8280995
log prob true 3.954855
log prob true 2.974538
log prob true 3.519886
log prob true 3.8494642
log prob true 2.6773617
log prob true 4.0298147
log prob true 3.4890459
log prob true 3.832913
log prob true 3.0063748
log prob true 3.0947096
log prob true 2.7092845
log prob true 3.4752789
log prob true 4.1087365
log prob true 3.5611684
log prob true 3.4654653
log prob true 2.8042104
log prob true 3.8395205
log prob true 4.078045
log prob true 3.6870315
log prob true 3.1021676
log prob true 2.7903507
log prob true 3.4959657
log prob true 3.845489
log prob true 2.822512
log prob true 4.1775846
log prob true 3.6159825
log prob true 3.3729384
log prob true 3.9420154
log prob true 3.5426583
log prob true 4.039355
log prob true 3.8677778
log prob true 2.8368876
log prob true 3.543196
log prob true 3.8161743
log prob true 2.6176064
log prob true 4.380007
log prob true 4.1121683
log prob true 4.171365
log prob true 3.559235
log prob true 2.9922986
log prob true 2.9288778
log prob true 3.8531275
log prob true 4.1436863
log prob true 3.9221914
log prob true 3.948849
log prob true 3.3038611
log prob true 4.192049
log prob true 4.427208
log prob true 4.116375
log prob true 3.4286437
log prob true 2.921559
log prob true 3.3315883
log prob true 3.8208394
log prob true 3.0424519
log prob true 4.3064585
log prob true 4.13796
log prob true 3.7891269
log prob true 4.1665616
log prob true 3.8772159
log prob true 4.305869
log prob true 4.179915
log prob true 3.2500186
log prob true 3.5252612
log prob true 4.080231
log prob true 2.9539828
log prob true 4.2435427
log prob true 3.8529713
log prob true 4.040775
log prob true 3.1662073
log prob true 3.3321455
log prob true 2.793104
log prob true 3.6766484
log prob true 4.111999
log prob true 3.5210316
log prob true 3.0714755
log prob true 3.046995
log prob true 4.055755
log prob true 4.4924006
log prob true 3.888936
log prob true 3.2516353
log prob true 2.7533374
log prob true 3.6391184
log prob true 3.8786469
log prob true 2.987967
log prob true 4.215919
log prob true 3.9830594
log prob true 3.81783
log prob true 4.101676
log prob true 3.7577133
log prob true 4.046894
log prob true 4.064315
log prob true 3.1345656
log prob true 3.5455365
log prob true 3.8538756
log prob true 2.9103734
script complete
