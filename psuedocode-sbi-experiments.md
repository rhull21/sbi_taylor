Updated 02052022

PseudoCode:

Questions:
    * ~~Build in Notebook?~~
    * ~~Maybe train an ensemble of 5 surrogates, just in case you can't find a biased one...~~
    * L.H.C. for Experiment 1 training fraction
    * ~~Noise for simulator, positive AND negative?~~ (it is...)
    * MAF, how does it work?
    * 'merged' posterior = desirable, or mixing information....
    * Set up MC
    
References:
    * `02_05_lstm_A` = LSTM for Experiments 1, and `2_1, 2_2a, 2_2b`
    


0. Shared Qualities of Experiments and Abstracted From Psuedocode:
    * Domain Selection, ParFlow Ensemble Definition, Data Compression for LSTM training
1. Experiment 1: "find parameters for synthetic observation(s) reserved from the trained surrogate model with a defined degree of randomness"
    1. Build Surrogate of ParFlow streamflow, `lstm_A`
        * Structure: 
            * `q_{t+1} = lstm_exp1(forcings_{t->t-14}, theta1, theta2)`
        * Hyperparameters:
            * MC_Dropout = False
            * bs = 50 # batch size
            * num_epochs = 300 # number of times iterating through the model
            * learning_rate = 0.001 # rate of learning
            * input_size = 10 # nodes on the input (should be number of features)
            * hidden_size = 10 # number of nodes in hidden layer 
            * num_layers = 1 # number of hidden layers
            * num_classes = 1 # nodes in output (should be 1)
            * criterion = torch.nn.MSELoss()# MSELoss()    # mean-squared error for regression
            * optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # Adam as optimizer
        * Number of Surrogates Trained:
            * `num_members` = 1 
        * Training, Validation, Testing
            * `train_frac`, `val_frac`, `test_frac` = 0.7, 0.2, 0.1
            * `train_frac`, `val_frac` sampled L.H.C. in theta space
        * Metrics of Performance
            * `NSE`, `RMSE` on test fraction, `q_PF v q_surr`
    2. Construct amortized (full) P(`theta | q_surr_all`) via a Neural Density Estimator, or `NDE`
        * Prior 
            * Uniform across entire space of theta1, theta2
        * Simulator (streamflow generated by lstm for construction of P(`theta | q_surr_all`))
            * `q_surr_i` = `lstm_A` + `noise`, or:
                * `q_surr_i = q_in + q_in * torch.randn(y_in.shape) * f_noise`, where
                    * `q_in` : 'deterministic' full streamflow time series (len=350) generated by surrogate
                    * `q_surr_i`  : 'stochastic' streamflow time series to be used to construct P(`theta | q_surr_all`), given:
                        * product of (1) `y_in`, (2) a vector (len=350) of random (gaussian) noise 0-1 times, and (3) a noise factor (`f_noise`) for scaling   
                            * `f_noise` is some factor to control the amplitude of randomness (default=`1e-02`)
        * NDE Architecture and Parameters
            * method ='SNPE' # (Sequential Neural Posterior Estimation)
            * model = 'maf' # (Masked Autoregressive Flow)
            * hidden_features = 10
            * num_transforms = 2
            * others=defaults, from: 
                * `https://www.mackelab.org/sbi/reference/#sbi.utils.get_nn_models.posterior_nn`
                * `https://www.mackelab.org/sbi/reference/#sbi.inference.snpe.snpe_c.SNPE_C`
        *  P(`theta | q_surr_all`) realization:
            * `L_sims` = 10 # Number of conditional probability models generated
                * NOTE: Need multiple because of instability (non convergence)
            * `n_sims` = 1000 # Number of forward runs of surrogate to generate suitable posterior / L_sim
                * NOTE: Adding more doesn't in general increase performance
    3. Sample posterior, P(`theta | q_surr_all = q_surr_obs`)
        * @ `n_obs=100` synthetic observations, `q_surr_obs`
            * This set of observations, `{q_surr_obs}`, from parameters pairs `{theta1, theta2}` sampled in a LHC way across the entire prior space.
                * Also includes set of parameters from `test_frac`
            * 'Observations', `{q_surr_obs}`, are forward simulations of `lstm_exp1({theta1, theta2})`
                * Deterministic (no noise added)
        * For each realization, `L_sims` = 10, of P(`theta | q_surr_all`), sample at observation `q_surr_obs`: 
            * `L_samples` = 10 # of times taking a sample per invdividual observation `q_surr_obs`
                * NOTE: Need multiple because of instability (from initialization)
            * `n_samples` = 5000 # of samples per individual observation `q_surr_obs`
                * NOTE: Adding more doesn't in general increase performance
        * Two expressions of the posterior, P(`theta | q_surr_all = q_surr_obs`), could be explored: 
            1. Many 'Differentiated' posteriors P(`theta | q_surr_all` = `q_surr_obs`)
                * There will be `L_sims=10` * `L_samples=10` unique expressions of the posterior distributions in this experiment for all `n_obs=100` observations
            2. One 'Merged' posterior P(`theta | q_surr_all` = `q_surr_obs`): 
                * Construct 'merged' posterior P(`theta | q_surr_all` = `q_surr_obs`), such that:
                    - `posterior_array_merged` = `torch.empty(2, 10*10*5000)`
                    - For `L_sim` in `L_sims`: # for each realization, `P(theta | q_surr_all)`
                        - For `L_samp` in `L_samples`: # for sample sample
                            - # sample `L_sim` 5000 times, and add to `posterior_array_merged`
            * Comment, retaining differentiated posterior is probably more robust representation of uncertainty, although the merged posterior could be an easier object to manage.
    4. Interpretive Plots:
        * Performance in Parameter Space:
            * Individual for `n_obs=100` synthetic observations, `q_surr_obs`:
                * Show each P(`theta | q_surr_all` = `q_surr_obs`) as pairplot
            * Bulk performance across parameter space for all `n_obs=100` synthetic observations
                * colorflod showing `1. Euclidean Distance`, `2. Determinant` and, `3. theta_true_log_prob`
                    1. `Euclidean Distance`
                        * the `mean` and `std` of (the distance between the true parameter and the mean of the array representing a sample) 
                            * over `L_samples = 10` posterior samples each of `L_sims = 10` P(`theta | q_surr_all`)  
                    2. `Determinant`
                        * the `mean` and `std` of (the determinant of the array representing a sample) 
                            * over `L_samples = 10` posterior samples each of `L_sims = 10` P(`theta | q_surr_all`) 
                    3. `theta_true_log_prob`
                        * based on some tolerance (see `genProbThetas`)
        * Performance in Streamflow Space:
            * Principle: 
                * Bootstrap n=50 parameters pairs `{theta1, theta2}` from P(`theta | q_surr_all` = `q_surr_obs`)
                * Run forward (+ noise) n=50 to generate streamflow accounting for uncertainties in:
                    1. Parameters
                    2. Simulator
            * Individual for each `n_obs=100` synthetic observations, `q_surr_obs`:
                * Show each (`q_surr_all | q_surr_obs`) as streamflow time series containing:
                    1. Central Tendency, `q_surr_all`
                    2. Max, Min bounds, `q_surr_all`
                    3. True, `q_surr_obs`
            * Bulk Performance:
                * colorflod showing `1. Mean NSE` of `q_surr_obs` v `q_surr_all`; `2. STD NSE` of `q_surr_all`
            
2. Experiment 2: find parameters for synthetic observation(s) reserved from the process-based model upon which the surrogate model was trained.   
    1. Dimension 1: Using the single surrogate model from `lstm_A`
        1. Use Surrogate of ParFlow streamflow, `lstm_A`
            - **Same as Experiment 1**
        2. Use amortized (full) P(`theta | q_surr_all`) via a Neural Density Estimator, or `NDE`
            - **Same as Experiment 1**
        3. Sample posterior, P(`theta | q_surr_all = q_PF_obs`)
            - **Similar to Experiment 1, except**
                - @ **`n_obs=18`** synthetic observations, **`q_PF_obs`** # only the parameter sets from test_frac for which ParFlow runs exist
        4. Interpretive Plots:
            - **Similar to Experiment 1, except**
                 - Performance in Parameter Space:
                    - Individual for **`n_obs=18`** synthetic observations, **`q_PF_obs`**:
                        - Show each P(`theta | q_surr_all` = **`q_PF_obs`**) as pairplot
                    - Bulk performance across parameter space for all **`n_obs=18`** synthetic observations
                        - **boxplots** showing `1. Euclidean Distance`, `2. Determinant` and, `3. theta_true_log_prob` for P(`theta | q_surr_all` = **`q_PF_obs`**)
                        - **boxplots showing `1. Euclidean Distance`, `2. Determinant` and, `3. theta_true_log_prob` for P(`theta | q_surr_all` = q_surr_obs)**
                            - NOTE: Want to be able to compare directly back to Experiment 1
                - Performance in Streamflow Space:
                    * Principle: 
                        - Bootstrap n=50 parameters pairs `{theta1, theta2}` from P(`theta | q_surr_all` = **`q_PF_obs`**)
                        - Run forward (+ noise) n=50 to generate streamflow accounting for uncertainties in:
                            1. Parameters
                            2. Simulator
                    - Individual for each **`n_obs=18`** synthetic observations, **`q_PF_obs`**:
                        - Show each (**`q_surr_all | q_PF_obs`**) as streamflow time series containing:
                            1. Central Tendency, `q_surr_all`
                            2. Max, Min bounds, `q_surr_all`
                            3. True, **`q_PF_obs`**
                    * Bulk Performance:
                        - **boxplots** showing `1. Mean NSE` of **`q_PF_obs`** v `q_surr_all`; `2. STD NSE` of `q_surr_all`
                        - **boxplots showing `1. Mean NSE` of `q_surr_obs` v `q_surr_all`; `2. STD NSE` of `q_surr_all`**
                            - NOTE: Want to be able to compare directly back to Experiment 1
    2. Dimension 2: Using heuristic and machine-learned summaries (compressions) of full time series
        a. Heuristic:
            0. **DIAGNOSE SOME HEURISTIC STATISTICS THAT SEEM LESS SUSCEPTIBLE TO MODEL BIAS**
                - Three ones that worked well for us before were:
                    - 10th flow quantile, 90th flow quantile, and the ordinal day at which half of the flow has occurred during the year of study. 
            1. Use Surrogate of ParFlow streamflow, `lstm_A`
                - **Same as Experiment 2, Dimension 1**
            2. **Construct** amortized (full) P(`theta | q_surr_all`) via a Neural Density Estimator, or `NDE`
                - **Similar to Experiment 2, Dimension 1, except**
                    - set stat_typ = `np.array([4,12,13])` (these are the summary stats)
                    - **this will implicitly ensure that all modeled quantities, `q_PF_obs` and `q_surr_all`, are expressed in terms of those three statistics**
            3. Sample posterior, P(`theta | q_surr_all = q_PF_obs`)
                - **Same as Experiment 2, Dimension 1**
            4. Interpretive Plots:
                - **Similar to Previous Experiments, except**
                    - **Adding BoxPlot Comparison to Previous**
        b. Machine-Learned:
            1. Use Surrogate of ParFlow streamflow, `lstm_A`
                - **Same as Experiment 2, Dimension 2a**
            2. **Construct** amortized (full) P(`theta | q_surr_all`) via a Neural Density Estimator, or `NDE`
                - **Similar to Experiment 2, Dimension 2a, except**
                    - set stat_method = `embed`, with a MLP construction
                    - set out_dim = `3`
                    - set embed_type = `MLP`
                    - **this will implicitly ensure that all modeled quantities, `q_PF_obs` and `q_surr_all`, are expressed in terms of ML summary statistics**
            3. Sample posterior, P(`theta | q_surr_all = q_PF_obs`)
                - **Same as Experiment 2, Dimension 2a**
            4. Interpretive Plots:
                - **Similar to Previous Experiments, except**
                    - **Adding BoxPlot Comparison to Previous**
    3. Dimension 3: Increase the diversity of our system representations to combat bias.
        a. Adding Monte Carlo drop-out to our surrogate model:
            1. **Build** Surrogate of ParFlow streamflow, `lstm_B`
                - **Similar to Experiment 2, Dimension 1, except**
                    * Add Monte Carlo Dropout (TBD)
                        * https://towardsdatascience.com/monte-carlo-dropout-7fd52f8b6571 
                        * Hyperparameters:
                            - **MC_Dropout = True**
            2. **Construct** amortized (full) P(`theta | q_surr_all`) via a Neural Density Estimator, or `NDE`
                - **Similar to Experiment 2, Dimension 1, except**
                    - w MC Dropout
                    - w fnoise turned off
            3. Sample posterior, P(`theta | q_surr_all = q_PF_obs`)
                - **Same as Experiment 2, Dimension 1**
            4. Interpretive Plots:
                - **Similar to Previous Experiments, except**
                    - **Adding BoxPlot Comparison to Previous**
        b. Ensemble of weakly-trained surrogates:
            1. **Build** Surrogate of ParFlow streamflow, `lstm_C`
                - **Similar to Experiment 2, Dimension 1, except**
                    * Structure: 
                        * `q_{t+1} = lstm_exp1(forcings_{t->t-14}, theta1, theta2)`
                    * Hyperparameters:
                        - **num_epochs = 100** # number of times iterating through the model
                            - trained for lower number of iterations
                    * Number of Surrogates Trained:
                        - **`num_members` = 10**
                            - larger number of ensemble members
                    * Training, Validation, Testing
                        - `train_frac`, `val_frac`, `test_frac` = **0.6, 0.3,** 0.1
                            - smaller train / test fraction
                    * Metrics of Performance
                        * `NSE`, `RMSE` on test fraction, `q_PF v q_surr`
            2. **Construct** amortized (full) P(`theta | q_surr_all`) via a Neural Density Estimator, or `NDE`
                - **Similar to Experiment 2, Dimension 1, except**
                    - w random selection from ensemble of 10 surrogates as source of stochasticity
            3. Sample posterior, P(`theta | q_surr_all = q_PF_obs`)
                - **Same as Experiment 2, Dimension 1**
            4. Interpretive Plots:
                - **Similar to Previous Experiments, except**
                    - **Adding BoxPlot Comparison to Previous**

3. Experiment 3: Results of SBI, given 'true' observed streamflow timeseries from USGS gage 09110000 at the Taylor River in Almont, CO. 
    
    
    

        
    


  